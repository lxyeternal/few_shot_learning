train: step: 0, loss: 0.6931471824645996, acc: 0.41, recall: 0.41000000000000003, precision: 0.27017364657814097, f_beta: 0.3041632267956127
train: step: 1, loss: 0.6931478381156921, acc: 0.5, recall: 0.5, precision: 0.5, f_beta: 0.3503118503118503
train: step: 2, loss: 0.6931458115577698, acc: 0.52, recall: 0.52, precision: 0.6302083333333333, f_beta: 0.39117199391172
train: step: 3, loss: 0.6931432485580444, acc: 0.5, recall: 0.5, precision: 0.25, f_beta: 0.3333333333333333
train: step: 4, loss: 0.6931473612785339, acc: 0.53, recall: 0.53, precision: 0.5300120048019208, f_beta: 0.5299529952995299
train: step: 5, loss: 0.6931532025337219, acc: 0.47, recall: 0.47, precision: 0.2422680412371134, f_beta: 0.31972789115646255
train: step: 6, loss: 0.6931350231170654, acc: 0.53, recall: 0.53, precision: 0.6578947368421053, f_beta: 0.4106583072100314
train: step: 7, loss: 0.6931469440460205, acc: 0.51, recall: 0.51, precision: 0.5101999184006528, f_beta: 0.5075871771681238
train: step: 8, loss: 0.6931483745574951, acc: 0.62, recall: 0.62, precision: 0.6231527093596059, f_beta: 0.6175523349436394
train: step: 9, loss: 0.6930375099182129, acc: 0.56, recall: 0.5599999999999999, precision: 0.6420454545454546, f_beta: 0.4857410004675081
train: step: 10, loss: 0.6931462287902832, acc: 0.56, recall: 0.56, precision: 0.5608766233766234, f_beta: 0.5584102769971899
train: step: 11, loss: 0.6931511163711548, acc: 0.5, recall: 0.5, precision: 0.5, f_beta: 0.3503118503118503
train: step: 12, loss: 0.6931479573249817, acc: 0.5, recall: 0.5, precision: 0.25, f_beta: 0.3333333333333333
train: step: 13, loss: 0.6931575536727905, acc: 0.5, recall: 0.5, precision: 0.5, f_beta: 0.3503118503118503
train: step: 14, loss: 0.6931473612785339, acc: 0.53, recall: 0.53, precision: 0.5310045473336089, f_beta: 0.5261619114830125
train: step: 15, loss: 0.693101704120636, acc: 0.54, recall: 0.54, precision: 0.7604166666666667, f_beta: 0.4165398274987316
train: step: 16, loss: 0.6931473016738892, acc: 0.49, recall: 0.49, precision: 0.46159754224270355, f_beta: 0.37430990062569014
train: step: 17, loss: 0.6931607127189636, acc: 0.49, recall: 0.49, precision: 0.4473684210526316, f_beta: 0.3605015673981191
train: step: 18, loss: 0.6931480169296265, acc: 0.45, recall: 0.45, precision: 0.449000407996736, f_beta: 0.4472917294744247
train: step: 19, loss: 0.6931469440460205, acc: 0.48, recall: 0.48000000000000004, precision: 0.41134751773049644, f_beta: 0.3551587301587301
train: step: 20, loss: 0.6931473612785339, acc: 0.54, recall: 0.54, precision: 0.5445632798573975, f_beta: 0.5279146141215106
train: step: 21, loss: 0.6931714415550232, acc: 0.46, recall: 0.46, precision: 0.3641304347826087, f_beta: 0.34434191355026716
train: step: 22, loss: 0.6931478977203369, acc: 0.47, recall: 0.47000000000000003, precision: 0.4689954526663911, f_beta: 0.46567194273616297
train: step: 23, loss: 0.6931569576263428, acc: 0.51, recall: 0.51, precision: 0.7525252525252526, f_beta: 0.3551783129359126
train: step: 24, loss: 0.6931430101394653, acc: 0.5, recall: 0.5, precision: 0.25, f_beta: 0.3333333333333333
train: step: 25, loss: 0.6931465864181519, acc: 0.55, recall: 0.55, precision: 0.5666666666666667, f_beta: 0.52
train: step: 26, loss: 0.6931312680244446, acc: 0.51, recall: 0.51, precision: 0.5221043324491601, f_beta: 0.43227899432278993
train: step: 27, loss: 0.6931503415107727, acc: 0.5, recall: 0.5, precision: 0.5, f_beta: 0.47916666666666663
train: step: 28, loss: 0.6931984424591064, acc: 0.45, recall: 0.44999999999999996, precision: 0.37231869254341166, f_beta: 0.35133860125014743
train: step: 29, loss: 0.6931465268135071, acc: 0.49, recall: 0.49, precision: 0.2474747474747475, f_beta: 0.32885906040268453
train: step: 30, loss: 0.6931455731391907, acc: 0.51, recall: 0.51, precision: 0.5526315789473684, f_beta: 0.38557993730407525
train: step: 31, loss: 0.6928796172142029, acc: 0.62, recall: 0.62, precision: 0.6378676470588236, f_beta: 0.6072757337742869
train: step: 32, loss: 0.6931469440460205, acc: 0.54, recall: 0.54, precision: 0.5445632798573975, f_beta: 0.5279146141215106
train: step: 33, loss: 0.6931453943252563, acc: 0.54, recall: 0.54, precision: 0.54, f_beta: 0.54
train: step: 34, loss: 0.6931492686271667, acc: 0.47, recall: 0.47000000000000003, precision: 0.4689954526663911, f_beta: 0.46567194273616297
train: step: 35, loss: 0.693123996257782, acc: 0.56, recall: 0.5599999999999999, precision: 0.6666666666666667, f_beta: 0.4761904761904763
train: step: 36, loss: 0.6931427121162415, acc: 0.51, recall: 0.51, precision: 0.5255362614913177, f_beta: 0.4221016629319495
train: step: 37, loss: 0.6931479573249817, acc: 0.5, recall: 0.5, precision: 0.5, f_beta: 0.3503118503118503
train: step: 38, loss: 0.6931139230728149, acc: 0.52, recall: 0.52, precision: 0.5555555555555556, f_beta: 0.42857142857142855
train: step: 39, loss: 0.6933653950691223, acc: 0.41, recall: 0.41000000000000003, precision: 0.3235294117647059, f_beta: 0.32763532763532766
train: step: 40, loss: 0.6931185126304626, acc: 0.52, recall: 0.52, precision: 0.6302083333333333, f_beta: 0.39117199391172
train: step: 41, loss: 0.693149209022522, acc: 0.5, recall: 0.5, precision: 0.5, f_beta: 0.49919871794871795
train: step: 42, loss: 0.692931592464447, acc: 0.56, recall: 0.56, precision: 0.5668449197860963, f_beta: 0.548440065681445
train: step: 43, loss: 0.6931483745574951, acc: 0.48, recall: 0.48, precision: 0.4320652173913043, f_beta: 0.36862554638173867
train: step: 44, loss: 0.693149745464325, acc: 0.44, recall: 0.44, precision: 0.43990384615384615, f_beta: 0.4397759103641457
train: step: 45, loss: 0.6931526064872742, acc: 0.49, recall: 0.49, precision: 0.4140893470790378, f_beta: 0.3453985367731998
train: step: 46, loss: 0.6931174397468567, acc: 0.55, recall: 0.55, precision: 0.5980392156862745, f_beta: 0.4871794871794872
train: step: 47, loss: 0.6931608319282532, acc: 0.52, recall: 0.52, precision: 0.7551020408163265, f_beta: 0.3762993762993764
train: step: 48, loss: 0.6931580901145935, acc: 0.37, recall: 0.37, precision: 0.36057486057486055, f_beta: 0.359169972535856
train: step: 49, loss: 0.6931564211845398, acc: 0.41, recall: 0.41000000000000003, precision: 0.40542244640605296, f_beta: 0.4027735600769308
train: step: 50, loss: 0.6931438446044922, acc: 0.54, recall: 0.54, precision: 0.5405844155844155, f_beta: 0.5383380168606985
train: step: 51, loss: 0.6931176781654358, acc: 0.53, recall: 0.53, precision: 0.6578947368421053, f_beta: 0.4106583072100314
train: step: 52, loss: 0.6931558847427368, acc: 0.46, recall: 0.46, precision: 0.4594155844155844, f_beta: 0.45804897631473307
train: step: 53, loss: 0.6931403875350952, acc: 0.58, recall: 0.5800000000000001, precision: 0.5801282051282051, f_beta: 0.5798319327731092
train: step: 54, loss: 0.693157970905304, acc: 0.43, recall: 0.43, precision: 0.42860057119543044, f_beta: 0.42719324690985827
train: step: 55, loss: 0.6909328699111938, acc: 0.58, recall: 0.5800000000000001, precision: 0.625, f_beta: 0.5384615384615385
train: step: 56, loss: 0.6931424140930176, acc: 0.48, recall: 0.48000000000000004, precision: 0.46612466124661245, f_beta: 0.4206773618538324
train: step: 57, loss: 0.6931151747703552, acc: 0.52, recall: 0.52, precision: 0.5202922077922079, f_beta: 0.5182657567242072
train: step: 58, loss: 0.6926743984222412, acc: 0.6, recall: 0.6, precision: 0.6370614035087719, f_beta: 0.570999570999571
train: step: 59, loss: 0.6946530938148499, acc: 0.43, recall: 0.43, precision: 0.3862897985705003, f_beta: 0.36939926983073346
train: step: 60, loss: 0.6912580728530884, acc: 0.55, recall: 0.55, precision: 0.5634195839675291, f_beta: 0.5248653785239151
train: step: 61, loss: 0.6917724013328552, acc: 0.54, recall: 0.54, precision: 0.5830564784053156, f_beta: 0.4715073529411765
train: step: 62, loss: 0.6938635110855103, acc: 0.47, recall: 0.47000000000000003, precision: 0.4694002447980416, f_beta: 0.467390212038991
train: step: 63, loss: 0.6943873763084412, acc: 0.39, recall: 0.39, precision: 0.3878008975928192, f_beta: 0.38699628178072554
train: step: 64, loss: 0.6928358674049377, acc: 0.57, recall: 0.57, precision: 0.5769230769230769, f_beta: 0.5601023017902813
train: step: 65, loss: 0.6933544874191284, acc: 0.47, recall: 0.47000000000000003, precision: 0.4689954526663911, f_beta: 0.46567194273616297
train: step: 66, loss: 0.6938201785087585, acc: 0.47, recall: 0.47, precision: 0.4411764705882353, f_beta: 0.396011396011396
train: step: 67, loss: 0.6933901309967041, acc: 0.48, recall: 0.48, precision: 0.47771836007130125, f_beta: 0.4663382594417077
train: step: 68, loss: 0.6928901076316833, acc: 0.51, recall: 0.51, precision: 0.5100361300682457, f_beta: 0.5095586027424682
train: step: 69, loss: 0.6933372616767883, acc: 0.52, recall: 0.52, precision: 0.5200320512820513, f_beta: 0.5198079231692677
train: step: 70, loss: 0.6930275559425354, acc: 0.56, recall: 0.56, precision: 0.5600961538461539, f_beta: 0.5598239295718288
train: step: 71, loss: 0.6924582719802856, acc: 0.51, recall: 0.51, precision: 0.510989010989011, f_beta: 0.4987212276214833
train: step: 72, loss: 0.6929414868354797, acc: 0.51, recall: 0.51, precision: 0.510508617065994, f_beta: 0.5039983804028748
train: step: 73, loss: 0.6922875046730042, acc: 0.54, recall: 0.54, precision: 0.6773049645390071, f_beta: 0.4295634920634921
train: step: 74, loss: 0.693115770816803, acc: 0.58, recall: 0.5800000000000001, precision: 0.5811688311688312, f_beta: 0.5784825371336814
train: step: 75, loss: 0.692974328994751, acc: 0.48, recall: 0.48000000000000004, precision: 0.41134751773049644, f_beta: 0.3551587301587301
train: step: 76, loss: 0.6927002668380737, acc: 0.53, recall: 0.53, precision: 0.5305997552019583, f_beta: 0.5276856597326902
train: step: 77, loss: 0.6934007406234741, acc: 0.47, recall: 0.47, precision: 0.4699879951980792, f_beta: 0.46994699469946993
train: step: 78, loss: 0.6929278373718262, acc: 0.5, recall: 0.5, precision: 0.5, f_beta: 0.486863711001642
train: step: 79, loss: 0.6923425793647766, acc: 0.49, recall: 0.49, precision: 0.48996386993175434, f_beta: 0.4895405865278751
train: step: 80, loss: 0.6927245855331421, acc: 0.57, recall: 0.5700000000000001, precision: 0.5707070707070707, f_beta: 0.5689223057644109
train: step: 81, loss: 0.6934006214141846, acc: 0.48, recall: 0.48, precision: 0.47916666666666663, f_beta: 0.4747474747474748
train: step: 82, loss: 0.6923604607582092, acc: 0.51, recall: 0.51, precision: 0.5305250305250305, f_beta: 0.4109868974636375
train: step: 83, loss: 0.6911473274230957, acc: 0.55, recall: 0.55, precision: 0.763157894736842, f_beta: 0.4357366771159875
train: step: 84, loss: 0.6930227875709534, acc: 0.56, recall: 0.56, precision: 0.5615763546798029, f_beta: 0.5571658615136876
train: step: 85, loss: 0.6934332847595215, acc: 0.5, recall: 0.5, precision: 0.5, f_beta: 0.47456914670029426
train: step: 86, loss: 0.6931997537612915, acc: 0.44, recall: 0.44, precision: 0.4285714285714286, f_beta: 0.41666666666666663
train: step: 87, loss: 0.6929782032966614, acc: 0.49, recall: 0.49, precision: 0.4898989898989899, f_beta: 0.48872180451127817
train: step: 88, loss: 0.6894389986991882, acc: 0.52, recall: 0.52, precision: 0.5555555555555556, f_beta: 0.42857142857142855
train: step: 89, loss: 0.6927008628845215, acc: 0.56, recall: 0.56, precision: 0.5600961538461539, f_beta: 0.5598239295718288
train: step: 90, loss: 0.6915010213851929, acc: 0.51, recall: 0.51, precision: 0.5141163184641445, f_beta: 0.47147017581706396
train: step: 91, loss: 0.6933397054672241, acc: 0.5, recall: 0.5, precision: 0.5, f_beta: 0.4949494949494949
train: step: 92, loss: 0.6932709217071533, acc: 0.48, recall: 0.48, precision: 0.46875, f_beta: 0.4285714285714286
train: step: 93, loss: 0.692414402961731, acc: 0.5, recall: 0.5, precision: 0.5, f_beta: 0.4505494505494506
train: step: 94, loss: 0.6924073696136475, acc: 0.55, recall: 0.55, precision: 0.5501806503412284, f_beta: 0.5495946351716544
train: step: 95, loss: 0.6964576840400696, acc: 0.42, recall: 0.42, precision: 0.410873440285205, f_beta: 0.40476190476190477
train: step: 96, loss: 0.6930875182151794, acc: 0.48, recall: 0.48, precision: 0.47987117552334946, f_beta: 0.47916666666666663
train: step: 97, loss: 0.6888365745544434, acc: 0.54, recall: 0.54, precision: 0.5830564784053156, f_beta: 0.4715073529411765
train: step: 98, loss: 0.6933597326278687, acc: 0.46, recall: 0.45999999999999996, precision: 0.4554367201426025, f_beta: 0.44581280788177335
train: step: 99, loss: 0.6925752758979797, acc: 0.6, recall: 0.6, precision: 0.6041666666666667, f_beta: 0.595959595959596
train: step: 100, loss: 0.6927751898765564, acc: 0.56, recall: 0.56, precision: 0.5625, f_beta: 0.5555555555555556
train: step: 101, loss: 0.6923871040344238, acc: 0.61, recall: 0.61, precision: 0.6103974307507025, f_beta: 0.6096486838154338
train: step: 102, loss: 0.693210780620575, acc: 0.46, recall: 0.46, precision: 0.45894909688013136, f_beta: 0.4565217391304348
train: step: 103, loss: 0.6868983507156372, acc: 0.57, recall: 0.57, precision: 0.6547303271441203, f_beta: 0.5017958521608157
train: step: 104, loss: 0.6938263177871704, acc: 0.46, recall: 0.45999999999999996, precision: 0.4053030303030303, f_beta: 0.3688639551192146
train: step: 105, loss: 0.6926113367080688, acc: 0.55, recall: 0.55, precision: 0.5584385226741468, f_beta: 0.5331465919701215
train: step: 106, loss: 0.6932042837142944, acc: 0.48, recall: 0.48, precision: 0.47771836007130125, f_beta: 0.4663382594417077
train: step: 107, loss: 0.693615734577179, acc: 0.47, recall: 0.47000000000000003, precision: 0.4694002447980416, f_beta: 0.467390212038991
train: step: 108, loss: 0.6932628750801086, acc: 0.45, recall: 0.45, precision: 0.449000407996736, f_beta: 0.4472917294744247
train: step: 109, loss: 0.6914549469947815, acc: 0.52, recall: 0.52, precision: 0.5886524822695035, f_beta: 0.40476190476190477
train: step: 110, loss: 0.6930138468742371, acc: 0.52, recall: 0.52, precision: 0.5201288244766505, f_beta: 0.5192307692307692
train: step: 111, loss: 0.6927726864814758, acc: 0.57, recall: 0.57, precision: 0.5988142292490118, f_beta: 0.5361881134721174
train: step: 112, loss: 0.6927129626274109, acc: 0.52, recall: 0.52, precision: 0.5205254515599343, f_beta: 0.5169082125603865
train: step: 113, loss: 0.6937000155448914, acc: 0.42, recall: 0.42000000000000004, precision: 0.4166666666666667, f_beta: 0.41414141414141414
train: step: 114, loss: 0.6941086649894714, acc: 0.5, recall: 0.5, precision: 0.5, f_beta: 0.4505494505494506
train: step: 115, loss: 0.6931536793708801, acc: 0.51, recall: 0.51, precision: 0.510989010989011, f_beta: 0.4987212276214833
train: step: 116, loss: 0.6925066113471985, acc: 0.55, recall: 0.55, precision: 0.5501806503412284, f_beta: 0.5495946351716544
train: step: 117, loss: 0.691721498966217, acc: 0.52, recall: 0.52, precision: 0.7551020408163265, f_beta: 0.3762993762993764
train: step: 118, loss: 0.693807065486908, acc: 0.52, recall: 0.52, precision: 0.5212224108658743, f_beta: 0.512987012987013
train: step: 119, loss: 0.6896334290504456, acc: 0.55, recall: 0.55, precision: 0.5525430853299707, f_beta: 0.5444883085332524
train: step: 120, loss: 0.6865970492362976, acc: 0.52, recall: 0.52, precision: 0.5372023809523809, f_beta: 0.4572591587516961
train: step: 121, loss: 0.6926847696304321, acc: 0.54, recall: 0.54, precision: 0.5519750519750519, f_beta: 0.5118845500848896
train: step: 122, loss: 0.6929354667663574, acc: 0.5, recall: 0.5, precision: 0.5, f_beta: 0.4967793880837359
train: step: 123, loss: 0.6902762055397034, acc: 0.56, recall: 0.5599999999999999, precision: 0.6420454545454546, f_beta: 0.4857410004675081
train: step: 124, loss: 0.693095326423645, acc: 0.45, recall: 0.45, precision: 0.4494949494949495, f_beta: 0.4486215538847118
train: step: 125, loss: 0.6917694807052612, acc: 0.48, recall: 0.48000000000000004, precision: 0.47619047619047616, f_beta: 0.45833333333333337
train: step: 126, loss: 0.6936827301979065, acc: 0.46, recall: 0.45999999999999996, precision: 0.45993589743589747, f_beta: 0.45978391356542614
train: step: 127, loss: 0.6936091780662537, acc: 0.47, recall: 0.47, precision: 0.46989160979526295, f_beta: 0.46952257031328193
train: step: 128, loss: 0.6894861459732056, acc: 0.53, recall: 0.53, precision: 0.5423489553924337, f_beta: 0.49304282170208175
train: step: 129, loss: 0.6931045651435852, acc: 0.51, recall: 0.51, precision: 0.510334849111203, f_beta: 0.5059985885673959
train: step: 130, loss: 0.6946255564689636, acc: 0.44, recall: 0.44, precision: 0.4125874125874126, f_beta: 0.3923611111111111
train: step: 131, loss: 0.6921145915985107, acc: 0.56, recall: 0.56, precision: 0.5636672325976231, f_beta: 0.5535714285714286
train: step: 132, loss: 0.693225622177124, acc: 0.48, recall: 0.48000000000000004, precision: 0.47947454844006565, f_beta: 0.47665056360708535
train: step: 133, loss: 0.693483829498291, acc: 0.53, recall: 0.53, precision: 0.54, f_beta: 0.4986666666666667
train: step: 134, loss: 0.6984084844589233, acc: 0.39, recall: 0.39, precision: 0.3664400194269063, f_beta: 0.36185793493043206
train: step: 135, loss: 0.6924861669540405, acc: 0.6, recall: 0.6000000000000001, precision: 0.6026272577996716, f_beta: 0.5974235104669887
train: step: 136, loss: 0.6974395513534546, acc: 0.39, recall: 0.39, precision: 0.38440521227406477, f_beta: 0.38252859601174205
train: step: 137, loss: 0.6930510997772217, acc: 0.5, recall: 0.5, precision: 0.5, f_beta: 0.46374946374946374
train: step: 138, loss: 0.6934058666229248, acc: 0.46, recall: 0.45999999999999996, precision: 0.4417249417249417, f_beta: 0.4140625
train: step: 139, loss: 0.694581925868988, acc: 0.47, recall: 0.47, precision: 0.4084249084249084, f_beta: 0.3629041952157711
train: step: 140, loss: 0.6928161382675171, acc: 0.48, recall: 0.48000000000000004, precision: 0.47947454844006565, f_beta: 0.47665056360708535
train: step: 141, loss: 0.6941410303115845, acc: 0.44, recall: 0.43999999999999995, precision: 0.41776315789473684, f_beta: 0.3993993993993994
train: step: 142, loss: 0.692811131477356, acc: 0.5, recall: 0.5, precision: 0.5, f_beta: 0.4967793880837359
train: step: 143, loss: 0.6911778450012207, acc: 0.52, recall: 0.52, precision: 0.525987525987526, f_beta: 0.49066213921901536
train: step: 144, loss: 0.6923606991767883, acc: 0.57, recall: 0.5700000000000001, precision: 0.5702529104777199, f_beta: 0.5696126513862476
train: step: 145, loss: 0.6931252479553223, acc: 0.49, recall: 0.49, precision: 0.48949138293400585, f_beta: 0.48375341633768604
train: step: 146, loss: 0.6932026147842407, acc: 0.51, recall: 0.51, precision: 0.510508617065994, f_beta: 0.5039983804028748
train: step: 147, loss: 0.6922277808189392, acc: 0.55, recall: 0.55, precision: 0.5607090820786789, f_beta: 0.529239460194581
train: step: 148, loss: 0.6930201649665833, acc: 0.52, recall: 0.52, precision: 0.52, f_beta: 0.52
train: step: 149, loss: 0.6927290558815002, acc: 0.53, recall: 0.53, precision: 0.530108390204737, f_beta: 0.5295766189570613
train: step: 150, loss: 0.6919190883636475, acc: 0.49, recall: 0.49, precision: 0.48228206945428775, f_beta: 0.4276736617663562
train: step: 151, loss: 0.6924624443054199, acc: 0.55, recall: 0.55, precision: 0.5501806503412284, f_beta: 0.5495946351716544
train: step: 152, loss: 0.6930716633796692, acc: 0.45, recall: 0.45, precision: 0.449000407996736, f_beta: 0.4472917294744247
train: step: 153, loss: 0.6933907866477966, acc: 0.45, recall: 0.44999999999999996, precision: 0.44505494505494503, f_beta: 0.4373401534526854
train: step: 154, loss: 0.689362108707428, acc: 0.53, recall: 0.53, precision: 0.6152073732718895, f_beta: 0.42338363390994965
train: step: 155, loss: 0.6926795840263367, acc: 0.43, recall: 0.43, precision: 0.42860057119543044, f_beta: 0.42719324690985827
train: step: 156, loss: 0.6927732229232788, acc: 0.55, recall: 0.55, precision: 0.5505050505050505, f_beta: 0.5488721804511277
train: step: 157, loss: 0.69288170337677, acc: 0.46, recall: 0.45999999999999996, precision: 0.4565972222222222, f_beta: 0.44920440636474906
train: step: 158, loss: 0.6930773258209229, acc: 0.51, recall: 0.51, precision: 0.510334849111203, f_beta: 0.5059985885673959
train: step: 159, loss: 0.6936944723129272, acc: 0.46, recall: 0.45999999999999996, precision: 0.4565972222222222, f_beta: 0.44920440636474906
train: step: 160, loss: 0.6933767199516296, acc: 0.45, recall: 0.44999999999999996, precision: 0.44637494637494635, f_beta: 0.44054521411860437
train: step: 161, loss: 0.6934112310409546, acc: 0.47, recall: 0.47000000000000003, precision: 0.4649368863955119, f_beta: 0.4501504305425874
train: step: 162, loss: 0.6927226185798645, acc: 0.52, recall: 0.52, precision: 0.5222816399286987, f_beta: 0.5073891625615763
train: step: 163, loss: 0.6932070851325989, acc: 0.48, recall: 0.48, precision: 0.47987117552334946, f_beta: 0.47916666666666663
train: step: 164, loss: 0.6900569796562195, acc: 0.54, recall: 0.54, precision: 0.5459558823529411, f_beta: 0.524596940884663
train: step: 165, loss: 0.692894458770752, acc: 0.53, recall: 0.53, precision: 0.5350631136044881, f_beta: 0.512397551613238
train: step: 166, loss: 0.687975287437439, acc: 0.6, recall: 0.6000000000000001, precision: 0.6006441223832528, f_beta: 0.5993589743589743
train: step: 167, loss: 0.6919018626213074, acc: 0.54, recall: 0.54, precision: 0.6358695652173914, f_beta: 0.4414764448761534
train: step: 168, loss: 0.6910961270332336, acc: 0.57, recall: 0.5700000000000001, precision: 0.6240255138199858, f_beta: 0.5174503422735944
train: step: 169, loss: 0.6935732960700989, acc: 0.43, recall: 0.43000000000000005, precision: 0.42492492492492495, f_beta: 0.42020140372291737
train: step: 170, loss: 0.6897095441818237, acc: 0.53, recall: 0.53, precision: 0.6578947368421053, f_beta: 0.4106583072100314
train: step: 171, loss: 0.6932610273361206, acc: 0.47, recall: 0.47, precision: 0.46989160979526295, f_beta: 0.46952257031328193
train: step: 172, loss: 0.6927667856216431, acc: 0.55, recall: 0.55, precision: 0.5753465943339361, f_beta: 0.508679986898133
train: step: 173, loss: 0.6928988099098206, acc: 0.51, recall: 0.51, precision: 0.5100361300682457, f_beta: 0.5095586027424682
train: step: 174, loss: 0.6932128071784973, acc: 0.53, recall: 0.53, precision: 0.530108390204737, f_beta: 0.5295766189570613
train: step: 175, loss: 0.6934518218040466, acc: 0.47, recall: 0.47, precision: 0.46989160979526295, f_beta: 0.46952257031328193
train: step: 176, loss: 0.6929094195365906, acc: 0.47, recall: 0.47, precision: 0.4699879951980792, f_beta: 0.46994699469946993
train: step: 177, loss: 0.6951904296875, acc: 0.53, recall: 0.53, precision: 0.5380517503805176, f_beta: 0.5037482842360891
train: step: 178, loss: 0.6940721273422241, acc: 0.51, recall: 0.51, precision: 0.5101999184006528, f_beta: 0.5075871771681238
train: step: 179, loss: 0.6927335262298584, acc: 0.55, recall: 0.55, precision: 0.5584385226741468, f_beta: 0.5331465919701215
train: step: 180, loss: 0.6930198669433594, acc: 0.54, recall: 0.54, precision: 0.5400641025641025, f_beta: 0.5398159263705482
train: step: 181, loss: 0.6932373046875, acc: 0.57, recall: 0.5700000000000001, precision: 0.5702529104777199, f_beta: 0.5696126513862476
train: step: 182, loss: 0.6941776871681213, acc: 0.47, recall: 0.47, precision: 0.4547920433996383, f_beta: 0.42133420679113437
train: step: 183, loss: 0.6923743486404419, acc: 0.52, recall: 0.52, precision: 0.5205254515599343, f_beta: 0.5169082125603865
train: step: 184, loss: 0.6879676580429077, acc: 0.53, recall: 0.53, precision: 0.5915750915750916, f_beta: 0.43502824858757067
train: step: 185, loss: 0.6932559013366699, acc: 0.57, recall: 0.5700000000000001, precision: 0.5702529104777199, f_beta: 0.5696126513862476
train: step: 186, loss: 0.6943680047988892, acc: 0.51, recall: 0.51, precision: 0.5305250305250305, f_beta: 0.4109868974636375
train: step: 187, loss: 0.6919768452644348, acc: 0.54, recall: 0.54, precision: 0.5459558823529411, f_beta: 0.524596940884663
train: step: 188, loss: 0.6925462484359741, acc: 0.58, recall: 0.5800000000000001, precision: 0.5833333333333333, f_beta: 0.5757575757575757
train: step: 189, loss: 0.6926250457763672, acc: 0.56, recall: 0.56, precision: 0.5603864734299517, f_beta: 0.5592948717948718
train: step: 190, loss: 0.687943696975708, acc: 0.55, recall: 0.55, precision: 0.5980392156862745, f_beta: 0.4871794871794872
train: step: 191, loss: 0.6922109127044678, acc: 0.57, recall: 0.5700000000000001, precision: 0.5849927149101506, f_beta: 0.5501621508525997
train: step: 192, loss: 0.6927171349525452, acc: 0.55, recall: 0.55, precision: 0.550999592003264, f_beta: 0.5477841422972565
train: step: 193, loss: 0.6918656826019287, acc: 0.51, recall: 0.51, precision: 0.5305250305250305, f_beta: 0.4109868974636375
train: step: 194, loss: 0.6931871175765991, acc: 0.52, recall: 0.52, precision: 0.5217013888888888, f_beta: 0.5104039167686658
train: step: 195, loss: 0.6899622082710266, acc: 0.5, recall: 0.5, precision: 0.5, f_beta: 0.4967793880837359
train: step: 196, loss: 0.6928421854972839, acc: 0.54, recall: 0.54, precision: 0.5445632798573975, f_beta: 0.5279146141215106
train: step: 197, loss: 0.6932489275932312, acc: 0.48, recall: 0.48, precision: 0.47916666666666663, f_beta: 0.4747474747474748
train: step: 198, loss: 0.6924833655357361, acc: 0.55, recall: 0.55, precision: 0.5525430853299707, f_beta: 0.5444883085332524
train: step: 199, loss: 0.6933632493019104, acc: 0.51, recall: 0.51, precision: 0.510334849111203, f_beta: 0.5059985885673959
train: step: 200, loss: 0.693718671798706, acc: 0.4, recall: 0.4, precision: 0.3993558776167472, f_beta: 0.3990384615384615
train: step: 201, loss: 0.6935508251190186, acc: 0.44, recall: 0.44, precision: 0.4285714285714286, f_beta: 0.41666666666666663
train: step: 202, loss: 0.6935973167419434, acc: 0.47, recall: 0.47, precision: 0.46847414880201765, f_beta: 0.46350845227249715
train: step: 203, loss: 0.6909781098365784, acc: 0.56, recall: 0.56, precision: 0.5744047619047619, f_beta: 0.537620849096259
train: step: 204, loss: 0.6935348510742188, acc: 0.5, recall: 0.5, precision: 0.5, f_beta: 0.36580416032470825
train: step: 205, loss: 0.6931872367858887, acc: 0.51, recall: 0.51, precision: 0.5101999184006528, f_beta: 0.5075871771681238
train: step: 206, loss: 0.691760778427124, acc: 0.59, recall: 0.5900000000000001, precision: 0.5917992656058751, f_beta: 0.5879811074263892
train: step: 207, loss: 0.6927633881568909, acc: 0.56, recall: 0.56, precision: 0.5615763546798029, f_beta: 0.5571658615136876
train: step: 208, loss: 0.6929340958595276, acc: 0.48, recall: 0.48, precision: 0.47401247401247404, f_beta: 0.44821731748726656
train: step: 209, loss: 0.6936166882514954, acc: 0.44, recall: 0.44, precision: 0.4125874125874126, f_beta: 0.3923611111111111
train: step: 210, loss: 0.6910492777824402, acc: 0.53, recall: 0.53, precision: 0.5423489553924337, f_beta: 0.49304282170208175
train: step: 211, loss: 0.6931794881820679, acc: 0.45, recall: 0.45, precision: 0.439290917921321, f_beta: 0.42462600690448793
train: step: 212, loss: 0.6930972337722778, acc: 0.52, recall: 0.52, precision: 0.5212224108658743, f_beta: 0.512987012987013
train: step: 213, loss: 0.6927574872970581, acc: 0.53, recall: 0.53, precision: 0.5303030303030303, f_beta: 0.5288220551378446
train: step: 214, loss: 0.6937822699546814, acc: 0.51, recall: 0.51, precision: 0.510334849111203, f_beta: 0.5059985885673959
train: step: 215, loss: 0.6931343078613281, acc: 0.49, recall: 0.49, precision: 0.48966515088879703, f_beta: 0.48583526565177937
train: step: 216, loss: 0.6897725462913513, acc: 0.61, recall: 0.61, precision: 0.6208791208791209, f_beta: 0.6010230179028133
train: step: 217, loss: 0.6936447620391846, acc: 0.44, recall: 0.44, precision: 0.43961352657004826, f_beta: 0.4391025641025641
train: step: 218, loss: 0.692837655544281, acc: 0.43, recall: 0.43, precision: 0.4150072850898494, f_beta: 0.40370331624646927
train: step: 219, loss: 0.6939697861671448, acc: 0.39, recall: 0.39, precision: 0.3896025692492975, f_beta: 0.3894505054549094
train: step: 220, loss: 0.693759024143219, acc: 0.51, recall: 0.51, precision: 0.5526315789473684, f_beta: 0.38557993730407525
train: step: 221, loss: 0.6946180462837219, acc: 0.41, recall: 0.41000000000000003, precision: 0.40347490347490345, f_beta: 0.39985759332723014
train: step: 222, loss: 0.692059338092804, acc: 0.58, recall: 0.5800000000000001, precision: 0.6488095238095238, f_beta: 0.5251017639077341
train: step: 223, loss: 0.6919963359832764, acc: 0.45, recall: 0.45, precision: 0.4114103472714387, f_beta: 0.38278532151273703
train: step: 224, loss: 0.691241443157196, acc: 0.59, recall: 0.59, precision: 0.6141552511415524, f_beta: 0.5670995670995671
train: step: 225, loss: 0.6934728026390076, acc: 0.49, recall: 0.49, precision: 0.4473684210526316, f_beta: 0.3605015673981191
train: step: 226, loss: 0.691352903842926, acc: 0.48, recall: 0.48, precision: 0.47996794871794873, f_beta: 0.4797919167667067
train: step: 227, loss: 0.6926273107528687, acc: 0.5, recall: 0.5, precision: 0.5, f_beta: 0.4949494949494949
train: step: 228, loss: 0.6934127807617188, acc: 0.5, recall: 0.5, precision: 0.5, f_beta: 0.4967793880837359
train: step: 229, loss: 0.6965520977973938, acc: 0.43, recall: 0.43, precision: 0.3945147679324894, f_beta: 0.37766131673763514
train: step: 230, loss: 0.6934782266616821, acc: 0.49, recall: 0.49, precision: 0.4898989898989899, f_beta: 0.48872180451127817
train: step: 231, loss: 0.6904195547103882, acc: 0.51, recall: 0.51, precision: 0.5526315789473684, f_beta: 0.38557993730407525
train: step: 232, loss: 0.6938138604164124, acc: 0.46, recall: 0.46, precision: 0.45894909688013136, f_beta: 0.4565217391304348
train: step: 233, loss: 0.6922778487205505, acc: 0.52, recall: 0.52, precision: 0.5238095238095237, f_beta: 0.5
train: step: 234, loss: 0.688733696937561, acc: 0.55, recall: 0.5499999999999999, precision: 0.6276813074565883, f_beta: 0.4692770373864842
train: step: 235, loss: 0.692823052406311, acc: 0.47, recall: 0.47000000000000003, precision: 0.4694002447980416, f_beta: 0.467390212038991
train: step: 236, loss: 0.6936749219894409, acc: 0.44, recall: 0.44000000000000006, precision: 0.4375, f_beta: 0.43434343434343436
train: step: 237, loss: 0.6923422813415527, acc: 0.52, recall: 0.52, precision: 0.52, f_beta: 0.52
train: step: 238, loss: 0.693142294883728, acc: 0.5, recall: 0.5, precision: 0.5, f_beta: 0.4949494949494949
train: step: 239, loss: 0.6938497424125671, acc: 0.42, recall: 0.42000000000000004, precision: 0.4007936507936508, f_beta: 0.3905002101723414
train: step: 240, loss: 0.6916607022285461, acc: 0.5, recall: 0.5, precision: 0.5, f_beta: 0.40476190476190477
train: step: 241, loss: 0.6931353211402893, acc: 0.51, recall: 0.51, precision: 0.51010101010101, f_beta: 0.5087719298245614
train: step: 242, loss: 0.6916167736053467, acc: 0.47, recall: 0.47, precision: 0.46194824961948244, f_beta: 0.4403970013726111
train: step: 243, loss: 0.6934455633163452, acc: 0.44, recall: 0.44, precision: 0.43990384615384615, f_beta: 0.4397759103641457
train: step: 244, loss: 0.6963749527931213, acc: 0.51, recall: 0.51, precision: 0.5384024577572964, f_beta: 0.3988467672678199
train: step: 245, loss: 0.6940645575523376, acc: 0.49, recall: 0.49, precision: 0.48785818358426425, f_beta: 0.46647138822052514
train: step: 246, loss: 0.6935793161392212, acc: 0.53, recall: 0.53, precision: 0.5364254492472074, f_beta: 0.5083167695365624
train: step: 247, loss: 0.6918628811836243, acc: 0.56, recall: 0.5599999999999999, precision: 0.5822368421052632, f_beta: 0.528099528099528
train: step: 248, loss: 0.6929208636283875, acc: 0.52, recall: 0.52, precision: 0.5201288244766505, f_beta: 0.5192307692307692
train: step: 249, loss: 0.6948138475418091, acc: 0.52, recall: 0.52, precision: 0.5274122807017544, f_beta: 0.4851994851994852
train: step: 250, loss: 0.6932996511459351, acc: 0.47, recall: 0.47, precision: 0.46782496782496785, f_beta: 0.4608890245142916
train: step: 251, loss: 0.693148672580719, acc: 0.45, recall: 0.45, precision: 0.4494949494949495, f_beta: 0.4486215538847118
train: step: 252, loss: 0.6929893493652344, acc: 0.43, recall: 0.43, precision: 0.4292929292929293, f_beta: 0.4285714285714286
train: step: 253, loss: 0.6934095025062561, acc: 0.45, recall: 0.45, precision: 0.449000407996736, f_beta: 0.4472917294744247
train: step: 254, loss: 0.7114900350570679, acc: 0.42, recall: 0.42000000000000004, precision: 0.3106060606060606, f_beta: 0.3221131369798972
train: step: 255, loss: 0.693436861038208, acc: 0.46, recall: 0.46, precision: 0.46, f_beta: 0.46
train: step: 256, loss: 0.6933482885360718, acc: 0.48, recall: 0.48, precision: 0.47996794871794873, f_beta: 0.4797919167667067
train: step: 257, loss: 0.6910210251808167, acc: 0.56, recall: 0.56, precision: 0.5615763546798029, f_beta: 0.5571658615136876
train: step: 258, loss: 0.694349467754364, acc: 0.52, recall: 0.52, precision: 0.5200320512820513, f_beta: 0.5198079231692677
train: step: 259, loss: 0.6921482682228088, acc: 0.56, recall: 0.56, precision: 0.5668449197860963, f_beta: 0.548440065681445
train: step: 260, loss: 0.6931658983230591, acc: 0.5, recall: 0.5, precision: 0.5, f_beta: 0.49919871794871795
train: step: 261, loss: 0.6935316324234009, acc: 0.41, recall: 0.41000000000000003, precision: 0.4099639855942377, f_beta: 0.4099409940994099
train: step: 262, loss: 0.6935700178146362, acc: 0.42, recall: 0.42, precision: 0.41883116883116883, f_beta: 0.4179044560417503
train: step: 263, loss: 0.6928271651268005, acc: 0.55, recall: 0.55, precision: 0.5516742455560149, f_beta: 0.5463252343986289
train: step: 264, loss: 0.6921937465667725, acc: 0.55, recall: 0.55, precision: 0.5607090820786789, f_beta: 0.529239460194581
train: step: 265, loss: 0.6948672533035278, acc: 0.52, recall: 0.52, precision: 0.53125, f_beta: 0.4725274725274725
train: step: 266, loss: 0.6931553483009338, acc: 0.48, recall: 0.48000000000000004, precision: 0.47619047619047616, f_beta: 0.45833333333333337
train: step: 267, loss: 0.6947268843650818, acc: 0.45, recall: 0.45, precision: 0.34737484737484736, f_beta: 0.338862844091838
train: step: 268, loss: 0.692973792552948, acc: 0.51, recall: 0.51, precision: 0.5101999184006528, f_beta: 0.5075871771681238
train: step: 269, loss: 0.6940982937812805, acc: 0.46, recall: 0.46, precision: 0.45039682539682535, f_beta: 0.43253467843631777
train: step: 270, loss: 0.6926649212837219, acc: 0.51, recall: 0.51, precision: 0.5107250107250108, f_beta: 0.5015766453056658
train: step: 271, loss: 0.6915556192398071, acc: 0.53, recall: 0.53, precision: 0.5915750915750916, f_beta: 0.43502824858757067
train: step: 272, loss: 0.6933200359344482, acc: 0.47, recall: 0.47000000000000003, precision: 0.4649368863955119, f_beta: 0.4501504305425874
train: step: 273, loss: 0.691567063331604, acc: 0.52, recall: 0.52, precision: 0.6302083333333333, f_beta: 0.39117199391172
train: step: 274, loss: 0.6870343685150146, acc: 0.56, recall: 0.56, precision: 0.7038043478260869, f_beta: 0.46576007770762506
train: step: 275, loss: 0.6932930946350098, acc: 0.47, recall: 0.47, precision: 0.45999999999999996, f_beta: 0.43466666666666665
train: step: 276, loss: 0.6918687224388123, acc: 0.63, recall: 0.63, precision: 0.6394251394251393, f_beta: 0.6236395076797885
train: step: 277, loss: 0.6941570043563843, acc: 0.4, recall: 0.4, precision: 0.3998397435897436, f_beta: 0.3997599039615846
train: step: 278, loss: 0.687065839767456, acc: 0.55, recall: 0.5499999999999999, precision: 0.6276813074565883, f_beta: 0.4692770373864842
train: step: 279, loss: 0.6936061382293701, acc: 0.45, recall: 0.44999999999999996, precision: 0.4498193496587716, f_beta: 0.44950455409868884
train: step: 280, loss: 0.7037539482116699, acc: 0.42, recall: 0.42, precision: 0.35119047619047616, f_beta: 0.34418815015829946
train: step: 281, loss: 0.6932401061058044, acc: 0.49, recall: 0.49, precision: 0.4899959983993597, f_beta: 0.48994899489948995
train: step: 282, loss: 0.6868075728416443, acc: 0.57, recall: 0.5700000000000001, precision: 0.7688172043010753, f_beta: 0.4724573671942093
train: step: 283, loss: 0.6914612650871277, acc: 0.54, recall: 0.54, precision: 0.5410509031198687, f_beta: 0.537037037037037
train: step: 284, loss: 0.6935324668884277, acc: 0.48, recall: 0.48, precision: 0.4782986111111111, f_beta: 0.46960424316605465
train: step: 285, loss: 0.6932820081710815, acc: 0.42, recall: 0.42000000000000004, precision: 0.4194847020933977, f_beta: 0.41907051282051283
train: step: 286, loss: 0.6933877468109131, acc: 0.55, recall: 0.55, precision: 0.5666666666666667, f_beta: 0.52
train: step: 287, loss: 0.6927188038825989, acc: 0.46, recall: 0.45999999999999996, precision: 0.448024948024948, f_beta: 0.42699490662139217
train: step: 288, loss: 0.6925082206726074, acc: 0.45, recall: 0.45, precision: 0.439290917921321, f_beta: 0.42462600690448793
train: step: 289, loss: 0.6934816241264343, acc: 0.5, recall: 0.5, precision: 0.5, f_beta: 0.49919871794871795
train: step: 290, loss: 0.6922924518585205, acc: 0.49, recall: 0.49, precision: 0.48901098901098905, f_beta: 0.47826086956521735
train: step: 291, loss: 0.6919283866882324, acc: 0.56, recall: 0.56, precision: 0.5625, f_beta: 0.5555555555555556
train: step: 292, loss: 0.6909921169281006, acc: 0.53, recall: 0.53, precision: 0.5423489553924337, f_beta: 0.49304282170208175
train: step: 293, loss: 0.6893700957298279, acc: 0.54, recall: 0.54, precision: 0.5416666666666667, f_beta: 0.5353535353535354
train: step: 294, loss: 0.6931673288345337, acc: 0.48, recall: 0.48, precision: 0.47987117552334946, f_beta: 0.47916666666666663
train: step: 295, loss: 0.6905143857002258, acc: 0.61, recall: 0.61, precision: 0.6395230847285642, f_beta: 0.5882166613873931
train: step: 296, loss: 0.6934588551521301, acc: 0.48, recall: 0.48, precision: 0.4782986111111111, f_beta: 0.46960424316605465
train: step: 297, loss: 0.6929639577865601, acc: 0.48, recall: 0.48, precision: 0.4787775891341256, f_beta: 0.4724025974025974
train: step: 298, loss: 0.6910481452941895, acc: 0.54, recall: 0.54, precision: 0.5625, f_beta: 0.4945054945054945
train: step: 299, loss: 0.6905055046081543, acc: 0.61, recall: 0.61, precision: 0.6243781094527363, f_beta: 0.5983935742971889
train: step: 300, loss: 0.6936217546463013, acc: 0.5, recall: 0.5, precision: 0.5, f_beta: 0.39290917921321034
train: step: 301, loss: 0.692442774772644, acc: 0.53, recall: 0.53, precision: 0.5350631136044881, f_beta: 0.512397551613238
train: step: 302, loss: 0.6931155920028687, acc: 0.55, recall: 0.55, precision: 0.5525430853299707, f_beta: 0.5444883085332524
train: step: 303, loss: 0.6939089894294739, acc: 0.45, recall: 0.45000000000000007, precision: 0.44745691467002946, f_beta: 0.44326348820730843
train: step: 304, loss: 0.6933979988098145, acc: 0.5, recall: 0.5, precision: 0.5, f_beta: 0.4967793880837359
train: step: 305, loss: 0.6942977905273438, acc: 0.55, recall: 0.55, precision: 0.5525430853299707, f_beta: 0.5444883085332524
train: step: 306, loss: 0.6904222369194031, acc: 0.53, recall: 0.53, precision: 0.5315258511979823, f_beta: 0.5242433444680635
train: step: 307, loss: 0.6934177875518799, acc: 0.48, recall: 0.48000000000000004, precision: 0.47619047619047616, f_beta: 0.45833333333333337
train: step: 308, loss: 0.688568115234375, acc: 0.53, recall: 0.53, precision: 0.6578947368421053, f_beta: 0.4106583072100314
train: step: 309, loss: 0.691976010799408, acc: 0.46, recall: 0.45999999999999996, precision: 0.4417249417249417, f_beta: 0.4140625
train: step: 310, loss: 0.6922038793563843, acc: 0.57, recall: 0.5700000000000001, precision: 0.5702529104777199, f_beta: 0.5696126513862476
train: step: 311, loss: 0.6999525427818298, acc: 0.42, recall: 0.42000000000000004, precision: 0.3106060606060606, f_beta: 0.3221131369798972
train: step: 312, loss: 0.691972017288208, acc: 0.51, recall: 0.51, precision: 0.5859106529209621, f_beta: 0.3710691823899371
train: step: 313, loss: 0.6897740960121155, acc: 0.55, recall: 0.55, precision: 0.5705815923207227, f_beta: 0.5146154675870995
train: step: 314, loss: 0.6936099529266357, acc: 0.5, recall: 0.5, precision: 0.5, f_beta: 0.40476190476190477
train: step: 315, loss: 0.6929552555084229, acc: 0.52, recall: 0.52, precision: 0.52, f_beta: 0.52
train: step: 316, loss: 0.693269670009613, acc: 0.45, recall: 0.45, precision: 0.449000407996736, f_beta: 0.4472917294744247
train: step: 317, loss: 0.6859741806983948, acc: 0.55, recall: 0.5499999999999999, precision: 0.6276813074565883, f_beta: 0.4692770373864842
train: step: 318, loss: 0.6852038502693176, acc: 0.55, recall: 0.55, precision: 0.763157894736842, f_beta: 0.4357366771159875
train: step: 319, loss: 0.6888152956962585, acc: 0.53, recall: 0.53, precision: 0.6152073732718895, f_beta: 0.42338363390994965
train: step: 320, loss: 0.6923672556877136, acc: 0.52, recall: 0.52, precision: 0.5212224108658743, f_beta: 0.512987012987013
train: step: 321, loss: 0.692623496055603, acc: 0.51, recall: 0.51, precision: 0.5100361300682457, f_beta: 0.5095586027424682
train: step: 322, loss: 0.6931421756744385, acc: 0.49, recall: 0.49, precision: 0.48901098901098905, f_beta: 0.47826086956521735
train: step: 323, loss: 0.6928674578666687, acc: 0.45, recall: 0.45000000000000007, precision: 0.44745691467002946, f_beta: 0.44326348820730843
train: step: 324, loss: 0.6934840679168701, acc: 0.47, recall: 0.47, precision: 0.46989160979526295, f_beta: 0.46952257031328193
train: step: 325, loss: 0.6938350200653076, acc: 0.49, recall: 0.49, precision: 0.4873160832064941, f_beta: 0.4615140956604371
train: step: 326, loss: 0.6922870874404907, acc: 0.51, recall: 0.51, precision: 0.5384024577572964, f_beta: 0.3988467672678199
train: step: 327, loss: 0.6910109519958496, acc: 0.51, recall: 0.51, precision: 0.5116877045348294, f_beta: 0.4916485112563544
train: step: 328, loss: 0.6943639516830444, acc: 0.41, recall: 0.41000000000000003, precision: 0.40542244640605296, f_beta: 0.4027735600769308
train: step: 329, loss: 0.6917719841003418, acc: 0.53, recall: 0.53, precision: 0.5663129973474801, f_beta: 0.45545128026879855
train: step: 330, loss: 0.6925816535949707, acc: 0.53, recall: 0.53, precision: 0.5303030303030303, f_beta: 0.5288220551378446
train: step: 331, loss: 0.6926817297935486, acc: 0.49, recall: 0.49, precision: 0.4898989898989899, f_beta: 0.48872180451127817
train: step: 332, loss: 0.6934983134269714, acc: 0.51, recall: 0.51, precision: 0.5101999184006528, f_beta: 0.5075871771681238
train: step: 333, loss: 0.6899653673171997, acc: 0.52, recall: 0.52, precision: 0.5886524822695035, f_beta: 0.40476190476190477
train: step: 334, loss: 0.6949841976165771, acc: 0.47, recall: 0.47, precision: 0.2422680412371134, f_beta: 0.31972789115646255
train: step: 335, loss: 0.6924492716789246, acc: 0.49, recall: 0.49, precision: 0.48831229546517063, f_beta: 0.47089947089947093
train: step: 336, loss: 0.6937785148620605, acc: 0.48, recall: 0.48000000000000004, precision: 0.47947454844006565, f_beta: 0.47665056360708535
train: step: 337, loss: 0.6923596262931824, acc: 0.59, recall: 0.5900000000000001, precision: 0.5903251706142112, f_beta: 0.5896306676008407
train: step: 338, loss: 0.6930708289146423, acc: 0.47, recall: 0.47, precision: 0.4699879951980792, f_beta: 0.46994699469946993
train: step: 339, loss: 0.6929564476013184, acc: 0.55, recall: 0.55, precision: 0.550999592003264, f_beta: 0.5477841422972565
train: step: 340, loss: 0.6938499212265015, acc: 0.4, recall: 0.4, precision: 0.4, f_beta: 0.4000000000000001
train: step: 341, loss: 0.693107008934021, acc: 0.5, recall: 0.5, precision: 0.5, f_beta: 0.46943972835314096
train: step: 342, loss: 0.6927874088287354, acc: 0.49, recall: 0.49, precision: 0.48966515088879703, f_beta: 0.48583526565177937
train: step: 343, loss: 0.6921776533126831, acc: 0.61, recall: 0.61, precision: 0.6100440176070427, f_beta: 0.60996099609961
train: step: 344, loss: 0.6931099891662598, acc: 0.51, recall: 0.51, precision: 0.510334849111203, f_beta: 0.5059985885673959
train: step: 345, loss: 0.6934629678726196, acc: 0.48, recall: 0.48000000000000004, precision: 0.41134751773049644, f_beta: 0.3551587301587301
train: step: 346, loss: 0.6937349438667297, acc: 0.43, recall: 0.43, precision: 0.4150072850898494, f_beta: 0.40370331624646927
train: step: 347, loss: 0.6928675770759583, acc: 0.48, recall: 0.48, precision: 0.47771836007130125, f_beta: 0.4663382594417077
train: step: 348, loss: 0.6928622722625732, acc: 0.55, recall: 0.55, precision: 0.5500200080032013, f_beta: 0.54995499549955
train: step: 349, loss: 0.693146824836731, acc: 0.55, recall: 0.55, precision: 0.5505050505050505, f_beta: 0.5488721804511277
train: step: 350, loss: 0.6960089802742004, acc: 0.46, recall: 0.45999999999999996, precision: 0.4565972222222222, f_beta: 0.44920440636474906
train: step: 351, loss: 0.6938605308532715, acc: 0.4, recall: 0.4, precision: 0.3993558776167472, f_beta: 0.3990384615384615
train: step: 352, loss: 0.6898737549781799, acc: 0.52, recall: 0.52, precision: 0.5201288244766505, f_beta: 0.5192307692307692
train: step: 353, loss: 0.6926805377006531, acc: 0.53, recall: 0.53, precision: 0.5303030303030303, f_beta: 0.5288220551378446
train: step: 354, loss: 0.6927329301834106, acc: 0.48, recall: 0.48, precision: 0.4751984126984127, f_beta: 0.45355191256830596
train: step: 355, loss: 0.6921218633651733, acc: 0.5, recall: 0.5, precision: 0.5, f_beta: 0.4574652777777778
train: step: 356, loss: 0.6932106018066406, acc: 0.52, recall: 0.52, precision: 0.52, f_beta: 0.52
train: step: 357, loss: 0.6929411292076111, acc: 0.5, recall: 0.5, precision: 0.5, f_beta: 0.49919871794871795
train: step: 358, loss: 0.6902413368225098, acc: 0.57, recall: 0.5700000000000001, precision: 0.5887874175545409, f_beta: 0.5459824728117411
train: step: 359, loss: 0.693098783493042, acc: 0.52, recall: 0.52, precision: 0.52, f_beta: 0.52
train: step: 360, loss: 0.6936607956886292, acc: 0.45, recall: 0.44999999999999996, precision: 0.4498193496587716, f_beta: 0.44950455409868884
train: step: 361, loss: 0.6938025951385498, acc: 0.5, recall: 0.5, precision: 0.5, f_beta: 0.36580416032470825
train: step: 362, loss: 0.6937041282653809, acc: 0.54, recall: 0.54, precision: 0.5416666666666667, f_beta: 0.5353535353535354
train: step: 363, loss: 0.6931556463241577, acc: 0.46, recall: 0.45999999999999996, precision: 0.4554367201426025, f_beta: 0.44581280788177335
train: step: 364, loss: 0.6933072805404663, acc: 0.47, recall: 0.47, precision: 0.46703296703296704, f_beta: 0.4578005115089514
train: step: 365, loss: 0.6928483843803406, acc: 0.51, recall: 0.51, precision: 0.510508617065994, f_beta: 0.5039983804028748
train: step: 366, loss: 0.6933656334877014, acc: 0.52, recall: 0.52, precision: 0.5238095238095237, f_beta: 0.5
train: step: 367, loss: 0.6930292248725891, acc: 0.53, recall: 0.53, precision: 0.5310045473336089, f_beta: 0.5261619114830125
train: step: 368, loss: 0.6930629014968872, acc: 0.49, recall: 0.49, precision: 0.48996386993175434, f_beta: 0.4895405865278751
train: step: 369, loss: 0.6932587623596191, acc: 0.49, recall: 0.49, precision: 0.4899959983993597, f_beta: 0.48994899489948995
train: step: 370, loss: 0.6856871843338013, acc: 0.54, recall: 0.54, precision: 0.5946969696969697, f_beta: 0.46236559139784944
train: step: 371, loss: 0.6909550428390503, acc: 0.51, recall: 0.51, precision: 0.5150693188667872, f_beta: 0.4650070968446337
train: step: 372, loss: 0.6930261254310608, acc: 0.51, recall: 0.51, precision: 0.51010101010101, f_beta: 0.5087719298245614
train: step: 373, loss: 0.6932877898216248, acc: 0.49, recall: 0.49, precision: 0.4898989898989899, f_beta: 0.48872180451127817
train: step: 374, loss: 0.692758321762085, acc: 0.51, recall: 0.51, precision: 0.5141163184641445, f_beta: 0.47147017581706396
train: step: 375, loss: 0.692850649356842, acc: 0.54, recall: 0.54, precision: 0.5402576489533011, f_beta: 0.5392628205128205
train: step: 376, loss: 0.6933866143226624, acc: 0.4, recall: 0.4, precision: 0.39583333333333337, f_beta: 0.3939393939393939
train: step: 377, loss: 0.6931093335151672, acc: 0.5, recall: 0.5, precision: 0.5, f_beta: 0.5
train: step: 378, loss: 0.6924471259117126, acc: 0.55, recall: 0.55, precision: 0.5516742455560149, f_beta: 0.5463252343986289
train: step: 379, loss: 0.6931062340736389, acc: 0.46, recall: 0.45999999999999996, precision: 0.4554367201426025, f_beta: 0.44581280788177335
train: step: 380, loss: 0.6919950842857361, acc: 0.6, recall: 0.6000000000000001, precision: 0.601461038961039, f_beta: 0.5985547972701726
train: step: 381, loss: 0.6912792921066284, acc: 0.54, recall: 0.54, precision: 0.5946969696969697, f_beta: 0.46236559139784944
train: step: 382, loss: 0.6953519582748413, acc: 0.47, recall: 0.47, precision: 0.4336870026525199, f_beta: 0.3859344224307728
train: step: 383, loss: 0.6956037878990173, acc: 0.43, recall: 0.43, precision: 0.37597448618001417, f_beta: 0.36034115138592754
train: step: 384, loss: 0.6932289004325867, acc: 0.58, recall: 0.5800000000000001, precision: 0.5805152979066023, f_beta: 0.5793269230769231
train: step: 385, loss: 0.691417932510376, acc: 0.52, recall: 0.52, precision: 0.5222816399286987, f_beta: 0.5073891625615763
train: step: 386, loss: 0.6943084597587585, acc: 0.54, recall: 0.54, precision: 0.5496031746031746, f_beta: 0.5166036149642708
train: step: 387, loss: 0.6941298842430115, acc: 0.47, recall: 0.47, precision: 0.4696969696969697, f_beta: 0.468671679197995
train: step: 388, loss: 0.693095326423645, acc: 0.44, recall: 0.44, precision: 0.43912337662337664, f_beta: 0.43797671617824174
train: step: 389, loss: 0.6955239176750183, acc: 0.42, recall: 0.42, precision: 0.375, f_beta: 0.3626373626373627
train: step: 390, loss: 0.6904050707817078, acc: 0.55, recall: 0.55, precision: 0.5607090820786789, f_beta: 0.529239460194581
train: step: 391, loss: 0.6845695972442627, acc: 0.58, recall: 0.58, precision: 0.7222222222222222, f_beta: 0.5
train: step: 392, loss: 0.6930596232414246, acc: 0.47, recall: 0.47000000000000003, precision: 0.4649368863955119, f_beta: 0.4501504305425874
train: step: 393, loss: 0.6918972730636597, acc: 0.5, recall: 0.5, precision: 0.5, f_beta: 0.5
train: step: 394, loss: 0.6932180523872375, acc: 0.46, recall: 0.45999999999999996, precision: 0.4565972222222222, f_beta: 0.44920440636474906
train: step: 395, loss: 0.6938433051109314, acc: 0.43, recall: 0.43, precision: 0.4181860682561945, f_beta: 0.4086523498288204
train: step: 396, loss: 0.6939501762390137, acc: 0.43, recall: 0.43, precision: 0.42997198879551823, f_beta: 0.42994299429942995
train: step: 397, loss: 0.6911174058914185, acc: 0.47, recall: 0.47000000000000003, precision: 0.4689954526663911, f_beta: 0.46567194273616297
train: step: 398, loss: 0.6922241449356079, acc: 0.54, recall: 0.54, precision: 0.5434027777777778, f_beta: 0.5308037535699714
train: step: 399, loss: 0.6954970359802246, acc: 0.5, recall: 0.5, precision: 0.5, f_beta: 0.37996031746031744
train: step: 400, loss: 0.686799168586731, acc: 0.53, recall: 0.53, precision: 0.7577319587628866, f_beta: 0.39673982800667434
train: step: 401, loss: 0.6916983723640442, acc: 0.52, recall: 0.52, precision: 0.5291375291375291, f_beta: 0.47916666666666663
train: step: 402, loss: 0.6927443742752075, acc: 0.57, recall: 0.5700000000000001, precision: 0.5735603194619587, f_beta: 0.5647332725984411
train: step: 403, loss: 0.6914609670639038, acc: 0.52, recall: 0.52, precision: 0.52, f_beta: 0.52
train: step: 404, loss: 0.6941207051277161, acc: 0.46, recall: 0.46, precision: 0.45833333333333337, f_beta: 0.4545454545454545
train: step: 405, loss: 0.6906157732009888, acc: 0.54, recall: 0.54, precision: 0.5519750519750519, f_beta: 0.5118845500848896
train: step: 406, loss: 0.6940677165985107, acc: 0.4, recall: 0.4, precision: 0.3998397435897436, f_beta: 0.3997599039615846
train: step: 407, loss: 0.6930204629898071, acc: 0.51, recall: 0.51, precision: 0.5141163184641445, f_beta: 0.47147017581706396
train: step: 408, loss: 0.6933843493461609, acc: 0.49, recall: 0.49, precision: 0.48949138293400585, f_beta: 0.48375341633768604
train: step: 409, loss: 0.6933631300926208, acc: 0.53, recall: 0.53, precision: 0.5310045473336089, f_beta: 0.5261619114830125
train: step: 410, loss: 0.6908116936683655, acc: 0.51, recall: 0.51, precision: 0.5526315789473684, f_beta: 0.38557993730407525
train: step: 411, loss: 0.6925092339515686, acc: 0.55, recall: 0.55, precision: 0.5536250536250535, f_beta: 0.5422642660970399
train: step: 412, loss: 0.6930907964706421, acc: 0.49, recall: 0.49, precision: 0.4898000815993472, f_beta: 0.48748869460355737
train: step: 413, loss: 0.693183422088623, acc: 0.57, recall: 0.5700000000000001, precision: 0.5713994288045696, f_beta: 0.567882624861823
train: step: 414, loss: 0.6937307715415955, acc: 0.49, recall: 0.49, precision: 0.48869289914066033, f_beta: 0.4748223663886314
train: step: 415, loss: 0.6925985217094421, acc: 0.56, recall: 0.56, precision: 0.5636672325976231, f_beta: 0.5535714285714286
train: step: 416, loss: 0.6930923461914062, acc: 0.53, recall: 0.53, precision: 0.5423489553924337, f_beta: 0.49304282170208175
train: step: 417, loss: 0.6933643221855164, acc: 0.5, recall: 0.5, precision: 0.5, f_beta: 0.4967793880837359
train: step: 418, loss: 0.6939948201179504, acc: 0.5, recall: 0.5, precision: 0.5, f_beta: 0.39290917921321034
train: step: 419, loss: 0.6934553384780884, acc: 0.49, recall: 0.49, precision: 0.4837556855100714, f_beta: 0.4357782940590773
train: step: 420, loss: 0.6909592151641846, acc: 0.53, recall: 0.53, precision: 0.5423489553924337, f_beta: 0.49304282170208175
train: step: 421, loss: 0.6928045749664307, acc: 0.52, recall: 0.52, precision: 0.5208333333333334, f_beta: 0.5151515151515151
train: step: 422, loss: 0.6876856088638306, acc: 0.53, recall: 0.53, precision: 0.5915750915750916, f_beta: 0.43502824858757067
train: step: 423, loss: 0.6912536025047302, acc: 0.54, recall: 0.54, precision: 0.5476190476190477, f_beta: 0.5208333333333333
train: step: 424, loss: 0.6943938732147217, acc: 0.5, recall: 0.5, precision: 0.5, f_beta: 0.49000407996736023
train: step: 425, loss: 0.6927787065505981, acc: 0.53, recall: 0.53, precision: 0.530108390204737, f_beta: 0.5295766189570613
train: step: 426, loss: 0.6900116205215454, acc: 0.55, recall: 0.55, precision: 0.5584385226741468, f_beta: 0.5331465919701215
train: step: 427, loss: 0.6932252645492554, acc: 0.47, recall: 0.47, precision: 0.4696969696969697, f_beta: 0.468671679197995
train: step: 428, loss: 0.6904332041740417, acc: 0.46, recall: 0.45999999999999996, precision: 0.4451754385964912, f_beta: 0.42084942084942084
train: step: 429, loss: 0.6892231702804565, acc: 0.53, recall: 0.53, precision: 0.576608784473953, f_beta: 0.44568935015921696
train: step: 430, loss: 0.6918449401855469, acc: 0.59, recall: 0.5900000000000001, precision: 0.5903251706142112, f_beta: 0.5896306676008407
train: step: 431, loss: 0.6948650479316711, acc: 0.45, recall: 0.44999999999999996, precision: 0.44505494505494503, f_beta: 0.4373401534526854
train: step: 432, loss: 0.6926754713058472, acc: 0.54, recall: 0.54, precision: 0.54, f_beta: 0.54
train: step: 433, loss: 0.6860973834991455, acc: 0.55, recall: 0.55, precision: 0.6920122887864824, f_beta: 0.4479205005520795
train: step: 434, loss: 0.6940464973449707, acc: 0.49, recall: 0.49, precision: 0.48949138293400585, f_beta: 0.48375341633768604
train: step: 435, loss: 0.6916868686676025, acc: 0.51, recall: 0.51, precision: 0.5196078431372548, f_beta: 0.4415954415954416
train: step: 436, loss: 0.6954708695411682, acc: 0.45, recall: 0.45, precision: 0.30798771121351765, f_beta: 0.3252361673414305
train: step: 437, loss: 0.6931645274162292, acc: 0.5, recall: 0.5, precision: 0.5, f_beta: 0.4574652777777778
train: step: 438, loss: 0.6906273365020752, acc: 0.5, recall: 0.5, precision: 0.5, f_beta: 0.486863711001642
train: step: 439, loss: 0.6932446360588074, acc: 0.42, recall: 0.42, precision: 0.41883116883116883, f_beta: 0.4179044560417503
train: step: 440, loss: 0.6904749870300293, acc: 0.6, recall: 0.6, precision: 0.6148897058823529, f_beta: 0.586606035551881
train: step: 441, loss: 0.6930285096168518, acc: 0.49, recall: 0.49, precision: 0.4866666666666667, f_beta: 0.45599999999999996
train: step: 442, loss: 0.6930343508720398, acc: 0.55, recall: 0.55, precision: 0.5505050505050505, f_beta: 0.5488721804511277
train: step: 443, loss: 0.6923217177391052, acc: 0.61, recall: 0.61, precision: 0.6243781094527363, f_beta: 0.5983935742971889
train: step: 444, loss: 0.6938587427139282, acc: 0.54, recall: 0.54, precision: 0.5677506775067751, f_beta: 0.4875222816399287
train: step: 445, loss: 0.6907097697257996, acc: 0.48, recall: 0.48, precision: 0.47401247401247404, f_beta: 0.44821731748726656
train: step: 446, loss: 0.6956014037132263, acc: 0.49, recall: 0.49, precision: 0.4849306811332128, f_beta: 0.44317065181788406
train: step: 447, loss: 0.690836489200592, acc: 0.54, recall: 0.54, precision: 0.5496031746031746, f_beta: 0.5166036149642708
train: step: 448, loss: 0.6926378011703491, acc: 0.48, recall: 0.48, precision: 0.47086247086247085, f_beta: 0.4357638888888889
train: step: 449, loss: 0.6917855739593506, acc: 0.46, recall: 0.46, precision: 0.4375, f_beta: 0.4065934065934066
train: step: 450, loss: 0.692909836769104, acc: 0.53, recall: 0.53, precision: 0.5315258511979823, f_beta: 0.5242433444680635
train: step: 451, loss: 0.6819055080413818, acc: 0.58, recall: 0.58, precision: 0.7222222222222222, f_beta: 0.5
train: step: 452, loss: 0.6928454041481018, acc: 0.56, recall: 0.56, precision: 0.5615763546798029, f_beta: 0.5571658615136876
train: step: 453, loss: 0.692579984664917, acc: 0.48, recall: 0.48, precision: 0.4725877192982456, f_beta: 0.4422994422994423
train: step: 454, loss: 0.6923090219497681, acc: 0.58, recall: 0.58, precision: 0.6165501165501166, f_beta: 0.5442708333333334
train: step: 455, loss: 0.6923572421073914, acc: 0.47, recall: 0.47, precision: 0.466078697421981, f_beta: 0.4542271650705386
train: step: 456, loss: 0.6902967095375061, acc: 0.54, recall: 0.54, precision: 0.5625, f_beta: 0.4945054945054945
train: step: 457, loss: 0.6920129656791687, acc: 0.5, recall: 0.5, precision: 0.5, f_beta: 0.36580416032470825
train: step: 458, loss: 0.693898618221283, acc: 0.5, recall: 0.5, precision: 0.5, f_beta: 0.49000407996736023
train: step: 459, loss: 0.6928018927574158, acc: 0.53, recall: 0.53, precision: 0.5321750321750323, f_beta: 0.5219204557013528
train: step: 460, loss: 0.6927865743637085, acc: 0.53, recall: 0.53, precision: 0.5305997552019583, f_beta: 0.5276856597326902
train: step: 461, loss: 0.6908863186836243, acc: 0.52, recall: 0.52, precision: 0.5415282392026578, f_beta: 0.4485294117647059
train: step: 462, loss: 0.6878777146339417, acc: 0.54, recall: 0.54, precision: 0.7604166666666667, f_beta: 0.4165398274987316
train: step: 463, loss: 0.6932689547538757, acc: 0.54, recall: 0.54, precision: 0.5410509031198687, f_beta: 0.537037037037037
train: step: 464, loss: 0.6888536810874939, acc: 0.56, recall: 0.5599999999999999, precision: 0.5822368421052632, f_beta: 0.528099528099528
train: step: 465, loss: 0.6934211850166321, acc: 0.5, recall: 0.5, precision: 0.5, f_beta: 0.4574652777777778
train: step: 466, loss: 0.6914485096931458, acc: 0.52, recall: 0.52, precision: 0.5274122807017544, f_beta: 0.4851994851994852
train: step: 467, loss: 0.6933539509773254, acc: 0.53, recall: 0.53, precision: 0.530108390204737, f_beta: 0.5295766189570613
train: step: 468, loss: 0.6928008198738098, acc: 0.51, recall: 0.51, precision: 0.5101999184006528, f_beta: 0.5075871771681238
train: step: 469, loss: 0.6921386122703552, acc: 0.53, recall: 0.53, precision: 0.5487329434697856, f_beta: 0.4800309768779732
train: step: 470, loss: 0.6923981308937073, acc: 0.57, recall: 0.5700000000000001, precision: 0.5700280112044818, f_beta: 0.56995699569957
train: step: 471, loss: 0.6906675100326538, acc: 0.53, recall: 0.53, precision: 0.5487329434697856, f_beta: 0.4800309768779732
train: step: 472, loss: 0.6962683200836182, acc: 0.49, recall: 0.49, precision: 0.4803921568627451, f_beta: 0.4188034188034188
train: step: 473, loss: 0.6948182582855225, acc: 0.44, recall: 0.44, precision: 0.3983739837398374, f_beta: 0.3761140819964349
train: step: 474, loss: 0.6884620785713196, acc: 0.54, recall: 0.54, precision: 0.5459558823529411, f_beta: 0.524596940884663
train: step: 475, loss: 0.6915328502655029, acc: 0.48, recall: 0.48, precision: 0.4584717607973422, f_beta: 0.4025735294117647
train: step: 476, loss: 0.6935827136039734, acc: 0.47, recall: 0.47, precision: 0.4696969696969697, f_beta: 0.468671679197995
train: step: 477, loss: 0.6930866837501526, acc: 0.5, recall: 0.5, precision: 0.5, f_beta: 0.4967793880837359
train: step: 478, loss: 0.6931972503662109, acc: 0.46, recall: 0.45999999999999996, precision: 0.45974235104669886, f_beta: 0.4591346153846153
train: step: 479, loss: 0.6957741379737854, acc: 0.52, recall: 0.52, precision: 0.5415282392026578, f_beta: 0.4485294117647059
train: step: 480, loss: 0.6947538256645203, acc: 0.51, recall: 0.51, precision: 0.5221043324491601, f_beta: 0.43227899432278993
train: step: 481, loss: 0.693648099899292, acc: 0.51, recall: 0.51, precision: 0.510508617065994, f_beta: 0.5039983804028748
train: step: 482, loss: 0.6927163004875183, acc: 0.56, recall: 0.56, precision: 0.5636672325976231, f_beta: 0.5535714285714286
train: step: 483, loss: 0.6931704878807068, acc: 0.5, recall: 0.5, precision: 0.5, f_beta: 0.4967793880837359
train: step: 484, loss: 0.6936458349227905, acc: 0.48, recall: 0.48, precision: 0.4787775891341256, f_beta: 0.4724025974025974
train: step: 485, loss: 0.6933830976486206, acc: 0.49, recall: 0.49, precision: 0.48949138293400585, f_beta: 0.48375341633768604
train: step: 486, loss: 0.6937634348869324, acc: 0.44, recall: 0.44000000000000006, precision: 0.4375, f_beta: 0.43434343434343436
train: step: 487, loss: 0.6939916014671326, acc: 0.5, recall: 0.5, precision: 0.5, f_beta: 0.42555147058823534
train: step: 488, loss: 0.6929754018783569, acc: 0.55, recall: 0.55, precision: 0.554945054945055, f_beta: 0.5396419437340153
train: step: 489, loss: 0.6933784484863281, acc: 0.57, recall: 0.5700000000000001, precision: 0.5702529104777199, f_beta: 0.5696126513862476
train: step: 490, loss: 0.6934185028076172, acc: 0.44, recall: 0.44, precision: 0.43961352657004826, f_beta: 0.4391025641025641
train: step: 491, loss: 0.696124792098999, acc: 0.49, recall: 0.49, precision: 0.4140893470790378, f_beta: 0.3453985367731998
train: step: 492, loss: 0.6926471590995789, acc: 0.55, recall: 0.55, precision: 0.5505050505050505, f_beta: 0.5488721804511277
train: step: 493, loss: 0.6930183172225952, acc: 0.54, recall: 0.54, precision: 0.5677506775067751, f_beta: 0.4875222816399287
train: step: 494, loss: 0.6963319182395935, acc: 0.49, recall: 0.49, precision: 0.4744637385086823, f_beta: 0.3985139757046821
train: step: 495, loss: 0.6928234696388245, acc: 0.48, recall: 0.48000000000000004, precision: 0.47947454844006565, f_beta: 0.47665056360708535
train: step: 496, loss: 0.690958559513092, acc: 0.56, recall: 0.56, precision: 0.5744047619047619, f_beta: 0.537620849096259
train: step: 497, loss: 0.6927280426025391, acc: 0.52, recall: 0.52, precision: 0.5208333333333334, f_beta: 0.5151515151515151
train: step: 498, loss: 0.6926522850990295, acc: 0.49, recall: 0.49, precision: 0.4694749694749695, f_beta: 0.38694554633970435
train: step: 499, loss: 0.6927458047866821, acc: 0.48, recall: 0.48, precision: 0.47401247401247404, f_beta: 0.44821731748726656
train: step: 500, loss: 0.6933310031890869, acc: 0.46, recall: 0.45999999999999996, precision: 0.45974235104669886, f_beta: 0.4591346153846153
train: step: 501, loss: 0.6931352019309998, acc: 0.52, recall: 0.52, precision: 0.5217013888888888, f_beta: 0.5104039167686658
train: step: 502, loss: 0.6921283602714539, acc: 0.57, recall: 0.5700000000000001, precision: 0.5702529104777199, f_beta: 0.5696126513862476
train: step: 503, loss: 0.6927980184555054, acc: 0.54, recall: 0.54, precision: 0.5416666666666667, f_beta: 0.5353535353535354
train: step: 504, loss: 0.7015191912651062, acc: 0.5, recall: 0.5, precision: 0.5, f_beta: 0.37996031746031744
train: step: 505, loss: 0.6914923191070557, acc: 0.47, recall: 0.47, precision: 0.46782496782496785, f_beta: 0.4608890245142916
train: step: 506, loss: 0.6935675144195557, acc: 0.44, recall: 0.44, precision: 0.40625, f_beta: 0.3846153846153846
train: step: 507, loss: 0.6913055181503296, acc: 0.57, recall: 0.5700000000000001, precision: 0.6054852320675106, f_beta: 0.5305164319248826
train: step: 508, loss: 0.6923045516014099, acc: 0.49, recall: 0.49, precision: 0.48927498927498925, f_beta: 0.48123283490997865
train: step: 509, loss: 0.6921607255935669, acc: 0.56, recall: 0.56, precision: 0.5668449197860963, f_beta: 0.548440065681445
train: step: 510, loss: 0.6933518052101135, acc: 0.5, recall: 0.5, precision: 0.5, f_beta: 0.49919871794871795
train: step: 511, loss: 0.6907599568367004, acc: 0.54, recall: 0.54, precision: 0.5476190476190477, f_beta: 0.5208333333333333
train: step: 512, loss: 0.6846710443496704, acc: 0.56, recall: 0.56, precision: 0.7659574468085106, f_beta: 0.4543650793650793
train: step: 513, loss: 0.6933075189590454, acc: 0.5, recall: 0.5, precision: 0.5, f_beta: 0.49979991996798717
train: step: 514, loss: 0.6931511759757996, acc: 0.46, recall: 0.45999999999999996, precision: 0.45993589743589747, f_beta: 0.45978391356542614
train: step: 515, loss: 0.6904397010803223, acc: 0.52, recall: 0.52, precision: 0.5886524822695035, f_beta: 0.40476190476190477
train: step: 516, loss: 0.6914626359939575, acc: 0.5, recall: 0.5, precision: 0.5, f_beta: 0.43464495703301675
train: step: 517, loss: 0.6868032813072205, acc: 0.53, recall: 0.53, precision: 0.6578947368421053, f_beta: 0.4106583072100314
train: step: 518, loss: 0.6934092044830322, acc: 0.54, recall: 0.54, precision: 0.5402576489533011, f_beta: 0.5392628205128205
train: step: 519, loss: 0.6919065117835999, acc: 0.61, recall: 0.61, precision: 0.6136833402232327, f_beta: 0.6068152031454783
train: step: 520, loss: 0.6926705837249756, acc: 0.54, recall: 0.54, precision: 0.5424448217317488, f_beta: 0.5332792207792207
train: step: 521, loss: 0.6928814053535461, acc: 0.44, recall: 0.44, precision: 0.43912337662337664, f_beta: 0.43797671617824174
train: step: 522, loss: 0.6931734681129456, acc: 0.49, recall: 0.49, precision: 0.4140893470790378, f_beta: 0.3453985367731998
train: step: 523, loss: 0.6925836801528931, acc: 0.52, recall: 0.52, precision: 0.5201288244766505, f_beta: 0.5192307692307692
train: step: 524, loss: 0.6931142210960388, acc: 0.51, recall: 0.51, precision: 0.5100040016006402, f_beta: 0.50995099509951
train: step: 525, loss: 0.6891324520111084, acc: 0.52, recall: 0.52, precision: 0.5248015873015872, f_beta: 0.49558638083228246
train: step: 526, loss: 0.6924830675125122, acc: 0.41, recall: 0.41000000000000003, precision: 0.40820073439412485, f_beta: 0.4070947643452919
train: step: 527, loss: 0.6894692778587341, acc: 0.53, recall: 0.53, precision: 0.7577319587628866, f_beta: 0.39673982800667434
train: step: 528, loss: 0.6935074329376221, acc: 0.49, recall: 0.49, precision: 0.48966515088879703, f_beta: 0.48583526565177937
train: step: 529, loss: 0.6932732462882996, acc: 0.54, recall: 0.54, precision: 0.5405844155844155, f_beta: 0.5383380168606985
train: step: 530, loss: 0.6909210681915283, acc: 0.45, recall: 0.45, precision: 0.449000407996736, f_beta: 0.4472917294744247
train: step: 531, loss: 0.693193256855011, acc: 0.56, recall: 0.56, precision: 0.5600961538461539, f_beta: 0.5598239295718288
train: step: 532, loss: 0.6931180357933044, acc: 0.52, recall: 0.52, precision: 0.5200320512820513, f_beta: 0.5198079231692677
train: step: 533, loss: 0.69349205493927, acc: 0.49, recall: 0.49, precision: 0.48785818358426425, f_beta: 0.46647138822052514
train: step: 534, loss: 0.6984429359436035, acc: 0.43, recall: 0.43, precision: 0.4181860682561945, f_beta: 0.4086523498288204
train: step: 535, loss: 0.6917939186096191, acc: 0.56, recall: 0.56, precision: 0.5603864734299517, f_beta: 0.5592948717948718
train: step: 536, loss: 0.6922964453697205, acc: 0.57, recall: 0.57, precision: 0.575075075075075, f_beta: 0.562608076492727
train: step: 537, loss: 0.6903293132781982, acc: 0.58, recall: 0.58, precision: 0.603950103950104, f_beta: 0.5543293718166383
train: step: 538, loss: 0.6948434710502625, acc: 0.48, recall: 0.48000000000000004, precision: 0.41134751773049644, f_beta: 0.3551587301587301
train: step: 539, loss: 0.6941204071044922, acc: 0.39, recall: 0.39, precision: 0.38631665977676727, f_beta: 0.385018651073697
train: step: 540, loss: 0.6925142407417297, acc: 0.53, recall: 0.53, precision: 0.576608784473953, f_beta: 0.44568935015921696
train: step: 541, loss: 0.6867133378982544, acc: 0.54, recall: 0.54, precision: 0.7604166666666667, f_beta: 0.4165398274987316
train: step: 542, loss: 0.6930564641952515, acc: 0.5, recall: 0.5, precision: 0.5, f_beta: 0.47916666666666663
train: step: 543, loss: 0.6924994587898254, acc: 0.51, recall: 0.51, precision: 0.5100040016006402, f_beta: 0.50995099509951
train: step: 544, loss: 0.6937940120697021, acc: 0.42, recall: 0.42, precision: 0.42, f_beta: 0.41999999999999993
train: step: 545, loss: 0.6923498511314392, acc: 0.56, recall: 0.5599999999999999, precision: 0.5651041666666667, f_beta: 0.5512035903712771
train: step: 546, loss: 0.6927825212478638, acc: 0.53, recall: 0.53, precision: 0.5303030303030303, f_beta: 0.5288220551378446
train: step: 547, loss: 0.6926706433296204, acc: 0.55, recall: 0.55, precision: 0.5501806503412284, f_beta: 0.5495946351716544
train: step: 548, loss: 0.6868691444396973, acc: 0.56, recall: 0.56, precision: 0.59375, f_beta: 0.5164835164835164
train: step: 549, loss: 0.6923775672912598, acc: 0.51, recall: 0.51, precision: 0.5150693188667872, f_beta: 0.4650070968446337
train: step: 550, loss: 0.689046323299408, acc: 0.63, recall: 0.63, precision: 0.6343530384456387, f_beta: 0.6269785260610949
train: step: 551, loss: 0.693263828754425, acc: 0.49, recall: 0.49, precision: 0.2474747474747475, f_beta: 0.32885906040268453
train: step: 552, loss: 0.6928684115409851, acc: 0.48, recall: 0.48, precision: 0.47987117552334946, f_beta: 0.47916666666666663
train: step: 553, loss: 0.6919978857040405, acc: 0.66, recall: 0.66, precision: 0.6602564102564102, f_beta: 0.6598639455782312
train: step: 554, loss: 0.69354647397995, acc: 0.49, recall: 0.49, precision: 0.48927498927498925, f_beta: 0.48123283490997865
train: step: 555, loss: 0.6940719485282898, acc: 0.45, recall: 0.44999999999999996, precision: 0.42941840767927725, f_beta: 0.40675223816201056
train: step: 556, loss: 0.6924630999565125, acc: 0.51, recall: 0.51, precision: 0.5113071008593397, f_beta: 0.49541756770672435
train: step: 557, loss: 0.694109320640564, acc: 0.48, recall: 0.48, precision: 0.47916666666666663, f_beta: 0.4747474747474748
train: step: 558, loss: 0.6934321522712708, acc: 0.46, recall: 0.46, precision: 0.46, f_beta: 0.46
train: step: 559, loss: 0.6927094459533691, acc: 0.52, recall: 0.52, precision: 0.5208333333333334, f_beta: 0.5151515151515151
train: step: 560, loss: 0.6925672888755798, acc: 0.51, recall: 0.51, precision: 0.5113071008593397, f_beta: 0.49541756770672435
train: step: 561, loss: 0.6909092664718628, acc: 0.57, recall: 0.5700000000000001, precision: 0.6372549019607843, f_beta: 0.50997150997151
train: step: 562, loss: 0.6931658387184143, acc: 0.51, recall: 0.51, precision: 0.510334849111203, f_beta: 0.5059985885673959
train: step: 563, loss: 0.6878626942634583, acc: 0.54, recall: 0.54, precision: 0.6358695652173914, f_beta: 0.4414764448761534
train: step: 564, loss: 0.6918962001800537, acc: 0.59, recall: 0.59, precision: 0.5965250965250966, f_beta: 0.5829518868884142
train: step: 565, loss: 0.6934260725975037, acc: 0.49, recall: 0.49, precision: 0.48927498927498925, f_beta: 0.48123283490997865
train: step: 566, loss: 0.6901520490646362, acc: 0.48, recall: 0.48, precision: 0.46875, f_beta: 0.4285714285714286
train: step: 567, loss: 0.6920877695083618, acc: 0.47, recall: 0.47, precision: 0.46703296703296704, f_beta: 0.4578005115089514
train: step: 568, loss: 0.692671537399292, acc: 0.46, recall: 0.46, precision: 0.4523809523809524, f_beta: 0.4375
train: step: 569, loss: 0.6930040121078491, acc: 0.55, recall: 0.55, precision: 0.5500200080032013, f_beta: 0.54995499549955
train: step: 570, loss: 0.6939194202423096, acc: 0.42, recall: 0.42000000000000004, precision: 0.4178981937602627, f_beta: 0.4162640901771337
train: step: 571, loss: 0.6933037638664246, acc: 0.46, recall: 0.45999999999999996, precision: 0.45993589743589747, f_beta: 0.45978391356542614
train: step: 572, loss: 0.6892548203468323, acc: 0.62, recall: 0.62, precision: 0.6336898395721926, f_beta: 0.6100164203612479
train: step: 573, loss: 0.6936109066009521, acc: 0.51, recall: 0.51, precision: 0.51010101010101, f_beta: 0.5087719298245614
train: step: 574, loss: 0.6932588815689087, acc: 0.46, recall: 0.46, precision: 0.46, f_beta: 0.46
train: step: 575, loss: 0.6939487457275391, acc: 0.42, recall: 0.42000000000000004, precision: 0.4198717948717949, f_beta: 0.41976790716286516
train: step: 576, loss: 0.692574143409729, acc: 0.57, recall: 0.5700000000000001, precision: 0.5700280112044818, f_beta: 0.56995699569957
train: step: 577, loss: 0.6933659315109253, acc: 0.52, recall: 0.52, precision: 0.5201288244766505, f_beta: 0.5192307692307692
train: step: 578, loss: 0.69170743227005, acc: 0.58, recall: 0.5800000000000001, precision: 0.5833333333333333, f_beta: 0.5757575757575757
train: step: 579, loss: 0.6944051384925842, acc: 0.39, recall: 0.39, precision: 0.3888888888888889, f_beta: 0.38847117794486213
train: step: 580, loss: 0.6928828358650208, acc: 0.57, recall: 0.5700000000000001, precision: 0.5700280112044818, f_beta: 0.56995699569957
train: step: 581, loss: 0.6897517442703247, acc: 0.53, recall: 0.53, precision: 0.5380517503805176, f_beta: 0.5037482842360891
train: step: 582, loss: 0.6905757188796997, acc: 0.5, recall: 0.5, precision: 0.5, f_beta: 0.486863711001642
train: step: 583, loss: 0.6906499266624451, acc: 0.5, recall: 0.5, precision: 0.5, f_beta: 0.46374946374946374
train: step: 584, loss: 0.6930475831031799, acc: 0.54, recall: 0.54, precision: 0.5405844155844155, f_beta: 0.5383380168606985
train: step: 585, loss: 0.6921281218528748, acc: 0.52, recall: 0.52, precision: 0.5201288244766505, f_beta: 0.5192307692307692
train: step: 586, loss: 0.6844556331634521, acc: 0.55, recall: 0.55, precision: 0.763157894736842, f_beta: 0.4357366771159875
train: step: 587, loss: 0.6987318396568298, acc: 0.49, recall: 0.49, precision: 0.4473684210526316, f_beta: 0.3605015673981191
train: step: 588, loss: 0.6927052140235901, acc: 0.54, recall: 0.54, precision: 0.5445632798573975, f_beta: 0.5279146141215106
train: step: 589, loss: 0.6938065886497498, acc: 0.48, recall: 0.48, precision: 0.46279761904761907, f_beta: 0.4120307553143374
train: step: 590, loss: 0.6928625702857971, acc: 0.53, recall: 0.53, precision: 0.5310045473336089, f_beta: 0.5261619114830125
train: step: 591, loss: 0.6928977966308594, acc: 0.48, recall: 0.48, precision: 0.47987117552334946, f_beta: 0.47916666666666663
train: step: 592, loss: 0.6916788220405579, acc: 0.57, recall: 0.5700000000000001, precision: 0.5818139317438056, f_beta: 0.5538956323270049
train: step: 593, loss: 0.69305819272995, acc: 0.5, recall: 0.5, precision: 0.5, f_beta: 0.4967793880837359
train: step: 594, loss: 0.6930825710296631, acc: 0.53, recall: 0.53, precision: 0.5310045473336089, f_beta: 0.5261619114830125
train: step: 595, loss: 0.6930633783340454, acc: 0.49, recall: 0.49, precision: 0.48996386993175434, f_beta: 0.4895405865278751
train: step: 596, loss: 0.6983017921447754, acc: 0.48, recall: 0.48, precision: 0.46875, f_beta: 0.4285714285714286
train: step: 597, loss: 0.6917949914932251, acc: 0.55, recall: 0.55, precision: 0.5500200080032013, f_beta: 0.54995499549955
train: step: 598, loss: 0.6926669478416443, acc: 0.56, recall: 0.56, precision: 0.5668449197860963, f_beta: 0.548440065681445
train: step: 599, loss: 0.6936724185943604, acc: 0.49, recall: 0.49, precision: 0.48949138293400585, f_beta: 0.48375341633768604
train: step: 600, loss: 0.6925264000892639, acc: 0.56, recall: 0.56, precision: 0.5689338235294117, f_beta: 0.545266639107069
train: step: 601, loss: 0.6934916973114014, acc: 0.54, recall: 0.54, precision: 0.5445632798573975, f_beta: 0.5279146141215106
train: step: 602, loss: 0.6933613419532776, acc: 0.5, recall: 0.5, precision: 0.5, f_beta: 0.486863711001642
train: step: 603, loss: 0.6918845176696777, acc: 0.51, recall: 0.51, precision: 0.5255362614913177, f_beta: 0.4221016629319495
train: step: 604, loss: 0.6928799152374268, acc: 0.52, recall: 0.52, precision: 0.5201288244766505, f_beta: 0.5192307692307692
train: step: 605, loss: 0.6932822465896606, acc: 0.49, recall: 0.49, precision: 0.4899959983993597, f_beta: 0.48994899489948995
train: step: 606, loss: 0.6950680613517761, acc: 0.38, recall: 0.38, precision: 0.37684729064039413, f_beta: 0.37600644122383253
train: step: 607, loss: 0.693483293056488, acc: 0.51, recall: 0.51, precision: 0.5100361300682457, f_beta: 0.5095586027424682
train: step: 608, loss: 0.6934606432914734, acc: 0.52, recall: 0.52, precision: 0.5473484848484849, f_beta: 0.43899018232819076
train: step: 609, loss: 0.6935976147651672, acc: 0.48, recall: 0.48, precision: 0.47771836007130125, f_beta: 0.4663382594417077
train: step: 610, loss: 0.6929463148117065, acc: 0.5, recall: 0.5, precision: 0.5, f_beta: 0.36580416032470825
train: step: 611, loss: 0.6929759383201599, acc: 0.46, recall: 0.45999999999999996, precision: 0.4554367201426025, f_beta: 0.44581280788177335
train: step: 612, loss: 0.6930561661720276, acc: 0.53, recall: 0.53, precision: 0.5303030303030303, f_beta: 0.5288220551378446
train: step: 613, loss: 0.6922553777694702, acc: 0.56, recall: 0.56, precision: 0.5608766233766234, f_beta: 0.5584102769971899
train: step: 614, loss: 0.6925221085548401, acc: 0.54, recall: 0.54, precision: 0.5402576489533011, f_beta: 0.5392628205128205
train: step: 615, loss: 0.6909700036048889, acc: 0.49, recall: 0.49, precision: 0.4849306811332128, f_beta: 0.44317065181788406
train: step: 616, loss: 0.6919553875923157, acc: 0.47, recall: 0.47, precision: 0.4547920433996383, f_beta: 0.42133420679113437
train: step: 617, loss: 0.6931377649307251, acc: 0.6, recall: 0.6, precision: 0.6001602564102564, f_beta: 0.5998399359743898
train: step: 618, loss: 0.6883307099342346, acc: 0.59, recall: 0.59, precision: 0.5989010989010989, f_beta: 0.5805626598465473
train: step: 619, loss: 0.695412278175354, acc: 0.49, recall: 0.49, precision: 0.48927498927498925, f_beta: 0.48123283490997865
train: step: 620, loss: 0.6930158138275146, acc: 0.55, recall: 0.55, precision: 0.550999592003264, f_beta: 0.5477841422972565
train: step: 621, loss: 0.6941020488739014, acc: 0.41, recall: 0.41000000000000003, precision: 0.40542244640605296, f_beta: 0.4027735600769308
train: step: 622, loss: 0.6932128667831421, acc: 0.47, recall: 0.47, precision: 0.46989160979526295, f_beta: 0.46952257031328193
train: step: 623, loss: 0.6935842037200928, acc: 0.45, recall: 0.45, precision: 0.4187784275503573, f_beta: 0.3915256112401814
train: step: 624, loss: 0.6936454772949219, acc: 0.47, recall: 0.47000000000000003, precision: 0.4468462083628632, f_beta: 0.4052294916395466
train: step: 625, loss: 0.692646861076355, acc: 0.52, recall: 0.52, precision: 0.5217013888888888, f_beta: 0.5104039167686658
train: step: 626, loss: 0.6903098225593567, acc: 0.53, recall: 0.53, precision: 0.5663129973474801, f_beta: 0.45545128026879855
train: step: 627, loss: 0.6828739047050476, acc: 0.58, recall: 0.58, precision: 0.7717391304347826, f_beta: 0.4900437105390967
train: step: 628, loss: 0.6934037208557129, acc: 0.48, recall: 0.48000000000000004, precision: 0.47947454844006565, f_beta: 0.47665056360708535
train: step: 629, loss: 0.6923079490661621, acc: 0.52, recall: 0.52, precision: 0.6302083333333333, f_beta: 0.39117199391172
train: step: 630, loss: 0.6932520866394043, acc: 0.48, recall: 0.48, precision: 0.48, f_beta: 0.48
train: step: 631, loss: 0.6927263140678406, acc: 0.49, recall: 0.49, precision: 0.48927498927498925, f_beta: 0.48123283490997865
train: step: 632, loss: 0.6937465667724609, acc: 0.48, recall: 0.48, precision: 0.4782986111111111, f_beta: 0.46960424316605465
train: step: 633, loss: 0.6929779648780823, acc: 0.5, recall: 0.5, precision: 0.5, f_beta: 0.43464495703301675
train: step: 634, loss: 0.6952903866767883, acc: 0.46, recall: 0.45999999999999996, precision: 0.4169435215946844, f_beta: 0.3795955882352941
train: step: 635, loss: 0.6926035284996033, acc: 0.54, recall: 0.54, precision: 0.5459558823529411, f_beta: 0.524596940884663
train: step: 636, loss: 0.6925601959228516, acc: 0.53, recall: 0.53, precision: 0.530108390204737, f_beta: 0.5295766189570613
train: step: 637, loss: 0.6929907202720642, acc: 0.49, recall: 0.49, precision: 0.48949138293400585, f_beta: 0.48375341633768604
train: step: 638, loss: 0.6929109692573547, acc: 0.49, recall: 0.49, precision: 0.4849306811332128, f_beta: 0.44317065181788406
train: step: 639, loss: 0.692291796207428, acc: 0.56, recall: 0.5599999999999999, precision: 0.5651041666666667, f_beta: 0.5512035903712771
train: step: 640, loss: 0.6902103424072266, acc: 0.52, recall: 0.52, precision: 0.6302083333333333, f_beta: 0.39117199391172
train: step: 641, loss: 0.6934881806373596, acc: 0.47, recall: 0.47, precision: 0.46989160979526295, f_beta: 0.46952257031328193
train: step: 642, loss: 0.6935898065567017, acc: 0.47, recall: 0.47000000000000003, precision: 0.4694002447980416, f_beta: 0.467390212038991
train: step: 643, loss: 0.6930732727050781, acc: 0.53, recall: 0.53, precision: 0.5305997552019583, f_beta: 0.5276856597326902
train: step: 644, loss: 0.693168044090271, acc: 0.52, recall: 0.52, precision: 0.5201288244766505, f_beta: 0.5192307692307692
train: step: 645, loss: 0.6927242279052734, acc: 0.57, recall: 0.5700000000000001, precision: 0.5702529104777199, f_beta: 0.5696126513862476
train: step: 646, loss: 0.6934366822242737, acc: 0.52, recall: 0.52, precision: 0.5201288244766505, f_beta: 0.5192307692307692
train: step: 647, loss: 0.69300377368927, acc: 0.49, recall: 0.49, precision: 0.46159754224270355, f_beta: 0.37430990062569014
train: step: 648, loss: 0.6923482418060303, acc: 0.49, recall: 0.49, precision: 0.4899959983993597, f_beta: 0.48994899489948995
train: step: 649, loss: 0.6963184475898743, acc: 0.48, recall: 0.48, precision: 0.47702205882352944, f_beta: 0.46258784621744525
train: step: 650, loss: 0.6935871839523315, acc: 0.44, recall: 0.43999999999999995, precision: 0.43489583333333337, f_beta: 0.4288045695634435
train: step: 651, loss: 0.6936294436454773, acc: 0.44, recall: 0.44, precision: 0.43961352657004826, f_beta: 0.4391025641025641
train: step: 652, loss: 0.6932858824729919, acc: 0.44, recall: 0.44000000000000006, precision: 0.4375, f_beta: 0.43434343434343436
train: step: 653, loss: 0.6939204931259155, acc: 0.42, recall: 0.42000000000000004, precision: 0.4198717948717949, f_beta: 0.41976790716286516
train: step: 654, loss: 0.6933594346046448, acc: 0.45, recall: 0.45, precision: 0.4499799919967987, f_beta: 0.44994499449944997
train: step: 655, loss: 0.6930439472198486, acc: 0.54, recall: 0.54, precision: 0.5402576489533011, f_beta: 0.5392628205128205
train: step: 656, loss: 0.6922893524169922, acc: 0.58, recall: 0.5800000000000001, precision: 0.5833333333333333, f_beta: 0.5757575757575757
train: step: 657, loss: 0.6941785216331482, acc: 0.42, recall: 0.42, precision: 0.41883116883116883, f_beta: 0.4179044560417503
train: step: 658, loss: 0.6931952834129333, acc: 0.51, recall: 0.51, precision: 0.5113071008593397, f_beta: 0.49541756770672435
train: step: 659, loss: 0.6918409466743469, acc: 0.52, recall: 0.52, precision: 0.5238095238095237, f_beta: 0.5
train: step: 660, loss: 0.6927054524421692, acc: 0.56, recall: 0.56, precision: 0.5636672325976231, f_beta: 0.5535714285714286
train: step: 661, loss: 0.6925980448722839, acc: 0.5, recall: 0.5, precision: 0.5, f_beta: 0.49979991996798717
train: step: 662, loss: 0.6914693713188171, acc: 0.53, recall: 0.53, precision: 0.5531537916371367, f_beta: 0.4725620020199753
train: step: 663, loss: 0.693051278591156, acc: 0.49, recall: 0.49, precision: 0.48831229546517063, f_beta: 0.47089947089947093
train: step: 664, loss: 0.6848008632659912, acc: 0.55, recall: 0.55, precision: 0.6526251526251526, f_beta: 0.45906959971150385
train: step: 665, loss: 0.6939892768859863, acc: 0.43, recall: 0.43000000000000005, precision: 0.42643968053804115, f_beta: 0.42301852414211966
train: step: 666, loss: 0.6914544105529785, acc: 0.61, recall: 0.61, precision: 0.6285647498831229, f_beta: 0.5953937130407718
train: step: 667, loss: 0.6911517977714539, acc: 0.49, recall: 0.49, precision: 0.4873160832064941, f_beta: 0.4615140956604371
train: step: 668, loss: 0.6930705904960632, acc: 0.43, recall: 0.43, precision: 0.42860057119543044, f_beta: 0.42719324690985827
train: step: 669, loss: 0.6928300261497498, acc: 0.52, recall: 0.52, precision: 0.52, f_beta: 0.52
train: step: 670, loss: 0.6935643553733826, acc: 0.49, recall: 0.49, precision: 0.48949138293400585, f_beta: 0.48375341633768604
train: step: 671, loss: 0.6955626606941223, acc: 0.46, recall: 0.46, precision: 0.4523809523809524, f_beta: 0.4375
train: step: 672, loss: 0.692898690700531, acc: 0.51, recall: 0.51, precision: 0.5526315789473684, f_beta: 0.38557993730407525
train: step: 673, loss: 0.6924970149993896, acc: 0.44, recall: 0.44000000000000006, precision: 0.42203742203742206, f_beta: 0.40577249575551777
train: step: 674, loss: 0.6916956901550293, acc: 0.62, recall: 0.62, precision: 0.6336898395721926, f_beta: 0.6100164203612479
train: step: 675, loss: 0.6928182244300842, acc: 0.56, recall: 0.5599999999999999, precision: 0.5651041666666667, f_beta: 0.5512035903712771
train: step: 676, loss: 0.6938000321388245, acc: 0.46, recall: 0.46, precision: 0.45833333333333337, f_beta: 0.4545454545454545
train: step: 677, loss: 0.6925513744354248, acc: 0.48, recall: 0.48, precision: 0.47970779220779225, f_beta: 0.4781212364512244
train: step: 678, loss: 0.6925510168075562, acc: 0.52, recall: 0.52, precision: 0.5202922077922079, f_beta: 0.5182657567242072
train: step: 679, loss: 0.692145824432373, acc: 0.51, recall: 0.51, precision: 0.7525252525252526, f_beta: 0.3551783129359126
train: step: 680, loss: 0.6925697922706604, acc: 0.52, recall: 0.52, precision: 0.5212224108658743, f_beta: 0.512987012987013
train: step: 681, loss: 0.6926681995391846, acc: 0.52, recall: 0.52, precision: 0.5205254515599343, f_beta: 0.5169082125603865
train: step: 682, loss: 0.6924902200698853, acc: 0.49, recall: 0.49, precision: 0.4849306811332128, f_beta: 0.44317065181788406
train: step: 683, loss: 0.692213237285614, acc: 0.59, recall: 0.5900000000000001, precision: 0.5917992656058751, f_beta: 0.5879811074263892
train: step: 684, loss: 0.6930195689201355, acc: 0.56, recall: 0.5599999999999999, precision: 0.5651041666666667, f_beta: 0.5512035903712771
train: step: 685, loss: 0.6939160823822021, acc: 0.51, recall: 0.51, precision: 0.510334849111203, f_beta: 0.5059985885673959
train: step: 686, loss: 0.6943158507347107, acc: 0.49, recall: 0.49, precision: 0.4140893470790378, f_beta: 0.3453985367731998
train: step: 687, loss: 0.6891947388648987, acc: 0.56, recall: 0.56, precision: 0.5744047619047619, f_beta: 0.537620849096259
train: step: 688, loss: 0.6925858855247498, acc: 0.53, recall: 0.53, precision: 0.5300120048019208, f_beta: 0.5299529952995299
train: step: 689, loss: 0.6929475665092468, acc: 0.54, recall: 0.54, precision: 0.5400641025641025, f_beta: 0.5398159263705482
train: step: 690, loss: 0.6920424699783325, acc: 0.45, recall: 0.44999999999999996, precision: 0.42941840767927725, f_beta: 0.40675223816201056
train: step: 691, loss: 0.6931076049804688, acc: 0.53, recall: 0.53, precision: 0.5303030303030303, f_beta: 0.5288220551378446
train: step: 692, loss: 0.693329930305481, acc: 0.46, recall: 0.46, precision: 0.45894909688013136, f_beta: 0.4565217391304348
train: step: 693, loss: 0.6929795742034912, acc: 0.5, recall: 0.5, precision: 0.5, f_beta: 0.46374946374946374
train: step: 694, loss: 0.6936419010162354, acc: 0.46, recall: 0.46, precision: 0.4540441176470588, f_beta: 0.44191814799503926
train: step: 695, loss: 0.6935874819755554, acc: 0.46, recall: 0.46, precision: 0.4594155844155844, f_beta: 0.45804897631473307
train: step: 696, loss: 0.6933202147483826, acc: 0.44, recall: 0.44, precision: 0.43842364532019706, f_beta: 0.4363929146537842
train: step: 697, loss: 0.6941580176353455, acc: 0.51, recall: 0.51, precision: 0.5177179305457122, f_beta: 0.45011783189316573
train: step: 698, loss: 0.6934956908226013, acc: 0.45, recall: 0.44999999999999996, precision: 0.4498193496587716, f_beta: 0.44950455409868884
train: step: 699, loss: 0.6885201334953308, acc: 0.57, recall: 0.5700000000000001, precision: 0.5849927149101506, f_beta: 0.5501621508525997
train: step: 700, loss: 0.6930758953094482, acc: 0.48, recall: 0.48, precision: 0.48, f_beta: 0.48
train: step: 701, loss: 0.6913109421730042, acc: 0.52, recall: 0.52, precision: 0.5679347826086957, f_beta: 0.4171928120446819
train: step: 702, loss: 0.6910703182220459, acc: 0.5, recall: 0.5, precision: 0.5, f_beta: 0.415614773258532
train: step: 703, loss: 0.6925598978996277, acc: 0.56, recall: 0.56, precision: 0.5636672325976231, f_beta: 0.5535714285714286
train: step: 704, loss: 0.6925990581512451, acc: 0.53, recall: 0.53, precision: 0.533921302578019, f_beta: 0.5160127690248172
train: step: 705, loss: 0.6934583187103271, acc: 0.46, recall: 0.45999999999999996, precision: 0.45974235104669886, f_beta: 0.4591346153846153
train: step: 706, loss: 0.6923114657402039, acc: 0.51, recall: 0.51, precision: 0.5305250305250305, f_beta: 0.4109868974636375
train: step: 707, loss: 0.6939680576324463, acc: 0.5, recall: 0.5, precision: 0.5, f_beta: 0.4505494505494506
train: step: 708, loss: 0.6927888989448547, acc: 0.48, recall: 0.48, precision: 0.47916666666666663, f_beta: 0.4747474747474748
train: step: 709, loss: 0.6927933692932129, acc: 0.5, recall: 0.5, precision: 0.5, f_beta: 0.4505494505494506
train: step: 710, loss: 0.6941168308258057, acc: 0.48, recall: 0.48, precision: 0.47996794871794873, f_beta: 0.4797919167667067
train: step: 711, loss: 0.6935231685638428, acc: 0.51, recall: 0.51, precision: 0.5141163184641445, f_beta: 0.47147017581706396
train: step: 712, loss: 0.6941512823104858, acc: 0.47, recall: 0.47, precision: 0.4699879951980792, f_beta: 0.46994699469946993
train: step: 713, loss: 0.6938562989234924, acc: 0.46, recall: 0.45999999999999996, precision: 0.4451754385964912, f_beta: 0.42084942084942084
train: step: 714, loss: 0.6916825771331787, acc: 0.51, recall: 0.51, precision: 0.5859106529209621, f_beta: 0.3710691823899371
train: step: 715, loss: 0.6904721260070801, acc: 0.51, recall: 0.51, precision: 0.5177179305457122, f_beta: 0.45011783189316573
train: step: 716, loss: 0.6929369568824768, acc: 0.51, recall: 0.51, precision: 0.510334849111203, f_beta: 0.5059985885673959
train: step: 717, loss: 0.6912594437599182, acc: 0.5, recall: 0.5, precision: 0.5, f_beta: 0.3503118503118503
train: step: 718, loss: 0.6922775506973267, acc: 0.55, recall: 0.55, precision: 0.5500200080032013, f_beta: 0.54995499549955
train: step: 719, loss: 0.6935961842536926, acc: 0.47, recall: 0.47000000000000003, precision: 0.4689954526663911, f_beta: 0.46567194273616297
train: step: 720, loss: 0.6939918994903564, acc: 0.5, recall: 0.5, precision: 0.5, f_beta: 0.4981934965877158
train: step: 721, loss: 0.6928247809410095, acc: 0.52, recall: 0.52, precision: 0.5217013888888888, f_beta: 0.5104039167686658
train: step: 722, loss: 0.6929578185081482, acc: 0.57, recall: 0.5700000000000001, precision: 0.6137102014294997, f_beta: 0.5242836596968691
train: step: 723, loss: 0.6924022436141968, acc: 0.52, recall: 0.52, precision: 0.5679347826086957, f_beta: 0.4171928120446819
train: step: 724, loss: 0.6938618421554565, acc: 0.49, recall: 0.49, precision: 0.48996386993175434, f_beta: 0.4895405865278751
train: step: 725, loss: 0.6927933692932129, acc: 0.57, recall: 0.5700000000000001, precision: 0.5713994288045696, f_beta: 0.567882624861823
train: step: 726, loss: 0.6926093101501465, acc: 0.48, recall: 0.48, precision: 0.46875, f_beta: 0.4285714285714286
train: step: 727, loss: 0.693717896938324, acc: 0.49, recall: 0.49, precision: 0.4898000815993472, f_beta: 0.48748869460355737
train: step: 728, loss: 0.6917588114738464, acc: 0.48, recall: 0.48, precision: 0.47702205882352944, f_beta: 0.46258784621744525
train: step: 729, loss: 0.6896378993988037, acc: 0.47, recall: 0.47, precision: 0.46703296703296704, f_beta: 0.4578005115089514
train: step: 730, loss: 0.6933273077011108, acc: 0.49, recall: 0.49, precision: 0.48831229546517063, f_beta: 0.47089947089947093
train: step: 731, loss: 0.6922557353973389, acc: 0.48, recall: 0.48, precision: 0.4782986111111111, f_beta: 0.46960424316605465
train: step: 732, loss: 0.6932403445243835, acc: 0.47, recall: 0.47, precision: 0.46703296703296704, f_beta: 0.4578005115089514
train: step: 733, loss: 0.6935975551605225, acc: 0.52, recall: 0.52, precision: 0.7551020408163265, f_beta: 0.3762993762993764
train: step: 734, loss: 0.6940708160400391, acc: 0.44, recall: 0.44, precision: 0.43912337662337664, f_beta: 0.43797671617824174
train: step: 735, loss: 0.6911139488220215, acc: 0.54, recall: 0.54, precision: 0.5496031746031746, f_beta: 0.5166036149642708
train: step: 736, loss: 0.6938648819923401, acc: 0.45, recall: 0.45, precision: 0.4499799919967987, f_beta: 0.44994499449944997
train: step: 737, loss: 0.690352201461792, acc: 0.54, recall: 0.54, precision: 0.7604166666666667, f_beta: 0.4165398274987316
train: step: 738, loss: 0.6934205889701843, acc: 0.48, recall: 0.48000000000000004, precision: 0.47947454844006565, f_beta: 0.47665056360708535
train: step: 739, loss: 0.6932269930839539, acc: 0.51, recall: 0.51, precision: 0.510989010989011, f_beta: 0.4987212276214833
train: step: 740, loss: 0.6923317909240723, acc: 0.48, recall: 0.48, precision: 0.47987117552334946, f_beta: 0.47916666666666663
train: step: 741, loss: 0.6944478750228882, acc: 0.37, recall: 0.37, precision: 0.36994797919167666, f_beta: 0.36993699369936994
train: step: 742, loss: 0.6895931363105774, acc: 0.51, recall: 0.51, precision: 0.5150693188667872, f_beta: 0.4650070968446337
train: step: 743, loss: 0.6918904185295105, acc: 0.53, recall: 0.53, precision: 0.5588235294117647, f_beta: 0.46438746438746437
train: step: 744, loss: 0.6977097988128662, acc: 0.46, recall: 0.46, precision: 0.4375, f_beta: 0.4065934065934066
train: step: 745, loss: 0.6931520700454712, acc: 0.47, recall: 0.47000000000000003, precision: 0.4689954526663911, f_beta: 0.46567194273616297
train: step: 746, loss: 0.6928625702857971, acc: 0.55, recall: 0.55, precision: 0.554945054945055, f_beta: 0.5396419437340153
train: step: 747, loss: 0.6946051120758057, acc: 0.38, recall: 0.38, precision: 0.37922705314009664, f_beta: 0.37900641025641024
train: step: 748, loss: 0.6935446262359619, acc: 0.47, recall: 0.47, precision: 0.466078697421981, f_beta: 0.4542271650705386
train: step: 749, loss: 0.6901381611824036, acc: 0.54, recall: 0.54, precision: 0.6773049645390071, f_beta: 0.4295634920634921
train: step: 750, loss: 0.6941617727279663, acc: 0.46, recall: 0.46, precision: 0.45833333333333337, f_beta: 0.4545454545454545
train: step: 751, loss: 0.6921865940093994, acc: 0.47, recall: 0.47, precision: 0.45999999999999996, f_beta: 0.43466666666666665
train: step: 752, loss: 0.6925758123397827, acc: 0.57, recall: 0.5700000000000001, precision: 0.5849927149101506, f_beta: 0.5501621508525997
train: step: 753, loss: 0.6919861435890198, acc: 0.64, recall: 0.64, precision: 0.6458333333333334, f_beta: 0.6363636363636365
train: step: 754, loss: 0.6937179565429688, acc: 0.42, recall: 0.42000000000000004, precision: 0.4151103565365025, f_beta: 0.411525974025974
train: step: 755, loss: 0.6929126977920532, acc: 0.53, recall: 0.53, precision: 0.5303030303030303, f_beta: 0.5288220551378446
train: step: 756, loss: 0.6930479407310486, acc: 0.47, recall: 0.47, precision: 0.46782496782496785, f_beta: 0.4608890245142916
train: step: 757, loss: 0.6932122111320496, acc: 0.48, recall: 0.48, precision: 0.48, f_beta: 0.48
train: step: 758, loss: 0.6946964263916016, acc: 0.43, recall: 0.43000000000000005, precision: 0.42492492492492495, f_beta: 0.42020140372291737
train: step: 759, loss: 0.6935113668441772, acc: 0.49, recall: 0.49, precision: 0.4898989898989899, f_beta: 0.48872180451127817
train: step: 760, loss: 0.6928526163101196, acc: 0.51, recall: 0.51, precision: 0.5100361300682457, f_beta: 0.5095586027424682
train: step: 761, loss: 0.6943759918212891, acc: 0.45, recall: 0.45, precision: 0.4415614773258532, f_beta: 0.4294013901857039
train: step: 762, loss: 0.6923035383224487, acc: 0.57, recall: 0.5700000000000001, precision: 0.5713994288045696, f_beta: 0.567882624861823
train: step: 763, loss: 0.6932384967803955, acc: 0.48, recall: 0.48, precision: 0.47987117552334946, f_beta: 0.47916666666666663
train: step: 764, loss: 0.6862476468086243, acc: 0.6, recall: 0.6, precision: 0.6299376299376299, f_beta: 0.5755517826825127
train: step: 765, loss: 0.6904399991035461, acc: 0.51, recall: 0.51, precision: 0.5121418164157359, f_beta: 0.4873940788785438
train: step: 766, loss: 0.6932662725448608, acc: 0.46, recall: 0.45999999999999996, precision: 0.45974235104669886, f_beta: 0.4591346153846153
train: step: 767, loss: 0.6939662098884583, acc: 0.44, recall: 0.44, precision: 0.43990384615384615, f_beta: 0.4397759103641457
train: step: 768, loss: 0.693132221698761, acc: 0.49, recall: 0.49, precision: 0.48949138293400585, f_beta: 0.48375341633768604
train: step: 769, loss: 0.6929790377616882, acc: 0.47, recall: 0.47, precision: 0.4696969696969697, f_beta: 0.468671679197995
train: step: 770, loss: 0.6909738183021545, acc: 0.56, recall: 0.56, precision: 0.59375, f_beta: 0.5164835164835164
train: step: 771, loss: 0.6936794519424438, acc: 0.42, recall: 0.42000000000000004, precision: 0.4178981937602627, f_beta: 0.4162640901771337
train: step: 772, loss: 0.6978744268417358, acc: 0.46, recall: 0.45999999999999996, precision: 0.4169435215946844, f_beta: 0.3795955882352941
train: step: 773, loss: 0.6906748414039612, acc: 0.54, recall: 0.54, precision: 0.7604166666666667, f_beta: 0.4165398274987316
train: step: 774, loss: 0.6929751634597778, acc: 0.49, recall: 0.49, precision: 0.4898989898989899, f_beta: 0.48872180451127817
train: step: 775, loss: 0.6917614936828613, acc: 0.52, recall: 0.52, precision: 0.5274122807017544, f_beta: 0.4851994851994852
train: step: 776, loss: 0.693410336971283, acc: 0.51, recall: 0.51, precision: 0.5101999184006528, f_beta: 0.5075871771681238
train: step: 777, loss: 0.6907708644866943, acc: 0.54, recall: 0.54, precision: 0.5459558823529411, f_beta: 0.524596940884663
train: step: 778, loss: 0.6929511427879333, acc: 0.47, recall: 0.47, precision: 0.4699879951980792, f_beta: 0.46994699469946993
train: step: 779, loss: 0.6926006078720093, acc: 0.52, recall: 0.52, precision: 0.5679347826086957, f_beta: 0.4171928120446819
train: step: 780, loss: 0.6938152313232422, acc: 0.51, recall: 0.51, precision: 0.510508617065994, f_beta: 0.5039983804028748
train: step: 781, loss: 0.6917346119880676, acc: 0.49, recall: 0.49, precision: 0.48785818358426425, f_beta: 0.46647138822052514
train: step: 782, loss: 0.6968929171562195, acc: 0.47, recall: 0.47, precision: 0.4336870026525199, f_beta: 0.3859344224307728
train: step: 783, loss: 0.6933190822601318, acc: 0.51, recall: 0.51, precision: 0.5150693188667872, f_beta: 0.4650070968446337
train: step: 784, loss: 0.6918386816978455, acc: 0.53, recall: 0.53, precision: 0.5303030303030303, f_beta: 0.5288220551378446
train: step: 785, loss: 0.6919345855712891, acc: 0.51, recall: 0.51, precision: 0.5859106529209621, f_beta: 0.3710691823899371
train: step: 786, loss: 0.6923213005065918, acc: 0.55, recall: 0.55, precision: 0.5501806503412284, f_beta: 0.5495946351716544
train: step: 787, loss: 0.6897130012512207, acc: 0.55, recall: 0.55, precision: 0.5666666666666667, f_beta: 0.52
train: step: 788, loss: 0.6921809315681458, acc: 0.58, recall: 0.5800000000000001, precision: 0.5801282051282051, f_beta: 0.5798319327731092
train: step: 789, loss: 0.6936338543891907, acc: 0.43, recall: 0.43, precision: 0.42860057119543044, f_beta: 0.42719324690985827
train: step: 790, loss: 0.6932356357574463, acc: 0.48, recall: 0.48, precision: 0.48, f_beta: 0.48
train: step: 791, loss: 0.6930091381072998, acc: 0.51, recall: 0.51, precision: 0.5107250107250108, f_beta: 0.5015766453056658
train: step: 792, loss: 0.6932138800621033, acc: 0.55, recall: 0.55, precision: 0.5565355042966984, f_beta: 0.5366079703429101
train: step: 793, loss: 0.6907795071601868, acc: 0.47, recall: 0.47000000000000003, precision: 0.4649368863955119, f_beta: 0.4501504305425874
train: step: 794, loss: 0.692211389541626, acc: 0.53, recall: 0.53, precision: 0.576608784473953, f_beta: 0.44568935015921696
train: step: 795, loss: 0.6913242340087891, acc: 0.49, recall: 0.49, precision: 0.4837556855100714, f_beta: 0.4357782940590773
train: step: 796, loss: 0.6933708786964417, acc: 0.48, recall: 0.48, precision: 0.4787775891341256, f_beta: 0.4724025974025974
train: step: 797, loss: 0.693535327911377, acc: 0.46, recall: 0.45999999999999996, precision: 0.45974235104669886, f_beta: 0.4591346153846153
train: step: 798, loss: 0.692527711391449, acc: 0.51, recall: 0.51, precision: 0.5177179305457122, f_beta: 0.45011783189316573
train: step: 799, loss: 0.6950097680091858, acc: 0.5, recall: 0.5, precision: 0.5, f_beta: 0.415614773258532
train: step: 800, loss: 0.6906376481056213, acc: 0.6, recall: 0.6, precision: 0.707641196013289, f_beta: 0.5404411764705881
train: step: 801, loss: 0.6933637261390686, acc: 0.54, recall: 0.54, precision: 0.54, f_beta: 0.54
train: step: 802, loss: 0.6941511631011963, acc: 0.53, recall: 0.53, precision: 0.5380517503805176, f_beta: 0.5037482842360891
train: step: 803, loss: 0.6924904584884644, acc: 0.46, recall: 0.46, precision: 0.45039682539682535, f_beta: 0.43253467843631777
train: step: 804, loss: 0.694179356098175, acc: 0.44, recall: 0.44000000000000006, precision: 0.42203742203742206, f_beta: 0.40577249575551777
train: step: 805, loss: 0.6930317878723145, acc: 0.5, recall: 0.5, precision: 0.5, f_beta: 0.49979991996798717
train: step: 806, loss: 0.6932879686355591, acc: 0.5, recall: 0.5, precision: 0.5, f_beta: 0.4967793880837359
train: step: 807, loss: 0.69384765625, acc: 0.45, recall: 0.45, precision: 0.44832575444398515, f_beta: 0.44550861982054646
train: step: 808, loss: 0.6919680833816528, acc: 0.54, recall: 0.54, precision: 0.5434027777777778, f_beta: 0.5308037535699714
train: step: 809, loss: 0.6865501403808594, acc: 0.56, recall: 0.56, precision: 0.5636672325976231, f_beta: 0.5535714285714286
train: step: 810, loss: 0.6926724314689636, acc: 0.53, recall: 0.53, precision: 0.530108390204737, f_beta: 0.5295766189570613
train: step: 811, loss: 0.6938566565513611, acc: 0.48, recall: 0.48, precision: 0.47996794871794873, f_beta: 0.4797919167667067
train: step: 812, loss: 0.6933937072753906, acc: 0.45, recall: 0.45, precision: 0.439290917921321, f_beta: 0.42462600690448793
train: step: 813, loss: 0.6935504078865051, acc: 0.49, recall: 0.49, precision: 0.48966515088879703, f_beta: 0.48583526565177937
train: step: 814, loss: 0.6926507353782654, acc: 0.46, recall: 0.45999999999999996, precision: 0.448024948024948, f_beta: 0.42699490662139217
train: step: 815, loss: 0.6936699151992798, acc: 0.48, recall: 0.48, precision: 0.4751984126984127, f_beta: 0.45355191256830596
train: step: 816, loss: 0.6927591562271118, acc: 0.52, recall: 0.52, precision: 0.5338753387533876, f_beta: 0.46524064171123003
train: step: 817, loss: 0.6946203708648682, acc: 0.49, recall: 0.49, precision: 0.4858836815358555, f_beta: 0.44989752993204624
train: step: 818, loss: 0.6922599673271179, acc: 0.52, recall: 0.52, precision: 0.5238095238095237, f_beta: 0.5
train: step: 819, loss: 0.6913999915122986, acc: 0.62, recall: 0.62, precision: 0.6644736842105263, f_beta: 0.5924495924495925
train: step: 820, loss: 0.6911406517028809, acc: 0.54, recall: 0.54, precision: 0.54, f_beta: 0.54
train: step: 821, loss: 0.6927860975265503, acc: 0.49, recall: 0.49, precision: 0.48869289914066033, f_beta: 0.4748223663886314
train: step: 822, loss: 0.6908787488937378, acc: 0.47, recall: 0.47, precision: 0.45999999999999996, f_beta: 0.43466666666666665
train: step: 823, loss: 0.6934251189231873, acc: 0.42, recall: 0.42, precision: 0.410873440285205, f_beta: 0.40476190476190477
train: step: 824, loss: 0.6933615207672119, acc: 0.52, recall: 0.52, precision: 0.5248015873015872, f_beta: 0.49558638083228246
train: step: 825, loss: 0.6933372020721436, acc: 0.5, recall: 0.5, precision: 0.5, f_beta: 0.4981934965877158
train: step: 826, loss: 0.692838191986084, acc: 0.51, recall: 0.51, precision: 0.5107250107250108, f_beta: 0.5015766453056658
train: step: 827, loss: 0.6940205097198486, acc: 0.45, recall: 0.45, precision: 0.4494949494949495, f_beta: 0.4486215538847118
train: step: 828, loss: 0.6951308250427246, acc: 0.46, recall: 0.45999999999999996, precision: 0.4451754385964912, f_beta: 0.42084942084942084
train: step: 829, loss: 0.6927627325057983, acc: 0.58, recall: 0.5800000000000001, precision: 0.5811688311688312, f_beta: 0.5784825371336814
train: step: 830, loss: 0.6929707527160645, acc: 0.49, recall: 0.49, precision: 0.48927498927498925, f_beta: 0.48123283490997865
train: step: 831, loss: 0.6947906017303467, acc: 0.44, recall: 0.44, precision: 0.4331550802139037, f_beta: 0.4252873563218391
train: step: 832, loss: 0.6922709941864014, acc: 0.52, recall: 0.52, precision: 0.5679347826086957, f_beta: 0.4171928120446819
train: step: 833, loss: 0.6936732530593872, acc: 0.48, recall: 0.48, precision: 0.47916666666666663, f_beta: 0.4747474747474748
train: step: 834, loss: 0.6927024722099304, acc: 0.55, recall: 0.55, precision: 0.550999592003264, f_beta: 0.5477841422972565
train: step: 835, loss: 0.6938562989234924, acc: 0.45, recall: 0.45, precision: 0.449000407996736, f_beta: 0.4472917294744247
train: step: 836, loss: 0.692660927772522, acc: 0.55, recall: 0.55, precision: 0.554945054945055, f_beta: 0.5396419437340153
train: step: 837, loss: 0.6901435256004333, acc: 0.56, recall: 0.56, precision: 0.5874125874125874, f_beta: 0.5225694444444444
train: step: 838, loss: 0.6943027973175049, acc: 0.37, recall: 0.37, precision: 0.3656469615543613, f_beta: 0.3648553281580804
train: step: 839, loss: 0.6926542520523071, acc: 0.58, recall: 0.5800000000000001, precision: 0.5801282051282051, f_beta: 0.5798319327731092
train: step: 840, loss: 0.6910662651062012, acc: 0.6, recall: 0.6000000000000001, precision: 0.6190476190476191, f_beta: 0.5833333333333333
train: step: 841, loss: 0.6935827136039734, acc: 0.44, recall: 0.44000000000000006, precision: 0.4375, f_beta: 0.43434343434343436
train: step: 842, loss: 0.692598283290863, acc: 0.52, recall: 0.52, precision: 0.5205254515599343, f_beta: 0.5169082125603865
train: step: 843, loss: 0.6934353709220886, acc: 0.49, recall: 0.49, precision: 0.4898989898989899, f_beta: 0.48872180451127817
train: step: 844, loss: 0.6920732259750366, acc: 0.5, recall: 0.5, precision: 0.5, f_beta: 0.43464495703301675
train: step: 845, loss: 0.6934217810630798, acc: 0.5, recall: 0.5, precision: 0.5, f_beta: 0.4981934965877158
train: step: 846, loss: 0.693348228931427, acc: 0.52, recall: 0.52, precision: 0.5205254515599343, f_beta: 0.5169082125603865
train: step: 847, loss: 0.6918664574623108, acc: 0.51, recall: 0.51, precision: 0.7525252525252526, f_beta: 0.3551783129359126
train: step: 848, loss: 0.6908870935440063, acc: 0.52, recall: 0.52, precision: 0.7551020408163265, f_beta: 0.3762993762993764
train: step: 849, loss: 0.6927588582038879, acc: 0.54, recall: 0.54, precision: 0.5424448217317488, f_beta: 0.5332792207792207
train: step: 850, loss: 0.6943227648735046, acc: 0.45, recall: 0.44999999999999996, precision: 0.44637494637494635, f_beta: 0.44054521411860437
train: step: 851, loss: 0.6864866614341736, acc: 0.48, recall: 0.48000000000000004, precision: 0.47619047619047616, f_beta: 0.45833333333333337
train: step: 852, loss: 0.6942160129547119, acc: 0.5, recall: 0.5, precision: 0.5, f_beta: 0.486863711001642
train: step: 853, loss: 0.6903587579727173, acc: 0.53, recall: 0.53, precision: 0.7577319587628866, f_beta: 0.39673982800667434
train: step: 854, loss: 0.6926062703132629, acc: 0.55, recall: 0.55, precision: 0.5505050505050505, f_beta: 0.5488721804511277
train: step: 855, loss: 0.6938320398330688, acc: 0.42, recall: 0.42000000000000004, precision: 0.4166666666666667, f_beta: 0.41414141414141414
train: step: 856, loss: 0.6941790580749512, acc: 0.51, recall: 0.51, precision: 0.5221043324491601, f_beta: 0.43227899432278993
train: step: 857, loss: 0.6924587488174438, acc: 0.58, recall: 0.58, precision: 0.58, f_beta: 0.58
train: step: 858, loss: 0.6923149824142456, acc: 0.49, recall: 0.49, precision: 0.4849306811332128, f_beta: 0.44317065181788406
train: step: 859, loss: 0.6938437819480896, acc: 0.47, recall: 0.47000000000000003, precision: 0.4694002447980416, f_beta: 0.467390212038991
train: step: 860, loss: 0.6902883052825928, acc: 0.47, recall: 0.47, precision: 0.46989160979526295, f_beta: 0.46952257031328193
train: step: 861, loss: 0.6920835375785828, acc: 0.61, recall: 0.61, precision: 0.6136833402232327, f_beta: 0.6068152031454783
train: step: 862, loss: 0.6929348707199097, acc: 0.55, recall: 0.55, precision: 0.5584385226741468, f_beta: 0.5331465919701215
train: step: 863, loss: 0.6931997537612915, acc: 0.48, recall: 0.48, precision: 0.47996794871794873, f_beta: 0.4797919167667067
train: step: 864, loss: 0.6934289336204529, acc: 0.51, recall: 0.51, precision: 0.5100361300682457, f_beta: 0.5095586027424682
train: step: 865, loss: 0.6929376125335693, acc: 0.5, recall: 0.5, precision: 0.5, f_beta: 0.48325754443985114
train: step: 866, loss: 0.6927429437637329, acc: 0.53, recall: 0.53, precision: 0.530108390204737, f_beta: 0.5295766189570613
train: step: 867, loss: 0.6934449076652527, acc: 0.48, recall: 0.48, precision: 0.47970779220779225, f_beta: 0.4781212364512244
train: step: 868, loss: 0.6909008622169495, acc: 0.51, recall: 0.51, precision: 0.5113071008593397, f_beta: 0.49541756770672435
train: step: 869, loss: 0.6941588521003723, acc: 0.49, recall: 0.49, precision: 0.4140893470790378, f_beta: 0.3453985367731998
train: step: 870, loss: 0.6927990913391113, acc: 0.56, recall: 0.56, precision: 0.5615763546798029, f_beta: 0.5571658615136876
train: step: 871, loss: 0.695058286190033, acc: 0.49, recall: 0.49, precision: 0.4140893470790378, f_beta: 0.3453985367731998
train: step: 872, loss: 0.6932617425918579, acc: 0.5, recall: 0.5, precision: 0.5, f_beta: 0.5
train: step: 873, loss: 0.6929290890693665, acc: 0.46, recall: 0.46, precision: 0.46, f_beta: 0.46
train: step: 874, loss: 0.6923611164093018, acc: 0.54, recall: 0.54, precision: 0.5416666666666667, f_beta: 0.5353535353535354
train: step: 875, loss: 0.6944315433502197, acc: 0.49, recall: 0.49, precision: 0.48996386993175434, f_beta: 0.4895405865278751
train: step: 876, loss: 0.6906258463859558, acc: 0.51, recall: 0.51, precision: 0.5196078431372548, f_beta: 0.4415954415954416
train: step: 877, loss: 0.6930946111679077, acc: 0.49, recall: 0.49, precision: 0.4899959983993597, f_beta: 0.48994899489948995
train: step: 878, loss: 0.6920814514160156, acc: 0.48, recall: 0.48, precision: 0.4725877192982456, f_beta: 0.4422994422994423
train: step: 879, loss: 0.6968142986297607, acc: 0.44, recall: 0.44, precision: 0.4331550802139037, f_beta: 0.4252873563218391
train: step: 880, loss: 0.6673131585121155, acc: 0.63, recall: 0.63, precision: 0.7549019607843137, f_beta: 0.5783475783475783
train: step: 881, loss: 0.6925596594810486, acc: 0.6, recall: 0.6000000000000001, precision: 0.601461038961039, f_beta: 0.5985547972701726
train: step: 882, loss: 0.6909578442573547, acc: 0.58, recall: 0.58, precision: 0.603950103950104, f_beta: 0.5543293718166383
train: step: 883, loss: 0.6936482787132263, acc: 0.46, recall: 0.46, precision: 0.45894909688013136, f_beta: 0.4565217391304348
train: step: 884, loss: 0.6934092044830322, acc: 0.45, recall: 0.44999999999999996, precision: 0.44505494505494503, f_beta: 0.4373401534526854
train: step: 885, loss: 0.6932180523872375, acc: 0.48, recall: 0.48000000000000004, precision: 0.46612466124661245, f_beta: 0.4206773618538324
train: step: 886, loss: 0.6936464905738831, acc: 0.44, recall: 0.44000000000000006, precision: 0.42203742203742206, f_beta: 0.40577249575551777
train: step: 887, loss: 0.6933285593986511, acc: 0.44, recall: 0.44, precision: 0.43912337662337664, f_beta: 0.43797671617824174
train: step: 888, loss: 0.6939170360565186, acc: 0.45, recall: 0.45, precision: 0.4494949494949495, f_beta: 0.4486215538847118
train: step: 889, loss: 0.6913885474205017, acc: 0.57, recall: 0.5700000000000001, precision: 0.5791497060153776, f_beta: 0.557203171661003
train: step: 890, loss: 0.6921217441558838, acc: 0.51, recall: 0.51, precision: 0.5196078431372548, f_beta: 0.4415954415954416
train: step: 891, loss: 0.690977156162262, acc: 0.52, recall: 0.52, precision: 0.5415282392026578, f_beta: 0.4485294117647059
train: step: 892, loss: 0.6925485730171204, acc: 0.54, recall: 0.54, precision: 0.5424448217317488, f_beta: 0.5332792207792207
train: step: 893, loss: 0.6933835744857788, acc: 0.5, recall: 0.5, precision: 0.5, f_beta: 0.49979991996798717
train: step: 894, loss: 0.6929444074630737, acc: 0.52, recall: 0.52, precision: 0.5200320512820513, f_beta: 0.5198079231692677
train: step: 895, loss: 0.6953004598617554, acc: 0.51, recall: 0.51, precision: 0.5162443144899285, f_beta: 0.4579046354685253
train: step: 896, loss: 0.6918770670890808, acc: 0.55, recall: 0.55, precision: 0.5516742455560149, f_beta: 0.5463252343986289
train: step: 897, loss: 0.693461537361145, acc: 0.5, recall: 0.5, precision: 0.5, f_beta: 0.5
train: step: 898, loss: 0.6930890083312988, acc: 0.54, recall: 0.54, precision: 0.5459558823529411, f_beta: 0.524596940884663
train: step: 899, loss: 0.6942622661590576, acc: 0.49, recall: 0.49, precision: 0.48996386993175434, f_beta: 0.4895405865278751
train: step: 900, loss: 0.6928346753120422, acc: 0.53, recall: 0.53, precision: 0.5300120048019208, f_beta: 0.5299529952995299
train: step: 901, loss: 0.6918933391571045, acc: 0.6, recall: 0.6, precision: 0.6061120543293719, f_beta: 0.5941558441558441
train: step: 902, loss: 0.6911223530769348, acc: 0.44, recall: 0.44, precision: 0.4331550802139037, f_beta: 0.4252873563218391
train: step: 903, loss: 0.6933218240737915, acc: 0.42, recall: 0.42000000000000004, precision: 0.4194847020933977, f_beta: 0.41907051282051283
train: step: 904, loss: 0.6920351386070251, acc: 0.64, recall: 0.6399999999999999, precision: 0.6420454545454546, f_beta: 0.6386993175431553
train: step: 905, loss: 0.6963334679603577, acc: 0.41, recall: 0.41000000000000003, precision: 0.3010610079575597, f_beta: 0.31641756459274706
train: step: 906, loss: 0.6933809518814087, acc: 0.49, recall: 0.49, precision: 0.48996386993175434, f_beta: 0.4895405865278751
train: step: 907, loss: 0.6922098398208618, acc: 0.57, recall: 0.5700000000000001, precision: 0.5713994288045696, f_beta: 0.567882624861823
train: step: 908, loss: 0.6913741230964661, acc: 0.57, recall: 0.5700000000000001, precision: 0.6240255138199858, f_beta: 0.5174503422735944
train: step: 909, loss: 0.687408447265625, acc: 0.53, recall: 0.53, precision: 0.6152073732718895, f_beta: 0.42338363390994965
train: step: 910, loss: 0.6912756562232971, acc: 0.52, recall: 0.52, precision: 0.5372023809523809, f_beta: 0.4572591587516961
train: step: 911, loss: 0.6941792964935303, acc: 0.38, recall: 0.38, precision: 0.3782467532467533, f_beta: 0.37775993576876754
train: step: 912, loss: 0.6927182078361511, acc: 0.51, recall: 0.51, precision: 0.5101999184006528, f_beta: 0.5075871771681238
train: step: 913, loss: 0.6929936408996582, acc: 0.51, recall: 0.51, precision: 0.5100040016006402, f_beta: 0.50995099509951
train: step: 914, loss: 0.6913577318191528, acc: 0.57, recall: 0.57, precision: 0.5769230769230769, f_beta: 0.5601023017902813
train: step: 915, loss: 0.6932399868965149, acc: 0.47, recall: 0.47, precision: 0.46782496782496785, f_beta: 0.4608890245142916
train: step: 916, loss: 0.6929314136505127, acc: 0.56, recall: 0.56, precision: 0.5625, f_beta: 0.5555555555555556
train: step: 917, loss: 0.6933414340019226, acc: 0.51, recall: 0.51, precision: 0.510508617065994, f_beta: 0.5039983804028748
train: step: 918, loss: 0.6937457323074341, acc: 0.57, recall: 0.57, precision: 0.5933333333333333, f_beta: 0.5413333333333333
train: step: 919, loss: 0.6894904971122742, acc: 0.61, recall: 0.61, precision: 0.6657625075346594, f_beta: 0.5741893219783819
train: step: 920, loss: 0.6923708319664001, acc: 0.52, recall: 0.52, precision: 0.5229779411764706, f_beta: 0.5039272426622571
train: step: 921, loss: 0.6919043660163879, acc: 0.52, recall: 0.52, precision: 0.5338753387533876, f_beta: 0.46524064171123003
train: step: 922, loss: 0.6932051777839661, acc: 0.47, recall: 0.47, precision: 0.46782496782496785, f_beta: 0.4608890245142916
train: step: 923, loss: 0.6931737661361694, acc: 0.48, recall: 0.48, precision: 0.4782986111111111, f_beta: 0.46960424316605465
train: step: 924, loss: 0.6925270557403564, acc: 0.57, recall: 0.5700000000000001, precision: 0.5723439437784208, f_beta: 0.5664885573142453
train: step: 925, loss: 0.6944283843040466, acc: 0.44, recall: 0.44, precision: 0.3754152823920266, f_beta: 0.35661764705882354
train: step: 926, loss: 0.6929876208305359, acc: 0.5, recall: 0.5, precision: 0.5, f_beta: 0.47916666666666663
train: step: 927, loss: 0.6912527680397034, acc: 0.58, recall: 0.5800000000000001, precision: 0.5952380952380952, f_beta: 0.5625
train: step: 928, loss: 0.692824125289917, acc: 0.5, recall: 0.5, precision: 0.5, f_beta: 0.43464495703301675
train: step: 929, loss: 0.6923798322677612, acc: 0.51, recall: 0.51, precision: 0.5305250305250305, f_beta: 0.4109868974636375
train: step: 930, loss: 0.6850849986076355, acc: 0.63, recall: 0.63, precision: 0.6469923111714156, f_beta: 0.6189887756152817
train: step: 931, loss: 0.6925731897354126, acc: 0.51, recall: 0.51, precision: 0.5113071008593397, f_beta: 0.49541756770672435
train: step: 932, loss: 0.6930980086326599, acc: 0.49, recall: 0.49, precision: 0.48996386993175434, f_beta: 0.4895405865278751
train: step: 933, loss: 0.6932952404022217, acc: 0.52, recall: 0.52, precision: 0.5201288244766505, f_beta: 0.5192307692307692
train: step: 934, loss: 0.6928662061691284, acc: 0.54, recall: 0.54, precision: 0.54, f_beta: 0.54
train: step: 935, loss: 0.6942875385284424, acc: 0.44, recall: 0.44000000000000006, precision: 0.4375, f_beta: 0.43434343434343436
train: step: 936, loss: 0.693846583366394, acc: 0.44, recall: 0.44, precision: 0.43842364532019706, f_beta: 0.4363929146537842
train: step: 937, loss: 0.693005383014679, acc: 0.51, recall: 0.51, precision: 0.5116877045348294, f_beta: 0.4916485112563544
train: step: 938, loss: 0.693449854850769, acc: 0.49, recall: 0.49, precision: 0.4898000815993472, f_beta: 0.48748869460355737
train: step: 939, loss: 0.693840503692627, acc: 0.46, recall: 0.45999999999999996, precision: 0.45974235104669886, f_beta: 0.4591346153846153
train: step: 940, loss: 0.6936245560646057, acc: 0.49, recall: 0.49, precision: 0.4837556855100714, f_beta: 0.4357782940590773
train: step: 941, loss: 0.6928012371063232, acc: 0.52, recall: 0.52, precision: 0.5238095238095237, f_beta: 0.5
train: step: 942, loss: 0.6931270360946655, acc: 0.52, recall: 0.52, precision: 0.5202922077922079, f_beta: 0.5182657567242072
train: step: 943, loss: 0.6933990716934204, acc: 0.49, recall: 0.49, precision: 0.4866666666666667, f_beta: 0.45599999999999996
train: step: 944, loss: 0.6951504349708557, acc: 0.41, recall: 0.41, precision: 0.3538011695906432, f_beta: 0.3472729284212855
train: step: 945, loss: 0.694871723651886, acc: 0.4, recall: 0.4, precision: 0.38859180035650626, f_beta: 0.3842364532019704
train: step: 946, loss: 0.6931889057159424, acc: 0.43, recall: 0.43000000000000005, precision: 0.42643968053804115, f_beta: 0.42301852414211966
train: step: 947, loss: 0.6919732689857483, acc: 0.54, recall: 0.54, precision: 0.5744047619047619, f_beta: 0.47987336047037543
train: step: 948, loss: 0.6889553070068359, acc: 0.54, recall: 0.54, precision: 0.7604166666666667, f_beta: 0.4165398274987316
train: step: 949, loss: 0.6930000185966492, acc: 0.52, recall: 0.52, precision: 0.5222816399286987, f_beta: 0.5073891625615763
train: step: 950, loss: 0.6928996443748474, acc: 0.53, recall: 0.53, precision: 0.5303030303030303, f_beta: 0.5288220551378446
train: step: 951, loss: 0.692939281463623, acc: 0.5, recall: 0.5, precision: 0.5, f_beta: 0.4981934965877158
train: step: 952, loss: 0.6919487118721008, acc: 0.53, recall: 0.53, precision: 0.5364254492472074, f_beta: 0.5083167695365624
train: step: 953, loss: 0.6941055059432983, acc: 0.44, recall: 0.43999999999999995, precision: 0.43489583333333337, f_beta: 0.4288045695634435
train: step: 954, loss: 0.6928713917732239, acc: 0.57, recall: 0.5700000000000001, precision: 0.5707070707070707, f_beta: 0.5689223057644109
train: step: 955, loss: 0.6937878131866455, acc: 0.44, recall: 0.44, precision: 0.43842364532019706, f_beta: 0.4363929146537842
train: step: 956, loss: 0.6936174035072327, acc: 0.45, recall: 0.45, precision: 0.449000407996736, f_beta: 0.4472917294744247
train: step: 957, loss: 0.693260669708252, acc: 0.52, recall: 0.52, precision: 0.5208333333333334, f_beta: 0.5151515151515151
train: step: 958, loss: 0.6923415660858154, acc: 0.54, recall: 0.54, precision: 0.54, f_beta: 0.54
train: step: 959, loss: 0.6934842467308044, acc: 0.49, recall: 0.49, precision: 0.48996386993175434, f_beta: 0.4895405865278751
train: step: 960, loss: 0.6930335164070129, acc: 0.52, recall: 0.52, precision: 0.5274122807017544, f_beta: 0.4851994851994852
train: step: 961, loss: 0.6941278576850891, acc: 0.46, recall: 0.45999999999999996, precision: 0.45993589743589747, f_beta: 0.45978391356542614
train: step: 962, loss: 0.6915709972381592, acc: 0.58, recall: 0.5800000000000001, precision: 0.6488095238095238, f_beta: 0.5251017639077341
train: step: 963, loss: 0.6923245191574097, acc: 0.56, recall: 0.56, precision: 0.5608766233766234, f_beta: 0.5584102769971899
train: step: 964, loss: 0.6928029656410217, acc: 0.49, recall: 0.49, precision: 0.48901098901098905, f_beta: 0.47826086956521735
train: step: 965, loss: 0.6940664649009705, acc: 0.45, recall: 0.45000000000000007, precision: 0.44745691467002946, f_beta: 0.44326348820730843
train: step: 966, loss: 0.6933043599128723, acc: 0.51, recall: 0.51, precision: 0.51010101010101, f_beta: 0.5087719298245614
train: step: 967, loss: 0.6932525634765625, acc: 0.51, recall: 0.51, precision: 0.51010101010101, f_beta: 0.5087719298245614
train: step: 968, loss: 0.6919874548912048, acc: 0.59, recall: 0.5900000000000001, precision: 0.5917992656058751, f_beta: 0.5879811074263892
train: step: 969, loss: 0.6895522475242615, acc: 0.58, recall: 0.5800000000000001, precision: 0.5952380952380952, f_beta: 0.5625
train: step: 970, loss: 0.6850820779800415, acc: 0.6, recall: 0.6000000000000001, precision: 0.6693766937669376, f_beta: 0.554367201426025
train: step: 971, loss: 0.6927722692489624, acc: 0.47, recall: 0.47, precision: 0.4696969696969697, f_beta: 0.468671679197995
train: step: 972, loss: 0.6848480105400085, acc: 0.57, recall: 0.57, precision: 0.7136752136752136, f_beta: 0.483110950835437
train: step: 973, loss: 0.6924574375152588, acc: 0.56, recall: 0.56, precision: 0.5625, f_beta: 0.5555555555555556
train: step: 974, loss: 0.6934671998023987, acc: 0.44, recall: 0.44, precision: 0.44, f_beta: 0.44
train: step: 975, loss: 0.6928887963294983, acc: 0.51, recall: 0.51, precision: 0.51010101010101, f_beta: 0.5087719298245614
train: step: 976, loss: 0.6929298639297485, acc: 0.53, recall: 0.53, precision: 0.530108390204737, f_beta: 0.5295766189570613
train: step: 977, loss: 0.6929909586906433, acc: 0.52, recall: 0.52, precision: 0.5200320512820513, f_beta: 0.5198079231692677
train: step: 978, loss: 0.6718074083328247, acc: 0.6, recall: 0.6, precision: 0.6860119047619048, f_beta: 0.5477159656264134
train: step: 979, loss: 0.693062424659729, acc: 0.46, recall: 0.46, precision: 0.45894909688013136, f_beta: 0.4565217391304348
train: step: 980, loss: 0.69062340259552, acc: 0.51, recall: 0.51, precision: 0.5133333333333334, f_beta: 0.4773333333333334
train: step: 981, loss: 0.6931613683700562, acc: 0.44, recall: 0.43999999999999995, precision: 0.43489583333333337, f_beta: 0.4288045695634435
train: step: 982, loss: 0.6925111413002014, acc: 0.49, recall: 0.49, precision: 0.4898000815993472, f_beta: 0.48748869460355737
train: step: 983, loss: 0.6935544013977051, acc: 0.48, recall: 0.48, precision: 0.47970779220779225, f_beta: 0.4781212364512244
train: step: 984, loss: 0.6945338249206543, acc: 0.41, recall: 0.41000000000000003, precision: 0.40542244640605296, f_beta: 0.4027735600769308
train: step: 985, loss: 0.6909198760986328, acc: 0.51, recall: 0.51, precision: 0.5526315789473684, f_beta: 0.38557993730407525
train: step: 986, loss: 0.6886898875236511, acc: 0.53, recall: 0.53, precision: 0.5588235294117647, f_beta: 0.46438746438746437
train: step: 987, loss: 0.6928157210350037, acc: 0.57, recall: 0.5700000000000001, precision: 0.5700280112044818, f_beta: 0.56995699569957
train: step: 988, loss: 0.6916466355323792, acc: 0.66, recall: 0.6599999999999999, precision: 0.6642036124794746, f_beta: 0.6578099838969405
train: step: 989, loss: 0.6949842572212219, acc: 0.44, recall: 0.44, precision: 0.3983739837398374, f_beta: 0.3761140819964349
train: step: 990, loss: 0.6944674849510193, acc: 0.41, recall: 0.41000000000000003, precision: 0.40542244640605296, f_beta: 0.4027735600769308
train: step: 991, loss: 0.6944527626037598, acc: 0.47, recall: 0.47000000000000003, precision: 0.45126705653021437, f_beta: 0.4136519526496294
train: step: 992, loss: 0.6960266828536987, acc: 0.51, recall: 0.51, precision: 0.5384024577572964, f_beta: 0.3988467672678199
train: step: 993, loss: 0.6920111179351807, acc: 0.5, recall: 0.5, precision: 0.5, f_beta: 0.4949494949494949
train: step: 994, loss: 0.6929664015769958, acc: 0.52, recall: 0.52, precision: 0.5208333333333334, f_beta: 0.5151515151515151
train: step: 995, loss: 0.6806812286376953, acc: 0.57, recall: 0.57, precision: 0.5933333333333333, f_beta: 0.5413333333333333
train: step: 996, loss: 0.674330472946167, acc: 0.58, recall: 0.5800000000000001, precision: 0.6488095238095238, f_beta: 0.5251017639077341
train: step: 997, loss: 0.693585216999054, acc: 0.54, recall: 0.54, precision: 0.5416666666666667, f_beta: 0.5353535353535354
train: step: 998, loss: 0.6916617751121521, acc: 0.51, recall: 0.51, precision: 0.5107250107250108, f_beta: 0.5015766453056658
train: step: 999, loss: 0.692608118057251, acc: 0.47, recall: 0.47, precision: 0.466078697421981, f_beta: 0.4542271650705386
train: step: 1000, loss: 0.6932751536369324, acc: 0.42, recall: 0.42000000000000004, precision: 0.4198717948717949, f_beta: 0.41976790716286516
train: step: 1001, loss: 0.691846489906311, acc: 0.55, recall: 0.55, precision: 0.5525430853299707, f_beta: 0.5444883085332524
train: step: 1002, loss: 0.6927192807197571, acc: 0.52, recall: 0.52, precision: 0.5291375291375291, f_beta: 0.47916666666666663
train: step: 1003, loss: 0.6921495795249939, acc: 0.55, recall: 0.55, precision: 0.5516742455560149, f_beta: 0.5463252343986289
train: step: 1004, loss: 0.6943851709365845, acc: 0.36, recall: 0.36, precision: 0.359775641025641, f_beta: 0.35974389755902364
train: step: 1005, loss: 0.6934725046157837, acc: 0.52, recall: 0.52, precision: 0.5200320512820513, f_beta: 0.5198079231692677
train: step: 1006, loss: 0.7205103039741516, acc: 0.4, recall: 0.39999999999999997, precision: 0.33062330623306235, f_beta: 0.3315508021390375
train: step: 1007, loss: 0.6747307777404785, acc: 0.68, recall: 0.6799999999999999, precision: 0.7622377622377623, f_beta: 0.6527777777777779
train: step: 1008, loss: 0.6936043500900269, acc: 0.46, recall: 0.46, precision: 0.45833333333333337, f_beta: 0.4545454545454545
train: step: 1009, loss: 0.695740818977356, acc: 0.45, recall: 0.44999999999999996, precision: 0.44505494505494503, f_beta: 0.4373401534526854
train: step: 1010, loss: 0.693555474281311, acc: 0.4, recall: 0.39999999999999997, precision: 0.3851102941176471, f_beta: 0.37990905332782143
train: step: 1011, loss: 0.6937696933746338, acc: 0.47, recall: 0.47, precision: 0.4699879951980792, f_beta: 0.46994699469946993
train: step: 1012, loss: 0.6923646330833435, acc: 0.53, recall: 0.53, precision: 0.5300120048019208, f_beta: 0.5299529952995299
train: step: 1013, loss: 0.6942110657691956, acc: 0.39, recall: 0.39, precision: 0.36047691527143577, f_beta: 0.3559286242213071
train: step: 1014, loss: 0.6931371092796326, acc: 0.52, recall: 0.52, precision: 0.5205254515599343, f_beta: 0.5169082125603865
train: step: 1015, loss: 0.6934978365898132, acc: 0.46, recall: 0.45999999999999996, precision: 0.4565972222222222, f_beta: 0.44920440636474906
train: step: 1016, loss: 0.6936386823654175, acc: 0.41, recall: 0.41000000000000003, precision: 0.40347490347490345, f_beta: 0.39985759332723014
train: step: 1017, loss: 0.6942833662033081, acc: 0.48, recall: 0.48, precision: 0.47771836007130125, f_beta: 0.4663382594417077
train: step: 1018, loss: 0.6932604908943176, acc: 0.49, recall: 0.49, precision: 0.48901098901098905, f_beta: 0.47826086956521735
train: step: 1019, loss: 0.6932126879692078, acc: 0.49, recall: 0.49, precision: 0.48996386993175434, f_beta: 0.4895405865278751
train: step: 1020, loss: 0.689706027507782, acc: 0.56, recall: 0.56, precision: 0.6245847176079734, f_beta: 0.4944852941176471
train: step: 1021, loss: 0.6932836771011353, acc: 0.48, recall: 0.48, precision: 0.47916666666666663, f_beta: 0.4747474747474748
train: step: 1022, loss: 0.6933850049972534, acc: 0.55, recall: 0.55, precision: 0.5525430853299707, f_beta: 0.5444883085332524
train: step: 1023, loss: 0.6942304968833923, acc: 0.42, recall: 0.42, precision: 0.410873440285205, f_beta: 0.40476190476190477
train: step: 1024, loss: 0.6935864090919495, acc: 0.45, recall: 0.45, precision: 0.4494949494949495, f_beta: 0.4486215538847118
train: step: 1025, loss: 0.6940743923187256, acc: 0.45, recall: 0.45000000000000007, precision: 0.44745691467002946, f_beta: 0.44326348820730843
train: step: 1026, loss: 0.6927303075790405, acc: 0.48, recall: 0.48000000000000004, precision: 0.47619047619047616, f_beta: 0.45833333333333337
train: step: 1027, loss: 0.6931446194648743, acc: 0.55, recall: 0.55, precision: 0.5500200080032013, f_beta: 0.54995499549955
train: step: 1028, loss: 0.6949312686920166, acc: 0.43, recall: 0.43, precision: 0.37597448618001417, f_beta: 0.36034115138592754
train: step: 1029, loss: 0.6929662227630615, acc: 0.57, recall: 0.5700000000000001, precision: 0.5702529104777199, f_beta: 0.5696126513862476
train: step: 1030, loss: 0.6923819184303284, acc: 0.51, recall: 0.51, precision: 0.510508617065994, f_beta: 0.5039983804028748
train: step: 1031, loss: 0.6951786279678345, acc: 0.41, recall: 0.41000000000000003, precision: 0.38584474885844744, f_beta: 0.3770457185091331
train: step: 1032, loss: 0.6929160356521606, acc: 0.57, recall: 0.5700000000000001, precision: 0.5713994288045696, f_beta: 0.567882624861823
train: step: 1033, loss: 0.6934394240379333, acc: 0.52, recall: 0.52, precision: 0.5217013888888888, f_beta: 0.5104039167686658
train: step: 1034, loss: 0.6885392665863037, acc: 0.53, recall: 0.53, precision: 0.7577319587628866, f_beta: 0.39673982800667434
train: step: 1035, loss: 0.6721360683441162, acc: 0.58, recall: 0.58, precision: 0.7222222222222222, f_beta: 0.5
train: step: 1036, loss: 0.6921190619468689, acc: 0.57, recall: 0.5700000000000001, precision: 0.5707070707070707, f_beta: 0.5689223057644109
train: step: 1037, loss: 0.69355309009552, acc: 0.45, recall: 0.45, precision: 0.449000407996736, f_beta: 0.4472917294744247
train: step: 1038, loss: 0.6929379105567932, acc: 0.46, recall: 0.46, precision: 0.45039682539682535, f_beta: 0.43253467843631777
train: step: 1039, loss: 0.6932626962661743, acc: 0.48, recall: 0.48, precision: 0.47401247401247404, f_beta: 0.44821731748726656
train: step: 1040, loss: 0.6939895153045654, acc: 0.38, recall: 0.38, precision: 0.375, f_beta: 0.37373737373737376
train: step: 1041, loss: 0.6928346753120422, acc: 0.48, recall: 0.48, precision: 0.46875, f_beta: 0.4285714285714286
train: step: 1042, loss: 0.6928258538246155, acc: 0.52, recall: 0.52, precision: 0.5202922077922079, f_beta: 0.5182657567242072
train: step: 1043, loss: 0.6936004161834717, acc: 0.48, recall: 0.48, precision: 0.47916666666666663, f_beta: 0.4747474747474748
train: step: 1044, loss: 0.6929334402084351, acc: 0.48, recall: 0.48, precision: 0.46875, f_beta: 0.4285714285714286
train: step: 1045, loss: 0.6926537156105042, acc: 0.53, recall: 0.53, precision: 0.533921302578019, f_beta: 0.5160127690248172
train: step: 1046, loss: 0.6926981210708618, acc: 0.59, recall: 0.59, precision: 0.594577553593947, f_beta: 0.58497823666363
train: step: 1047, loss: 0.6930195093154907, acc: 0.49, recall: 0.49, precision: 0.48996386993175434, f_beta: 0.4895405865278751
train: step: 1048, loss: 0.6927414536476135, acc: 0.59, recall: 0.5900000000000001, precision: 0.5903251706142112, f_beta: 0.5896306676008407
train: step: 1049, loss: 0.6919975876808167, acc: 0.55, recall: 0.55, precision: 0.5812215724496426, f_beta: 0.5021573182874212
train: step: 1050, loss: 0.6933248043060303, acc: 0.53, recall: 0.53, precision: 0.530108390204737, f_beta: 0.5295766189570613
train: step: 1051, loss: 0.6909814476966858, acc: 0.51, recall: 0.51, precision: 0.5177179305457122, f_beta: 0.45011783189316573
train: step: 1052, loss: 0.6936180591583252, acc: 0.43, recall: 0.43, precision: 0.42997198879551823, f_beta: 0.42994299429942995
train: step: 1053, loss: 0.691038966178894, acc: 0.52, recall: 0.52, precision: 0.6302083333333333, f_beta: 0.39117199391172
train: step: 1054, loss: 0.693263828754425, acc: 0.51, recall: 0.51, precision: 0.5100040016006402, f_beta: 0.50995099509951
train: step: 1055, loss: 0.69366854429245, acc: 0.48, recall: 0.48, precision: 0.47970779220779225, f_beta: 0.4781212364512244
train: step: 1056, loss: 0.6938474178314209, acc: 0.45, recall: 0.45, precision: 0.449000407996736, f_beta: 0.4472917294744247
train: step: 1057, loss: 0.7000095844268799, acc: 0.49, recall: 0.49, precision: 0.4866666666666667, f_beta: 0.45599999999999996
train: step: 1058, loss: 0.6929185390472412, acc: 0.49, recall: 0.49, precision: 0.4898989898989899, f_beta: 0.48872180451127817
train: step: 1059, loss: 0.6938167810440063, acc: 0.51, recall: 0.51, precision: 0.5384024577572964, f_beta: 0.3988467672678199
train: step: 1060, loss: 0.6935188174247742, acc: 0.47, recall: 0.47000000000000003, precision: 0.4694002447980416, f_beta: 0.467390212038991
train: step: 1061, loss: 0.6926255226135254, acc: 0.49, recall: 0.49, precision: 0.4899959983993597, f_beta: 0.48994899489948995
train: step: 1062, loss: 0.6851913332939148, acc: 0.56, recall: 0.5599999999999999, precision: 0.6420454545454546, f_beta: 0.4857410004675081
train: step: 1063, loss: 0.6934517621994019, acc: 0.53, recall: 0.53, precision: 0.5350631136044881, f_beta: 0.512397551613238
train: step: 1064, loss: 0.6923304796218872, acc: 0.56, recall: 0.56, precision: 0.5615763546798029, f_beta: 0.5571658615136876
train: step: 1065, loss: 0.6929157376289368, acc: 0.54, recall: 0.54, precision: 0.5410509031198687, f_beta: 0.537037037037037
train: step: 1066, loss: 0.6908061504364014, acc: 0.59, recall: 0.59, precision: 0.5965250965250966, f_beta: 0.5829518868884142
train: step: 1067, loss: 0.6921988129615784, acc: 0.55, recall: 0.55, precision: 0.5565355042966984, f_beta: 0.5366079703429101
train: step: 1068, loss: 0.6935913562774658, acc: 0.54, recall: 0.54, precision: 0.5405844155844155, f_beta: 0.5383380168606985
train: step: 1069, loss: 0.6912629008293152, acc: 0.51, recall: 0.51, precision: 0.5141163184641445, f_beta: 0.47147017581706396
train: step: 1070, loss: 0.6855895519256592, acc: 0.58, recall: 0.58, precision: 0.7222222222222222, f_beta: 0.5
train: step: 1071, loss: 0.6939048767089844, acc: 0.48, recall: 0.48, precision: 0.47970779220779225, f_beta: 0.4781212364512244
train: step: 1072, loss: 0.6924241781234741, acc: 0.55, recall: 0.55, precision: 0.554945054945055, f_beta: 0.5396419437340153
train: step: 1073, loss: 0.6928735375404358, acc: 0.55, recall: 0.55, precision: 0.5501806503412284, f_beta: 0.5495946351716544
train: step: 1074, loss: 0.6913179159164429, acc: 0.51, recall: 0.51, precision: 0.5121418164157359, f_beta: 0.4873940788785438
train: step: 1075, loss: 0.6922866106033325, acc: 0.52, recall: 0.52, precision: 0.52, f_beta: 0.52
train: step: 1076, loss: 0.6819427609443665, acc: 0.56, recall: 0.56, precision: 0.7038043478260869, f_beta: 0.46576007770762506
train: step: 1077, loss: 0.6934192180633545, acc: 0.48, recall: 0.48, precision: 0.48, f_beta: 0.48
train: step: 1078, loss: 0.7020896673202515, acc: 0.42, recall: 0.42, precision: 0.2777777777777778, f_beta: 0.3095238095238095
train: step: 1079, loss: 0.6923414468765259, acc: 0.51, recall: 0.51, precision: 0.51010101010101, f_beta: 0.5087719298245614
train: step: 1080, loss: 0.6923869252204895, acc: 0.55, recall: 0.55, precision: 0.5980392156862745, f_beta: 0.4871794871794872
train: step: 1081, loss: 0.6893346309661865, acc: 0.51, recall: 0.51, precision: 0.510989010989011, f_beta: 0.4987212276214833
train: step: 1082, loss: 0.6928778290748596, acc: 0.56, recall: 0.56, precision: 0.5615763546798029, f_beta: 0.5571658615136876
train: step: 1083, loss: 0.6902220249176025, acc: 0.52, recall: 0.52, precision: 0.5238095238095237, f_beta: 0.5
train: step: 1084, loss: 0.6919922828674316, acc: 0.53, recall: 0.53, precision: 0.5303030303030303, f_beta: 0.5288220551378446
train: step: 1085, loss: 0.693770170211792, acc: 0.48, recall: 0.48, precision: 0.47987117552334946, f_beta: 0.47916666666666663
train: step: 1086, loss: 0.6933376789093018, acc: 0.45, recall: 0.45, precision: 0.4499799919967987, f_beta: 0.44994499449944997
train: step: 1087, loss: 0.6946062445640564, acc: 0.51, recall: 0.51, precision: 0.5305250305250305, f_beta: 0.4109868974636375
train: step: 1088, loss: 0.6887948513031006, acc: 0.54, recall: 0.54, precision: 0.7604166666666667, f_beta: 0.4165398274987316
train: step: 1089, loss: 0.6923225522041321, acc: 0.5, recall: 0.5, precision: 0.5, f_beta: 0.43464495703301675
train: step: 1090, loss: 0.6930609941482544, acc: 0.48, recall: 0.48000000000000004, precision: 0.47947454844006565, f_beta: 0.47665056360708535
train: step: 1091, loss: 0.6934060454368591, acc: 0.48, recall: 0.48, precision: 0.47987117552334946, f_beta: 0.47916666666666663
train: step: 1092, loss: 0.6937049627304077, acc: 0.48, recall: 0.48, precision: 0.4751984126984127, f_beta: 0.45355191256830596
train: step: 1093, loss: 0.6919639706611633, acc: 0.44, recall: 0.44, precision: 0.4285714285714286, f_beta: 0.41666666666666663
train: step: 1094, loss: 0.6934326887130737, acc: 0.44, recall: 0.44, precision: 0.43990384615384615, f_beta: 0.4397759103641457
train: step: 1095, loss: 0.6930679082870483, acc: 0.54, recall: 0.54, precision: 0.5410509031198687, f_beta: 0.537037037037037
train: step: 1096, loss: 0.6929013729095459, acc: 0.46, recall: 0.46, precision: 0.4375, f_beta: 0.4065934065934066
train: step: 1097, loss: 0.6934592723846436, acc: 0.5, recall: 0.5, precision: 0.5, f_beta: 0.49979991996798717
train: step: 1098, loss: 0.7002859711647034, acc: 0.45, recall: 0.44999999999999996, precision: 0.43333333333333335, f_beta: 0.41333333333333333
train: step: 1099, loss: 0.6916203498840332, acc: 0.53, recall: 0.53, precision: 0.533921302578019, f_beta: 0.5160127690248172
train: step: 1100, loss: 0.693470299243927, acc: 0.49, recall: 0.49, precision: 0.48869289914066033, f_beta: 0.4748223663886314
train: step: 1101, loss: 0.6921681761741638, acc: 0.5, recall: 0.5, precision: 0.5, f_beta: 0.4429590017825312
train: step: 1102, loss: 0.6922223567962646, acc: 0.57, recall: 0.5700000000000001, precision: 0.5700280112044818, f_beta: 0.56995699569957
train: step: 1103, loss: 0.6939282417297363, acc: 0.45, recall: 0.45, precision: 0.449000407996736, f_beta: 0.4472917294744247
train: step: 1104, loss: 0.696668803691864, acc: 0.48, recall: 0.48, precision: 0.47086247086247085, f_beta: 0.4357638888888889
train: step: 1105, loss: 0.6933776140213013, acc: 0.52, recall: 0.52, precision: 0.5229779411764706, f_beta: 0.5039272426622571
train: step: 1106, loss: 0.6936801671981812, acc: 0.46, recall: 0.45999999999999996, precision: 0.4565972222222222, f_beta: 0.44920440636474906
train: step: 1107, loss: 0.6940944194793701, acc: 0.4, recall: 0.4, precision: 0.4, f_beta: 0.4000000000000001
train: step: 1108, loss: 0.6935648322105408, acc: 0.47, recall: 0.47, precision: 0.466078697421981, f_beta: 0.4542271650705386
train: step: 1109, loss: 0.6923741698265076, acc: 0.53, recall: 0.53, precision: 0.5303030303030303, f_beta: 0.5288220551378446
train: step: 1110, loss: 0.6921795010566711, acc: 0.53, recall: 0.53, precision: 0.532967032967033, f_beta: 0.5191815856777493
train: step: 1111, loss: 0.6940820217132568, acc: 0.45, recall: 0.44999999999999996, precision: 0.44637494637494635, f_beta: 0.44054521411860437
train: step: 1112, loss: 0.6922042965888977, acc: 0.55, recall: 0.55, precision: 0.5500200080032013, f_beta: 0.54995499549955
train: step: 1113, loss: 0.688041090965271, acc: 0.53, recall: 0.53, precision: 0.7577319587628866, f_beta: 0.39673982800667434
train: step: 1114, loss: 0.6930038332939148, acc: 0.51, recall: 0.51, precision: 0.5100040016006402, f_beta: 0.50995099509951
train: step: 1115, loss: 0.684724748134613, acc: 0.55, recall: 0.55, precision: 0.6920122887864824, f_beta: 0.4479205005520795
train: step: 1116, loss: 0.6912014484405518, acc: 0.5, recall: 0.5, precision: 0.5, f_beta: 0.42555147058823534
train: step: 1117, loss: 0.6928252577781677, acc: 0.54, recall: 0.54, precision: 0.5445632798573975, f_beta: 0.5279146141215106
train: step: 1118, loss: 0.6876267790794373, acc: 0.54, recall: 0.54, precision: 0.5476190476190477, f_beta: 0.5208333333333333
train: step: 1119, loss: 0.6932736039161682, acc: 0.5, recall: 0.5, precision: 0.5, f_beta: 0.49269480519480513
train: step: 1120, loss: 0.693112850189209, acc: 0.49, recall: 0.49, precision: 0.48966515088879703, f_beta: 0.48583526565177937
train: step: 1121, loss: 0.6944988965988159, acc: 0.47, recall: 0.47, precision: 0.46194824961948244, f_beta: 0.4403970013726111
train: step: 1122, loss: 0.6912286281585693, acc: 0.57, recall: 0.57, precision: 0.5988142292490118, f_beta: 0.5361881134721174
train: step: 1123, loss: 0.6928800940513611, acc: 0.52, recall: 0.52, precision: 0.5217013888888888, f_beta: 0.5104039167686658
train: step: 1124, loss: 0.693585991859436, acc: 0.47, recall: 0.47, precision: 0.4699879951980792, f_beta: 0.46994699469946993
train: step: 1125, loss: 0.6938135623931885, acc: 0.46, recall: 0.46, precision: 0.45039682539682535, f_beta: 0.43253467843631777
train: step: 1126, loss: 0.6937503218650818, acc: 0.42, recall: 0.42, precision: 0.42, f_beta: 0.41999999999999993
train: step: 1127, loss: 0.693495512008667, acc: 0.47, recall: 0.47, precision: 0.46703296703296704, f_beta: 0.4578005115089514
train: step: 1128, loss: 0.6891318559646606, acc: 0.51, recall: 0.51, precision: 0.5113071008593397, f_beta: 0.49541756770672435
train: step: 1129, loss: 0.7109304070472717, acc: 0.45, recall: 0.44999999999999996, precision: 0.3894783377541998, f_beta: 0.36276213648476424
train: step: 1130, loss: 0.6929099559783936, acc: 0.54, recall: 0.54, precision: 0.5548245614035088, f_beta: 0.5066495066495066
train: step: 1131, loss: 0.693931519985199, acc: 0.51, recall: 0.51, precision: 0.5101999184006528, f_beta: 0.5075871771681238
train: step: 1132, loss: 0.6968353390693665, acc: 0.49, recall: 0.49, precision: 0.4803921568627451, f_beta: 0.4188034188034188
train: step: 1133, loss: 0.6950972676277161, acc: 0.48, recall: 0.48, precision: 0.47401247401247404, f_beta: 0.44821731748726656
train: step: 1134, loss: 0.6930703520774841, acc: 0.46, recall: 0.45999999999999996, precision: 0.4565972222222222, f_beta: 0.44920440636474906
train: step: 1135, loss: 0.6931648850440979, acc: 0.48, recall: 0.48, precision: 0.47702205882352944, f_beta: 0.46258784621744525
train: step: 1136, loss: 0.6940411329269409, acc: 0.52, recall: 0.52, precision: 0.53125, f_beta: 0.4725274725274725
train: step: 1137, loss: 0.6936142444610596, acc: 0.5, recall: 0.5, precision: 0.5, f_beta: 0.4967793880837359
train: step: 1138, loss: 0.6916694641113281, acc: 0.57, recall: 0.57, precision: 0.575075075075075, f_beta: 0.562608076492727
train: step: 1139, loss: 0.690358579158783, acc: 0.53, recall: 0.53, precision: 0.532967032967033, f_beta: 0.5191815856777493
train: step: 1140, loss: 0.6930286288261414, acc: 0.51, recall: 0.51, precision: 0.5141163184641445, f_beta: 0.47147017581706396
train: step: 1141, loss: 0.6929630041122437, acc: 0.49, recall: 0.49, precision: 0.48869289914066033, f_beta: 0.4748223663886314
train: step: 1142, loss: 0.6932007074356079, acc: 0.51, recall: 0.51, precision: 0.5107250107250108, f_beta: 0.5015766453056658
train: step: 1143, loss: 0.6891375780105591, acc: 0.53, recall: 0.53, precision: 0.5588235294117647, f_beta: 0.46438746438746437
train: step: 1144, loss: 0.6929466128349304, acc: 0.49, recall: 0.49, precision: 0.48949138293400585, f_beta: 0.48375341633768604
train: step: 1145, loss: 0.6936132311820984, acc: 0.38, recall: 0.38, precision: 0.3782467532467533, f_beta: 0.37775993576876754
train: step: 1146, loss: 0.6923891305923462, acc: 0.51, recall: 0.51, precision: 0.5121418164157359, f_beta: 0.4873940788785438
train: step: 1147, loss: 0.6931988596916199, acc: 0.52, recall: 0.52, precision: 0.5205254515599343, f_beta: 0.5169082125603865
train: step: 1148, loss: 0.693312406539917, acc: 0.46, recall: 0.46, precision: 0.4594155844155844, f_beta: 0.45804897631473307
train: step: 1149, loss: 0.6925681829452515, acc: 0.62, recall: 0.62, precision: 0.6428571428571428, f_beta: 0.6041666666666667
train: step: 1150, loss: 0.6928912401199341, acc: 0.53, recall: 0.53, precision: 0.5315258511979823, f_beta: 0.5242433444680635
train: step: 1151, loss: 0.6934100985527039, acc: 0.52, recall: 0.52, precision: 0.5201288244766505, f_beta: 0.5192307692307692
train: step: 1152, loss: 0.6941750049591064, acc: 0.45, recall: 0.45, precision: 0.4494949494949495, f_beta: 0.4486215538847118
train: step: 1153, loss: 0.6933395862579346, acc: 0.45, recall: 0.45, precision: 0.44832575444398515, f_beta: 0.44550861982054646
train: step: 1154, loss: 0.6928250789642334, acc: 0.54, recall: 0.54, precision: 0.5434027777777778, f_beta: 0.5308037535699714
train: step: 1155, loss: 0.6925411224365234, acc: 0.52, recall: 0.52, precision: 0.5202922077922079, f_beta: 0.5182657567242072
train: step: 1156, loss: 0.6967626810073853, acc: 0.47, recall: 0.47000000000000003, precision: 0.3847926267281106, f_beta: 0.3497730339835603
train: step: 1157, loss: 0.6926868557929993, acc: 0.46, recall: 0.45999999999999996, precision: 0.45755517826825126, f_beta: 0.4521103896103896
train: step: 1158, loss: 0.6929813623428345, acc: 0.5, recall: 0.5, precision: 0.25, f_beta: 0.3333333333333333
train: step: 1159, loss: 0.6935192942619324, acc: 0.47, recall: 0.47000000000000003, precision: 0.46357455075279264, f_beta: 0.44554869756250653
train: step: 1160, loss: 0.6917034387588501, acc: 0.52, recall: 0.52, precision: 0.5555555555555556, f_beta: 0.42857142857142855
train: step: 1161, loss: 0.6922602653503418, acc: 0.54, recall: 0.54, precision: 0.5496031746031746, f_beta: 0.5166036149642708
train: step: 1162, loss: 0.6936931610107422, acc: 0.44, recall: 0.44, precision: 0.44, f_beta: 0.44
train: step: 1163, loss: 0.6919860243797302, acc: 0.48, recall: 0.48, precision: 0.4725877192982456, f_beta: 0.4422994422994423
train: step: 1164, loss: 0.6837194561958313, acc: 0.57, recall: 0.57, precision: 0.6547303271441203, f_beta: 0.5017958521608157
train: step: 1165, loss: 0.6912006139755249, acc: 0.53, recall: 0.53, precision: 0.5588235294117647, f_beta: 0.46438746438746437
train: step: 1166, loss: 0.6932559013366699, acc: 0.53, recall: 0.53, precision: 0.5380517503805176, f_beta: 0.5037482842360891
train: step: 1167, loss: 0.691246747970581, acc: 0.53, recall: 0.53, precision: 0.576608784473953, f_beta: 0.44568935015921696
train: step: 1168, loss: 0.6947858929634094, acc: 0.5, recall: 0.5, precision: 0.5, f_beta: 0.3503118503118503
train: step: 1169, loss: 0.6888380646705627, acc: 0.49, recall: 0.49, precision: 0.48228206945428775, f_beta: 0.4276736617663562
train: step: 1170, loss: 0.6937239766120911, acc: 0.55, recall: 0.55, precision: 0.5500200080032013, f_beta: 0.54995499549955
train: step: 1171, loss: 0.6939111948013306, acc: 0.44, recall: 0.44, precision: 0.43961352657004826, f_beta: 0.4391025641025641
train: step: 1172, loss: 0.6920495629310608, acc: 0.51, recall: 0.51, precision: 0.5107250107250108, f_beta: 0.5015766453056658
train: step: 1173, loss: 0.6935268640518188, acc: 0.48, recall: 0.48, precision: 0.47996794871794873, f_beta: 0.4797919167667067
train: step: 1174, loss: 0.6927615404129028, acc: 0.52, recall: 0.52, precision: 0.52, f_beta: 0.52
train: step: 1175, loss: 0.6925961971282959, acc: 0.59, recall: 0.5900000000000001, precision: 0.5909090909090908, f_beta: 0.5889724310776943
train: step: 1176, loss: 0.693217396736145, acc: 0.51, recall: 0.51, precision: 0.510508617065994, f_beta: 0.5039983804028748
train: step: 1177, loss: 0.6918599009513855, acc: 0.49, recall: 0.49, precision: 0.48785818358426425, f_beta: 0.46647138822052514
train: step: 1178, loss: 0.693709135055542, acc: 0.51, recall: 0.51, precision: 0.5162443144899285, f_beta: 0.4579046354685253
train: step: 1179, loss: 0.6927424669265747, acc: 0.55, recall: 0.55, precision: 0.5505050505050505, f_beta: 0.5488721804511277
train: step: 1180, loss: 0.6915029883384705, acc: 0.51, recall: 0.51, precision: 0.5177179305457122, f_beta: 0.45011783189316573
train: step: 1181, loss: 0.6926763653755188, acc: 0.54, recall: 0.54, precision: 0.5410509031198687, f_beta: 0.537037037037037
train: step: 1182, loss: 0.6929705142974854, acc: 0.54, recall: 0.54, precision: 0.5424448217317488, f_beta: 0.5332792207792207
train: step: 1183, loss: 0.6933194994926453, acc: 0.53, recall: 0.53, precision: 0.5364254492472074, f_beta: 0.5083167695365624
train: step: 1184, loss: 0.6928169131278992, acc: 0.59, recall: 0.5900000000000001, precision: 0.5909090909090908, f_beta: 0.5889724310776943
train: step: 1185, loss: 0.6928567290306091, acc: 0.51, recall: 0.51, precision: 0.5100040016006402, f_beta: 0.50995099509951
train: step: 1186, loss: 0.6925697326660156, acc: 0.55, recall: 0.55, precision: 0.5536250536250535, f_beta: 0.5422642660970399
train: step: 1187, loss: 0.6951231360435486, acc: 0.47, recall: 0.47, precision: 0.45999999999999996, f_beta: 0.43466666666666665
train: step: 1188, loss: 0.6936583518981934, acc: 0.43, recall: 0.43, precision: 0.4292929292929293, f_beta: 0.4285714285714286
train: step: 1189, loss: 0.6909148693084717, acc: 0.51, recall: 0.51, precision: 0.5526315789473684, f_beta: 0.38557993730407525
train: step: 1190, loss: 0.6935506463050842, acc: 0.51, recall: 0.51, precision: 0.5100361300682457, f_beta: 0.5095586027424682
train: step: 1191, loss: 0.6935645937919617, acc: 0.53, recall: 0.53, precision: 0.5321750321750323, f_beta: 0.5219204557013528
train: step: 1192, loss: 0.6932855248451233, acc: 0.51, recall: 0.51, precision: 0.510508617065994, f_beta: 0.5039983804028748
train: step: 1193, loss: 0.6937329769134521, acc: 0.42, recall: 0.42000000000000004, precision: 0.4198717948717949, f_beta: 0.41976790716286516
train: step: 1194, loss: 0.6945958137512207, acc: 0.49, recall: 0.49, precision: 0.2474747474747475, f_beta: 0.32885906040268453
train: step: 1195, loss: 0.6948822140693665, acc: 0.5, recall: 0.5, precision: 0.5, f_beta: 0.4429590017825312
train: step: 1196, loss: 0.6926147937774658, acc: 0.55, recall: 0.55, precision: 0.5584385226741468, f_beta: 0.5331465919701215
train: step: 1197, loss: 0.6927950382232666, acc: 0.56, recall: 0.56, precision: 0.5603864734299517, f_beta: 0.5592948717948718
train: step: 1198, loss: 0.6926348209381104, acc: 0.53, recall: 0.53, precision: 0.5303030303030303, f_beta: 0.5288220551378446
train: step: 1199, loss: 0.6932953596115112, acc: 0.54, recall: 0.54, precision: 0.5459558823529411, f_beta: 0.524596940884663
train: step: 1200, loss: 0.6888738870620728, acc: 0.54, recall: 0.54, precision: 0.5625, f_beta: 0.4945054945054945
train: step: 1201, loss: 0.6924371123313904, acc: 0.56, recall: 0.56, precision: 0.5608766233766234, f_beta: 0.5584102769971899
train: step: 1202, loss: 0.69324791431427, acc: 0.5, recall: 0.5, precision: 0.5, f_beta: 0.4949494949494949
train: step: 1203, loss: 0.6894463300704956, acc: 0.53, recall: 0.53, precision: 0.5423489553924337, f_beta: 0.49304282170208175
train: step: 1204, loss: 0.6936643123626709, acc: 0.49, recall: 0.49, precision: 0.48949138293400585, f_beta: 0.48375341633768604
train: step: 1205, loss: 0.6921330094337463, acc: 0.5, recall: 0.5, precision: 0.5, f_beta: 0.47916666666666663
train: step: 1206, loss: 0.6936700344085693, acc: 0.46, recall: 0.45999999999999996, precision: 0.4554367201426025, f_beta: 0.44581280788177335
train: step: 1207, loss: 0.692291259765625, acc: 0.55, recall: 0.55, precision: 0.5501806503412284, f_beta: 0.5495946351716544
train: step: 1208, loss: 0.6898866295814514, acc: 0.56, recall: 0.56, precision: 0.6245847176079734, f_beta: 0.4944852941176471
train: step: 1209, loss: 0.6931259036064148, acc: 0.51, recall: 0.51, precision: 0.51010101010101, f_beta: 0.5087719298245614
train: step: 1210, loss: 0.6935408115386963, acc: 0.49, recall: 0.49, precision: 0.4866666666666667, f_beta: 0.45599999999999996
train: step: 1211, loss: 0.6927765011787415, acc: 0.51, recall: 0.51, precision: 0.5100040016006402, f_beta: 0.50995099509951
train: step: 1212, loss: 0.6916730403900146, acc: 0.47, recall: 0.47, precision: 0.4547920433996383, f_beta: 0.42133420679113437
train: step: 1213, loss: 0.6942756175994873, acc: 0.33, recall: 0.33, precision: 0.3282828282828283, f_beta: 0.32832080200501257
train: step: 1214, loss: 0.6931100487709045, acc: 0.5, recall: 0.5, precision: 0.5, f_beta: 0.4505494505494506
train: step: 1215, loss: 0.6946566104888916, acc: 0.43, recall: 0.43, precision: 0.42860057119543044, f_beta: 0.42719324690985827
train: step: 1216, loss: 0.6931817531585693, acc: 0.51, recall: 0.51, precision: 0.5113071008593397, f_beta: 0.49541756770672435
train: step: 1217, loss: 0.6943740248680115, acc: 0.55, recall: 0.55, precision: 0.5516742455560149, f_beta: 0.5463252343986289
train: step: 1218, loss: 0.6899508833885193, acc: 0.51, recall: 0.51, precision: 0.5221043324491601, f_beta: 0.43227899432278993
train: step: 1219, loss: 0.6935375928878784, acc: 0.51, recall: 0.51, precision: 0.5177179305457122, f_beta: 0.45011783189316573
train: step: 1220, loss: 0.6937136054039001, acc: 0.44, recall: 0.44, precision: 0.43990384615384615, f_beta: 0.4397759103641457
train: step: 1221, loss: 0.6914064288139343, acc: 0.55, recall: 0.55, precision: 0.5607090820786789, f_beta: 0.529239460194581
train: step: 1222, loss: 0.6908637285232544, acc: 0.5, recall: 0.5, precision: 0.5, f_beta: 0.3503118503118503
train: step: 1223, loss: 0.6931961178779602, acc: 0.55, recall: 0.55, precision: 0.5536250536250535, f_beta: 0.5422642660970399
train: step: 1224, loss: 0.6930476427078247, acc: 0.53, recall: 0.53, precision: 0.5300120048019208, f_beta: 0.5299529952995299
train: step: 1225, loss: 0.6927882432937622, acc: 0.52, recall: 0.52, precision: 0.5201288244766505, f_beta: 0.5192307692307692
train: step: 1226, loss: 0.6943172216415405, acc: 0.34, recall: 0.34, precision: 0.3302207130730051, f_beta: 0.33035714285714285
train: step: 1227, loss: 0.6931514739990234, acc: 0.5, recall: 0.5, precision: 0.5, f_beta: 0.49979991996798717
train: step: 1228, loss: 0.693816065788269, acc: 0.43, recall: 0.43, precision: 0.42860057119543044, f_beta: 0.42719324690985827
train: step: 1229, loss: 0.6929472088813782, acc: 0.54, recall: 0.54, precision: 0.5405844155844155, f_beta: 0.5383380168606985
train: step: 1230, loss: 0.6925422549247742, acc: 0.51, recall: 0.51, precision: 0.5101999184006528, f_beta: 0.5075871771681238
train: step: 1231, loss: 0.6934862732887268, acc: 0.49, recall: 0.49, precision: 0.4898000815993472, f_beta: 0.48748869460355737
train: step: 1232, loss: 0.6933752298355103, acc: 0.48, recall: 0.48, precision: 0.36979166666666663, f_beta: 0.34043632673769664
train: step: 1233, loss: 0.6931493282318115, acc: 0.46, recall: 0.45999999999999996, precision: 0.45993589743589747, f_beta: 0.45978391356542614
train: step: 1234, loss: 0.6936405897140503, acc: 0.49, recall: 0.49, precision: 0.48927498927498925, f_beta: 0.48123283490997865
train: step: 1235, loss: 0.6926664710044861, acc: 0.61, recall: 0.61, precision: 0.6243781094527363, f_beta: 0.5983935742971889
train: step: 1236, loss: 0.6900782585144043, acc: 0.56, recall: 0.56, precision: 0.6116071428571428, f_beta: 0.5024875621890548
train: step: 1237, loss: 0.6928163766860962, acc: 0.51, recall: 0.51, precision: 0.5107250107250108, f_beta: 0.5015766453056658
train: step: 1238, loss: 0.6927967071533203, acc: 0.53, recall: 0.53, precision: 0.5315258511979823, f_beta: 0.5242433444680635
train: step: 1239, loss: 0.6924405694007874, acc: 0.56, recall: 0.56, precision: 0.5603864734299517, f_beta: 0.5592948717948718
train: step: 1240, loss: 0.6925235986709595, acc: 0.57, recall: 0.5700000000000001, precision: 0.5707070707070707, f_beta: 0.5689223057644109
train: step: 1241, loss: 0.691478967666626, acc: 0.53, recall: 0.53, precision: 0.6578947368421053, f_beta: 0.4106583072100314
train: step: 1242, loss: 0.6927606463432312, acc: 0.58, recall: 0.58, precision: 0.58, f_beta: 0.58
train: step: 1243, loss: 0.6935103535652161, acc: 0.45, recall: 0.44999999999999996, precision: 0.4498193496587716, f_beta: 0.44950455409868884
train: step: 1244, loss: 0.693196713924408, acc: 0.46, recall: 0.45999999999999996, precision: 0.45974235104669886, f_beta: 0.4591346153846153
train: step: 1245, loss: 0.6931430697441101, acc: 0.48, recall: 0.48, precision: 0.47401247401247404, f_beta: 0.44821731748726656
train: step: 1246, loss: 0.6930437684059143, acc: 0.5, recall: 0.5, precision: 0.5, f_beta: 0.4967793880837359
train: step: 1247, loss: 0.6932510137557983, acc: 0.52, recall: 0.52, precision: 0.5202922077922079, f_beta: 0.5182657567242072
train: step: 1248, loss: 0.693175196647644, acc: 0.5, recall: 0.5, precision: 0.5, f_beta: 0.5
train: step: 1249, loss: 0.6919270157814026, acc: 0.59, recall: 0.5900000000000001, precision: 0.6092763477416221, f_beta: 0.5710848415106183
train: step: 1250, loss: 0.6922228336334229, acc: 0.54, recall: 0.54, precision: 0.5625, f_beta: 0.4945054945054945
train: step: 1251, loss: 0.6961625814437866, acc: 0.44, recall: 0.44, precision: 0.3983739837398374, f_beta: 0.3761140819964349
train: step: 1252, loss: 0.6928600072860718, acc: 0.53, recall: 0.53, precision: 0.530108390204737, f_beta: 0.5295766189570613
train: step: 1253, loss: 0.6915568709373474, acc: 0.43, recall: 0.43, precision: 0.3945147679324894, f_beta: 0.37766131673763514
train: step: 1254, loss: 0.6929227709770203, acc: 0.51, recall: 0.51, precision: 0.5100040016006402, f_beta: 0.50995099509951
train: step: 1255, loss: 0.6931089758872986, acc: 0.6, recall: 0.6, precision: 0.6041666666666667, f_beta: 0.595959595959596
train: step: 1256, loss: 0.6935688257217407, acc: 0.46, recall: 0.45999999999999996, precision: 0.45755517826825126, f_beta: 0.4521103896103896
train: step: 1257, loss: 0.6873414516448975, acc: 0.53, recall: 0.53, precision: 0.6578947368421053, f_beta: 0.4106583072100314
train: step: 1258, loss: 0.6929047107696533, acc: 0.55, recall: 0.55, precision: 0.5500200080032013, f_beta: 0.54995499549955
train: step: 1259, loss: 0.6931386590003967, acc: 0.48, recall: 0.48, precision: 0.4782986111111111, f_beta: 0.46960424316605465
train: step: 1260, loss: 0.6932373642921448, acc: 0.47, recall: 0.47, precision: 0.42339121552604697, f_beta: 0.3749262884774148
train: step: 1261, loss: 0.6934112310409546, acc: 0.51, recall: 0.51, precision: 0.5100040016006402, f_beta: 0.50995099509951
train: step: 1262, loss: 0.6932761669158936, acc: 0.52, recall: 0.52, precision: 0.5217013888888888, f_beta: 0.5104039167686658
train: step: 1263, loss: 0.6867278218269348, acc: 0.61, recall: 0.61, precision: 0.6948972360028349, f_beta: 0.5623386825272136
train: step: 1264, loss: 0.6930348873138428, acc: 0.5, recall: 0.5, precision: 0.5, f_beta: 0.49979991996798717
train: step: 1265, loss: 0.6936080455780029, acc: 0.43, recall: 0.43, precision: 0.42997198879551823, f_beta: 0.42994299429942995
train: step: 1266, loss: 0.6912201642990112, acc: 0.52, recall: 0.52, precision: 0.5248015873015872, f_beta: 0.49558638083228246
train: step: 1267, loss: 0.6921172142028809, acc: 0.58, recall: 0.5800000000000001, precision: 0.5805152979066023, f_beta: 0.5793269230769231
train: step: 1268, loss: 0.692674458026886, acc: 0.55, recall: 0.55, precision: 0.5501806503412284, f_beta: 0.5495946351716544
train: step: 1269, loss: 0.693504273891449, acc: 0.52, recall: 0.52, precision: 0.5208333333333334, f_beta: 0.5151515151515151
train: step: 1270, loss: 0.6928093433380127, acc: 0.56, recall: 0.56, precision: 0.5668449197860963, f_beta: 0.548440065681445
train: step: 1271, loss: 0.69317227602005, acc: 0.47, recall: 0.47000000000000003, precision: 0.4694002447980416, f_beta: 0.467390212038991
train: step: 1272, loss: 0.6931599974632263, acc: 0.44, recall: 0.44, precision: 0.4310661764705882, f_beta: 0.4212484497726333
train: step: 1273, loss: 0.6916834115982056, acc: 0.49, recall: 0.49, precision: 0.46159754224270355, f_beta: 0.37430990062569014
train: step: 1274, loss: 0.6942093372344971, acc: 0.44, recall: 0.44000000000000006, precision: 0.4363327674023769, f_beta: 0.43181818181818177
train: step: 1275, loss: 0.692241370677948, acc: 0.59, recall: 0.59, precision: 0.594577553593947, f_beta: 0.58497823666363
train: step: 1276, loss: 0.6931332945823669, acc: 0.51, recall: 0.51, precision: 0.510334849111203, f_beta: 0.5059985885673959
train: step: 1277, loss: 0.6928154826164246, acc: 0.41, recall: 0.41000000000000003, precision: 0.4010989010989011, f_beta: 0.3964194373401535
train: step: 1278, loss: 0.6932029724121094, acc: 0.51, recall: 0.51, precision: 0.7525252525252526, f_beta: 0.3551783129359126
train: step: 1279, loss: 0.6938794851303101, acc: 0.45, recall: 0.45, precision: 0.4499799919967987, f_beta: 0.44994499449944997
train: step: 1280, loss: 0.6918851733207703, acc: 0.68, recall: 0.6799999999999999, precision: 0.6875, f_beta: 0.6767676767676767
train: step: 1281, loss: 0.692905068397522, acc: 0.47, recall: 0.47, precision: 0.46782496782496785, f_beta: 0.4608890245142916
train: step: 1282, loss: 0.6938613653182983, acc: 0.52, recall: 0.52, precision: 0.5229779411764706, f_beta: 0.5039272426622571
train: step: 1283, loss: 0.6932905316352844, acc: 0.52, recall: 0.52, precision: 0.52, f_beta: 0.52
train: step: 1284, loss: 0.6924388408660889, acc: 0.46, recall: 0.46, precision: 0.45039682539682535, f_beta: 0.43253467843631777
train: step: 1285, loss: 0.6944538950920105, acc: 0.39, recall: 0.39, precision: 0.38631665977676727, f_beta: 0.385018651073697
train: step: 1286, loss: 0.6932565569877625, acc: 0.44, recall: 0.44000000000000006, precision: 0.4363327674023769, f_beta: 0.43181818181818177
train: step: 1287, loss: 0.6919938921928406, acc: 0.48, recall: 0.48000000000000004, precision: 0.46612466124661245, f_beta: 0.4206773618538324
train: step: 1288, loss: 0.6937053799629211, acc: 0.47, recall: 0.47, precision: 0.4699879951980792, f_beta: 0.46994699469946993
train: step: 1289, loss: 0.6924067735671997, acc: 0.52, recall: 0.52, precision: 0.5473484848484849, f_beta: 0.43899018232819076
train: step: 1290, loss: 0.6933598518371582, acc: 0.47, recall: 0.47, precision: 0.46989160979526295, f_beta: 0.46952257031328193
train: step: 1291, loss: 0.6934208869934082, acc: 0.46, recall: 0.45999999999999996, precision: 0.45993589743589747, f_beta: 0.45978391356542614
train: step: 1292, loss: 0.6941055059432983, acc: 0.4, recall: 0.4, precision: 0.3998397435897436, f_beta: 0.3997599039615846
train: step: 1293, loss: 0.6940774321556091, acc: 0.39, recall: 0.39, precision: 0.3896025692492975, f_beta: 0.3894505054549094
train: step: 1294, loss: 0.6938236355781555, acc: 0.44, recall: 0.43999999999999995, precision: 0.43489583333333337, f_beta: 0.4288045695634435
train: step: 1295, loss: 0.6918854713439941, acc: 0.51, recall: 0.51, precision: 0.7525252525252526, f_beta: 0.3551783129359126
train: step: 1296, loss: 0.6930980682373047, acc: 0.45, recall: 0.45, precision: 0.44832575444398515, f_beta: 0.44550861982054646
train: step: 1297, loss: 0.6922361254692078, acc: 0.51, recall: 0.51, precision: 0.51010101010101, f_beta: 0.5087719298245614
train: step: 1298, loss: 0.6921930909156799, acc: 0.57, recall: 0.5700000000000001, precision: 0.5818139317438056, f_beta: 0.5538956323270049
train: step: 1299, loss: 0.6930721402168274, acc: 0.49, recall: 0.49, precision: 0.48901098901098905, f_beta: 0.47826086956521735
train: step: 1300, loss: 0.6929025053977966, acc: 0.48, recall: 0.48, precision: 0.47996794871794873, f_beta: 0.4797919167667067
train: step: 1301, loss: 0.6941626667976379, acc: 0.38, recall: 0.38, precision: 0.37684729064039413, f_beta: 0.37600644122383253
train: step: 1302, loss: 0.6924688816070557, acc: 0.54, recall: 0.54, precision: 0.5445632798573975, f_beta: 0.5279146141215106
train: step: 1303, loss: 0.6935144066810608, acc: 0.45, recall: 0.45, precision: 0.4246534056660639, f_beta: 0.39949776176438473
train: step: 1304, loss: 0.692355215549469, acc: 0.57, recall: 0.5700000000000001, precision: 0.5702529104777199, f_beta: 0.5696126513862476
train: step: 1305, loss: 0.692240297794342, acc: 0.52, recall: 0.52, precision: 0.5217013888888888, f_beta: 0.5104039167686658
train: step: 1306, loss: 0.6945604085922241, acc: 0.41, recall: 0.41000000000000003, precision: 0.4099639855942377, f_beta: 0.4099409940994099
train: step: 1307, loss: 0.6930882334709167, acc: 0.44, recall: 0.44, precision: 0.43842364532019706, f_beta: 0.4363929146537842
train: step: 1308, loss: 0.6932494640350342, acc: 0.5, recall: 0.5, precision: 0.5, f_beta: 0.49000407996736023
train: step: 1309, loss: 0.6932117938995361, acc: 0.52, recall: 0.52, precision: 0.5208333333333334, f_beta: 0.5151515151515151
train: step: 1310, loss: 0.6931498050689697, acc: 0.52, recall: 0.52, precision: 0.5217013888888888, f_beta: 0.5104039167686658
train: step: 1311, loss: 0.6933783888816833, acc: 0.53, recall: 0.53, precision: 0.5315258511979823, f_beta: 0.5242433444680635
train: step: 1312, loss: 0.69126296043396, acc: 0.56, recall: 0.5599999999999999, precision: 0.5822368421052632, f_beta: 0.528099528099528
train: step: 1313, loss: 0.6931843757629395, acc: 0.49, recall: 0.49, precision: 0.4899959983993597, f_beta: 0.48994899489948995
train: step: 1314, loss: 0.6916096210479736, acc: 0.6, recall: 0.6, precision: 0.6085069444444444, f_beta: 0.5920032639738881
train: step: 1315, loss: 0.6927595734596252, acc: 0.49, recall: 0.49, precision: 0.48831229546517063, f_beta: 0.47089947089947093
train: step: 1316, loss: 0.6922034621238708, acc: 0.52, recall: 0.52, precision: 0.5205254515599343, f_beta: 0.5169082125603865
train: step: 1317, loss: 0.6931362748146057, acc: 0.51, recall: 0.51, precision: 0.5100361300682457, f_beta: 0.5095586027424682
train: step: 1318, loss: 0.6937596201896667, acc: 0.47, recall: 0.47, precision: 0.4699879951980792, f_beta: 0.46994699469946993
train: step: 1319, loss: 0.6930057406425476, acc: 0.49, recall: 0.49, precision: 0.47789566755084, f_beta: 0.40910670837678137
train: step: 1320, loss: 0.6919606924057007, acc: 0.58, recall: 0.5800000000000001, precision: 0.5833333333333333, f_beta: 0.5757575757575757
train: step: 1321, loss: 0.6930596828460693, acc: 0.49, recall: 0.49, precision: 0.4898000815993472, f_beta: 0.48748869460355737
train: step: 1322, loss: 0.6918560862541199, acc: 0.55, recall: 0.55, precision: 0.6105216622458002, f_beta: 0.4786235662148072
train: step: 1323, loss: 0.6931332349777222, acc: 0.53, recall: 0.53, precision: 0.5321750321750323, f_beta: 0.5219204557013528
train: step: 1324, loss: 0.6927464008331299, acc: 0.54, recall: 0.54, precision: 0.5402576489533011, f_beta: 0.5392628205128205
train: step: 1325, loss: 0.6936008334159851, acc: 0.49, recall: 0.49, precision: 0.4837556855100714, f_beta: 0.4357782940590773
train: step: 1326, loss: 0.6941487193107605, acc: 0.47, recall: 0.47000000000000003, precision: 0.45126705653021437, f_beta: 0.4136519526496294
train: step: 1327, loss: 0.6923854351043701, acc: 0.57, recall: 0.5700000000000001, precision: 0.5791497060153776, f_beta: 0.557203171661003
train: step: 1328, loss: 0.6959441900253296, acc: 0.36, recall: 0.36, precision: 0.20930232558139536, f_beta: 0.2647058823529412
train: step: 1329, loss: 0.6925905346870422, acc: 0.57, recall: 0.5700000000000001, precision: 0.5700280112044818, f_beta: 0.56995699569957
train: step: 1330, loss: 0.693371593952179, acc: 0.47, recall: 0.47, precision: 0.46847414880201765, f_beta: 0.46350845227249715
train: step: 1331, loss: 0.6931149363517761, acc: 0.54, recall: 0.54, precision: 0.5416666666666667, f_beta: 0.5353535353535354
train: step: 1332, loss: 0.6933081746101379, acc: 0.49, recall: 0.49, precision: 0.48831229546517063, f_beta: 0.47089947089947093
train: step: 1333, loss: 0.6926016807556152, acc: 0.51, recall: 0.51, precision: 0.5121418164157359, f_beta: 0.4873940788785438
train: step: 1334, loss: 0.6908661723136902, acc: 0.55, recall: 0.55, precision: 0.5705815923207227, f_beta: 0.5146154675870995
train: step: 1335, loss: 0.6929411888122559, acc: 0.5, recall: 0.5, precision: 0.5, f_beta: 0.4981934965877158
train: step: 1336, loss: 0.6940711140632629, acc: 0.52, recall: 0.52, precision: 0.5217013888888888, f_beta: 0.5104039167686658
train: step: 1337, loss: 0.6917495727539062, acc: 0.46, recall: 0.46, precision: 0.4375, f_beta: 0.4065934065934066
train: step: 1338, loss: 0.6928594708442688, acc: 0.5, recall: 0.5, precision: 0.5, f_beta: 0.46374946374946374
train: step: 1339, loss: 0.6935372948646545, acc: 0.47, recall: 0.47000000000000003, precision: 0.4689954526663911, f_beta: 0.46567194273616297
train: step: 1340, loss: 0.6922933459281921, acc: 0.47, recall: 0.47, precision: 0.46194824961948244, f_beta: 0.4403970013726111
train: step: 1341, loss: 0.6928892731666565, acc: 0.51, recall: 0.51, precision: 0.5100040016006402, f_beta: 0.50995099509951
train: step: 1342, loss: 0.6924459934234619, acc: 0.48, recall: 0.48, precision: 0.46875, f_beta: 0.4285714285714286
train: step: 1343, loss: 0.6898214221000671, acc: 0.59, recall: 0.5900000000000001, precision: 0.6594613749114103, f_beta: 0.539894512400404
train: step: 1344, loss: 0.6906030178070068, acc: 0.52, recall: 0.52, precision: 0.5372023809523809, f_beta: 0.4572591587516961
train: step: 1345, loss: 0.692872166633606, acc: 0.5, recall: 0.5, precision: 0.5, f_beta: 0.4981934965877158
train: step: 1346, loss: 0.6954986453056335, acc: 0.35, recall: 0.35000000000000003, precision: 0.3246844319775596, f_beta: 0.32565618840128646
train: step: 1347, loss: 0.693033754825592, acc: 0.49, recall: 0.49, precision: 0.48927498927498925, f_beta: 0.48123283490997865
train: step: 1348, loss: 0.692889928817749, acc: 0.52, recall: 0.52, precision: 0.5201288244766505, f_beta: 0.5192307692307692
train: step: 1349, loss: 0.6930555105209351, acc: 0.48, recall: 0.48, precision: 0.4782986111111111, f_beta: 0.46960424316605465
train: step: 1350, loss: 0.6925243139266968, acc: 0.56, recall: 0.56, precision: 0.5714285714285714, f_beta: 0.5416666666666666
train: step: 1351, loss: 0.6938208937644958, acc: 0.47, recall: 0.47, precision: 0.45765104460756634, f_beta: 0.42832488404702834
train: step: 1352, loss: 0.6937549710273743, acc: 0.44, recall: 0.44, precision: 0.43990384615384615, f_beta: 0.4397759103641457
train: step: 1353, loss: 0.6933793425559998, acc: 0.51, recall: 0.51, precision: 0.5126839167935058, f_beta: 0.48263118994826315
train: step: 1354, loss: 0.6917725205421448, acc: 0.58, recall: 0.5800000000000001, precision: 0.5801282051282051, f_beta: 0.5798319327731092
train: step: 1355, loss: 0.6935241222381592, acc: 0.49, recall: 0.49, precision: 0.48966515088879703, f_beta: 0.48583526565177937
train: step: 1356, loss: 0.6915833950042725, acc: 0.49, recall: 0.49, precision: 0.4803921568627451, f_beta: 0.4188034188034188
train: step: 1357, loss: 0.6937095522880554, acc: 0.45, recall: 0.45, precision: 0.44832575444398515, f_beta: 0.44550861982054646
train: step: 1358, loss: 0.6925735473632812, acc: 0.53, recall: 0.53, precision: 0.5364254492472074, f_beta: 0.5083167695365624
train: step: 1359, loss: 0.6938909888267517, acc: 0.54, recall: 0.54, precision: 0.5410509031198687, f_beta: 0.537037037037037
train: step: 1360, loss: 0.6927263736724854, acc: 0.53, recall: 0.53, precision: 0.530108390204737, f_beta: 0.5295766189570613
train: step: 1361, loss: 0.692922055721283, acc: 0.47, recall: 0.47000000000000003, precision: 0.4694002447980416, f_beta: 0.467390212038991
train: step: 1362, loss: 0.6928713917732239, acc: 0.53, recall: 0.53, precision: 0.533921302578019, f_beta: 0.5160127690248172
train: step: 1363, loss: 0.6931654214859009, acc: 0.55, recall: 0.55, precision: 0.5525430853299707, f_beta: 0.5444883085332524
train: step: 1364, loss: 0.6935538649559021, acc: 0.48, recall: 0.48, precision: 0.48, f_beta: 0.48
train: step: 1365, loss: 0.693505048751831, acc: 0.5, recall: 0.5, precision: 0.5, f_beta: 0.47456914670029426
train: step: 1366, loss: 0.6929180026054382, acc: 0.53, recall: 0.53, precision: 0.5300120048019208, f_beta: 0.5299529952995299
train: step: 1367, loss: 0.6901625990867615, acc: 0.6, recall: 0.6, precision: 0.6456876456876457, f_beta: 0.5659722222222222
train: step: 1368, loss: 0.6928912401199341, acc: 0.5, recall: 0.5, precision: 0.5, f_beta: 0.49000407996736023
train: step: 1369, loss: 0.6934698224067688, acc: 0.44, recall: 0.44, precision: 0.43961352657004826, f_beta: 0.4391025641025641
train: step: 1370, loss: 0.6930274963378906, acc: 0.47, recall: 0.47000000000000003, precision: 0.4468462083628632, f_beta: 0.4052294916395466
train: step: 1371, loss: 0.6933651566505432, acc: 0.46, recall: 0.46, precision: 0.4594155844155844, f_beta: 0.45804897631473307
train: step: 1372, loss: 0.6935455799102783, acc: 0.5, recall: 0.5, precision: 0.5, f_beta: 0.4967793880837359
train: step: 1373, loss: 0.6938017010688782, acc: 0.48, recall: 0.48, precision: 0.47702205882352944, f_beta: 0.46258784621744525
train: step: 1374, loss: 0.692923367023468, acc: 0.51, recall: 0.51, precision: 0.510334849111203, f_beta: 0.5059985885673959
train: step: 1375, loss: 0.6928145885467529, acc: 0.57, recall: 0.5700000000000001, precision: 0.5702529104777199, f_beta: 0.5696126513862476
train: step: 1376, loss: 0.692992091178894, acc: 0.56, recall: 0.56, precision: 0.5603864734299517, f_beta: 0.5592948717948718
train: step: 1377, loss: 0.6930133104324341, acc: 0.52, recall: 0.52, precision: 0.5200320512820513, f_beta: 0.5198079231692677
train: step: 1378, loss: 0.6921716332435608, acc: 0.5, recall: 0.5, precision: 0.5, f_beta: 0.43464495703301675
train: step: 1379, loss: 0.6934200525283813, acc: 0.5, recall: 0.5, precision: 0.5, f_beta: 0.46374946374946374
train: step: 1380, loss: 0.6932138800621033, acc: 0.49, recall: 0.49, precision: 0.4873160832064941, f_beta: 0.4615140956604371
train: step: 1381, loss: 0.6939148902893066, acc: 0.44, recall: 0.44000000000000006, precision: 0.4363327674023769, f_beta: 0.43181818181818177
train: step: 1382, loss: 0.6921787858009338, acc: 0.56, recall: 0.56, precision: 0.5689338235294117, f_beta: 0.545266639107069
train: step: 1383, loss: 0.6929869651794434, acc: 0.49, recall: 0.49, precision: 0.48996386993175434, f_beta: 0.4895405865278751
train: step: 1384, loss: 0.6944826245307922, acc: 0.42, recall: 0.42, precision: 0.40476190476190477, f_beta: 0.39583333333333337
train: step: 1385, loss: 0.6923784017562866, acc: 0.49, recall: 0.49, precision: 0.4899959983993597, f_beta: 0.48994899489948995
train: step: 1386, loss: 0.6930384635925293, acc: 0.53, recall: 0.53, precision: 0.5300120048019208, f_beta: 0.5299529952995299
train: step: 1387, loss: 0.6924206018447876, acc: 0.58, recall: 0.5800000000000001, precision: 0.5833333333333333, f_beta: 0.5757575757575757
train: step: 1388, loss: 0.6925960779190063, acc: 0.53, recall: 0.53, precision: 0.5310045473336089, f_beta: 0.5261619114830125
train: step: 1389, loss: 0.6921433210372925, acc: 0.49, recall: 0.49, precision: 0.48831229546517063, f_beta: 0.47089947089947093
train: step: 1390, loss: 0.6930932402610779, acc: 0.51, recall: 0.51, precision: 0.5100040016006402, f_beta: 0.50995099509951
train: step: 1391, loss: 0.6928694248199463, acc: 0.53, recall: 0.53, precision: 0.5310045473336089, f_beta: 0.5261619114830125
train: step: 1392, loss: 0.6940762996673584, acc: 0.39, recall: 0.39, precision: 0.38631665977676727, f_beta: 0.385018651073697
train: step: 1393, loss: 0.6933352947235107, acc: 0.5, recall: 0.5, precision: 0.5, f_beta: 0.4967793880837359
train: step: 1394, loss: 0.6932686567306519, acc: 0.48, recall: 0.48, precision: 0.47970779220779225, f_beta: 0.4781212364512244
train: step: 1395, loss: 0.6934491991996765, acc: 0.47, recall: 0.47000000000000003, precision: 0.4689954526663911, f_beta: 0.46567194273616297
train: step: 1396, loss: 0.6924661993980408, acc: 0.54, recall: 0.54, precision: 0.5410509031198687, f_beta: 0.537037037037037
train: step: 1397, loss: 0.6924395561218262, acc: 0.58, recall: 0.5800000000000001, precision: 0.5833333333333333, f_beta: 0.5757575757575757
train: step: 1398, loss: 0.6930267810821533, acc: 0.51, recall: 0.51, precision: 0.5100040016006402, f_beta: 0.50995099509951
train: step: 1399, loss: 0.6922146081924438, acc: 0.6, recall: 0.6, precision: 0.6001602564102564, f_beta: 0.5998399359743898
train: step: 1400, loss: 0.6927425265312195, acc: 0.59, recall: 0.59, precision: 0.5989010989010989, f_beta: 0.5805626598465473
train: step: 1401, loss: 0.6935346722602844, acc: 0.47, recall: 0.47, precision: 0.4699879951980792, f_beta: 0.46994699469946993
train: step: 1402, loss: 0.6926617622375488, acc: 0.54, recall: 0.54, precision: 0.5410509031198687, f_beta: 0.537037037037037
train: step: 1403, loss: 0.6927188634872437, acc: 0.52, recall: 0.52, precision: 0.5415282392026578, f_beta: 0.4485294117647059
train: step: 1404, loss: 0.6926313042640686, acc: 0.54, recall: 0.54, precision: 0.5410509031198687, f_beta: 0.537037037037037
train: step: 1405, loss: 0.6931681036949158, acc: 0.45, recall: 0.45000000000000007, precision: 0.44745691467002946, f_beta: 0.44326348820730843
train: step: 1406, loss: 0.6930321455001831, acc: 0.45, recall: 0.45, precision: 0.4494949494949495, f_beta: 0.4486215538847118
train: step: 1407, loss: 0.6940352916717529, acc: 0.47, recall: 0.47, precision: 0.46194824961948244, f_beta: 0.4403970013726111
train: step: 1408, loss: 0.6920682787895203, acc: 0.57, recall: 0.57, precision: 0.575075075075075, f_beta: 0.562608076492727
train: step: 1409, loss: 0.6945703625679016, acc: 0.43, recall: 0.43, precision: 0.4150072850898494, f_beta: 0.40370331624646927
train: step: 1410, loss: 0.6930015683174133, acc: 0.49, recall: 0.49, precision: 0.4898989898989899, f_beta: 0.48872180451127817
train: step: 1411, loss: 0.6938985586166382, acc: 0.48, recall: 0.48, precision: 0.4787775891341256, f_beta: 0.4724025974025974
train: step: 1412, loss: 0.6949162483215332, acc: 0.45, recall: 0.45, precision: 0.4246534056660639, f_beta: 0.39949776176438473
train: step: 1413, loss: 0.6934006214141846, acc: 0.49, recall: 0.49, precision: 0.4898989898989899, f_beta: 0.48872180451127817
train: step: 1414, loss: 0.6928402781486511, acc: 0.49, recall: 0.49, precision: 0.48228206945428775, f_beta: 0.4276736617663562
train: step: 1415, loss: 0.6935918927192688, acc: 0.45, recall: 0.45, precision: 0.4415614773258532, f_beta: 0.4294013901857039
train: step: 1416, loss: 0.6930733323097229, acc: 0.51, recall: 0.51, precision: 0.5100040016006402, f_beta: 0.50995099509951
train: step: 1417, loss: 0.6933745741844177, acc: 0.47, recall: 0.47, precision: 0.466078697421981, f_beta: 0.4542271650705386
train: step: 1418, loss: 0.6928320527076721, acc: 0.54, recall: 0.54, precision: 0.5410509031198687, f_beta: 0.537037037037037
train: step: 1419, loss: 0.6926496028900146, acc: 0.52, recall: 0.52, precision: 0.5222816399286987, f_beta: 0.5073891625615763
train: step: 1420, loss: 0.6914764642715454, acc: 0.51, recall: 0.51, precision: 0.5141163184641445, f_beta: 0.47147017581706396
train: step: 1421, loss: 0.6905623078346252, acc: 0.51, recall: 0.51, precision: 0.5526315789473684, f_beta: 0.38557993730407525
train: step: 1422, loss: 0.693187952041626, acc: 0.49, recall: 0.49, precision: 0.48996386993175434, f_beta: 0.4895405865278751
train: step: 1423, loss: 0.6938258409500122, acc: 0.44, recall: 0.44, precision: 0.43912337662337664, f_beta: 0.43797671617824174
train: step: 1424, loss: 0.6928017139434814, acc: 0.53, recall: 0.53, precision: 0.5487329434697856, f_beta: 0.4800309768779732
train: step: 1425, loss: 0.6927894353866577, acc: 0.54, recall: 0.54, precision: 0.5400641025641025, f_beta: 0.5398159263705482
train: step: 1426, loss: 0.691853404045105, acc: 0.61, recall: 0.61, precision: 0.6100440176070427, f_beta: 0.60996099609961
train: step: 1427, loss: 0.6927193999290466, acc: 0.5, recall: 0.5, precision: 0.5, f_beta: 0.4574652777777778
train: step: 1428, loss: 0.6933315992355347, acc: 0.49, recall: 0.49, precision: 0.4898989898989899, f_beta: 0.48872180451127817
train: step: 1429, loss: 0.6931653618812561, acc: 0.51, recall: 0.51, precision: 0.5100040016006402, f_beta: 0.50995099509951
train: step: 1430, loss: 0.6924984455108643, acc: 0.59, recall: 0.5900000000000001, precision: 0.5909090909090908, f_beta: 0.5889724310776943
train: step: 1431, loss: 0.6929173469543457, acc: 0.54, recall: 0.54, precision: 0.5405844155844155, f_beta: 0.5383380168606985
train: step: 1432, loss: 0.6929389238357544, acc: 0.52, recall: 0.52, precision: 0.5201288244766505, f_beta: 0.5192307692307692
train: step: 1433, loss: 0.6927013993263245, acc: 0.51, recall: 0.51, precision: 0.510989010989011, f_beta: 0.4987212276214833
train: step: 1434, loss: 0.6932384371757507, acc: 0.51, recall: 0.51, precision: 0.5305250305250305, f_beta: 0.4109868974636375
train: step: 1435, loss: 0.6931899189949036, acc: 0.55, recall: 0.55, precision: 0.5525430853299707, f_beta: 0.5444883085332524
train: step: 1436, loss: 0.6931902170181274, acc: 0.52, recall: 0.52, precision: 0.5208333333333334, f_beta: 0.5151515151515151
train: step: 1437, loss: 0.6925312876701355, acc: 0.54, recall: 0.54, precision: 0.5459558823529411, f_beta: 0.524596940884663
train: step: 1438, loss: 0.6932951211929321, acc: 0.51, recall: 0.51, precision: 0.5133333333333334, f_beta: 0.4773333333333334
train: step: 1439, loss: 0.693701982498169, acc: 0.56, recall: 0.56, precision: 0.59375, f_beta: 0.5164835164835164
train: step: 1440, loss: 0.6925210356712341, acc: 0.53, recall: 0.53, precision: 0.5364254492472074, f_beta: 0.5083167695365624
train: step: 1441, loss: 0.6907109022140503, acc: 0.54, recall: 0.54, precision: 0.5625, f_beta: 0.4945054945054945
train: step: 1442, loss: 0.6936348676681519, acc: 0.47, recall: 0.47, precision: 0.46703296703296704, f_beta: 0.4578005115089514
train: step: 1443, loss: 0.692677915096283, acc: 0.55, recall: 0.55, precision: 0.5500200080032013, f_beta: 0.54995499549955
train: step: 1444, loss: 0.6923694610595703, acc: 0.54, recall: 0.54, precision: 0.5410509031198687, f_beta: 0.537037037037037
train: step: 1445, loss: 0.6928425431251526, acc: 0.5, recall: 0.5, precision: 0.5, f_beta: 0.49919871794871795
train: step: 1446, loss: 0.6933538317680359, acc: 0.53, recall: 0.53, precision: 0.5423489553924337, f_beta: 0.49304282170208175
train: step: 1447, loss: 0.6914314031600952, acc: 0.62, recall: 0.62, precision: 0.62, f_beta: 0.62
train: step: 1448, loss: 0.6926645636558533, acc: 0.51, recall: 0.51, precision: 0.5101999184006528, f_beta: 0.5075871771681238
train: step: 1449, loss: 0.6901904940605164, acc: 0.57, recall: 0.5700000000000001, precision: 0.5887874175545409, f_beta: 0.5459824728117411
train: step: 1450, loss: 0.6918076872825623, acc: 0.43, recall: 0.43, precision: 0.3862897985705003, f_beta: 0.36939926983073346
train: step: 1451, loss: 0.6917153000831604, acc: 0.66, recall: 0.6599999999999999, precision: 0.6623376623376623, f_beta: 0.6587715776796468
train: step: 1452, loss: 0.6931427717208862, acc: 0.5, recall: 0.5, precision: 0.5, f_beta: 0.415614773258532
train: step: 1453, loss: 0.6924704909324646, acc: 0.51, recall: 0.51, precision: 0.5100361300682457, f_beta: 0.5095586027424682
train: step: 1454, loss: 0.6938303112983704, acc: 0.36, recall: 0.36, precision: 0.35909822866344604, f_beta: 0.3589743589743589
train: step: 1455, loss: 0.6934601664543152, acc: 0.5, recall: 0.5, precision: 0.5, f_beta: 0.47916666666666663
train: step: 1456, loss: 0.6930525302886963, acc: 0.52, recall: 0.52, precision: 0.5201288244766505, f_beta: 0.5192307692307692
train: step: 1457, loss: 0.6915591359138489, acc: 0.56, recall: 0.5599999999999999, precision: 0.6420454545454546, f_beta: 0.4857410004675081
train: step: 1458, loss: 0.6933650374412537, acc: 0.5, recall: 0.5, precision: 0.5, f_beta: 0.47916666666666663
train: step: 1459, loss: 0.6927253603935242, acc: 0.53, recall: 0.53, precision: 0.5321750321750323, f_beta: 0.5219204557013528
train: step: 1460, loss: 0.6931535601615906, acc: 0.48, recall: 0.48, precision: 0.47996794871794873, f_beta: 0.4797919167667067
train: step: 1461, loss: 0.6932880282402039, acc: 0.47, recall: 0.47000000000000003, precision: 0.4689954526663911, f_beta: 0.46567194273616297
train: step: 1462, loss: 0.6932361721992493, acc: 0.47, recall: 0.47000000000000003, precision: 0.4689954526663911, f_beta: 0.46567194273616297
train: step: 1463, loss: 0.6929428577423096, acc: 0.56, recall: 0.56, precision: 0.56, f_beta: 0.56
train: step: 1464, loss: 0.6887422204017639, acc: 0.59, recall: 0.5900000000000001, precision: 0.6092763477416221, f_beta: 0.5710848415106183
train: step: 1465, loss: 0.6913239359855652, acc: 0.5, recall: 0.5, precision: 0.5, f_beta: 0.37996031746031744
train: step: 1466, loss: 0.6878328919410706, acc: 0.53, recall: 0.53, precision: 0.7577319587628866, f_beta: 0.39673982800667434
train: step: 1467, loss: 0.6914373636245728, acc: 0.5, recall: 0.5, precision: 0.5, f_beta: 0.46943972835314096
train: step: 1468, loss: 0.693888783454895, acc: 0.47, recall: 0.47, precision: 0.4699879951980792, f_beta: 0.46994699469946993
train: step: 1469, loss: 0.6920450329780579, acc: 0.56, recall: 0.56, precision: 0.6016260162601625, f_beta: 0.5098039215686274
train: step: 1470, loss: 0.6937859058380127, acc: 0.51, recall: 0.51, precision: 0.51010101010101, f_beta: 0.5087719298245614
train: step: 1471, loss: 0.6934100985527039, acc: 0.47, recall: 0.47, precision: 0.4696969696969697, f_beta: 0.468671679197995
train: step: 1472, loss: 0.6895027160644531, acc: 0.59, recall: 0.59, precision: 0.6141552511415524, f_beta: 0.5670995670995671
train: step: 1473, loss: 0.696779191493988, acc: 0.49, recall: 0.49, precision: 0.48831229546517063, f_beta: 0.47089947089947093
train: step: 1474, loss: 0.6929340958595276, acc: 0.5, recall: 0.5, precision: 0.5, f_beta: 0.49919871794871795
train: step: 1475, loss: 0.6925882697105408, acc: 0.54, recall: 0.54, precision: 0.5410509031198687, f_beta: 0.537037037037037
train: step: 1476, loss: 0.6923658847808838, acc: 0.52, recall: 0.52, precision: 0.5372023809523809, f_beta: 0.4572591587516961
train: step: 1477, loss: 0.7058955430984497, acc: 0.48, recall: 0.48, precision: 0.45265151515151514, f_beta: 0.3922393641888733
train: step: 1478, loss: 0.6790152788162231, acc: 0.59, recall: 0.59, precision: 0.635623869801085, f_beta: 0.5523528769516323
train: step: 1479, loss: 0.6921853423118591, acc: 0.52, recall: 0.52, precision: 0.5212224108658743, f_beta: 0.512987012987013
train: step: 1480, loss: 0.692363977432251, acc: 0.55, recall: 0.55, precision: 0.5525430853299707, f_beta: 0.5444883085332524
train: step: 1481, loss: 0.6894024610519409, acc: 0.54, recall: 0.54, precision: 0.5476190476190477, f_beta: 0.5208333333333333
train: step: 1482, loss: 0.6946613192558289, acc: 0.47, recall: 0.47000000000000003, precision: 0.4468462083628632, f_beta: 0.4052294916395466
train: step: 1483, loss: 0.6932635307312012, acc: 0.48, recall: 0.48, precision: 0.4584717607973422, f_beta: 0.4025735294117647
train: step: 1484, loss: 0.6770625114440918, acc: 0.57, recall: 0.57, precision: 0.7136752136752136, f_beta: 0.483110950835437
train: step: 1485, loss: 0.6942251324653625, acc: 0.44, recall: 0.44, precision: 0.43990384615384615, f_beta: 0.4397759103641457
train: step: 1486, loss: 0.6941254138946533, acc: 0.39, recall: 0.39, precision: 0.3878008975928192, f_beta: 0.38699628178072554
train: step: 1487, loss: 0.6909019351005554, acc: 0.47, recall: 0.47, precision: 0.4336870026525199, f_beta: 0.3859344224307728
train: step: 1488, loss: 0.6846469044685364, acc: 0.53, recall: 0.53, precision: 0.533921302578019, f_beta: 0.5160127690248172
train: step: 1489, loss: 0.6908948421478271, acc: 0.5, recall: 0.5, precision: 0.5, f_beta: 0.486863711001642
train: step: 1490, loss: 0.6960468292236328, acc: 0.49, recall: 0.49, precision: 0.48901098901098905, f_beta: 0.47826086956521735
train: step: 1491, loss: 0.6923715472221375, acc: 0.56, recall: 0.56, precision: 0.5615763546798029, f_beta: 0.5571658615136876
train: step: 1492, loss: 0.7010780572891235, acc: 0.55, recall: 0.55, precision: 0.5634195839675291, f_beta: 0.5248653785239151
train: step: 1493, loss: 0.6942447423934937, acc: 0.52, recall: 0.52, precision: 0.525987525987526, f_beta: 0.49066213921901536
train: step: 1494, loss: 0.6930487155914307, acc: 0.51, recall: 0.51, precision: 0.510334849111203, f_beta: 0.5059985885673959
train: step: 1495, loss: 0.693461537361145, acc: 0.49, recall: 0.49, precision: 0.48869289914066033, f_beta: 0.4748223663886314
train: step: 1496, loss: 0.6908406019210815, acc: 0.55, recall: 0.5499999999999999, precision: 0.6276813074565883, f_beta: 0.4692770373864842
train: step: 1497, loss: 0.6933587789535522, acc: 0.5, recall: 0.5, precision: 0.5, f_beta: 0.4967793880837359
train: step: 1498, loss: 0.682650089263916, acc: 0.54, recall: 0.54, precision: 0.6358695652173914, f_beta: 0.4414764448761534
train: step: 1499, loss: 0.6857894062995911, acc: 0.55, recall: 0.5499999999999999, precision: 0.6276813074565883, f_beta: 0.4692770373864842
train: step: 1500, loss: 0.6858322620391846, acc: 0.56, recall: 0.56, precision: 0.6245847176079734, f_beta: 0.4944852941176471
train: step: 1501, loss: 0.6930939555168152, acc: 0.52, recall: 0.52, precision: 0.5201288244766505, f_beta: 0.5192307692307692
train: step: 1502, loss: 0.6897479295730591, acc: 0.51, recall: 0.51, precision: 0.5305250305250305, f_beta: 0.4109868974636375
train: step: 1503, loss: 0.6892319321632385, acc: 0.52, recall: 0.52, precision: 0.5208333333333334, f_beta: 0.5151515151515151
train: step: 1504, loss: 0.6906483173370361, acc: 0.52, recall: 0.52, precision: 0.5886524822695035, f_beta: 0.40476190476190477
train: step: 1505, loss: 0.6925041079521179, acc: 0.59, recall: 0.5900000000000001, precision: 0.5903251706142112, f_beta: 0.5896306676008407
train: step: 1506, loss: 0.6924669146537781, acc: 0.54, recall: 0.54, precision: 0.54, f_beta: 0.54
train: step: 1507, loss: 0.6918584704399109, acc: 0.61, recall: 0.6100000000000001, precision: 0.6111111111111112, f_beta: 0.6090225563909775
train: step: 1508, loss: 0.6981402635574341, acc: 0.46, recall: 0.46, precision: 0.4540441176470588, f_beta: 0.44191814799503926
train: step: 1509, loss: 0.6951343417167664, acc: 0.49, recall: 0.49, precision: 0.4849306811332128, f_beta: 0.44317065181788406
train: step: 1510, loss: 0.6948354840278625, acc: 0.49, recall: 0.49, precision: 0.4803921568627451, f_beta: 0.4188034188034188
train: step: 1511, loss: 0.6936630010604858, acc: 0.47, recall: 0.47000000000000003, precision: 0.4694002447980416, f_beta: 0.467390212038991
train: step: 1512, loss: 0.6918571591377258, acc: 0.51, recall: 0.51, precision: 0.5526315789473684, f_beta: 0.38557993730407525
train: step: 1513, loss: 0.6932786703109741, acc: 0.51, recall: 0.51, precision: 0.5100361300682457, f_beta: 0.5095586027424682
train: step: 1514, loss: 0.6939151287078857, acc: 0.47, recall: 0.47, precision: 0.466078697421981, f_beta: 0.4542271650705386
train: step: 1515, loss: 0.6932043433189392, acc: 0.45, recall: 0.44999999999999996, precision: 0.44637494637494635, f_beta: 0.44054521411860437
train: step: 1516, loss: 0.6881890892982483, acc: 0.57, recall: 0.5700000000000001, precision: 0.6137102014294997, f_beta: 0.5242836596968691
train: step: 1517, loss: 0.6835611462593079, acc: 0.56, recall: 0.56, precision: 0.6116071428571428, f_beta: 0.5024875621890548
train: step: 1518, loss: 0.7074080109596252, acc: 0.42, recall: 0.42, precision: 0.40476190476190477, f_beta: 0.39583333333333337
train: step: 1519, loss: 0.6849066019058228, acc: 0.56, recall: 0.5599999999999999, precision: 0.5822368421052632, f_beta: 0.528099528099528
train: step: 1520, loss: 0.6925694942474365, acc: 0.54, recall: 0.54, precision: 0.5434027777777778, f_beta: 0.5308037535699714
train: step: 1521, loss: 0.6916250586509705, acc: 0.51, recall: 0.51, precision: 0.5150693188667872, f_beta: 0.4650070968446337
train: step: 1522, loss: 0.6922253370285034, acc: 0.52, recall: 0.52, precision: 0.5238095238095237, f_beta: 0.5
train: step: 1523, loss: 0.6895533800125122, acc: 0.51, recall: 0.51, precision: 0.51010101010101, f_beta: 0.5087719298245614
train: step: 1524, loss: 0.6933490037918091, acc: 0.5, recall: 0.5, precision: 0.5, f_beta: 0.415614773258532
train: step: 1525, loss: 0.693240225315094, acc: 0.54, recall: 0.54, precision: 0.5400641025641025, f_beta: 0.5398159263705482
train: step: 1526, loss: 0.6927024722099304, acc: 0.51, recall: 0.51, precision: 0.5221043324491601, f_beta: 0.43227899432278993
train: step: 1527, loss: 0.6948432326316833, acc: 0.53, recall: 0.53, precision: 0.54, f_beta: 0.4986666666666667
train: step: 1528, loss: 0.6908496618270874, acc: 0.53, recall: 0.53, precision: 0.54, f_beta: 0.4986666666666667
train: step: 1529, loss: 0.6920876502990723, acc: 0.5, recall: 0.5, precision: 0.5, f_beta: 0.46374946374946374
train: step: 1530, loss: 0.6815701127052307, acc: 0.57, recall: 0.57, precision: 0.6787538304392238, f_beta: 0.4928647246137516
train: step: 1531, loss: 0.6917752027511597, acc: 0.54, recall: 0.54, precision: 0.5476190476190477, f_beta: 0.5208333333333333
train: step: 1532, loss: 0.687084436416626, acc: 0.6, recall: 0.6, precision: 0.6456876456876457, f_beta: 0.5659722222222222
train: step: 1533, loss: 0.6835029721260071, acc: 0.56, recall: 0.5599999999999999, precision: 0.6666666666666667, f_beta: 0.4761904761904763
train: step: 1534, loss: 0.6930835247039795, acc: 0.55, recall: 0.55, precision: 0.5505050505050505, f_beta: 0.5488721804511277
train: step: 1535, loss: 0.6930294036865234, acc: 0.51, recall: 0.51, precision: 0.5116877045348294, f_beta: 0.4916485112563544
train: step: 1536, loss: 0.6936226487159729, acc: 0.48, recall: 0.48000000000000004, precision: 0.47947454844006565, f_beta: 0.47665056360708535
train: step: 1537, loss: 0.6860257983207703, acc: 0.56, recall: 0.56, precision: 0.5744047619047619, f_beta: 0.537620849096259
train: step: 1538, loss: 0.6933857798576355, acc: 0.48, recall: 0.48, precision: 0.47916666666666663, f_beta: 0.4747474747474748
train: step: 1539, loss: 0.6877483129501343, acc: 0.5, recall: 0.5, precision: 0.5, f_beta: 0.46374946374946374
train: step: 1540, loss: 0.6982811093330383, acc: 0.46, recall: 0.45999999999999996, precision: 0.448024948024948, f_beta: 0.42699490662139217
train: step: 1541, loss: 0.6938705444335938, acc: 0.44, recall: 0.44, precision: 0.43990384615384615, f_beta: 0.4397759103641457
train: step: 1542, loss: 0.6946241855621338, acc: 0.44, recall: 0.44000000000000006, precision: 0.4375, f_beta: 0.43434343434343436
train: step: 1543, loss: 0.6926267147064209, acc: 0.57, recall: 0.5700000000000001, precision: 0.5702529104777199, f_beta: 0.5696126513862476
train: step: 1544, loss: 0.6930550336837769, acc: 0.46, recall: 0.45999999999999996, precision: 0.4565972222222222, f_beta: 0.44920440636474906
train: step: 1545, loss: 0.6935144066810608, acc: 0.48, recall: 0.48, precision: 0.48, f_beta: 0.48
train: step: 1546, loss: 0.6971560120582581, acc: 0.49, recall: 0.49, precision: 0.4473684210526316, f_beta: 0.3605015673981191
train: step: 1547, loss: 0.6934181451797485, acc: 0.5, recall: 0.5, precision: 0.5, f_beta: 0.4981934965877158
train: step: 1548, loss: 0.6949403882026672, acc: 0.45, recall: 0.45, precision: 0.4246534056660639, f_beta: 0.39949776176438473
train: step: 1549, loss: 0.6765311360359192, acc: 0.58, recall: 0.58, precision: 0.6661129568106312, f_beta: 0.5174632352941175
train: step: 1550, loss: 0.6929328441619873, acc: 0.6, recall: 0.6, precision: 0.6001602564102564, f_beta: 0.5998399359743898
train: step: 1551, loss: 0.6944820284843445, acc: 0.49, recall: 0.49, precision: 0.4858836815358555, f_beta: 0.44989752993204624
train: step: 1552, loss: 0.6927517652511597, acc: 0.51, recall: 0.51, precision: 0.5100361300682457, f_beta: 0.5095586027424682
train: step: 1553, loss: 0.6947609186172485, acc: 0.41, recall: 0.41, precision: 0.3948106591865358, f_beta: 0.3879033094719369
train: step: 1554, loss: 0.6885181665420532, acc: 0.53, recall: 0.53, precision: 0.5350631136044881, f_beta: 0.512397551613238
train: step: 1555, loss: 0.686317503452301, acc: 0.57, recall: 0.57, precision: 0.5988142292490118, f_beta: 0.5361881134721174
train: step: 1556, loss: 0.6918776631355286, acc: 0.55, recall: 0.55, precision: 0.5525430853299707, f_beta: 0.5444883085332524
train: step: 1557, loss: 0.6916036009788513, acc: 0.55, recall: 0.55, precision: 0.5584385226741468, f_beta: 0.5331465919701215
train: step: 1558, loss: 0.6901680827140808, acc: 0.57, recall: 0.5700000000000001, precision: 0.5791497060153776, f_beta: 0.557203171661003
train: step: 1559, loss: 0.6934985518455505, acc: 0.47, recall: 0.47000000000000003, precision: 0.4689954526663911, f_beta: 0.46567194273616297
train: step: 1560, loss: 0.7058565020561218, acc: 0.49, recall: 0.49, precision: 0.46159754224270355, f_beta: 0.37430990062569014
train: step: 1561, loss: 0.6910944581031799, acc: 0.53, recall: 0.53, precision: 0.5305997552019583, f_beta: 0.5276856597326902
train: step: 1562, loss: 0.6917585134506226, acc: 0.5, recall: 0.5, precision: 0.5, f_beta: 0.48325754443985114
train: step: 1563, loss: 0.6942185163497925, acc: 0.41, recall: 0.41, precision: 0.39823609226594303, f_beta: 0.3924415611162599
train: step: 1564, loss: 0.6892074346542358, acc: 0.53, recall: 0.53, precision: 0.5915750915750916, f_beta: 0.43502824858757067
train: step: 1565, loss: 0.6923272609710693, acc: 0.57, recall: 0.5700000000000001, precision: 0.5700280112044818, f_beta: 0.56995699569957
train: step: 1566, loss: 0.693961501121521, acc: 0.5, recall: 0.5, precision: 0.5, f_beta: 0.37996031746031744
train: step: 1567, loss: 0.6930278539657593, acc: 0.51, recall: 0.51, precision: 0.5100361300682457, f_beta: 0.5095586027424682
train: step: 1568, loss: 0.6902056336402893, acc: 0.47, recall: 0.47, precision: 0.4547920433996383, f_beta: 0.42133420679113437
train: step: 1569, loss: 0.6927300095558167, acc: 0.51, recall: 0.51, precision: 0.5133333333333334, f_beta: 0.4773333333333334
train: step: 1570, loss: 0.6937379240989685, acc: 0.51, recall: 0.51, precision: 0.51010101010101, f_beta: 0.5087719298245614
train: step: 1571, loss: 0.6904640197753906, acc: 0.57, recall: 0.57, precision: 0.5933333333333333, f_beta: 0.5413333333333333
train: step: 1572, loss: 0.6866418719291687, acc: 0.54, recall: 0.54, precision: 0.5946969696969697, f_beta: 0.46236559139784944
train: step: 1573, loss: 0.6943397521972656, acc: 0.46, recall: 0.46, precision: 0.4375, f_beta: 0.4065934065934066
train: step: 1574, loss: 0.6809322237968445, acc: 0.56, recall: 0.5599999999999999, precision: 0.6666666666666667, f_beta: 0.4761904761904763
train: step: 1575, loss: 0.6892278790473938, acc: 0.7, recall: 0.7, precision: 0.7012882447665056, f_beta: 0.6995192307692307
train: step: 1576, loss: 0.6906411051750183, acc: 0.55, recall: 0.55, precision: 0.5666666666666667, f_beta: 0.52
train: step: 1577, loss: 0.6932317614555359, acc: 0.49, recall: 0.49, precision: 0.4858836815358555, f_beta: 0.44989752993204624
train: step: 1578, loss: 0.6917182803153992, acc: 0.58, recall: 0.5800000000000001, precision: 0.5833333333333333, f_beta: 0.5757575757575757
train: step: 1579, loss: 0.6909101009368896, acc: 0.58, recall: 0.5800000000000001, precision: 0.625, f_beta: 0.5384615384615385
train: step: 1580, loss: 0.6913909912109375, acc: 0.54, recall: 0.54, precision: 0.5548245614035088, f_beta: 0.5066495066495066
train: step: 1581, loss: 0.6922619342803955, acc: 0.58, recall: 0.5800000000000001, precision: 0.5801282051282051, f_beta: 0.5798319327731092
train: step: 1582, loss: 0.6923926472663879, acc: 0.58, recall: 0.5800000000000001, precision: 0.5821018062397373, f_beta: 0.5772946859903382
train: step: 1583, loss: 0.6923660039901733, acc: 0.54, recall: 0.54, precision: 0.5830564784053156, f_beta: 0.4715073529411765
train: step: 1584, loss: 0.692061185836792, acc: 0.54, recall: 0.54, precision: 0.5459558823529411, f_beta: 0.524596940884663
train: step: 1585, loss: 0.6913467645645142, acc: 0.48, recall: 0.48, precision: 0.4725877192982456, f_beta: 0.4422994422994423
train: step: 1586, loss: 0.6903135180473328, acc: 0.67, recall: 0.6699999999999999, precision: 0.6706142111601766, f_beta: 0.6697027324592133
train: step: 1587, loss: 0.6931659579277039, acc: 0.5, recall: 0.5, precision: 0.5, f_beta: 0.486863711001642
train: step: 1588, loss: 0.6906508803367615, acc: 0.51, recall: 0.51, precision: 0.5141163184641445, f_beta: 0.47147017581706396
train: step: 1589, loss: 0.6883775591850281, acc: 0.75, recall: 0.75, precision: 0.7549979600163199, f_beta: 0.7487689679429204
train: step: 1590, loss: 0.6869013905525208, acc: 0.56, recall: 0.56, precision: 0.7659574468085106, f_beta: 0.4543650793650793
train: step: 1591, loss: 0.6881973147392273, acc: 0.67, recall: 0.6699999999999999, precision: 0.6868131868131868, f_beta: 0.6624040920716112
train: step: 1592, loss: 0.6908256411552429, acc: 0.63, recall: 0.63, precision: 0.6366120218579234, f_beta: 0.6254681647940075
train: step: 1593, loss: 0.691913902759552, acc: 0.6, recall: 0.6000000000000001, precision: 0.6190476190476191, f_beta: 0.5833333333333333
train: step: 1594, loss: 0.6887133121490479, acc: 0.66, recall: 0.6599999999999999, precision: 0.6838235294117647, f_beta: 0.6486151302190988
train: step: 1595, loss: 0.6921167969703674, acc: 0.55, recall: 0.55, precision: 0.5565355042966984, f_beta: 0.5366079703429101
train: step: 1596, loss: 0.6907256841659546, acc: 0.63, recall: 0.63, precision: 0.6300520208083233, f_beta: 0.6299629962996299
train: step: 1597, loss: 0.6928149461746216, acc: 0.5, recall: 0.5, precision: 0.5, f_beta: 0.4981934965877158
train: step: 1598, loss: 0.6929447054862976, acc: 0.48, recall: 0.48, precision: 0.47702205882352944, f_beta: 0.46258784621744525
train: step: 1599, loss: 0.6912993788719177, acc: 0.52, recall: 0.52, precision: 0.5200320512820513, f_beta: 0.5198079231692677
train: step: 1600, loss: 0.6930531859397888, acc: 0.52, recall: 0.52, precision: 0.5208333333333334, f_beta: 0.5151515151515151
train: step: 1601, loss: 0.6913923025131226, acc: 0.59, recall: 0.59, precision: 0.627046866177301, f_beta: 0.5577607593571352
train: step: 1602, loss: 0.6914506554603577, acc: 0.59, recall: 0.5900000000000001, precision: 0.5903251706142112, f_beta: 0.5896306676008407
train: step: 1603, loss: 0.6929025053977966, acc: 0.56, recall: 0.56, precision: 0.5608766233766234, f_beta: 0.5584102769971899
train: step: 1604, loss: 0.6538812518119812, acc: 0.62, recall: 0.62, precision: 0.7032520325203252, f_beta: 0.5766488413547237
train: step: 1605, loss: 0.6914034485816956, acc: 0.57, recall: 0.57, precision: 0.575075075075075, f_beta: 0.562608076492727
train: step: 1606, loss: 0.6665534377098083, acc: 0.59, recall: 0.59, precision: 0.6764705882352942, f_beta: 0.5327635327635327
train: step: 1607, loss: 0.6924797892570496, acc: 0.53, recall: 0.53, precision: 0.5350631136044881, f_beta: 0.512397551613238
train: step: 1608, loss: 0.6925166845321655, acc: 0.51, recall: 0.51, precision: 0.5162443144899285, f_beta: 0.4579046354685253
train: step: 1609, loss: 0.6917862892150879, acc: 0.57, recall: 0.5700000000000001, precision: 0.5723439437784208, f_beta: 0.5664885573142453
train: step: 1610, loss: 0.6874644756317139, acc: 0.6, recall: 0.6, precision: 0.6001602564102564, f_beta: 0.5998399359743898
train: step: 1611, loss: 0.693000316619873, acc: 0.55, recall: 0.55, precision: 0.5705815923207227, f_beta: 0.5146154675870995
train: step: 1612, loss: 0.6919535994529724, acc: 0.59, recall: 0.59, precision: 0.5900360144057624, f_beta: 0.5899589958995899
train: step: 1613, loss: 0.693458080291748, acc: 0.5, recall: 0.5, precision: 0.5, f_beta: 0.47916666666666663
train: step: 1614, loss: 0.6839961409568787, acc: 0.53, recall: 0.53, precision: 0.6152073732718895, f_beta: 0.42338363390994965
train: step: 1615, loss: 0.6923092603683472, acc: 0.59, recall: 0.5900000000000001, precision: 0.5930136420008267, f_beta: 0.5866518802298619
train: step: 1616, loss: 0.692531406879425, acc: 0.56, recall: 0.56, precision: 0.5625, f_beta: 0.5555555555555556
train: step: 1617, loss: 0.6832861304283142, acc: 0.54, recall: 0.54, precision: 0.5830564784053156, f_beta: 0.4715073529411765
train: step: 1618, loss: 0.671485960483551, acc: 0.62, recall: 0.62, precision: 0.7032520325203252, f_beta: 0.5766488413547237
train: step: 1619, loss: 0.6885882019996643, acc: 0.54, recall: 0.54, precision: 0.7604166666666667, f_beta: 0.4165398274987316
train: step: 1620, loss: 0.6833963990211487, acc: 0.64, recall: 0.64, precision: 0.6559714795008913, f_beta: 0.6305418719211822
train: step: 1621, loss: 0.6791671514511108, acc: 0.53, recall: 0.53, precision: 0.6578947368421053, f_beta: 0.4106583072100314
train: step: 1622, loss: 0.6860253214836121, acc: 0.68, recall: 0.68, precision: 0.7338877338877339, f_beta: 0.6604414261460101
train: step: 1623, loss: 0.691953182220459, acc: 0.55, recall: 0.55, precision: 0.6105216622458002, f_beta: 0.4786235662148072
train: step: 1624, loss: 0.6933768391609192, acc: 0.54, recall: 0.54, precision: 0.5496031746031746, f_beta: 0.5166036149642708
train: step: 1625, loss: 0.6978595852851868, acc: 0.45, recall: 0.45, precision: 0.4187784275503573, f_beta: 0.3915256112401814
train: step: 1626, loss: 0.6884052753448486, acc: 0.59, recall: 0.59, precision: 0.5989010989010989, f_beta: 0.5805626598465473
train: step: 1627, loss: 0.6908605098724365, acc: 0.67, recall: 0.67, precision: 0.6786464901219, f_beta: 0.6659580929243851
train: step: 1628, loss: 0.688632607460022, acc: 0.65, recall: 0.65, precision: 0.6505419510236852, f_beta: 0.6496847162446201
train: step: 1629, loss: 0.6580061912536621, acc: 0.83, recall: 0.83, precision: 0.853925353925354, f_beta: 0.8270776116366596
train: step: 1630, loss: 0.6629197001457214, acc: 0.65, recall: 0.65, precision: 0.7117447769621683, f_beta: 0.6224786970121886
train: step: 1631, loss: 0.6927356123924255, acc: 0.56, recall: 0.56, precision: 0.5689338235294117, f_beta: 0.545266639107069
train: step: 1632, loss: 0.6914319396018982, acc: 0.6, recall: 0.6, precision: 0.6299376299376299, f_beta: 0.5755517826825127
train: step: 1633, loss: 0.6885384917259216, acc: 0.56, recall: 0.56, precision: 0.5874125874125874, f_beta: 0.5225694444444444
train: step: 1634, loss: 0.6925666332244873, acc: 0.54, recall: 0.54, precision: 0.5459558823529411, f_beta: 0.524596940884663
train: step: 1635, loss: 0.6943187713623047, acc: 0.41, recall: 0.41000000000000003, precision: 0.40542244640605296, f_beta: 0.4027735600769308
train: step: 1636, loss: 0.6920936703681946, acc: 0.58, recall: 0.5800000000000001, precision: 0.5992063492063493, f_beta: 0.5586380832282472
train: step: 1637, loss: 0.6957075595855713, acc: 0.37, recall: 0.37, precision: 0.36953030911280615, f_beta: 0.36943248924031635
train: step: 1638, loss: 0.6922572255134583, acc: 0.54, recall: 0.54, precision: 0.5424448217317488, f_beta: 0.5332792207792207
train: step: 1639, loss: 0.6924223303794861, acc: 0.57, recall: 0.5700000000000001, precision: 0.5702529104777199, f_beta: 0.5696126513862476
train: step: 1640, loss: 0.6915982961654663, acc: 0.64, recall: 0.64, precision: 0.640224358974359, f_beta: 0.6398559423769508
train: step: 1641, loss: 0.6935487985610962, acc: 0.48, recall: 0.48, precision: 0.48, f_beta: 0.48
train: step: 1642, loss: 0.6946213245391846, acc: 0.46, recall: 0.45999999999999996, precision: 0.4565972222222222, f_beta: 0.44920440636474906
train: step: 1643, loss: 0.6934321522712708, acc: 0.52, recall: 0.52, precision: 0.5205254515599343, f_beta: 0.5169082125603865
train: step: 1644, loss: 0.6936245560646057, acc: 0.45, recall: 0.44999999999999996, precision: 0.4498193496587716, f_beta: 0.44950455409868884
train: step: 1645, loss: 0.6943923830986023, acc: 0.41, recall: 0.41000000000000003, precision: 0.40820073439412485, f_beta: 0.4070947643452919
train: step: 1646, loss: 0.6924245357513428, acc: 0.53, recall: 0.53, precision: 0.530108390204737, f_beta: 0.5295766189570613
train: step: 1647, loss: 0.6933154463768005, acc: 0.52, recall: 0.52, precision: 0.5208333333333334, f_beta: 0.5151515151515151
train: step: 1648, loss: 0.6936242580413818, acc: 0.49, recall: 0.49, precision: 0.4898000815993472, f_beta: 0.48748869460355737
train: step: 1649, loss: 0.6932581067085266, acc: 0.52, recall: 0.52, precision: 0.5205254515599343, f_beta: 0.5169082125603865
train: step: 1650, loss: 0.6932588219642639, acc: 0.54, recall: 0.54, precision: 0.5476190476190477, f_beta: 0.5208333333333333
train: step: 1651, loss: 0.6941972374916077, acc: 0.48, recall: 0.48, precision: 0.4782986111111111, f_beta: 0.46960424316605465
train: step: 1652, loss: 0.6931672692298889, acc: 0.52, recall: 0.52, precision: 0.5212224108658743, f_beta: 0.512987012987013
train: step: 1653, loss: 0.6928182244300842, acc: 0.54, recall: 0.54, precision: 0.5402576489533011, f_beta: 0.5392628205128205
train: step: 1654, loss: 0.6933518052101135, acc: 0.44, recall: 0.44, precision: 0.44, f_beta: 0.44
train: step: 1655, loss: 0.6930485367774963, acc: 0.51, recall: 0.51, precision: 0.51010101010101, f_beta: 0.5087719298245614
train: step: 1656, loss: 0.6934815049171448, acc: 0.42, recall: 0.42000000000000004, precision: 0.4131944444444444, f_beta: 0.40840473276213796
train: step: 1657, loss: 0.6929866671562195, acc: 0.47, recall: 0.47, precision: 0.46989160979526295, f_beta: 0.46952257031328193
train: step: 1658, loss: 0.6927703619003296, acc: 0.53, recall: 0.53, precision: 0.5300120048019208, f_beta: 0.5299529952995299
train: step: 1659, loss: 0.6933569312095642, acc: 0.46, recall: 0.46, precision: 0.46, f_beta: 0.46
train: step: 1660, loss: 0.6932359337806702, acc: 0.55, recall: 0.55, precision: 0.5505050505050505, f_beta: 0.5488721804511277
train: step: 1661, loss: 0.693117082118988, acc: 0.52, recall: 0.52, precision: 0.5205254515599343, f_beta: 0.5169082125603865
train: step: 1662, loss: 0.6934681534767151, acc: 0.47, recall: 0.47, precision: 0.46989160979526295, f_beta: 0.46952257031328193
train: step: 1663, loss: 0.6931705474853516, acc: 0.48, recall: 0.48, precision: 0.4751984126984127, f_beta: 0.45355191256830596
train: step: 1664, loss: 0.6923577785491943, acc: 0.58, recall: 0.58, precision: 0.603950103950104, f_beta: 0.5543293718166383
train: step: 1665, loss: 0.6933072805404663, acc: 0.47, recall: 0.47, precision: 0.46989160979526295, f_beta: 0.46952257031328193
train: step: 1666, loss: 0.6943056583404541, acc: 0.44, recall: 0.44, precision: 0.4310661764705882, f_beta: 0.4212484497726333
train: step: 1667, loss: 0.6941463351249695, acc: 0.49, recall: 0.49, precision: 0.4899959983993597, f_beta: 0.48994899489948995
train: step: 1668, loss: 0.6932796239852905, acc: 0.48, recall: 0.48, precision: 0.48, f_beta: 0.48
train: step: 1669, loss: 0.6940425634384155, acc: 0.52, recall: 0.52, precision: 0.5200320512820513, f_beta: 0.5198079231692677
train: step: 1670, loss: 0.6930894255638123, acc: 0.55, recall: 0.55, precision: 0.550999592003264, f_beta: 0.5477841422972565
train: step: 1671, loss: 0.6924498081207275, acc: 0.55, recall: 0.55, precision: 0.5500200080032013, f_beta: 0.54995499549955
train: step: 1672, loss: 0.6934026479721069, acc: 0.51, recall: 0.51, precision: 0.5113071008593397, f_beta: 0.49541756770672435
train: step: 1673, loss: 0.6930918097496033, acc: 0.49, recall: 0.49, precision: 0.48966515088879703, f_beta: 0.48583526565177937
train: step: 1674, loss: 0.6936305165290833, acc: 0.46, recall: 0.46, precision: 0.45894909688013136, f_beta: 0.4565217391304348
train: step: 1675, loss: 0.6931331753730774, acc: 0.5, recall: 0.5, precision: 0.5, f_beta: 0.48325754443985114
train: step: 1676, loss: 0.6937131285667419, acc: 0.42, recall: 0.42, precision: 0.42, f_beta: 0.41999999999999993
train: step: 1677, loss: 0.6942223310470581, acc: 0.4, recall: 0.4, precision: 0.4, f_beta: 0.4000000000000001
train: step: 1678, loss: 0.6931490898132324, acc: 0.51, recall: 0.51, precision: 0.5101999184006528, f_beta: 0.5075871771681238
train: step: 1679, loss: 0.6929817199707031, acc: 0.52, recall: 0.52, precision: 0.52, f_beta: 0.52
train: step: 1680, loss: 0.6928520202636719, acc: 0.46, recall: 0.46, precision: 0.4594155844155844, f_beta: 0.45804897631473307
train: step: 1681, loss: 0.6925508379936218, acc: 0.55, recall: 0.55, precision: 0.5500200080032013, f_beta: 0.54995499549955
train: step: 1682, loss: 0.6917980313301086, acc: 0.59, recall: 0.5900000000000001, precision: 0.5930136420008267, f_beta: 0.5866518802298619
train: step: 1683, loss: 0.6935402750968933, acc: 0.48, recall: 0.48, precision: 0.48, f_beta: 0.48
train: step: 1684, loss: 0.6940158009529114, acc: 0.48, recall: 0.48, precision: 0.47987117552334946, f_beta: 0.47916666666666663
train: step: 1685, loss: 0.6936999559402466, acc: 0.45, recall: 0.45, precision: 0.44832575444398515, f_beta: 0.44550861982054646
train: step: 1686, loss: 0.6933402419090271, acc: 0.53, recall: 0.53, precision: 0.5300120048019208, f_beta: 0.5299529952995299
train: step: 1687, loss: 0.6925128698348999, acc: 0.59, recall: 0.59, precision: 0.5965250965250966, f_beta: 0.5829518868884142
train: step: 1688, loss: 0.6929418444633484, acc: 0.5, recall: 0.5, precision: 0.5, f_beta: 0.49919871794871795
train: step: 1689, loss: 0.6931087374687195, acc: 0.51, recall: 0.51, precision: 0.5101999184006528, f_beta: 0.5075871771681238
train: step: 1690, loss: 0.6925216913223267, acc: 0.49, recall: 0.49, precision: 0.4898000815993472, f_beta: 0.48748869460355737
train: step: 1691, loss: 0.6939952969551086, acc: 0.49, recall: 0.49, precision: 0.48996386993175434, f_beta: 0.4895405865278751
train: step: 1692, loss: 0.693287193775177, acc: 0.49, recall: 0.49, precision: 0.4899959983993597, f_beta: 0.48994899489948995
train: step: 1693, loss: 0.6925163865089417, acc: 0.55, recall: 0.55, precision: 0.5516742455560149, f_beta: 0.5463252343986289
train: step: 1694, loss: 0.6927577257156372, acc: 0.53, recall: 0.53, precision: 0.5305997552019583, f_beta: 0.5276856597326902
train: step: 1695, loss: 0.6938193440437317, acc: 0.46, recall: 0.45999999999999996, precision: 0.45993589743589747, f_beta: 0.45978391356542614
train: step: 1696, loss: 0.6926626563072205, acc: 0.56, recall: 0.56, precision: 0.5600961538461539, f_beta: 0.5598239295718288
train: step: 1697, loss: 0.6940507292747498, acc: 0.42, recall: 0.42, precision: 0.410873440285205, f_beta: 0.40476190476190477
train: step: 1698, loss: 0.6943498849868774, acc: 0.42, recall: 0.42000000000000004, precision: 0.4151103565365025, f_beta: 0.411525974025974
train: step: 1699, loss: 0.6938888430595398, acc: 0.45, recall: 0.45, precision: 0.44832575444398515, f_beta: 0.44550861982054646
train: step: 1700, loss: 0.692319929599762, acc: 0.55, recall: 0.55, precision: 0.554945054945055, f_beta: 0.5396419437340153
train: step: 1701, loss: 0.6915128231048584, acc: 0.63, recall: 0.63, precision: 0.6304696908871938, f_beta: 0.629666700030027
train: step: 1702, loss: 0.6924893260002136, acc: 0.52, recall: 0.52, precision: 0.525987525987526, f_beta: 0.49066213921901536
train: step: 1703, loss: 0.6930733323097229, acc: 0.49, recall: 0.49, precision: 0.4898989898989899, f_beta: 0.48872180451127817
train: step: 1704, loss: 0.6929351687431335, acc: 0.6, recall: 0.6, precision: 0.6, f_beta: 0.6
train: step: 1705, loss: 0.6925103068351746, acc: 0.52, recall: 0.52, precision: 0.5208333333333334, f_beta: 0.5151515151515151
train: step: 1706, loss: 0.69392991065979, acc: 0.43, recall: 0.43000000000000005, precision: 0.42492492492492495, f_beta: 0.42020140372291737
train: step: 1707, loss: 0.6944851875305176, acc: 0.45, recall: 0.45, precision: 0.449000407996736, f_beta: 0.4472917294744247
train: step: 1708, loss: 0.6926225423812866, acc: 0.53, recall: 0.53, precision: 0.5310045473336089, f_beta: 0.5261619114830125
train: step: 1709, loss: 0.693122923374176, acc: 0.51, recall: 0.51, precision: 0.5100040016006402, f_beta: 0.50995099509951
train: step: 1710, loss: 0.6922440528869629, acc: 0.57, recall: 0.5700000000000001, precision: 0.5700280112044818, f_beta: 0.56995699569957
train: step: 1711, loss: 0.6942997574806213, acc: 0.45, recall: 0.44999999999999996, precision: 0.4498193496587716, f_beta: 0.44950455409868884
train: step: 1712, loss: 0.693061351776123, acc: 0.52, recall: 0.52, precision: 0.5200320512820513, f_beta: 0.5198079231692677
train: step: 1713, loss: 0.6928784847259521, acc: 0.49, recall: 0.49, precision: 0.48949138293400585, f_beta: 0.48375341633768604
train: step: 1714, loss: 0.6936761736869812, acc: 0.46, recall: 0.45999999999999996, precision: 0.45974235104669886, f_beta: 0.4591346153846153
train: step: 1715, loss: 0.6928804516792297, acc: 0.53, recall: 0.53, precision: 0.532967032967033, f_beta: 0.5191815856777493
train: step: 1716, loss: 0.6932402849197388, acc: 0.47, recall: 0.47, precision: 0.4699879951980792, f_beta: 0.46994699469946993
train: step: 1717, loss: 0.6940343379974365, acc: 0.45, recall: 0.45, precision: 0.4434644957033017, f_beta: 0.4336319637524457
train: step: 1718, loss: 0.6937530040740967, acc: 0.42, recall: 0.42000000000000004, precision: 0.4178981937602627, f_beta: 0.4162640901771337
train: step: 1719, loss: 0.6925716400146484, acc: 0.52, recall: 0.52, precision: 0.5208333333333334, f_beta: 0.5151515151515151
train: step: 1720, loss: 0.6932597160339355, acc: 0.47, recall: 0.47, precision: 0.4699879951980792, f_beta: 0.46994699469946993
train: step: 1721, loss: 0.6926972270011902, acc: 0.56, recall: 0.56, precision: 0.5668449197860963, f_beta: 0.548440065681445
train: step: 1722, loss: 0.6921525001525879, acc: 0.58, recall: 0.5800000000000001, precision: 0.5805152979066023, f_beta: 0.5793269230769231
train: step: 1723, loss: 0.6927971839904785, acc: 0.53, recall: 0.53, precision: 0.5315258511979823, f_beta: 0.5242433444680635
train: step: 1724, loss: 0.6921240091323853, acc: 0.53, recall: 0.53, precision: 0.5350631136044881, f_beta: 0.512397551613238
train: step: 1725, loss: 0.6931365728378296, acc: 0.54, recall: 0.54, precision: 0.5400641025641025, f_beta: 0.5398159263705482
train: step: 1726, loss: 0.6930994987487793, acc: 0.53, recall: 0.53, precision: 0.5315258511979823, f_beta: 0.5242433444680635
train: step: 1727, loss: 0.6941044330596924, acc: 0.42, recall: 0.42000000000000004, precision: 0.4198717948717949, f_beta: 0.41976790716286516
train: step: 1728, loss: 0.6926261186599731, acc: 0.55, recall: 0.55, precision: 0.5516742455560149, f_beta: 0.5463252343986289
train: step: 1729, loss: 0.6944517493247986, acc: 0.41, recall: 0.41000000000000003, precision: 0.4099639855942377, f_beta: 0.4099409940994099
train: step: 1730, loss: 0.6939495801925659, acc: 0.45, recall: 0.45, precision: 0.449000407996736, f_beta: 0.4472917294744247
train: step: 1731, loss: 0.6928050518035889, acc: 0.49, recall: 0.49, precision: 0.4898989898989899, f_beta: 0.48872180451127817
train: step: 1732, loss: 0.6945406198501587, acc: 0.41, recall: 0.41000000000000003, precision: 0.4099639855942377, f_beta: 0.4099409940994099
train: step: 1733, loss: 0.6928479075431824, acc: 0.52, recall: 0.52, precision: 0.5222816399286987, f_beta: 0.5073891625615763
train: step: 1734, loss: 0.6925462484359741, acc: 0.52, recall: 0.52, precision: 0.5200320512820513, f_beta: 0.5198079231692677
train: step: 1735, loss: 0.6933538317680359, acc: 0.55, recall: 0.55, precision: 0.5500200080032013, f_beta: 0.54995499549955
train: step: 1736, loss: 0.6935436129570007, acc: 0.43, recall: 0.43, precision: 0.42307692307692313, f_beta: 0.41687979539641945
train: step: 1737, loss: 0.6932045221328735, acc: 0.54, recall: 0.54, precision: 0.5434027777777778, f_beta: 0.5308037535699714
train: step: 1738, loss: 0.6939778923988342, acc: 0.46, recall: 0.46, precision: 0.4594155844155844, f_beta: 0.45804897631473307
train: step: 1739, loss: 0.6945236921310425, acc: 0.4, recall: 0.4, precision: 0.3993558776167472, f_beta: 0.3990384615384615
train: step: 1740, loss: 0.6931207180023193, acc: 0.53, recall: 0.53, precision: 0.532967032967033, f_beta: 0.5191815856777493
train: step: 1741, loss: 0.6932647824287415, acc: 0.44, recall: 0.44, precision: 0.4331550802139037, f_beta: 0.4252873563218391
train: step: 1742, loss: 0.6920682787895203, acc: 0.57, recall: 0.5700000000000001, precision: 0.5713994288045696, f_beta: 0.567882624861823
train: step: 1743, loss: 0.6933309435844421, acc: 0.45, recall: 0.45, precision: 0.4499799919967987, f_beta: 0.44994499449944997
train: step: 1744, loss: 0.6944420337677002, acc: 0.47, recall: 0.47, precision: 0.46782496782496785, f_beta: 0.4608890245142916
train: step: 1745, loss: 0.6930114030838013, acc: 0.54, recall: 0.54, precision: 0.5400641025641025, f_beta: 0.5398159263705482
train: step: 1746, loss: 0.6934736371040344, acc: 0.47, recall: 0.47, precision: 0.4696969696969697, f_beta: 0.468671679197995
train: step: 1747, loss: 0.6925628185272217, acc: 0.54, recall: 0.54, precision: 0.5410509031198687, f_beta: 0.537037037037037
train: step: 1748, loss: 0.6938846111297607, acc: 0.44, recall: 0.44, precision: 0.43842364532019706, f_beta: 0.4363929146537842
train: step: 1749, loss: 0.6927604675292969, acc: 0.55, recall: 0.55, precision: 0.5501806503412284, f_beta: 0.5495946351716544
train: step: 1750, loss: 0.6940465569496155, acc: 0.46, recall: 0.45999999999999996, precision: 0.45993589743589747, f_beta: 0.45978391356542614
train: step: 1751, loss: 0.6926008462905884, acc: 0.51, recall: 0.51, precision: 0.510334849111203, f_beta: 0.5059985885673959
train: step: 1752, loss: 0.6929975748062134, acc: 0.51, recall: 0.51, precision: 0.5100361300682457, f_beta: 0.5095586027424682
train: step: 1753, loss: 0.6933020949363708, acc: 0.49, recall: 0.49, precision: 0.48966515088879703, f_beta: 0.48583526565177937
train: step: 1754, loss: 0.6933025121688843, acc: 0.49, recall: 0.49, precision: 0.48966515088879703, f_beta: 0.48583526565177937
train: step: 1755, loss: 0.6940228939056396, acc: 0.46, recall: 0.45999999999999996, precision: 0.45993589743589747, f_beta: 0.45978391356542614
train: step: 1756, loss: 0.6929279565811157, acc: 0.52, recall: 0.52, precision: 0.5205254515599343, f_beta: 0.5169082125603865
train: step: 1757, loss: 0.6942329406738281, acc: 0.47, recall: 0.47, precision: 0.46989160979526295, f_beta: 0.46952257031328193
train: step: 1758, loss: 0.6940428018569946, acc: 0.39, recall: 0.39, precision: 0.3888888888888889, f_beta: 0.38847117794486213
train: step: 1759, loss: 0.6926681399345398, acc: 0.55, recall: 0.55, precision: 0.5516742455560149, f_beta: 0.5463252343986289
train: step: 1760, loss: 0.6920223236083984, acc: 0.56, recall: 0.56, precision: 0.5615763546798029, f_beta: 0.5571658615136876
train: step: 1761, loss: 0.6935822367668152, acc: 0.54, recall: 0.54, precision: 0.5410509031198687, f_beta: 0.537037037037037
train: step: 1762, loss: 0.6933982372283936, acc: 0.47, recall: 0.47, precision: 0.46989160979526295, f_beta: 0.46952257031328193
train: step: 1763, loss: 0.6923514604568481, acc: 0.5, recall: 0.5, precision: 0.5, f_beta: 0.47456914670029426
train: step: 1764, loss: 0.6932824850082397, acc: 0.46, recall: 0.45999999999999996, precision: 0.45993589743589747, f_beta: 0.45978391356542614
train: step: 1765, loss: 0.6937114596366882, acc: 0.48, recall: 0.48, precision: 0.47086247086247085, f_beta: 0.4357638888888889
train: step: 1766, loss: 0.692464292049408, acc: 0.58, recall: 0.58, precision: 0.58, f_beta: 0.58
train: step: 1767, loss: 0.6939751505851746, acc: 0.44, recall: 0.44, precision: 0.43961352657004826, f_beta: 0.4391025641025641
train: step: 1768, loss: 0.6932437419891357, acc: 0.5, recall: 0.5, precision: 0.5, f_beta: 0.49979991996798717
train: step: 1769, loss: 0.693851113319397, acc: 0.51, recall: 0.51, precision: 0.5100361300682457, f_beta: 0.5095586027424682
train: step: 1770, loss: 0.6936528086662292, acc: 0.42, recall: 0.42000000000000004, precision: 0.4131944444444444, f_beta: 0.40840473276213796
train: step: 1771, loss: 0.6944310665130615, acc: 0.41, recall: 0.41000000000000003, precision: 0.4096748293857888, f_beta: 0.40946852166950254
train: step: 1772, loss: 0.6936103105545044, acc: 0.47, recall: 0.47, precision: 0.4696969696969697, f_beta: 0.468671679197995
train: step: 1773, loss: 0.6933664083480835, acc: 0.53, recall: 0.53, precision: 0.5300120048019208, f_beta: 0.5299529952995299
train: step: 1774, loss: 0.6938865184783936, acc: 0.47, recall: 0.47000000000000003, precision: 0.4694002447980416, f_beta: 0.467390212038991
train: step: 1775, loss: 0.6928695440292358, acc: 0.53, recall: 0.53, precision: 0.5310045473336089, f_beta: 0.5261619114830125
train: step: 1776, loss: 0.6934223771095276, acc: 0.51, recall: 0.51, precision: 0.5100361300682457, f_beta: 0.5095586027424682
train: step: 1777, loss: 0.6933215260505676, acc: 0.49, recall: 0.49, precision: 0.4898989898989899, f_beta: 0.48872180451127817
train: step: 1778, loss: 0.6929367184638977, acc: 0.5, recall: 0.5, precision: 0.5, f_beta: 0.49979991996798717
train: step: 1779, loss: 0.6941959857940674, acc: 0.44, recall: 0.43999999999999995, precision: 0.43489583333333337, f_beta: 0.4288045695634435
train: step: 1780, loss: 0.6930455565452576, acc: 0.54, recall: 0.54, precision: 0.5434027777777778, f_beta: 0.5308037535699714
train: step: 1781, loss: 0.6924651861190796, acc: 0.52, recall: 0.52, precision: 0.5248015873015872, f_beta: 0.49558638083228246
train: step: 1782, loss: 0.6927923560142517, acc: 0.52, recall: 0.52, precision: 0.5202922077922079, f_beta: 0.5182657567242072
train: step: 1783, loss: 0.6929734945297241, acc: 0.53, recall: 0.53, precision: 0.533921302578019, f_beta: 0.5160127690248172
train: step: 1784, loss: 0.6937296986579895, acc: 0.46, recall: 0.46, precision: 0.45894909688013136, f_beta: 0.4565217391304348
train: step: 1785, loss: 0.6928753852844238, acc: 0.48, recall: 0.48, precision: 0.47996794871794873, f_beta: 0.4797919167667067
train: step: 1786, loss: 0.6927728056907654, acc: 0.5, recall: 0.5, precision: 0.5, f_beta: 0.4967793880837359
train: step: 1787, loss: 0.6933894157409668, acc: 0.49, recall: 0.49, precision: 0.4899959983993597, f_beta: 0.48994899489948995
train: step: 1788, loss: 0.6921465992927551, acc: 0.57, recall: 0.5700000000000001, precision: 0.5700280112044818, f_beta: 0.56995699569957
train: step: 1789, loss: 0.692046582698822, acc: 0.59, recall: 0.59, precision: 0.601763907734057, f_beta: 0.5777983729790959
train: step: 1790, loss: 0.6935508847236633, acc: 0.52, recall: 0.52, precision: 0.5205254515599343, f_beta: 0.5169082125603865
train: step: 1791, loss: 0.6921970248222351, acc: 0.57, recall: 0.5700000000000001, precision: 0.5713994288045696, f_beta: 0.567882624861823
train: step: 1792, loss: 0.6932867169380188, acc: 0.52, recall: 0.52, precision: 0.5208333333333334, f_beta: 0.5151515151515151
train: step: 1793, loss: 0.6930267214775085, acc: 0.55, recall: 0.55, precision: 0.550999592003264, f_beta: 0.5477841422972565
train: step: 1794, loss: 0.6929337978363037, acc: 0.5, recall: 0.5, precision: 0.5, f_beta: 0.4981934965877158
train: step: 1795, loss: 0.693206250667572, acc: 0.57, recall: 0.57, precision: 0.5769230769230769, f_beta: 0.5601023017902813
train: step: 1796, loss: 0.6939666271209717, acc: 0.45, recall: 0.44999999999999996, precision: 0.4498193496587716, f_beta: 0.44950455409868884
train: step: 1797, loss: 0.6943860054016113, acc: 0.41, recall: 0.41000000000000003, precision: 0.40909090909090906, f_beta: 0.4085213032581453
train: step: 1798, loss: 0.6934161186218262, acc: 0.44, recall: 0.44, precision: 0.43912337662337664, f_beta: 0.43797671617824174
train: step: 1799, loss: 0.6931161284446716, acc: 0.48, recall: 0.48, precision: 0.47970779220779225, f_beta: 0.4781212364512244
train: step: 1800, loss: 0.692969799041748, acc: 0.46, recall: 0.45999999999999996, precision: 0.45974235104669886, f_beta: 0.4591346153846153
train: step: 1801, loss: 0.6939060091972351, acc: 0.46, recall: 0.46, precision: 0.4594155844155844, f_beta: 0.45804897631473307
train: step: 1802, loss: 0.6933220028877258, acc: 0.52, recall: 0.52, precision: 0.5212224108658743, f_beta: 0.512987012987013
train: step: 1803, loss: 0.6937165856361389, acc: 0.46, recall: 0.46, precision: 0.46, f_beta: 0.46
train: step: 1804, loss: 0.693271815776825, acc: 0.51, recall: 0.51, precision: 0.5100361300682457, f_beta: 0.5095586027424682
train: step: 1805, loss: 0.6928932070732117, acc: 0.53, recall: 0.53, precision: 0.532967032967033, f_beta: 0.5191815856777493
train: step: 1806, loss: 0.6926001906394958, acc: 0.54, recall: 0.54, precision: 0.5434027777777778, f_beta: 0.5308037535699714
train: step: 1807, loss: 0.6940158009529114, acc: 0.47, recall: 0.47, precision: 0.46989160979526295, f_beta: 0.46952257031328193
train: step: 1808, loss: 0.692850649356842, acc: 0.52, recall: 0.52, precision: 0.5208333333333334, f_beta: 0.5151515151515151
train: step: 1809, loss: 0.6930274963378906, acc: 0.52, recall: 0.52, precision: 0.5222816399286987, f_beta: 0.5073891625615763
train: step: 1810, loss: 0.6935529112815857, acc: 0.5, recall: 0.5, precision: 0.5, f_beta: 0.4949494949494949
train: step: 1811, loss: 0.6923479437828064, acc: 0.52, recall: 0.52, precision: 0.5208333333333334, f_beta: 0.5151515151515151
train: step: 1812, loss: 0.6928806900978088, acc: 0.52, recall: 0.52, precision: 0.5205254515599343, f_beta: 0.5169082125603865
train: step: 1813, loss: 0.693888247013092, acc: 0.49, recall: 0.49, precision: 0.4898000815993472, f_beta: 0.48748869460355737
train: step: 1814, loss: 0.692121684551239, acc: 0.52, recall: 0.52, precision: 0.5208333333333334, f_beta: 0.5151515151515151
train: step: 1815, loss: 0.6920942664146423, acc: 0.55, recall: 0.55, precision: 0.5501806503412284, f_beta: 0.5495946351716544
train: step: 1816, loss: 0.6936138272285461, acc: 0.46, recall: 0.46, precision: 0.45894909688013136, f_beta: 0.4565217391304348
train: step: 1817, loss: 0.6943585276603699, acc: 0.42, recall: 0.42000000000000004, precision: 0.4131944444444444, f_beta: 0.40840473276213796
train: step: 1818, loss: 0.6944373250007629, acc: 0.42, recall: 0.42, precision: 0.40808823529411764, f_beta: 0.4005787515502274
train: step: 1819, loss: 0.6933141350746155, acc: 0.57, recall: 0.5700000000000001, precision: 0.5713994288045696, f_beta: 0.567882624861823
train: step: 1820, loss: 0.692351758480072, acc: 0.55, recall: 0.55, precision: 0.5500200080032013, f_beta: 0.54995499549955
train: step: 1821, loss: 0.6935610771179199, acc: 0.46, recall: 0.45999999999999996, precision: 0.45993589743589747, f_beta: 0.45978391356542614
train: step: 1822, loss: 0.6931648254394531, acc: 0.54, recall: 0.54, precision: 0.5424448217317488, f_beta: 0.5332792207792207
train: step: 1823, loss: 0.6919969916343689, acc: 0.59, recall: 0.5900000000000001, precision: 0.5903251706142112, f_beta: 0.5896306676008407
train: step: 1824, loss: 0.6926984190940857, acc: 0.52, recall: 0.52, precision: 0.5200320512820513, f_beta: 0.5198079231692677
train: step: 1825, loss: 0.6918833255767822, acc: 0.57, recall: 0.5700000000000001, precision: 0.5723439437784208, f_beta: 0.5664885573142453
train: step: 1826, loss: 0.6937890648841858, acc: 0.47, recall: 0.47, precision: 0.4699879951980792, f_beta: 0.46994699469946993
train: step: 1827, loss: 0.6924363970756531, acc: 0.51, recall: 0.51, precision: 0.510989010989011, f_beta: 0.4987212276214833
train: step: 1828, loss: 0.6923275589942932, acc: 0.56, recall: 0.56, precision: 0.5600961538461539, f_beta: 0.5598239295718288
train: step: 1829, loss: 0.6929814219474792, acc: 0.47, recall: 0.47, precision: 0.46782496782496785, f_beta: 0.4608890245142916
train: step: 1830, loss: 0.6939705610275269, acc: 0.43, recall: 0.43000000000000005, precision: 0.42765605622157915, f_beta: 0.42534529690492995
train: step: 1831, loss: 0.6933292150497437, acc: 0.51, recall: 0.51, precision: 0.5101999184006528, f_beta: 0.5075871771681238
train: step: 1832, loss: 0.6934179663658142, acc: 0.46, recall: 0.45999999999999996, precision: 0.45993589743589747, f_beta: 0.45978391356542614
train: step: 1833, loss: 0.6926366686820984, acc: 0.49, recall: 0.49, precision: 0.4899959983993597, f_beta: 0.48994899489948995
train: step: 1834, loss: 0.6934301853179932, acc: 0.48, recall: 0.48, precision: 0.4782986111111111, f_beta: 0.46960424316605465
train: step: 1835, loss: 0.6938531398773193, acc: 0.41, recall: 0.41000000000000003, precision: 0.40542244640605296, f_beta: 0.4027735600769308
train: step: 1836, loss: 0.6934514045715332, acc: 0.5, recall: 0.5, precision: 0.5, f_beta: 0.4981934965877158
train: step: 1837, loss: 0.6935878992080688, acc: 0.46, recall: 0.45999999999999996, precision: 0.45974235104669886, f_beta: 0.4591346153846153
train: step: 1838, loss: 0.693387508392334, acc: 0.44, recall: 0.44, precision: 0.4331550802139037, f_beta: 0.4252873563218391
train: step: 1839, loss: 0.6922783851623535, acc: 0.58, recall: 0.5800000000000001, precision: 0.5821018062397373, f_beta: 0.5772946859903382
train: step: 1840, loss: 0.6924160122871399, acc: 0.55, recall: 0.55, precision: 0.5501806503412284, f_beta: 0.5495946351716544
train: step: 1841, loss: 0.6936626434326172, acc: 0.47, recall: 0.47, precision: 0.4699879951980792, f_beta: 0.46994699469946993
train: step: 1842, loss: 0.6924931406974792, acc: 0.56, recall: 0.56, precision: 0.5608766233766234, f_beta: 0.5584102769971899
train: step: 1843, loss: 0.6938375234603882, acc: 0.46, recall: 0.45999999999999996, precision: 0.45993589743589747, f_beta: 0.45978391356542614
train: step: 1844, loss: 0.6938633918762207, acc: 0.46, recall: 0.45999999999999996, precision: 0.4565972222222222, f_beta: 0.44920440636474906
train: step: 1845, loss: 0.6933915019035339, acc: 0.49, recall: 0.49, precision: 0.48996386993175434, f_beta: 0.4895405865278751
train: step: 1846, loss: 0.6931929588317871, acc: 0.47, recall: 0.47000000000000003, precision: 0.4694002447980416, f_beta: 0.467390212038991
train: step: 1847, loss: 0.6931549310684204, acc: 0.54, recall: 0.54, precision: 0.5424448217317488, f_beta: 0.5332792207792207
train: step: 1848, loss: 0.6926966309547424, acc: 0.51, recall: 0.51, precision: 0.510508617065994, f_beta: 0.5039983804028748
train: step: 1849, loss: 0.692831814289093, acc: 0.54, recall: 0.54, precision: 0.5459558823529411, f_beta: 0.524596940884663
train: step: 1850, loss: 0.6934929490089417, acc: 0.45, recall: 0.45, precision: 0.4499799919967987, f_beta: 0.44994499449944997
train: step: 1851, loss: 0.6927607655525208, acc: 0.51, recall: 0.51, precision: 0.5107250107250108, f_beta: 0.5015766453056658
train: step: 1852, loss: 0.6935242414474487, acc: 0.43, recall: 0.43, precision: 0.4292929292929293, f_beta: 0.4285714285714286
train: step: 1853, loss: 0.6926471590995789, acc: 0.52, recall: 0.52, precision: 0.52, f_beta: 0.52
train: step: 1854, loss: 0.6936540007591248, acc: 0.48, recall: 0.48, precision: 0.47987117552334946, f_beta: 0.47916666666666663
train: step: 1855, loss: 0.6945427060127258, acc: 0.46, recall: 0.45999999999999996, precision: 0.45974235104669886, f_beta: 0.4591346153846153
train: step: 1856, loss: 0.6927472949028015, acc: 0.54, recall: 0.54, precision: 0.5405844155844155, f_beta: 0.5383380168606985
train: step: 1857, loss: 0.6939164996147156, acc: 0.43, recall: 0.43, precision: 0.4292929292929293, f_beta: 0.4285714285714286
train: step: 1858, loss: 0.6929269433021545, acc: 0.52, recall: 0.52, precision: 0.5212224108658743, f_beta: 0.512987012987013
train: step: 1859, loss: 0.6941405534744263, acc: 0.44, recall: 0.44, precision: 0.43912337662337664, f_beta: 0.43797671617824174
train: step: 1860, loss: 0.6929084062576294, acc: 0.5, recall: 0.5, precision: 0.5, f_beta: 0.49919871794871795
train: step: 1861, loss: 0.69248366355896, acc: 0.54, recall: 0.54, precision: 0.5416666666666667, f_beta: 0.5353535353535354
train: step: 1862, loss: 0.6924901008605957, acc: 0.52, recall: 0.52, precision: 0.5205254515599343, f_beta: 0.5169082125603865
train: step: 1863, loss: 0.6948139071464539, acc: 0.4, recall: 0.4, precision: 0.4, f_beta: 0.4000000000000001
train: step: 1864, loss: 0.6927286386489868, acc: 0.53, recall: 0.53, precision: 0.532967032967033, f_beta: 0.5191815856777493
train: step: 1865, loss: 0.6932618021965027, acc: 0.48, recall: 0.48, precision: 0.47987117552334946, f_beta: 0.47916666666666663
train: step: 1866, loss: 0.6940906643867493, acc: 0.47, recall: 0.47, precision: 0.4699879951980792, f_beta: 0.46994699469946993
train: step: 1867, loss: 0.692929208278656, acc: 0.54, recall: 0.54, precision: 0.5402576489533011, f_beta: 0.5392628205128205
train: step: 1868, loss: 0.6934551000595093, acc: 0.43, recall: 0.43000000000000005, precision: 0.42974708952228025, f_beta: 0.42948653788409574
train: step: 1869, loss: 0.6943802833557129, acc: 0.45, recall: 0.44999999999999996, precision: 0.44505494505494503, f_beta: 0.4373401534526854
train: step: 1870, loss: 0.693591833114624, acc: 0.48, recall: 0.48, precision: 0.47916666666666663, f_beta: 0.4747474747474748
train: step: 1871, loss: 0.6932153105735779, acc: 0.49, recall: 0.49, precision: 0.4899959983993597, f_beta: 0.48994899489948995
train: step: 1872, loss: 0.6934228539466858, acc: 0.49, recall: 0.49, precision: 0.48927498927498925, f_beta: 0.48123283490997865
train: step: 1873, loss: 0.6925570964813232, acc: 0.54, recall: 0.54, precision: 0.5400641025641025, f_beta: 0.5398159263705482
train: step: 1874, loss: 0.694563627243042, acc: 0.39, recall: 0.39, precision: 0.3791208791208791, f_beta: 0.3759590792838875
train: step: 1875, loss: 0.692619800567627, acc: 0.53, recall: 0.53, precision: 0.530108390204737, f_beta: 0.5295766189570613
train: step: 1876, loss: 0.6932162642478943, acc: 0.47, recall: 0.47000000000000003, precision: 0.4694002447980416, f_beta: 0.467390212038991
train: step: 1877, loss: 0.6935334205627441, acc: 0.43, recall: 0.43000000000000005, precision: 0.42643968053804115, f_beta: 0.42301852414211966
train: step: 1878, loss: 0.6930024027824402, acc: 0.5, recall: 0.5, precision: 0.5, f_beta: 0.49979991996798717
train: step: 1879, loss: 0.6929340362548828, acc: 0.55, recall: 0.55, precision: 0.5505050505050505, f_beta: 0.5488721804511277
train: step: 1880, loss: 0.694335401058197, acc: 0.45, recall: 0.45, precision: 0.4499799919967987, f_beta: 0.44994499449944997
train: step: 1881, loss: 0.6937345862388611, acc: 0.43, recall: 0.43000000000000005, precision: 0.42765605622157915, f_beta: 0.42534529690492995
train: step: 1882, loss: 0.6928772926330566, acc: 0.52, recall: 0.52, precision: 0.5217013888888888, f_beta: 0.5104039167686658
train: step: 1883, loss: 0.6934376358985901, acc: 0.53, recall: 0.53, precision: 0.530108390204737, f_beta: 0.5295766189570613
train: step: 1884, loss: 0.6927130818367004, acc: 0.52, recall: 0.52, precision: 0.5201288244766505, f_beta: 0.5192307692307692
train: step: 1885, loss: 0.6919553875923157, acc: 0.54, recall: 0.54, precision: 0.5410509031198687, f_beta: 0.537037037037037
train: step: 1886, loss: 0.6921565532684326, acc: 0.53, recall: 0.53, precision: 0.533921302578019, f_beta: 0.5160127690248172
train: step: 1887, loss: 0.6925514340400696, acc: 0.49, recall: 0.49, precision: 0.4898989898989899, f_beta: 0.48872180451127817
train: step: 1888, loss: 0.6922321319580078, acc: 0.52, recall: 0.52, precision: 0.5208333333333334, f_beta: 0.5151515151515151
train: step: 1889, loss: 0.6921605467796326, acc: 0.58, recall: 0.5800000000000001, precision: 0.5801282051282051, f_beta: 0.5798319327731092
train: step: 1890, loss: 0.6926179528236389, acc: 0.47, recall: 0.47, precision: 0.46782496782496785, f_beta: 0.4608890245142916
train: step: 1891, loss: 0.6942305564880371, acc: 0.47, recall: 0.47, precision: 0.46989160979526295, f_beta: 0.46952257031328193
train: step: 1892, loss: 0.6936351656913757, acc: 0.47, recall: 0.47, precision: 0.46782496782496785, f_beta: 0.4608890245142916
train: step: 1893, loss: 0.6925605535507202, acc: 0.53, recall: 0.53, precision: 0.530108390204737, f_beta: 0.5295766189570613
train: step: 1894, loss: 0.6934589147567749, acc: 0.47, recall: 0.47, precision: 0.46989160979526295, f_beta: 0.46952257031328193
train: step: 1895, loss: 0.6932871341705322, acc: 0.54, recall: 0.54, precision: 0.5459558823529411, f_beta: 0.524596940884663
train: step: 1896, loss: 0.6930428147315979, acc: 0.47, recall: 0.47000000000000003, precision: 0.4689954526663911, f_beta: 0.46567194273616297
train: step: 1897, loss: 0.6939721703529358, acc: 0.45, recall: 0.45000000000000007, precision: 0.44745691467002946, f_beta: 0.44326348820730843
train: step: 1898, loss: 0.6941911578178406, acc: 0.44, recall: 0.44, precision: 0.43961352657004826, f_beta: 0.4391025641025641
train: step: 1899, loss: 0.6926579475402832, acc: 0.52, recall: 0.52, precision: 0.5200320512820513, f_beta: 0.5198079231692677
train: step: 1900, loss: 0.6941874027252197, acc: 0.49, recall: 0.49, precision: 0.48966515088879703, f_beta: 0.48583526565177937
train: step: 1901, loss: 0.6939411163330078, acc: 0.48, recall: 0.48, precision: 0.47970779220779225, f_beta: 0.4781212364512244
train: step: 1902, loss: 0.6933367252349854, acc: 0.52, recall: 0.52, precision: 0.5201288244766505, f_beta: 0.5192307692307692
train: step: 1903, loss: 0.6925948858261108, acc: 0.55, recall: 0.55, precision: 0.5536250536250535, f_beta: 0.5422642660970399
train: step: 1904, loss: 0.693634033203125, acc: 0.48, recall: 0.48, precision: 0.4782986111111111, f_beta: 0.46960424316605465
train: step: 1905, loss: 0.6927376389503479, acc: 0.49, recall: 0.49, precision: 0.4898989898989899, f_beta: 0.48872180451127817
train: step: 1906, loss: 0.6915707588195801, acc: 0.56, recall: 0.56, precision: 0.5608766233766234, f_beta: 0.5584102769971899
train: step: 1907, loss: 0.6923055052757263, acc: 0.57, recall: 0.5700000000000001, precision: 0.5723439437784208, f_beta: 0.5664885573142453
train: step: 1908, loss: 0.6934771537780762, acc: 0.44, recall: 0.44000000000000006, precision: 0.4375, f_beta: 0.43434343434343436
train: step: 1909, loss: 0.6927781701087952, acc: 0.58, recall: 0.5800000000000001, precision: 0.5805152979066023, f_beta: 0.5793269230769231
train: step: 1910, loss: 0.6935520768165588, acc: 0.45, recall: 0.45, precision: 0.449000407996736, f_beta: 0.4472917294744247
train: step: 1911, loss: 0.6928094625473022, acc: 0.52, recall: 0.52, precision: 0.5238095238095237, f_beta: 0.5
train: step: 1912, loss: 0.693687379360199, acc: 0.51, recall: 0.51, precision: 0.5113071008593397, f_beta: 0.49541756770672435
train: step: 1913, loss: 0.6934624314308167, acc: 0.46, recall: 0.46, precision: 0.45833333333333337, f_beta: 0.4545454545454545
train: step: 1914, loss: 0.6926674842834473, acc: 0.56, recall: 0.5599999999999999, precision: 0.5651041666666667, f_beta: 0.5512035903712771
train: step: 1915, loss: 0.6924187541007996, acc: 0.57, recall: 0.5700000000000001, precision: 0.5818139317438056, f_beta: 0.5538956323270049
train: step: 1916, loss: 0.692304790019989, acc: 0.52, recall: 0.52, precision: 0.52, f_beta: 0.52
train: step: 1917, loss: 0.6924048066139221, acc: 0.5, recall: 0.5, precision: 0.5, f_beta: 0.486863711001642
train: step: 1918, loss: 0.693193256855011, acc: 0.5, recall: 0.5, precision: 0.5, f_beta: 0.49979991996798717
train: step: 1919, loss: 0.6931235790252686, acc: 0.46, recall: 0.46, precision: 0.45833333333333337, f_beta: 0.4545454545454545
train: step: 1920, loss: 0.6940945982933044, acc: 0.49, recall: 0.49, precision: 0.48966515088879703, f_beta: 0.48583526565177937
train: step: 1921, loss: 0.6933391094207764, acc: 0.5, recall: 0.5, precision: 0.5, f_beta: 0.486863711001642
train: step: 1922, loss: 0.6937547326087952, acc: 0.46, recall: 0.46, precision: 0.4594155844155844, f_beta: 0.45804897631473307
train: step: 1923, loss: 0.6931864023208618, acc: 0.51, recall: 0.51, precision: 0.5121418164157359, f_beta: 0.4873940788785438
train: step: 1924, loss: 0.6924480199813843, acc: 0.55, recall: 0.55, precision: 0.550999592003264, f_beta: 0.5477841422972565
train: step: 1925, loss: 0.6930860280990601, acc: 0.53, recall: 0.53, precision: 0.532967032967033, f_beta: 0.5191815856777493
train: step: 1926, loss: 0.693332314491272, acc: 0.48, recall: 0.48, precision: 0.47987117552334946, f_beta: 0.47916666666666663
train: step: 1927, loss: 0.6933231949806213, acc: 0.46, recall: 0.45999999999999996, precision: 0.4565972222222222, f_beta: 0.44920440636474906
train: step: 1928, loss: 0.6929511427879333, acc: 0.57, recall: 0.5700000000000001, precision: 0.6054852320675106, f_beta: 0.5305164319248826
train: step: 1929, loss: 0.6925555467605591, acc: 0.56, recall: 0.56, precision: 0.5603864734299517, f_beta: 0.5592948717948718
train: step: 1930, loss: 0.6930201053619385, acc: 0.54, recall: 0.54, precision: 0.54, f_beta: 0.54
train: step: 1931, loss: 0.6931506395339966, acc: 0.52, recall: 0.52, precision: 0.5208333333333334, f_beta: 0.5151515151515151
train: step: 1932, loss: 0.6937496662139893, acc: 0.5, recall: 0.5, precision: 0.5, f_beta: 0.49919871794871795
train: step: 1933, loss: 0.693929135799408, acc: 0.39, recall: 0.39, precision: 0.3888888888888889, f_beta: 0.38847117794486213
train: step: 1934, loss: 0.6929591298103333, acc: 0.54, recall: 0.54, precision: 0.5496031746031746, f_beta: 0.5166036149642708
train: step: 1935, loss: 0.6933132410049438, acc: 0.5, recall: 0.5, precision: 0.5, f_beta: 0.5
train: step: 1936, loss: 0.6927749514579773, acc: 0.45, recall: 0.45, precision: 0.439290917921321, f_beta: 0.42462600690448793
train: step: 1937, loss: 0.6928937435150146, acc: 0.48, recall: 0.48, precision: 0.47970779220779225, f_beta: 0.4781212364512244
train: step: 1938, loss: 0.6938435435295105, acc: 0.47, recall: 0.47000000000000003, precision: 0.4649368863955119, f_beta: 0.4501504305425874
train: step: 1939, loss: 0.6928330063819885, acc: 0.45, recall: 0.44999999999999996, precision: 0.44637494637494635, f_beta: 0.44054521411860437
train: step: 1940, loss: 0.692435622215271, acc: 0.52, recall: 0.52, precision: 0.5201288244766505, f_beta: 0.5192307692307692
train: step: 1941, loss: 0.6926309466362, acc: 0.51, recall: 0.51, precision: 0.51010101010101, f_beta: 0.5087719298245614
train: step: 1942, loss: 0.6940489411354065, acc: 0.46, recall: 0.45999999999999996, precision: 0.45974235104669886, f_beta: 0.4591346153846153
train: step: 1943, loss: 0.6933512091636658, acc: 0.46, recall: 0.46, precision: 0.45039682539682535, f_beta: 0.43253467843631777
train: step: 1944, loss: 0.692003071308136, acc: 0.57, recall: 0.5700000000000001, precision: 0.5735603194619587, f_beta: 0.5647332725984411
train: step: 1945, loss: 0.6928427219390869, acc: 0.53, recall: 0.53, precision: 0.5303030303030303, f_beta: 0.5288220551378446
train: step: 1946, loss: 0.6928861737251282, acc: 0.53, recall: 0.53, precision: 0.5321750321750323, f_beta: 0.5219204557013528
train: step: 1947, loss: 0.6939293146133423, acc: 0.51, recall: 0.51, precision: 0.5101999184006528, f_beta: 0.5075871771681238
train: step: 1948, loss: 0.6941940784454346, acc: 0.48, recall: 0.48, precision: 0.47916666666666663, f_beta: 0.4747474747474748
train: step: 1949, loss: 0.6927244067192078, acc: 0.53, recall: 0.53, precision: 0.5350631136044881, f_beta: 0.512397551613238
train: step: 1950, loss: 0.6937779784202576, acc: 0.52, recall: 0.52, precision: 0.5201288244766505, f_beta: 0.5192307692307692
train: step: 1951, loss: 0.6934176087379456, acc: 0.52, recall: 0.52, precision: 0.52, f_beta: 0.52
train: step: 1952, loss: 0.6924702525138855, acc: 0.53, recall: 0.53, precision: 0.5350631136044881, f_beta: 0.512397551613238
train: step: 1953, loss: 0.6928188800811768, acc: 0.52, recall: 0.52, precision: 0.5217013888888888, f_beta: 0.5104039167686658
train: step: 1954, loss: 0.6924846172332764, acc: 0.53, recall: 0.53, precision: 0.5303030303030303, f_beta: 0.5288220551378446
train: step: 1955, loss: 0.6936560869216919, acc: 0.52, recall: 0.52, precision: 0.5202922077922079, f_beta: 0.5182657567242072
train: step: 1956, loss: 0.6947664022445679, acc: 0.4, recall: 0.4, precision: 0.3993558776167472, f_beta: 0.3990384615384615
train: step: 1957, loss: 0.6933892965316772, acc: 0.48, recall: 0.48000000000000004, precision: 0.47619047619047616, f_beta: 0.45833333333333337
train: step: 1958, loss: 0.6930681467056274, acc: 0.52, recall: 0.52, precision: 0.5200320512820513, f_beta: 0.5198079231692677
train: step: 1959, loss: 0.6943220496177673, acc: 0.47, recall: 0.47000000000000003, precision: 0.4689954526663911, f_beta: 0.46567194273616297
train: step: 1960, loss: 0.6933193802833557, acc: 0.47, recall: 0.47, precision: 0.46782496782496785, f_beta: 0.4608890245142916
train: step: 1961, loss: 0.6934773325920105, acc: 0.44, recall: 0.44, precision: 0.43990384615384615, f_beta: 0.4397759103641457
train: step: 1962, loss: 0.6932119131088257, acc: 0.5, recall: 0.5, precision: 0.5, f_beta: 0.4429590017825312
train: step: 1963, loss: 0.6919060349464417, acc: 0.57, recall: 0.5700000000000001, precision: 0.5707070707070707, f_beta: 0.5689223057644109
train: step: 1964, loss: 0.6925394535064697, acc: 0.54, recall: 0.54, precision: 0.5445632798573975, f_beta: 0.5279146141215106
train: step: 1965, loss: 0.6933539509773254, acc: 0.5, recall: 0.5, precision: 0.5, f_beta: 0.4967793880837359
train: step: 1966, loss: 0.6922200918197632, acc: 0.54, recall: 0.54, precision: 0.54, f_beta: 0.54
train: step: 1967, loss: 0.6941277980804443, acc: 0.43, recall: 0.43000000000000005, precision: 0.42492492492492495, f_beta: 0.42020140372291737
train: step: 1968, loss: 0.6930097937583923, acc: 0.53, recall: 0.53, precision: 0.5321750321750323, f_beta: 0.5219204557013528
train: step: 1969, loss: 0.6934894323348999, acc: 0.48, recall: 0.48, precision: 0.47970779220779225, f_beta: 0.4781212364512244
train: step: 1970, loss: 0.6926938891410828, acc: 0.53, recall: 0.53, precision: 0.5364254492472074, f_beta: 0.5083167695365624
train: step: 1971, loss: 0.6934707760810852, acc: 0.5, recall: 0.5, precision: 0.5, f_beta: 0.4949494949494949
train: step: 1972, loss: 0.6938771605491638, acc: 0.44, recall: 0.44, precision: 0.43961352657004826, f_beta: 0.4391025641025641
train: step: 1973, loss: 0.6940782070159912, acc: 0.44, recall: 0.44, precision: 0.43842364532019706, f_beta: 0.4363929146537842
train: step: 1974, loss: 0.6926849484443665, acc: 0.56, recall: 0.56, precision: 0.5636672325976231, f_beta: 0.5535714285714286
train: step: 1975, loss: 0.6935735940933228, acc: 0.49, recall: 0.49, precision: 0.48927498927498925, f_beta: 0.48123283490997865
train: step: 1976, loss: 0.6937941908836365, acc: 0.47, recall: 0.47000000000000003, precision: 0.4689954526663911, f_beta: 0.46567194273616297
train: step: 1977, loss: 0.6937739849090576, acc: 0.44, recall: 0.44, precision: 0.44, f_beta: 0.44
train: step: 1978, loss: 0.6918653249740601, acc: 0.57, recall: 0.5700000000000001, precision: 0.5723439437784208, f_beta: 0.5664885573142453
train: step: 1979, loss: 0.6932425498962402, acc: 0.5, recall: 0.5, precision: 0.5, f_beta: 0.49979991996798717
train: step: 1980, loss: 0.6920150518417358, acc: 0.57, recall: 0.5700000000000001, precision: 0.5723439437784208, f_beta: 0.5664885573142453
train: step: 1981, loss: 0.69330233335495, acc: 0.48, recall: 0.48, precision: 0.47987117552334946, f_beta: 0.47916666666666663
train: step: 1982, loss: 0.6922973394393921, acc: 0.53, recall: 0.53, precision: 0.530108390204737, f_beta: 0.5295766189570613
train: step: 1983, loss: 0.6934143304824829, acc: 0.48, recall: 0.48, precision: 0.48, f_beta: 0.48
train: step: 1984, loss: 0.6932952404022217, acc: 0.46, recall: 0.46, precision: 0.45894909688013136, f_beta: 0.4565217391304348
train: step: 1985, loss: 0.6934421062469482, acc: 0.49, recall: 0.49, precision: 0.4898000815993472, f_beta: 0.48748869460355737
train: step: 1986, loss: 0.6919509768486023, acc: 0.57, recall: 0.5700000000000001, precision: 0.5700280112044818, f_beta: 0.56995699569957
train: step: 1987, loss: 0.6944817304611206, acc: 0.43, recall: 0.43000000000000005, precision: 0.42974708952228025, f_beta: 0.42948653788409574
train: step: 1988, loss: 0.6919044256210327, acc: 0.52, recall: 0.52, precision: 0.5201288244766505, f_beta: 0.5192307692307692
train: step: 1989, loss: 0.6935942769050598, acc: 0.46, recall: 0.45999999999999996, precision: 0.45993589743589747, f_beta: 0.45978391356542614
train: step: 1990, loss: 0.6939808130264282, acc: 0.44, recall: 0.44, precision: 0.43990384615384615, f_beta: 0.4397759103641457
train: step: 1991, loss: 0.6930219531059265, acc: 0.53, recall: 0.53, precision: 0.5305997552019583, f_beta: 0.5276856597326902
train: step: 1992, loss: 0.6928489804267883, acc: 0.48, recall: 0.48, precision: 0.47970779220779225, f_beta: 0.4781212364512244
train: step: 1993, loss: 0.6940332055091858, acc: 0.44, recall: 0.44, precision: 0.43990384615384615, f_beta: 0.4397759103641457
train: step: 1994, loss: 0.6929474472999573, acc: 0.47, recall: 0.47000000000000003, precision: 0.4694002447980416, f_beta: 0.467390212038991
train: step: 1995, loss: 0.6937409043312073, acc: 0.47, recall: 0.47000000000000003, precision: 0.46357455075279264, f_beta: 0.44554869756250653
train: step: 1996, loss: 0.6929574608802795, acc: 0.53, recall: 0.53, precision: 0.54, f_beta: 0.4986666666666667
train: step: 1997, loss: 0.6927406787872314, acc: 0.48, recall: 0.48000000000000004, precision: 0.47619047619047616, f_beta: 0.45833333333333337
train: step: 1998, loss: 0.6928166151046753, acc: 0.57, recall: 0.5700000000000001, precision: 0.5700280112044818, f_beta: 0.56995699569957
train: step: 1999, loss: 0.6940545439720154, acc: 0.45, recall: 0.44999999999999996, precision: 0.4498193496587716, f_beta: 0.44950455409868884
train: step: 2000, loss: 0.692838728427887, acc: 0.49, recall: 0.49, precision: 0.48927498927498925, f_beta: 0.48123283490997865
train: step: 2001, loss: 0.692608118057251, acc: 0.55, recall: 0.55, precision: 0.5536250536250535, f_beta: 0.5422642660970399
train: step: 2002, loss: 0.6947397589683533, acc: 0.44, recall: 0.44000000000000006, precision: 0.42203742203742206, f_beta: 0.40577249575551777
train: step: 2003, loss: 0.6949471831321716, acc: 0.41, recall: 0.41000000000000003, precision: 0.40820073439412485, f_beta: 0.4070947643452919
train: step: 2004, loss: 0.6926559209823608, acc: 0.48, recall: 0.48, precision: 0.47996794871794873, f_beta: 0.4797919167667067
train: step: 2005, loss: 0.6940419673919678, acc: 0.46, recall: 0.46, precision: 0.4594155844155844, f_beta: 0.45804897631473307
train: step: 2006, loss: 0.6926681399345398, acc: 0.51, recall: 0.51, precision: 0.5116877045348294, f_beta: 0.4916485112563544
train: step: 2007, loss: 0.6934525370597839, acc: 0.47, recall: 0.47, precision: 0.46989160979526295, f_beta: 0.46952257031328193
train: step: 2008, loss: 0.6937648057937622, acc: 0.47, recall: 0.47000000000000003, precision: 0.4649368863955119, f_beta: 0.4501504305425874
train: step: 2009, loss: 0.6924830675125122, acc: 0.52, recall: 0.52, precision: 0.5205254515599343, f_beta: 0.5169082125603865
train: step: 2010, loss: 0.6927450299263, acc: 0.56, recall: 0.5599999999999999, precision: 0.5651041666666667, f_beta: 0.5512035903712771
train: step: 2011, loss: 0.6916604042053223, acc: 0.57, recall: 0.5700000000000001, precision: 0.5702529104777199, f_beta: 0.5696126513862476
train: step: 2012, loss: 0.6933214664459229, acc: 0.47, recall: 0.47, precision: 0.466078697421981, f_beta: 0.4542271650705386
train: step: 2013, loss: 0.6937507390975952, acc: 0.53, recall: 0.53, precision: 0.532967032967033, f_beta: 0.5191815856777493
train: step: 2014, loss: 0.6942808032035828, acc: 0.49, recall: 0.49, precision: 0.48996386993175434, f_beta: 0.4895405865278751
train: step: 2015, loss: 0.6931277513504028, acc: 0.46, recall: 0.46, precision: 0.45894909688013136, f_beta: 0.4565217391304348
train: step: 2016, loss: 0.6932215094566345, acc: 0.53, recall: 0.53, precision: 0.5303030303030303, f_beta: 0.5288220551378446
train: step: 2017, loss: 0.6929755210876465, acc: 0.51, recall: 0.51, precision: 0.510989010989011, f_beta: 0.4987212276214833
train: step: 2018, loss: 0.6940602660179138, acc: 0.46, recall: 0.46, precision: 0.45039682539682535, f_beta: 0.43253467843631777
train: step: 2019, loss: 0.6931263208389282, acc: 0.61, recall: 0.61, precision: 0.6103974307507025, f_beta: 0.6096486838154338
train: step: 2020, loss: 0.6938958764076233, acc: 0.51, recall: 0.51, precision: 0.510508617065994, f_beta: 0.5039983804028748
train: step: 2021, loss: 0.6936488151550293, acc: 0.46, recall: 0.46, precision: 0.4540441176470588, f_beta: 0.44191814799503926
train: step: 2022, loss: 0.693501353263855, acc: 0.38, recall: 0.38, precision: 0.38, f_beta: 0.38
train: step: 2023, loss: 0.6918956637382507, acc: 0.59, recall: 0.5900000000000001, precision: 0.5909090909090908, f_beta: 0.5889724310776943
train: step: 2024, loss: 0.6935669779777527, acc: 0.52, recall: 0.52, precision: 0.5201288244766505, f_beta: 0.5192307692307692
train: step: 2025, loss: 0.6931169033050537, acc: 0.48, recall: 0.48, precision: 0.4787775891341256, f_beta: 0.4724025974025974
train: step: 2026, loss: 0.693729043006897, acc: 0.48, recall: 0.48, precision: 0.47987117552334946, f_beta: 0.47916666666666663
train: step: 2027, loss: 0.6922881603240967, acc: 0.51, recall: 0.51, precision: 0.510508617065994, f_beta: 0.5039983804028748
train: step: 2028, loss: 0.6928324103355408, acc: 0.53, recall: 0.53, precision: 0.5305997552019583, f_beta: 0.5276856597326902
train: step: 2029, loss: 0.6938261389732361, acc: 0.44, recall: 0.44000000000000006, precision: 0.4363327674023769, f_beta: 0.43181818181818177
train: step: 2030, loss: 0.69329833984375, acc: 0.51, recall: 0.51, precision: 0.5113071008593397, f_beta: 0.49541756770672435
train: step: 2031, loss: 0.6939896941184998, acc: 0.48, recall: 0.48, precision: 0.47996794871794873, f_beta: 0.4797919167667067
train: step: 2032, loss: 0.6926612257957458, acc: 0.46, recall: 0.45999999999999996, precision: 0.4554367201426025, f_beta: 0.44581280788177335
train: step: 2033, loss: 0.693045973777771, acc: 0.5, recall: 0.5, precision: 0.5, f_beta: 0.49919871794871795
train: step: 2034, loss: 0.692095160484314, acc: 0.51, recall: 0.51, precision: 0.5101999184006528, f_beta: 0.5075871771681238
train: step: 2035, loss: 0.6926604509353638, acc: 0.56, recall: 0.56, precision: 0.5603864734299517, f_beta: 0.5592948717948718
train: step: 2036, loss: 0.6932153105735779, acc: 0.51, recall: 0.51, precision: 0.5126839167935058, f_beta: 0.48263118994826315
train: step: 2037, loss: 0.6941428184509277, acc: 0.44, recall: 0.44, precision: 0.43912337662337664, f_beta: 0.43797671617824174
train: step: 2038, loss: 0.692758321762085, acc: 0.5, recall: 0.5, precision: 0.5, f_beta: 0.486863711001642
train: step: 2039, loss: 0.6940736174583435, acc: 0.41, recall: 0.41000000000000003, precision: 0.40347490347490345, f_beta: 0.39985759332723014
train: step: 2040, loss: 0.6940168142318726, acc: 0.48, recall: 0.48, precision: 0.47987117552334946, f_beta: 0.47916666666666663
train: step: 2041, loss: 0.6928055286407471, acc: 0.5, recall: 0.5, precision: 0.5, f_beta: 0.486863711001642
train: step: 2042, loss: 0.6928629875183105, acc: 0.49, recall: 0.49, precision: 0.4898000815993472, f_beta: 0.48748869460355737
train: step: 2043, loss: 0.6929409503936768, acc: 0.55, recall: 0.55, precision: 0.5505050505050505, f_beta: 0.5488721804511277
train: step: 2044, loss: 0.6946256160736084, acc: 0.41, recall: 0.41000000000000003, precision: 0.4096748293857888, f_beta: 0.40946852166950254
train: step: 2045, loss: 0.6930776834487915, acc: 0.52, recall: 0.52, precision: 0.5208333333333334, f_beta: 0.5151515151515151
train: step: 2046, loss: 0.6920282244682312, acc: 0.61, recall: 0.61, precision: 0.6103974307507025, f_beta: 0.6096486838154338
train: step: 2047, loss: 0.692055344581604, acc: 0.56, recall: 0.56, precision: 0.5603864734299517, f_beta: 0.5592948717948718
train: step: 2048, loss: 0.693779706954956, acc: 0.48, recall: 0.48, precision: 0.47987117552334946, f_beta: 0.47916666666666663
train: step: 2049, loss: 0.6936219930648804, acc: 0.5, recall: 0.5, precision: 0.5, f_beta: 0.48325754443985114
train: step: 2050, loss: 0.6930773854255676, acc: 0.5, recall: 0.5, precision: 0.5, f_beta: 0.4949494949494949
train: step: 2051, loss: 0.6925386786460876, acc: 0.51, recall: 0.51, precision: 0.5101999184006528, f_beta: 0.5075871771681238
train: step: 2052, loss: 0.6936174631118774, acc: 0.46, recall: 0.45999999999999996, precision: 0.45974235104669886, f_beta: 0.4591346153846153
train: step: 2053, loss: 0.6931844353675842, acc: 0.48, recall: 0.48000000000000004, precision: 0.47619047619047616, f_beta: 0.45833333333333337
train: step: 2054, loss: 0.6932058930397034, acc: 0.48, recall: 0.48, precision: 0.47996794871794873, f_beta: 0.4797919167667067
train: step: 2055, loss: 0.692122220993042, acc: 0.6, recall: 0.6, precision: 0.6061120543293719, f_beta: 0.5941558441558441
train: step: 2056, loss: 0.6926447153091431, acc: 0.53, recall: 0.53, precision: 0.533921302578019, f_beta: 0.5160127690248172
train: step: 2057, loss: 0.6939318776130676, acc: 0.45, recall: 0.45000000000000007, precision: 0.44745691467002946, f_beta: 0.44326348820730843
train: step: 2058, loss: 0.693159818649292, acc: 0.46, recall: 0.46, precision: 0.4594155844155844, f_beta: 0.45804897631473307
train: step: 2059, loss: 0.6938720941543579, acc: 0.42, recall: 0.42000000000000004, precision: 0.4178981937602627, f_beta: 0.4162640901771337
train: step: 2060, loss: 0.6929302215576172, acc: 0.5, recall: 0.5, precision: 0.5, f_beta: 0.4967793880837359
train: step: 2061, loss: 0.6943961977958679, acc: 0.46, recall: 0.46, precision: 0.46, f_beta: 0.46
train: step: 2062, loss: 0.6921852827072144, acc: 0.56, recall: 0.56, precision: 0.5608766233766234, f_beta: 0.5584102769971899
train: step: 2063, loss: 0.6941559314727783, acc: 0.49, recall: 0.49, precision: 0.48966515088879703, f_beta: 0.48583526565177937
train: step: 2064, loss: 0.6905296444892883, acc: 0.67, recall: 0.6699999999999999, precision: 0.6733986128110976, f_beta: 0.6683750376846548
train: step: 2065, loss: 0.6928024291992188, acc: 0.56, recall: 0.56, precision: 0.5615763546798029, f_beta: 0.5571658615136876
train: step: 2066, loss: 0.6931963562965393, acc: 0.52, recall: 0.52, precision: 0.5202922077922079, f_beta: 0.5182657567242072
train: step: 2067, loss: 0.6916913390159607, acc: 0.6, recall: 0.6, precision: 0.6085069444444444, f_beta: 0.5920032639738881
train: step: 2068, loss: 0.693149209022522, acc: 0.56, recall: 0.56, precision: 0.5668449197860963, f_beta: 0.548440065681445
train: step: 2069, loss: 0.6930238604545593, acc: 0.49, recall: 0.49, precision: 0.48869289914066033, f_beta: 0.4748223663886314
train: step: 2070, loss: 0.6940124034881592, acc: 0.47, recall: 0.47, precision: 0.46703296703296704, f_beta: 0.4578005115089514
train: step: 2071, loss: 0.6930820345878601, acc: 0.53, recall: 0.53, precision: 0.5310045473336089, f_beta: 0.5261619114830125
train: step: 2072, loss: 0.691609263420105, acc: 0.63, recall: 0.63, precision: 0.6343530384456387, f_beta: 0.6269785260610949
train: step: 2073, loss: 0.6922762393951416, acc: 0.5, recall: 0.5, precision: 0.5, f_beta: 0.4967793880837359
train: step: 2074, loss: 0.6933220028877258, acc: 0.47, recall: 0.47000000000000003, precision: 0.46357455075279264, f_beta: 0.44554869756250653
train: step: 2075, loss: 0.6931936740875244, acc: 0.45, recall: 0.45, precision: 0.4434644957033017, f_beta: 0.4336319637524457
train: step: 2076, loss: 0.6921563148498535, acc: 0.54, recall: 0.54, precision: 0.5400641025641025, f_beta: 0.5398159263705482
train: step: 2077, loss: 0.6925657391548157, acc: 0.58, recall: 0.5800000000000001, precision: 0.5801282051282051, f_beta: 0.5798319327731092
train: step: 2078, loss: 0.6932501792907715, acc: 0.44, recall: 0.44, precision: 0.43990384615384615, f_beta: 0.4397759103641457
train: step: 2079, loss: 0.6930528879165649, acc: 0.47, recall: 0.47, precision: 0.4699879951980792, f_beta: 0.46994699469946993
train: step: 2080, loss: 0.6920101642608643, acc: 0.56, recall: 0.56, precision: 0.5600961538461539, f_beta: 0.5598239295718288
train: step: 2081, loss: 0.6942086219787598, acc: 0.41, recall: 0.41000000000000003, precision: 0.4099639855942377, f_beta: 0.4099409940994099
train: step: 2082, loss: 0.6924459338188171, acc: 0.55, recall: 0.55, precision: 0.550999592003264, f_beta: 0.5477841422972565
train: step: 2083, loss: 0.6949674487113953, acc: 0.4, recall: 0.4, precision: 0.398538961038961, f_beta: 0.397832195905259
train: step: 2084, loss: 0.6945794820785522, acc: 0.43, recall: 0.43000000000000005, precision: 0.42974708952228025, f_beta: 0.42948653788409574
train: step: 2085, loss: 0.6936604380607605, acc: 0.43, recall: 0.43, precision: 0.42307692307692313, f_beta: 0.41687979539641945
train: step: 2086, loss: 0.6926509141921997, acc: 0.5, recall: 0.5, precision: 0.5, f_beta: 0.4981934965877158
train: step: 2087, loss: 0.6925133466720581, acc: 0.54, recall: 0.54, precision: 0.5400641025641025, f_beta: 0.5398159263705482
train: step: 2088, loss: 0.6925740838050842, acc: 0.49, recall: 0.49, precision: 0.48869289914066033, f_beta: 0.4748223663886314
train: step: 2089, loss: 0.6929737329483032, acc: 0.5, recall: 0.5, precision: 0.5, f_beta: 0.48325754443985114
train: step: 2090, loss: 0.6928711533546448, acc: 0.56, recall: 0.56, precision: 0.5603864734299517, f_beta: 0.5592948717948718
train: step: 2091, loss: 0.693839967250824, acc: 0.44, recall: 0.44, precision: 0.43842364532019706, f_beta: 0.4363929146537842
train: step: 2092, loss: 0.6934877038002014, acc: 0.47, recall: 0.47, precision: 0.4696969696969697, f_beta: 0.468671679197995
train: step: 2093, loss: 0.6924558281898499, acc: 0.49, recall: 0.49, precision: 0.48869289914066033, f_beta: 0.4748223663886314
train: step: 2094, loss: 0.69317626953125, acc: 0.57, recall: 0.5700000000000001, precision: 0.5700280112044818, f_beta: 0.56995699569957
train: step: 2095, loss: 0.6925312876701355, acc: 0.58, recall: 0.5800000000000001, precision: 0.5811688311688312, f_beta: 0.5784825371336814
train: step: 2096, loss: 0.6925885677337646, acc: 0.59, recall: 0.59, precision: 0.5965250965250966, f_beta: 0.5829518868884142
train: step: 2097, loss: 0.6921618580818176, acc: 0.56, recall: 0.56, precision: 0.5636672325976231, f_beta: 0.5535714285714286
train: step: 2098, loss: 0.6925364136695862, acc: 0.52, recall: 0.52, precision: 0.5212224108658743, f_beta: 0.512987012987013
train: step: 2099, loss: 0.6927274465560913, acc: 0.51, recall: 0.51, precision: 0.5100040016006402, f_beta: 0.50995099509951
train: step: 2100, loss: 0.6924343109130859, acc: 0.55, recall: 0.55, precision: 0.5500200080032013, f_beta: 0.54995499549955
train: step: 2101, loss: 0.6942875385284424, acc: 0.48, recall: 0.48, precision: 0.48, f_beta: 0.48
train: step: 2102, loss: 0.6943362355232239, acc: 0.4, recall: 0.4, precision: 0.3998397435897436, f_beta: 0.3997599039615846
train: step: 2103, loss: 0.6935247182846069, acc: 0.47, recall: 0.47, precision: 0.46782496782496785, f_beta: 0.4608890245142916
train: step: 2104, loss: 0.6934236288070679, acc: 0.46, recall: 0.46, precision: 0.45894909688013136, f_beta: 0.4565217391304348
train: step: 2105, loss: 0.6941978335380554, acc: 0.46, recall: 0.46, precision: 0.4523809523809524, f_beta: 0.4375
train: step: 2106, loss: 0.6927452683448792, acc: 0.51, recall: 0.51, precision: 0.5100361300682457, f_beta: 0.5095586027424682
train: step: 2107, loss: 0.6928431987762451, acc: 0.56, recall: 0.56, precision: 0.5600961538461539, f_beta: 0.5598239295718288
train: step: 2108, loss: 0.6931743621826172, acc: 0.53, recall: 0.53, precision: 0.530108390204737, f_beta: 0.5295766189570613
train: step: 2109, loss: 0.6931068897247314, acc: 0.51, recall: 0.51, precision: 0.5100040016006402, f_beta: 0.50995099509951
train: step: 2110, loss: 0.6940548419952393, acc: 0.48, recall: 0.48, precision: 0.47996794871794873, f_beta: 0.4797919167667067
train: step: 2111, loss: 0.6922206282615662, acc: 0.57, recall: 0.5700000000000001, precision: 0.5707070707070707, f_beta: 0.5689223057644109
train: step: 2112, loss: 0.6933217644691467, acc: 0.45, recall: 0.45, precision: 0.4494949494949495, f_beta: 0.4486215538847118
train: step: 2113, loss: 0.6930326819419861, acc: 0.5, recall: 0.5, precision: 0.5, f_beta: 0.49979991996798717
train: step: 2114, loss: 0.6925234198570251, acc: 0.55, recall: 0.55, precision: 0.5634195839675291, f_beta: 0.5248653785239151
train: step: 2115, loss: 0.6927949786186218, acc: 0.54, recall: 0.54, precision: 0.5476190476190477, f_beta: 0.5208333333333333
train: step: 2116, loss: 0.6939013600349426, acc: 0.49, recall: 0.49, precision: 0.48966515088879703, f_beta: 0.48583526565177937
train: step: 2117, loss: 0.6930360198020935, acc: 0.51, recall: 0.51, precision: 0.5101999184006528, f_beta: 0.5075871771681238
train: step: 2118, loss: 0.6940779089927673, acc: 0.45, recall: 0.45, precision: 0.4499799919967987, f_beta: 0.44994499449944997
train: step: 2119, loss: 0.693169116973877, acc: 0.49, recall: 0.49, precision: 0.4898000815993472, f_beta: 0.48748869460355737
train: step: 2120, loss: 0.6920885443687439, acc: 0.61, recall: 0.61, precision: 0.6136833402232327, f_beta: 0.6068152031454783
train: step: 2121, loss: 0.6923527717590332, acc: 0.56, recall: 0.5599999999999999, precision: 0.5651041666666667, f_beta: 0.5512035903712771
train: step: 2122, loss: 0.6934629082679749, acc: 0.52, recall: 0.52, precision: 0.5222816399286987, f_beta: 0.5073891625615763
train: step: 2123, loss: 0.691882312297821, acc: 0.52, recall: 0.52, precision: 0.5248015873015872, f_beta: 0.49558638083228246
train: step: 2124, loss: 0.6937847137451172, acc: 0.47, recall: 0.47000000000000003, precision: 0.4689954526663911, f_beta: 0.46567194273616297
train: step: 2125, loss: 0.6952002048492432, acc: 0.39, recall: 0.39, precision: 0.38202488202488205, f_beta: 0.3795137829315431
train: step: 2126, loss: 0.6932632923126221, acc: 0.51, recall: 0.51, precision: 0.5101999184006528, f_beta: 0.5075871771681238
train: step: 2127, loss: 0.694267988204956, acc: 0.46, recall: 0.45999999999999996, precision: 0.4417249417249417, f_beta: 0.4140625
train: step: 2128, loss: 0.6927563548088074, acc: 0.48, recall: 0.48, precision: 0.47987117552334946, f_beta: 0.47916666666666663
train: step: 2129, loss: 0.6931572556495667, acc: 0.51, recall: 0.51, precision: 0.51010101010101, f_beta: 0.5087719298245614
train: step: 2130, loss: 0.6927998065948486, acc: 0.51, recall: 0.51, precision: 0.5221043324491601, f_beta: 0.43227899432278993
train: step: 2131, loss: 0.6933048367500305, acc: 0.47, recall: 0.47, precision: 0.46703296703296704, f_beta: 0.4578005115089514
train: step: 2132, loss: 0.693183958530426, acc: 0.53, recall: 0.53, precision: 0.530108390204737, f_beta: 0.5295766189570613
train: step: 2133, loss: 0.6932428479194641, acc: 0.49, recall: 0.49, precision: 0.48869289914066033, f_beta: 0.4748223663886314
train: step: 2134, loss: 0.6929593086242676, acc: 0.51, recall: 0.51, precision: 0.510508617065994, f_beta: 0.5039983804028748
train: step: 2135, loss: 0.6934319138526917, acc: 0.45, recall: 0.45, precision: 0.449000407996736, f_beta: 0.4472917294744247
train: step: 2136, loss: 0.6926921010017395, acc: 0.54, recall: 0.54, precision: 0.5410509031198687, f_beta: 0.537037037037037
train: step: 2137, loss: 0.6928537487983704, acc: 0.52, recall: 0.52, precision: 0.5212224108658743, f_beta: 0.512987012987013
train: step: 2138, loss: 0.6935754418373108, acc: 0.47, recall: 0.47, precision: 0.4696969696969697, f_beta: 0.468671679197995
train: step: 2139, loss: 0.6935964226722717, acc: 0.5, recall: 0.5, precision: 0.5, f_beta: 0.49919871794871795
train: step: 2140, loss: 0.6928899884223938, acc: 0.49, recall: 0.49, precision: 0.4899959983993597, f_beta: 0.48994899489948995
train: step: 2141, loss: 0.6922829151153564, acc: 0.59, recall: 0.59, precision: 0.5965250965250966, f_beta: 0.5829518868884142
train: step: 2142, loss: 0.6931746602058411, acc: 0.49, recall: 0.49, precision: 0.4899959983993597, f_beta: 0.48994899489948995
train: step: 2143, loss: 0.6934820413589478, acc: 0.47, recall: 0.47, precision: 0.46847414880201765, f_beta: 0.46350845227249715
train: step: 2144, loss: 0.6920878887176514, acc: 0.57, recall: 0.57, precision: 0.575075075075075, f_beta: 0.562608076492727
train: step: 2145, loss: 0.6936790943145752, acc: 0.42, recall: 0.42000000000000004, precision: 0.4178981937602627, f_beta: 0.4162640901771337
train: step: 2146, loss: 0.6937028765678406, acc: 0.46, recall: 0.45999999999999996, precision: 0.45993589743589747, f_beta: 0.45978391356542614
train: step: 2147, loss: 0.6922380328178406, acc: 0.54, recall: 0.54, precision: 0.5459558823529411, f_beta: 0.524596940884663
train: step: 2148, loss: 0.6934400200843811, acc: 0.49, recall: 0.49, precision: 0.4898989898989899, f_beta: 0.48872180451127817
train: step: 2149, loss: 0.693031370639801, acc: 0.48, recall: 0.48, precision: 0.47987117552334946, f_beta: 0.47916666666666663
train: step: 2150, loss: 0.6930572390556335, acc: 0.5, recall: 0.5, precision: 0.5, f_beta: 0.49000407996736023
train: step: 2151, loss: 0.6928427219390869, acc: 0.5, recall: 0.5, precision: 0.5, f_beta: 0.4967793880837359
train: step: 2152, loss: 0.6932921409606934, acc: 0.44, recall: 0.44, precision: 0.43961352657004826, f_beta: 0.4391025641025641
train: step: 2153, loss: 0.6918497681617737, acc: 0.62, recall: 0.62, precision: 0.6201923076923077, f_beta: 0.6198479391756702
train: step: 2154, loss: 0.6929768919944763, acc: 0.47, recall: 0.47000000000000003, precision: 0.4689954526663911, f_beta: 0.46567194273616297
train: step: 2155, loss: 0.6920133829116821, acc: 0.61, recall: 0.61, precision: 0.6208791208791209, f_beta: 0.6010230179028133
train: step: 2156, loss: 0.6931872367858887, acc: 0.53, recall: 0.53, precision: 0.533921302578019, f_beta: 0.5160127690248172
train: step: 2157, loss: 0.6930721402168274, acc: 0.52, recall: 0.52, precision: 0.5212224108658743, f_beta: 0.512987012987013
train: step: 2158, loss: 0.6941539645195007, acc: 0.41, recall: 0.41000000000000003, precision: 0.40820073439412485, f_beta: 0.4070947643452919
train: step: 2159, loss: 0.6933284997940063, acc: 0.51, recall: 0.51, precision: 0.5100040016006402, f_beta: 0.50995099509951
train: step: 2160, loss: 0.6926632523536682, acc: 0.54, recall: 0.54, precision: 0.5424448217317488, f_beta: 0.5332792207792207
train: step: 2161, loss: 0.693551242351532, acc: 0.45, recall: 0.45, precision: 0.4499799919967987, f_beta: 0.44994499449944997
train: step: 2162, loss: 0.6920703649520874, acc: 0.54, recall: 0.54, precision: 0.5459558823529411, f_beta: 0.524596940884663
train: step: 2163, loss: 0.6920708417892456, acc: 0.56, recall: 0.56, precision: 0.5615763546798029, f_beta: 0.5571658615136876
train: step: 2164, loss: 0.6919881701469421, acc: 0.56, recall: 0.56, precision: 0.5714285714285714, f_beta: 0.5416666666666666
train: step: 2165, loss: 0.6921729445457458, acc: 0.49, recall: 0.49, precision: 0.4898989898989899, f_beta: 0.48872180451127817
train: step: 2166, loss: 0.6930708885192871, acc: 0.55, recall: 0.55, precision: 0.5500200080032013, f_beta: 0.54995499549955
train: step: 2167, loss: 0.692180335521698, acc: 0.55, recall: 0.55, precision: 0.5565355042966984, f_beta: 0.5366079703429101
train: step: 2168, loss: 0.6940019726753235, acc: 0.39, recall: 0.39, precision: 0.3888888888888889, f_beta: 0.38847117794486213
train: step: 2169, loss: 0.6947537064552307, acc: 0.42, recall: 0.42000000000000004, precision: 0.4007936507936508, f_beta: 0.3905002101723414
train: step: 2170, loss: 0.6920631527900696, acc: 0.59, recall: 0.59, precision: 0.5900360144057624, f_beta: 0.5899589958995899
train: step: 2171, loss: 0.6927966475486755, acc: 0.5, recall: 0.5, precision: 0.5, f_beta: 0.4949494949494949
train: step: 2172, loss: 0.6933794617652893, acc: 0.52, recall: 0.52, precision: 0.5208333333333334, f_beta: 0.5151515151515151
train: step: 2173, loss: 0.6930400729179382, acc: 0.57, recall: 0.57, precision: 0.575075075075075, f_beta: 0.562608076492727
train: step: 2174, loss: 0.692976713180542, acc: 0.56, recall: 0.56, precision: 0.56, f_beta: 0.56
train: step: 2175, loss: 0.6942707896232605, acc: 0.44, recall: 0.44, precision: 0.43990384615384615, f_beta: 0.4397759103641457
train: step: 2176, loss: 0.6931135058403015, acc: 0.54, recall: 0.54, precision: 0.5405844155844155, f_beta: 0.5383380168606985
train: step: 2177, loss: 0.6930748224258423, acc: 0.49, recall: 0.49, precision: 0.4873160832064941, f_beta: 0.4615140956604371
train: step: 2178, loss: 0.6923096179962158, acc: 0.51, recall: 0.51, precision: 0.5113071008593397, f_beta: 0.49541756770672435
train: step: 2179, loss: 0.6938149333000183, acc: 0.41, recall: 0.41000000000000003, precision: 0.4069863579991732, f_beta: 0.40518197398931344
train: step: 2180, loss: 0.6938192844390869, acc: 0.46, recall: 0.46, precision: 0.46, f_beta: 0.46
train: step: 2181, loss: 0.6941553354263306, acc: 0.37, recall: 0.37, precision: 0.3674010607915137, f_beta: 0.36689779921615917
train: step: 2182, loss: 0.6937825083732605, acc: 0.46, recall: 0.45999999999999996, precision: 0.4554367201426025, f_beta: 0.44581280788177335
train: step: 2183, loss: 0.6939728260040283, acc: 0.43, recall: 0.43, precision: 0.4292929292929293, f_beta: 0.4285714285714286
train: step: 2184, loss: 0.694054126739502, acc: 0.45, recall: 0.45, precision: 0.4494949494949495, f_beta: 0.4486215538847118
train: step: 2185, loss: 0.6928092837333679, acc: 0.53, recall: 0.53, precision: 0.5310045473336089, f_beta: 0.5261619114830125
train: step: 2186, loss: 0.6923922896385193, acc: 0.53, recall: 0.53, precision: 0.530108390204737, f_beta: 0.5295766189570613
train: step: 2187, loss: 0.692487895488739, acc: 0.56, recall: 0.56, precision: 0.5600961538461539, f_beta: 0.5598239295718288
train: step: 2188, loss: 0.6923496127128601, acc: 0.57, recall: 0.5700000000000001, precision: 0.5700280112044818, f_beta: 0.56995699569957
train: step: 2189, loss: 0.6930946111679077, acc: 0.5, recall: 0.5, precision: 0.5, f_beta: 0.49919871794871795
train: step: 2190, loss: 0.6933732032775879, acc: 0.52, recall: 0.52, precision: 0.5200320512820513, f_beta: 0.5198079231692677
train: step: 2191, loss: 0.6926406025886536, acc: 0.54, recall: 0.54, precision: 0.5424448217317488, f_beta: 0.5332792207792207
train: step: 2192, loss: 0.6940850019454956, acc: 0.46, recall: 0.46, precision: 0.4594155844155844, f_beta: 0.45804897631473307
train: step: 2193, loss: 0.692505419254303, acc: 0.54, recall: 0.54, precision: 0.5410509031198687, f_beta: 0.537037037037037
train: step: 2194, loss: 0.6930325031280518, acc: 0.52, recall: 0.52, precision: 0.5274122807017544, f_beta: 0.4851994851994852
train: step: 2195, loss: 0.6927140951156616, acc: 0.47, recall: 0.47, precision: 0.46782496782496785, f_beta: 0.4608890245142916
train: step: 2196, loss: 0.6921384334564209, acc: 0.55, recall: 0.55, precision: 0.5536250536250535, f_beta: 0.5422642660970399
train: step: 2197, loss: 0.6928249597549438, acc: 0.52, recall: 0.52, precision: 0.5202922077922079, f_beta: 0.5182657567242072
train: step: 2198, loss: 0.6935267448425293, acc: 0.45, recall: 0.45, precision: 0.4499799919967987, f_beta: 0.44994499449944997
train: step: 2199, loss: 0.6931272149085999, acc: 0.53, recall: 0.53, precision: 0.5350631136044881, f_beta: 0.512397551613238
train: step: 2200, loss: 0.6924318075180054, acc: 0.54, recall: 0.54, precision: 0.5400641025641025, f_beta: 0.5398159263705482
train: step: 2201, loss: 0.6929981708526611, acc: 0.49, recall: 0.49, precision: 0.48996386993175434, f_beta: 0.4895405865278751
train: step: 2202, loss: 0.6942079663276672, acc: 0.43, recall: 0.43, precision: 0.42307692307692313, f_beta: 0.41687979539641945
train: step: 2203, loss: 0.6929522156715393, acc: 0.5, recall: 0.5, precision: 0.5, f_beta: 0.4949494949494949
train: step: 2204, loss: 0.6927453875541687, acc: 0.59, recall: 0.59, precision: 0.594577553593947, f_beta: 0.58497823666363
train: step: 2205, loss: 0.6932803988456726, acc: 0.48, recall: 0.48000000000000004, precision: 0.47947454844006565, f_beta: 0.47665056360708535
train: step: 2206, loss: 0.6938117742538452, acc: 0.46, recall: 0.45999999999999996, precision: 0.4554367201426025, f_beta: 0.44581280788177335
train: step: 2207, loss: 0.6932222843170166, acc: 0.53, recall: 0.53, precision: 0.530108390204737, f_beta: 0.5295766189570613
train: step: 2208, loss: 0.6937780976295471, acc: 0.47, recall: 0.47, precision: 0.466078697421981, f_beta: 0.4542271650705386
train: step: 2209, loss: 0.6931757926940918, acc: 0.45, recall: 0.45, precision: 0.44832575444398515, f_beta: 0.44550861982054646
train: step: 2210, loss: 0.6920313835144043, acc: 0.57, recall: 0.5700000000000001, precision: 0.5735603194619587, f_beta: 0.5647332725984411
train: step: 2211, loss: 0.6923994421958923, acc: 0.53, recall: 0.53, precision: 0.533921302578019, f_beta: 0.5160127690248172
train: step: 2212, loss: 0.6927696466445923, acc: 0.55, recall: 0.55, precision: 0.5584385226741468, f_beta: 0.5331465919701215
train: step: 2213, loss: 0.6928903460502625, acc: 0.54, recall: 0.54, precision: 0.5402576489533011, f_beta: 0.5392628205128205
train: step: 2214, loss: 0.6934839487075806, acc: 0.54, recall: 0.54, precision: 0.5677506775067751, f_beta: 0.4875222816399287
train: step: 2215, loss: 0.6939103603363037, acc: 0.48, recall: 0.48000000000000004, precision: 0.47947454844006565, f_beta: 0.47665056360708535
train: step: 2216, loss: 0.6923206448554993, acc: 0.52, recall: 0.52, precision: 0.5202922077922079, f_beta: 0.5182657567242072
train: step: 2217, loss: 0.6937812566757202, acc: 0.49, recall: 0.49, precision: 0.4898000815993472, f_beta: 0.48748869460355737
train: step: 2218, loss: 0.69287109375, acc: 0.54, recall: 0.54, precision: 0.5416666666666667, f_beta: 0.5353535353535354
train: step: 2219, loss: 0.6935520768165588, acc: 0.5, recall: 0.5, precision: 0.5, f_beta: 0.49979991996798717
train: step: 2220, loss: 0.6934875249862671, acc: 0.44, recall: 0.44, precision: 0.4331550802139037, f_beta: 0.4252873563218391
train: step: 2221, loss: 0.6934862732887268, acc: 0.46, recall: 0.45999999999999996, precision: 0.4565972222222222, f_beta: 0.44920440636474906
train: step: 2222, loss: 0.693210244178772, acc: 0.47, recall: 0.47, precision: 0.466078697421981, f_beta: 0.4542271650705386
train: step: 2223, loss: 0.6915610432624817, acc: 0.6, recall: 0.6, precision: 0.6041666666666667, f_beta: 0.595959595959596
train: step: 2224, loss: 0.6931481957435608, acc: 0.47, recall: 0.47, precision: 0.4696969696969697, f_beta: 0.468671679197995
train: step: 2225, loss: 0.6939089298248291, acc: 0.48, recall: 0.48, precision: 0.47996794871794873, f_beta: 0.4797919167667067
train: step: 2226, loss: 0.6926534175872803, acc: 0.52, recall: 0.52, precision: 0.5201288244766505, f_beta: 0.5192307692307692
train: step: 2227, loss: 0.693561315536499, acc: 0.51, recall: 0.51, precision: 0.5101999184006528, f_beta: 0.5075871771681238
train: step: 2228, loss: 0.6930672526359558, acc: 0.51, recall: 0.51, precision: 0.5107250107250108, f_beta: 0.5015766453056658
train: step: 2229, loss: 0.6918694972991943, acc: 0.55, recall: 0.55, precision: 0.5516742455560149, f_beta: 0.5463252343986289
train: step: 2230, loss: 0.6927924156188965, acc: 0.5, recall: 0.5, precision: 0.5, f_beta: 0.47456914670029426
train: step: 2231, loss: 0.6931048035621643, acc: 0.49, recall: 0.49, precision: 0.4898000815993472, f_beta: 0.48748869460355737
train: step: 2232, loss: 0.6935021877288818, acc: 0.48, recall: 0.48, precision: 0.4787775891341256, f_beta: 0.4724025974025974
train: step: 2233, loss: 0.6932098865509033, acc: 0.53, recall: 0.53, precision: 0.5380517503805176, f_beta: 0.5037482842360891
train: step: 2234, loss: 0.6924683451652527, acc: 0.55, recall: 0.55, precision: 0.5500200080032013, f_beta: 0.54995499549955
train: step: 2235, loss: 0.6931315064430237, acc: 0.44, recall: 0.44, precision: 0.43990384615384615, f_beta: 0.4397759103641457
train: step: 2236, loss: 0.6932392716407776, acc: 0.49, recall: 0.49, precision: 0.4899959983993597, f_beta: 0.48994899489948995
train: step: 2237, loss: 0.6922539472579956, acc: 0.55, recall: 0.55, precision: 0.5525430853299707, f_beta: 0.5444883085332524
train: step: 2238, loss: 0.6930850148200989, acc: 0.52, recall: 0.52, precision: 0.5222816399286987, f_beta: 0.5073891625615763
train: step: 2239, loss: 0.6925843954086304, acc: 0.52, recall: 0.52, precision: 0.5238095238095237, f_beta: 0.5
train: step: 2240, loss: 0.6932211518287659, acc: 0.49, recall: 0.49, precision: 0.48996386993175434, f_beta: 0.4895405865278751
train: step: 2241, loss: 0.6933130025863647, acc: 0.51, recall: 0.51, precision: 0.5100040016006402, f_beta: 0.50995099509951
train: step: 2242, loss: 0.6936909556388855, acc: 0.45, recall: 0.45, precision: 0.449000407996736, f_beta: 0.4472917294744247
train: step: 2243, loss: 0.6932833790779114, acc: 0.51, recall: 0.51, precision: 0.5113071008593397, f_beta: 0.49541756770672435
train: step: 2244, loss: 0.6938356161117554, acc: 0.45, recall: 0.45, precision: 0.4494949494949495, f_beta: 0.4486215538847118
train: step: 2245, loss: 0.6929923295974731, acc: 0.5, recall: 0.5, precision: 0.5, f_beta: 0.49919871794871795
train: step: 2246, loss: 0.6931222677230835, acc: 0.51, recall: 0.51, precision: 0.510334849111203, f_beta: 0.5059985885673959
train: step: 2247, loss: 0.6928473114967346, acc: 0.5, recall: 0.5, precision: 0.5, f_beta: 0.46943972835314096
train: step: 2248, loss: 0.6922416090965271, acc: 0.57, recall: 0.5700000000000001, precision: 0.5735603194619587, f_beta: 0.5647332725984411
train: step: 2249, loss: 0.6940315961837769, acc: 0.42, recall: 0.42000000000000004, precision: 0.4194847020933977, f_beta: 0.41907051282051283
train: step: 2250, loss: 0.6940931677818298, acc: 0.44, recall: 0.44, precision: 0.43990384615384615, f_beta: 0.4397759103641457
train: step: 2251, loss: 0.6934792995452881, acc: 0.42, recall: 0.42000000000000004, precision: 0.4198717948717949, f_beta: 0.41976790716286516
train: step: 2252, loss: 0.6925068497657776, acc: 0.53, recall: 0.53, precision: 0.5364254492472074, f_beta: 0.5083167695365624
train: step: 2253, loss: 0.6918399930000305, acc: 0.55, recall: 0.55, precision: 0.5500200080032013, f_beta: 0.54995499549955
train: step: 2254, loss: 0.6937525272369385, acc: 0.5, recall: 0.5, precision: 0.5, f_beta: 0.4981934965877158
train: step: 2255, loss: 0.6938582062721252, acc: 0.44, recall: 0.44000000000000006, precision: 0.4363327674023769, f_beta: 0.43181818181818177
train: step: 2256, loss: 0.6929555535316467, acc: 0.52, recall: 0.52, precision: 0.5200320512820513, f_beta: 0.5198079231692677
train: step: 2257, loss: 0.6931487917900085, acc: 0.48, recall: 0.48000000000000004, precision: 0.47619047619047616, f_beta: 0.45833333333333337
train: step: 2258, loss: 0.6929334402084351, acc: 0.58, recall: 0.58, precision: 0.58, f_beta: 0.58
train: step: 2259, loss: 0.6928809285163879, acc: 0.5, recall: 0.5, precision: 0.5, f_beta: 0.49000407996736023
train: step: 2260, loss: 0.6932528018951416, acc: 0.49, recall: 0.49, precision: 0.48949138293400585, f_beta: 0.48375341633768604
train: step: 2261, loss: 0.692629337310791, acc: 0.52, recall: 0.52, precision: 0.525987525987526, f_beta: 0.49066213921901536
train: step: 2262, loss: 0.6925163865089417, acc: 0.48, recall: 0.48, precision: 0.4782986111111111, f_beta: 0.46960424316605465
train: step: 2263, loss: 0.6942390203475952, acc: 0.45, recall: 0.45000000000000007, precision: 0.44745691467002946, f_beta: 0.44326348820730843
train: step: 2264, loss: 0.6935155987739563, acc: 0.43, recall: 0.43, precision: 0.42860057119543044, f_beta: 0.42719324690985827
train: step: 2265, loss: 0.6926062703132629, acc: 0.54, recall: 0.54, precision: 0.5416666666666667, f_beta: 0.5353535353535354
train: step: 2266, loss: 0.6933925747871399, acc: 0.46, recall: 0.45999999999999996, precision: 0.45974235104669886, f_beta: 0.4591346153846153
train: step: 2267, loss: 0.6933200359344482, acc: 0.51, recall: 0.51, precision: 0.5116877045348294, f_beta: 0.4916485112563544
train: step: 2268, loss: 0.6931983828544617, acc: 0.51, recall: 0.51, precision: 0.5100040016006402, f_beta: 0.50995099509951
train: step: 2269, loss: 0.6928333044052124, acc: 0.53, recall: 0.53, precision: 0.5364254492472074, f_beta: 0.5083167695365624
train: step: 2270, loss: 0.6945599317550659, acc: 0.42, recall: 0.42000000000000004, precision: 0.4178981937602627, f_beta: 0.4162640901771337
train: step: 2271, loss: 0.6926844120025635, acc: 0.55, recall: 0.55, precision: 0.5516742455560149, f_beta: 0.5463252343986289
train: step: 2272, loss: 0.6931931376457214, acc: 0.5, recall: 0.5, precision: 0.5, f_beta: 0.49979991996798717
train: step: 2273, loss: 0.6927677989006042, acc: 0.53, recall: 0.53, precision: 0.5315258511979823, f_beta: 0.5242433444680635
train: step: 2274, loss: 0.694021463394165, acc: 0.45, recall: 0.44999999999999996, precision: 0.44637494637494635, f_beta: 0.44054521411860437
train: step: 2275, loss: 0.6926011443138123, acc: 0.56, recall: 0.56, precision: 0.56, f_beta: 0.56
train: step: 2276, loss: 0.6924856305122375, acc: 0.5, recall: 0.5, precision: 0.5, f_beta: 0.49919871794871795
train: step: 2277, loss: 0.6942983865737915, acc: 0.42, recall: 0.42, precision: 0.38344988344988346, f_beta: 0.3706597222222222
train: step: 2278, loss: 0.6935154795646667, acc: 0.48, recall: 0.48, precision: 0.47916666666666663, f_beta: 0.4747474747474748
train: step: 2279, loss: 0.6928533911705017, acc: 0.53, recall: 0.53, precision: 0.5350631136044881, f_beta: 0.512397551613238
train: step: 2280, loss: 0.6926831007003784, acc: 0.54, recall: 0.54, precision: 0.5402576489533011, f_beta: 0.5392628205128205
train: step: 2281, loss: 0.6928572654724121, acc: 0.56, recall: 0.56, precision: 0.5600961538461539, f_beta: 0.5598239295718288
train: step: 2282, loss: 0.6929561495780945, acc: 0.5, recall: 0.5, precision: 0.5, f_beta: 0.5
train: step: 2283, loss: 0.6930064558982849, acc: 0.54, recall: 0.54, precision: 0.5400641025641025, f_beta: 0.5398159263705482
train: step: 2284, loss: 0.6931749582290649, acc: 0.54, recall: 0.54, precision: 0.5400641025641025, f_beta: 0.5398159263705482
train: step: 2285, loss: 0.6938078999519348, acc: 0.41, recall: 0.41, precision: 0.3948106591865358, f_beta: 0.3879033094719369
train: step: 2286, loss: 0.6933309435844421, acc: 0.5, recall: 0.5, precision: 0.5, f_beta: 0.486863711001642
train: step: 2287, loss: 0.6936435699462891, acc: 0.49, recall: 0.49, precision: 0.48949138293400585, f_beta: 0.48375341633768604
train: step: 2288, loss: 0.692641019821167, acc: 0.51, recall: 0.51, precision: 0.51010101010101, f_beta: 0.5087719298245614
train: step: 2289, loss: 0.693041205406189, acc: 0.57, recall: 0.5700000000000001, precision: 0.6054852320675106, f_beta: 0.5305164319248826
train: step: 2290, loss: 0.6931852698326111, acc: 0.55, recall: 0.55, precision: 0.550999592003264, f_beta: 0.5477841422972565
train: step: 2291, loss: 0.6930831074714661, acc: 0.52, recall: 0.52, precision: 0.5222816399286987, f_beta: 0.5073891625615763
train: step: 2292, loss: 0.693561851978302, acc: 0.5, recall: 0.5, precision: 0.5, f_beta: 0.4967793880837359
train: step: 2293, loss: 0.692710816860199, acc: 0.5, recall: 0.5, precision: 0.5, f_beta: 0.4949494949494949
train: step: 2294, loss: 0.6922006011009216, acc: 0.6, recall: 0.6000000000000001, precision: 0.6006441223832528, f_beta: 0.5993589743589743
train: step: 2295, loss: 0.693413496017456, acc: 0.54, recall: 0.54, precision: 0.5424448217317488, f_beta: 0.5332792207792207
train: step: 2296, loss: 0.6918022632598877, acc: 0.61, recall: 0.61, precision: 0.6155947877259353, f_beta: 0.6052232007288187
train: step: 2297, loss: 0.6929455399513245, acc: 0.49, recall: 0.49, precision: 0.48869289914066033, f_beta: 0.4748223663886314
train: step: 2298, loss: 0.693024754524231, acc: 0.54, recall: 0.54, precision: 0.5445632798573975, f_beta: 0.5279146141215106
train: step: 2299, loss: 0.6928489208221436, acc: 0.53, recall: 0.53, precision: 0.5315258511979823, f_beta: 0.5242433444680635
train: step: 2300, loss: 0.6921408772468567, acc: 0.55, recall: 0.55, precision: 0.5505050505050505, f_beta: 0.5488721804511277
train: step: 2301, loss: 0.6936576962471008, acc: 0.46, recall: 0.45999999999999996, precision: 0.4554367201426025, f_beta: 0.44581280788177335
train: step: 2302, loss: 0.6918477416038513, acc: 0.6, recall: 0.6, precision: 0.6001602564102564, f_beta: 0.5998399359743898
train: step: 2303, loss: 0.6936236023902893, acc: 0.44, recall: 0.43999999999999995, precision: 0.43489583333333337, f_beta: 0.4288045695634435
train: step: 2304, loss: 0.6932132244110107, acc: 0.54, recall: 0.54, precision: 0.5405844155844155, f_beta: 0.5383380168606985
train: step: 2305, loss: 0.6935014128684998, acc: 0.51, recall: 0.51, precision: 0.51010101010101, f_beta: 0.5087719298245614
train: step: 2306, loss: 0.6930825114250183, acc: 0.53, recall: 0.53, precision: 0.530108390204737, f_beta: 0.5295766189570613
train: step: 2307, loss: 0.692517101764679, acc: 0.54, recall: 0.54, precision: 0.54, f_beta: 0.54
train: step: 2308, loss: 0.6927037835121155, acc: 0.54, recall: 0.54, precision: 0.5400641025641025, f_beta: 0.5398159263705482
train: step: 2309, loss: 0.6938822269439697, acc: 0.49, recall: 0.49, precision: 0.4898989898989899, f_beta: 0.48872180451127817
train: step: 2310, loss: 0.6939914226531982, acc: 0.47, recall: 0.47, precision: 0.4696969696969697, f_beta: 0.468671679197995
train: step: 2311, loss: 0.6938736438751221, acc: 0.46, recall: 0.46, precision: 0.46, f_beta: 0.46
train: step: 2312, loss: 0.693807065486908, acc: 0.46, recall: 0.46, precision: 0.46, f_beta: 0.46
train: step: 2313, loss: 0.6942456960678101, acc: 0.47, recall: 0.47, precision: 0.46703296703296704, f_beta: 0.4578005115089514
train: step: 2314, loss: 0.6943228840827942, acc: 0.41, recall: 0.41000000000000003, precision: 0.4099639855942377, f_beta: 0.4099409940994099
train: step: 2315, loss: 0.6937992572784424, acc: 0.42, recall: 0.42000000000000004, precision: 0.4194847020933977, f_beta: 0.41907051282051283
train: step: 2316, loss: 0.6930782198905945, acc: 0.53, recall: 0.53, precision: 0.533921302578019, f_beta: 0.5160127690248172
train: step: 2317, loss: 0.6929833292961121, acc: 0.5, recall: 0.5, precision: 0.5, f_beta: 0.4967793880837359
train: step: 2318, loss: 0.6934530138969421, acc: 0.47, recall: 0.47, precision: 0.466078697421981, f_beta: 0.4542271650705386
train: step: 2319, loss: 0.6930782794952393, acc: 0.5, recall: 0.5, precision: 0.5, f_beta: 0.49000407996736023
train: step: 2320, loss: 0.6927441358566284, acc: 0.55, recall: 0.55, precision: 0.5500200080032013, f_beta: 0.54995499549955
train: step: 2321, loss: 0.6927711963653564, acc: 0.49, recall: 0.49, precision: 0.48785818358426425, f_beta: 0.46647138822052514
train: step: 2322, loss: 0.6922709941864014, acc: 0.51, recall: 0.51, precision: 0.5100361300682457, f_beta: 0.5095586027424682
train: step: 2323, loss: 0.6930103898048401, acc: 0.46, recall: 0.45999999999999996, precision: 0.45974235104669886, f_beta: 0.4591346153846153
train: step: 2324, loss: 0.6936893463134766, acc: 0.52, recall: 0.52, precision: 0.5202922077922079, f_beta: 0.5182657567242072
train: step: 2325, loss: 0.6924597024917603, acc: 0.56, recall: 0.56, precision: 0.5608766233766234, f_beta: 0.5584102769971899
train: step: 2326, loss: 0.6931993365287781, acc: 0.5, recall: 0.5, precision: 0.5, f_beta: 0.4949494949494949
train: step: 2327, loss: 0.6934297680854797, acc: 0.5, recall: 0.5, precision: 0.5, f_beta: 0.4949494949494949
train: step: 2328, loss: 0.6932694911956787, acc: 0.54, recall: 0.54, precision: 0.5410509031198687, f_beta: 0.537037037037037
train: step: 2329, loss: 0.6931552290916443, acc: 0.51, recall: 0.51, precision: 0.510334849111203, f_beta: 0.5059985885673959
train: step: 2330, loss: 0.6920726299285889, acc: 0.54, recall: 0.54, precision: 0.5410509031198687, f_beta: 0.537037037037037
train: step: 2331, loss: 0.693743109703064, acc: 0.47, recall: 0.47, precision: 0.4699879951980792, f_beta: 0.46994699469946993
train: step: 2332, loss: 0.6924784779548645, acc: 0.55, recall: 0.55, precision: 0.5501806503412284, f_beta: 0.5495946351716544
train: step: 2333, loss: 0.6929544806480408, acc: 0.56, recall: 0.56, precision: 0.5600961538461539, f_beta: 0.5598239295718288
train: step: 2334, loss: 0.693239152431488, acc: 0.53, recall: 0.53, precision: 0.5305997552019583, f_beta: 0.5276856597326902
train: step: 2335, loss: 0.6933395266532898, acc: 0.46, recall: 0.46, precision: 0.45894909688013136, f_beta: 0.4565217391304348
train: step: 2336, loss: 0.6941866874694824, acc: 0.41, recall: 0.41000000000000003, precision: 0.4099639855942377, f_beta: 0.4099409940994099
train: step: 2337, loss: 0.6932201385498047, acc: 0.47, recall: 0.47, precision: 0.46703296703296704, f_beta: 0.4578005115089514
train: step: 2338, loss: 0.6935076713562012, acc: 0.47, recall: 0.47, precision: 0.46782496782496785, f_beta: 0.4608890245142916
train: step: 2339, loss: 0.6927688121795654, acc: 0.53, recall: 0.53, precision: 0.5531537916371367, f_beta: 0.4725620020199753
train: step: 2340, loss: 0.6937155723571777, acc: 0.48, recall: 0.48, precision: 0.4787775891341256, f_beta: 0.4724025974025974
train: step: 2341, loss: 0.6941958069801331, acc: 0.43, recall: 0.43, precision: 0.42307692307692313, f_beta: 0.41687979539641945
train: step: 2342, loss: 0.6933345198631287, acc: 0.46, recall: 0.45999999999999996, precision: 0.45974235104669886, f_beta: 0.4591346153846153
train: step: 2343, loss: 0.6928972601890564, acc: 0.5, recall: 0.5, precision: 0.5, f_beta: 0.4949494949494949
train: step: 2344, loss: 0.6920299530029297, acc: 0.6, recall: 0.6000000000000001, precision: 0.6026272577996716, f_beta: 0.5974235104669887
train: step: 2345, loss: 0.6941419839859009, acc: 0.41, recall: 0.41000000000000003, precision: 0.39072365225837785, f_beta: 0.3827806255884507
train: step: 2346, loss: 0.691287636756897, acc: 0.58, recall: 0.5800000000000001, precision: 0.5805152979066023, f_beta: 0.5793269230769231
train: step: 2347, loss: 0.6934298872947693, acc: 0.52, recall: 0.52, precision: 0.5217013888888888, f_beta: 0.5104039167686658
train: step: 2348, loss: 0.6933037042617798, acc: 0.45, recall: 0.45, precision: 0.44832575444398515, f_beta: 0.44550861982054646
train: step: 2349, loss: 0.6917465329170227, acc: 0.61, recall: 0.61, precision: 0.6179751179751181, f_beta: 0.6032956972841013
train: step: 2350, loss: 0.6933132410049438, acc: 0.46, recall: 0.46, precision: 0.45894909688013136, f_beta: 0.4565217391304348
train: step: 2351, loss: 0.6936087012290955, acc: 0.47, recall: 0.47000000000000003, precision: 0.46357455075279264, f_beta: 0.44554869756250653
train: step: 2352, loss: 0.6937071084976196, acc: 0.46, recall: 0.46, precision: 0.46, f_beta: 0.46
train: step: 2353, loss: 0.6937653422355652, acc: 0.47, recall: 0.47, precision: 0.46847414880201765, f_beta: 0.46350845227249715
train: step: 2354, loss: 0.6917838454246521, acc: 0.58, recall: 0.5800000000000001, precision: 0.5805152979066023, f_beta: 0.5793269230769231
train: step: 2355, loss: 0.6926590204238892, acc: 0.53, recall: 0.53, precision: 0.5305997552019583, f_beta: 0.5276856597326902
train: step: 2356, loss: 0.6932430267333984, acc: 0.49, recall: 0.49, precision: 0.48831229546517063, f_beta: 0.47089947089947093
train: step: 2357, loss: 0.6928800344467163, acc: 0.57, recall: 0.5700000000000001, precision: 0.5723439437784208, f_beta: 0.5664885573142453
train: step: 2358, loss: 0.6921042799949646, acc: 0.54, recall: 0.54, precision: 0.5405844155844155, f_beta: 0.5383380168606985
train: step: 2359, loss: 0.6937642097473145, acc: 0.41, recall: 0.41000000000000003, precision: 0.40347490347490345, f_beta: 0.39985759332723014
train: step: 2360, loss: 0.6928988695144653, acc: 0.52, recall: 0.52, precision: 0.52, f_beta: 0.52
train: step: 2361, loss: 0.6928931474685669, acc: 0.48, recall: 0.48, precision: 0.47771836007130125, f_beta: 0.4663382594417077
train: step: 2362, loss: 0.6931231617927551, acc: 0.5, recall: 0.5, precision: 0.5, f_beta: 0.486863711001642
train: step: 2363, loss: 0.6928814649581909, acc: 0.54, recall: 0.54, precision: 0.5400641025641025, f_beta: 0.5398159263705482
train: step: 2364, loss: 0.6933879256248474, acc: 0.53, recall: 0.53, precision: 0.5350631136044881, f_beta: 0.512397551613238
train: step: 2365, loss: 0.692909300327301, acc: 0.49, recall: 0.49, precision: 0.48785818358426425, f_beta: 0.46647138822052514
train: step: 2366, loss: 0.6931512355804443, acc: 0.53, recall: 0.53, precision: 0.5300120048019208, f_beta: 0.5299529952995299
train: step: 2367, loss: 0.6928808093070984, acc: 0.54, recall: 0.54, precision: 0.5410509031198687, f_beta: 0.537037037037037
train: step: 2368, loss: 0.6921697854995728, acc: 0.56, recall: 0.5599999999999999, precision: 0.5651041666666667, f_beta: 0.5512035903712771
train: step: 2369, loss: 0.6933044195175171, acc: 0.52, recall: 0.52, precision: 0.5222816399286987, f_beta: 0.5073891625615763
train: step: 2370, loss: 0.6925541162490845, acc: 0.53, recall: 0.53, precision: 0.5380517503805176, f_beta: 0.5037482842360891
train: step: 2371, loss: 0.693147599697113, acc: 0.48, recall: 0.48, precision: 0.47970779220779225, f_beta: 0.4781212364512244
train: step: 2372, loss: 0.6928098201751709, acc: 0.56, recall: 0.56, precision: 0.5636672325976231, f_beta: 0.5535714285714286
train: step: 2373, loss: 0.6929358839988708, acc: 0.45, recall: 0.44999999999999996, precision: 0.4498193496587716, f_beta: 0.44950455409868884
train: step: 2374, loss: 0.6918761730194092, acc: 0.58, recall: 0.5800000000000001, precision: 0.5811688311688312, f_beta: 0.5784825371336814
train: step: 2375, loss: 0.6933043003082275, acc: 0.51, recall: 0.51, precision: 0.5100361300682457, f_beta: 0.5095586027424682
train: step: 2376, loss: 0.6940304636955261, acc: 0.4, recall: 0.4, precision: 0.38095238095238093, f_beta: 0.375
train: step: 2377, loss: 0.6939254999160767, acc: 0.49, recall: 0.49, precision: 0.4898989898989899, f_beta: 0.48872180451127817
train: step: 2378, loss: 0.6929059028625488, acc: 0.5, recall: 0.5, precision: 0.5, f_beta: 0.49919871794871795
train: step: 2379, loss: 0.692656934261322, acc: 0.51, recall: 0.51, precision: 0.5116877045348294, f_beta: 0.4916485112563544
train: step: 2380, loss: 0.6931561231613159, acc: 0.5, recall: 0.5, precision: 0.5, f_beta: 0.49919871794871795
train: step: 2381, loss: 0.6924879550933838, acc: 0.56, recall: 0.56, precision: 0.5636672325976231, f_beta: 0.5535714285714286
train: step: 2382, loss: 0.693153440952301, acc: 0.56, recall: 0.56, precision: 0.5608766233766234, f_beta: 0.5584102769971899
train: step: 2383, loss: 0.6935293078422546, acc: 0.49, recall: 0.49, precision: 0.4898989898989899, f_beta: 0.48872180451127817
train: step: 2384, loss: 0.6927255392074585, acc: 0.48, recall: 0.48, precision: 0.4782986111111111, f_beta: 0.46960424316605465
train: step: 2385, loss: 0.691720724105835, acc: 0.59, recall: 0.59, precision: 0.601763907734057, f_beta: 0.5777983729790959
train: step: 2386, loss: 0.6939383149147034, acc: 0.45, recall: 0.45, precision: 0.4499799919967987, f_beta: 0.44994499449944997
train: step: 2387, loss: 0.6930370330810547, acc: 0.49, recall: 0.49, precision: 0.48949138293400585, f_beta: 0.48375341633768604
train: step: 2388, loss: 0.6933424472808838, acc: 0.49, recall: 0.49, precision: 0.48966515088879703, f_beta: 0.48583526565177937
train: step: 2389, loss: 0.69243323802948, acc: 0.55, recall: 0.55, precision: 0.5516742455560149, f_beta: 0.5463252343986289
train: step: 2390, loss: 0.6919018626213074, acc: 0.63, recall: 0.63, precision: 0.6325989392084863, f_beta: 0.6281780725555222
train: step: 2391, loss: 0.6924886107444763, acc: 0.55, recall: 0.55, precision: 0.550999592003264, f_beta: 0.5477841422972565
train: step: 2392, loss: 0.6935268640518188, acc: 0.48, recall: 0.48, precision: 0.47987117552334946, f_beta: 0.47916666666666663
train: step: 2393, loss: 0.6933914422988892, acc: 0.53, recall: 0.53, precision: 0.5300120048019208, f_beta: 0.5299529952995299
train: step: 2394, loss: 0.6931989192962646, acc: 0.49, recall: 0.49, precision: 0.4898000815993472, f_beta: 0.48748869460355737
train: step: 2395, loss: 0.6921700239181519, acc: 0.53, recall: 0.53, precision: 0.5300120048019208, f_beta: 0.5299529952995299
train: step: 2396, loss: 0.6926310062408447, acc: 0.5, recall: 0.5, precision: 0.5, f_beta: 0.49269480519480513
train: step: 2397, loss: 0.6925660967826843, acc: 0.53, recall: 0.53, precision: 0.5303030303030303, f_beta: 0.5288220551378446
train: step: 2398, loss: 0.693021297454834, acc: 0.51, recall: 0.51, precision: 0.510508617065994, f_beta: 0.5039983804028748
train: step: 2399, loss: 0.6924485564231873, acc: 0.47, recall: 0.47, precision: 0.46847414880201765, f_beta: 0.46350845227249715
train: step: 2400, loss: 0.6926151514053345, acc: 0.58, recall: 0.5800000000000001, precision: 0.5833333333333333, f_beta: 0.5757575757575757
train: step: 2401, loss: 0.6929689645767212, acc: 0.56, recall: 0.56, precision: 0.5636672325976231, f_beta: 0.5535714285714286
train: step: 2402, loss: 0.6930059790611267, acc: 0.49, recall: 0.49, precision: 0.48996386993175434, f_beta: 0.4895405865278751
train: step: 2403, loss: 0.693848729133606, acc: 0.45, recall: 0.45, precision: 0.449000407996736, f_beta: 0.4472917294744247
train: step: 2404, loss: 0.693865180015564, acc: 0.46, recall: 0.45999999999999996, precision: 0.45755517826825126, f_beta: 0.4521103896103896
train: step: 2405, loss: 0.693907618522644, acc: 0.42, recall: 0.42000000000000004, precision: 0.4131944444444444, f_beta: 0.40840473276213796
train: step: 2406, loss: 0.692984938621521, acc: 0.55, recall: 0.55, precision: 0.550999592003264, f_beta: 0.5477841422972565
train: step: 2407, loss: 0.6927643418312073, acc: 0.51, recall: 0.51, precision: 0.51010101010101, f_beta: 0.5087719298245614
train: step: 2408, loss: 0.6928393840789795, acc: 0.56, recall: 0.56, precision: 0.5625, f_beta: 0.5555555555555556
train: step: 2409, loss: 0.6942371129989624, acc: 0.39, recall: 0.39, precision: 0.3888888888888889, f_beta: 0.38847117794486213
train: step: 2410, loss: 0.6935299038887024, acc: 0.47, recall: 0.47, precision: 0.46782496782496785, f_beta: 0.4608890245142916
train: step: 2411, loss: 0.6937361359596252, acc: 0.47, recall: 0.47, precision: 0.4699879951980792, f_beta: 0.46994699469946993
train: step: 2412, loss: 0.6927759647369385, acc: 0.53, recall: 0.53, precision: 0.5300120048019208, f_beta: 0.5299529952995299
train: step: 2413, loss: 0.692812979221344, acc: 0.54, recall: 0.54, precision: 0.5410509031198687, f_beta: 0.537037037037037
train: step: 2414, loss: 0.6930790543556213, acc: 0.53, recall: 0.53, precision: 0.5310045473336089, f_beta: 0.5261619114830125
train: step: 2415, loss: 0.6939918398857117, acc: 0.43, recall: 0.43000000000000005, precision: 0.42974708952228025, f_beta: 0.42948653788409574
train: step: 2416, loss: 0.6931349039077759, acc: 0.5, recall: 0.5, precision: 0.5, f_beta: 0.4981934965877158
train: step: 2417, loss: 0.6925977468490601, acc: 0.5, recall: 0.5, precision: 0.5, f_beta: 0.4949494949494949
train: step: 2418, loss: 0.692899763584137, acc: 0.56, recall: 0.56, precision: 0.5603864734299517, f_beta: 0.5592948717948718
train: step: 2419, loss: 0.6934016346931458, acc: 0.48, recall: 0.48, precision: 0.48, f_beta: 0.48
train: step: 2420, loss: 0.693757951259613, acc: 0.45, recall: 0.44999999999999996, precision: 0.4498193496587716, f_beta: 0.44950455409868884
train: step: 2421, loss: 0.6937271356582642, acc: 0.48, recall: 0.48, precision: 0.4787775891341256, f_beta: 0.4724025974025974
train: step: 2422, loss: 0.6936081051826477, acc: 0.47, recall: 0.47, precision: 0.4696969696969697, f_beta: 0.468671679197995
train: step: 2423, loss: 0.6932017803192139, acc: 0.47, recall: 0.47, precision: 0.46782496782496785, f_beta: 0.4608890245142916
train: step: 2424, loss: 0.6936960816383362, acc: 0.49, recall: 0.49, precision: 0.4898989898989899, f_beta: 0.48872180451127817
train: step: 2425, loss: 0.6924822926521301, acc: 0.56, recall: 0.56, precision: 0.5600961538461539, f_beta: 0.5598239295718288
train: step: 2426, loss: 0.6941074132919312, acc: 0.43, recall: 0.43, precision: 0.3627450980392157, f_beta: 0.35042735042735046
train: step: 2427, loss: 0.6922945976257324, acc: 0.55, recall: 0.55, precision: 0.5607090820786789, f_beta: 0.529239460194581
train: step: 2428, loss: 0.692156970500946, acc: 0.57, recall: 0.5700000000000001, precision: 0.5713994288045696, f_beta: 0.567882624861823
train: step: 2429, loss: 0.6929354071617126, acc: 0.51, recall: 0.51, precision: 0.5101999184006528, f_beta: 0.5075871771681238
train: step: 2430, loss: 0.6924037933349609, acc: 0.57, recall: 0.5700000000000001, precision: 0.5700280112044818, f_beta: 0.56995699569957
train: step: 2431, loss: 0.6923219561576843, acc: 0.56, recall: 0.56, precision: 0.5608766233766234, f_beta: 0.5584102769971899
train: step: 2432, loss: 0.694060742855072, acc: 0.42, recall: 0.42, precision: 0.41883116883116883, f_beta: 0.4179044560417503
train: step: 2433, loss: 0.6943069696426392, acc: 0.42, recall: 0.42000000000000004, precision: 0.4178981937602627, f_beta: 0.4162640901771337
train: step: 2434, loss: 0.6934351325035095, acc: 0.5, recall: 0.5, precision: 0.5, f_beta: 0.48325754443985114
train: step: 2435, loss: 0.6929538249969482, acc: 0.54, recall: 0.54, precision: 0.5424448217317488, f_beta: 0.5332792207792207
train: step: 2436, loss: 0.693051815032959, acc: 0.48, recall: 0.48, precision: 0.47086247086247085, f_beta: 0.4357638888888889
train: step: 2437, loss: 0.6932079792022705, acc: 0.52, recall: 0.52, precision: 0.5200320512820513, f_beta: 0.5198079231692677
train: step: 2438, loss: 0.6924008131027222, acc: 0.52, recall: 0.52, precision: 0.5201288244766505, f_beta: 0.5192307692307692
train: step: 2439, loss: 0.6929125785827637, acc: 0.49, recall: 0.49, precision: 0.4899959983993597, f_beta: 0.48994899489948995
train: step: 2440, loss: 0.6928934454917908, acc: 0.52, recall: 0.52, precision: 0.5291375291375291, f_beta: 0.47916666666666663
train: step: 2441, loss: 0.6940382122993469, acc: 0.43, recall: 0.43000000000000005, precision: 0.42974708952228025, f_beta: 0.42948653788409574
train: step: 2442, loss: 0.6928443908691406, acc: 0.5, recall: 0.5, precision: 0.5, f_beta: 0.4949494949494949
train: step: 2443, loss: 0.6939713358879089, acc: 0.39, recall: 0.39, precision: 0.3878008975928192, f_beta: 0.38699628178072554
train: step: 2444, loss: 0.6934450268745422, acc: 0.51, recall: 0.51, precision: 0.5121418164157359, f_beta: 0.4873940788785438
train: step: 2445, loss: 0.6931052207946777, acc: 0.56, recall: 0.56, precision: 0.5668449197860963, f_beta: 0.548440065681445
train: step: 2446, loss: 0.69288170337677, acc: 0.49, recall: 0.49, precision: 0.48996386993175434, f_beta: 0.4895405865278751
train: step: 2447, loss: 0.6925195455551147, acc: 0.54, recall: 0.54, precision: 0.5410509031198687, f_beta: 0.537037037037037
train: step: 2448, loss: 0.6926798224449158, acc: 0.54, recall: 0.54, precision: 0.5424448217317488, f_beta: 0.5332792207792207
train: step: 2449, loss: 0.6932264566421509, acc: 0.51, recall: 0.51, precision: 0.5126839167935058, f_beta: 0.48263118994826315
train: step: 2450, loss: 0.6940741539001465, acc: 0.44, recall: 0.44, precision: 0.43990384615384615, f_beta: 0.4397759103641457
train: step: 2451, loss: 0.6935663819313049, acc: 0.52, recall: 0.52, precision: 0.5200320512820513, f_beta: 0.5198079231692677
train: step: 2452, loss: 0.6938296556472778, acc: 0.45, recall: 0.45, precision: 0.4494949494949495, f_beta: 0.4486215538847118
train: step: 2453, loss: 0.6935923099517822, acc: 0.52, recall: 0.52, precision: 0.5212224108658743, f_beta: 0.512987012987013
train: step: 2454, loss: 0.6931098699569702, acc: 0.48, recall: 0.48, precision: 0.48, f_beta: 0.48
train: step: 2455, loss: 0.6916406154632568, acc: 0.58, recall: 0.58, precision: 0.58, f_beta: 0.58
train: step: 2456, loss: 0.6928922533988953, acc: 0.52, recall: 0.52, precision: 0.5200320512820513, f_beta: 0.5198079231692677
train: step: 2457, loss: 0.6923656463623047, acc: 0.56, recall: 0.56, precision: 0.5600961538461539, f_beta: 0.5598239295718288
train: step: 2458, loss: 0.6931800842285156, acc: 0.47, recall: 0.47000000000000003, precision: 0.4694002447980416, f_beta: 0.467390212038991
train: step: 2459, loss: 0.6940945982933044, acc: 0.43, recall: 0.43, precision: 0.4292929292929293, f_beta: 0.4285714285714286
train: step: 2460, loss: 0.6935400366783142, acc: 0.44, recall: 0.44, precision: 0.43912337662337664, f_beta: 0.43797671617824174
train: step: 2461, loss: 0.6934831738471985, acc: 0.51, recall: 0.51, precision: 0.5100040016006402, f_beta: 0.50995099509951
train: step: 2462, loss: 0.6931596994400024, acc: 0.51, recall: 0.51, precision: 0.510334849111203, f_beta: 0.5059985885673959
train: step: 2463, loss: 0.6929844617843628, acc: 0.55, recall: 0.55, precision: 0.5501806503412284, f_beta: 0.5495946351716544
train: step: 2464, loss: 0.6938457489013672, acc: 0.46, recall: 0.45999999999999996, precision: 0.45993589743589747, f_beta: 0.45978391356542614
train: step: 2465, loss: 0.6932076811790466, acc: 0.47, recall: 0.47000000000000003, precision: 0.4694002447980416, f_beta: 0.467390212038991
train: step: 2466, loss: 0.6940038800239563, acc: 0.44, recall: 0.44, precision: 0.43842364532019706, f_beta: 0.4363929146537842
train: step: 2467, loss: 0.6936207413673401, acc: 0.49, recall: 0.49, precision: 0.48927498927498925, f_beta: 0.48123283490997865
train: step: 2468, loss: 0.6920334100723267, acc: 0.58, recall: 0.5800000000000001, precision: 0.5805152979066023, f_beta: 0.5793269230769231
train: step: 2469, loss: 0.6938596367835999, acc: 0.45, recall: 0.45, precision: 0.4415614773258532, f_beta: 0.4294013901857039
train: step: 2470, loss: 0.6926441192626953, acc: 0.53, recall: 0.53, precision: 0.5315258511979823, f_beta: 0.5242433444680635
train: step: 2471, loss: 0.693276584148407, acc: 0.46, recall: 0.46, precision: 0.45894909688013136, f_beta: 0.4565217391304348
train: step: 2472, loss: 0.6928994655609131, acc: 0.5, recall: 0.5, precision: 0.5, f_beta: 0.4967793880837359
train: step: 2473, loss: 0.6926222443580627, acc: 0.53, recall: 0.53, precision: 0.5315258511979823, f_beta: 0.5242433444680635
train: step: 2474, loss: 0.6919283270835876, acc: 0.56, recall: 0.56, precision: 0.5603864734299517, f_beta: 0.5592948717948718
train: step: 2475, loss: 0.6935377717018127, acc: 0.48, recall: 0.48000000000000004, precision: 0.47947454844006565, f_beta: 0.47665056360708535
train: step: 2476, loss: 0.6928862929344177, acc: 0.52, recall: 0.52, precision: 0.5205254515599343, f_beta: 0.5169082125603865
train: step: 2477, loss: 0.6941046118736267, acc: 0.41, recall: 0.41000000000000003, precision: 0.4099639855942377, f_beta: 0.4099409940994099
train: step: 2478, loss: 0.6929190158843994, acc: 0.49, recall: 0.49, precision: 0.4899959983993597, f_beta: 0.48994899489948995
train: step: 2479, loss: 0.6930004358291626, acc: 0.51, recall: 0.51, precision: 0.5100361300682457, f_beta: 0.5095586027424682
train: step: 2480, loss: 0.6934614777565002, acc: 0.47, recall: 0.47000000000000003, precision: 0.4689954526663911, f_beta: 0.46567194273616297
train: step: 2481, loss: 0.6926097869873047, acc: 0.58, recall: 0.58, precision: 0.5848896434634975, f_beta: 0.5738636363636364
train: step: 2482, loss: 0.6928584575653076, acc: 0.48, recall: 0.48, precision: 0.47987117552334946, f_beta: 0.47916666666666663
train: step: 2483, loss: 0.6928672790527344, acc: 0.44, recall: 0.44000000000000006, precision: 0.4375, f_beta: 0.43434343434343436
train: step: 2484, loss: 0.6940275430679321, acc: 0.46, recall: 0.46, precision: 0.45833333333333337, f_beta: 0.4545454545454545
train: step: 2485, loss: 0.6933035254478455, acc: 0.53, recall: 0.53, precision: 0.5303030303030303, f_beta: 0.5288220551378446
train: step: 2486, loss: 0.6925515532493591, acc: 0.54, recall: 0.54, precision: 0.5416666666666667, f_beta: 0.5353535353535354
train: step: 2487, loss: 0.693821132183075, acc: 0.5, recall: 0.5, precision: 0.5, f_beta: 0.49269480519480513
train: step: 2488, loss: 0.6927634477615356, acc: 0.55, recall: 0.55, precision: 0.550999592003264, f_beta: 0.5477841422972565
train: step: 2489, loss: 0.6937274932861328, acc: 0.49, recall: 0.49, precision: 0.4898989898989899, f_beta: 0.48872180451127817
train: step: 2490, loss: 0.6938681602478027, acc: 0.45, recall: 0.45, precision: 0.449000407996736, f_beta: 0.4472917294744247
train: step: 2491, loss: 0.6942903399467468, acc: 0.41, recall: 0.41000000000000003, precision: 0.4096748293857888, f_beta: 0.40946852166950254
train: step: 2492, loss: 0.6936801075935364, acc: 0.44, recall: 0.44, precision: 0.4331550802139037, f_beta: 0.4252873563218391
train: step: 2493, loss: 0.6941805481910706, acc: 0.42, recall: 0.42000000000000004, precision: 0.4131944444444444, f_beta: 0.40840473276213796
train: step: 2494, loss: 0.6929327249526978, acc: 0.49, recall: 0.49, precision: 0.48996386993175434, f_beta: 0.4895405865278751
train: step: 2495, loss: 0.6941373944282532, acc: 0.4, recall: 0.4, precision: 0.398538961038961, f_beta: 0.397832195905259
train: step: 2496, loss: 0.6939045190811157, acc: 0.42, recall: 0.42000000000000004, precision: 0.4131944444444444, f_beta: 0.40840473276213796
train: step: 2497, loss: 0.6945013999938965, acc: 0.42, recall: 0.42000000000000004, precision: 0.4131944444444444, f_beta: 0.40840473276213796
train: step: 2498, loss: 0.6921477317810059, acc: 0.55, recall: 0.55, precision: 0.5516742455560149, f_beta: 0.5463252343986289
train: step: 2499, loss: 0.6931352019309998, acc: 0.53, recall: 0.53, precision: 0.5303030303030303, f_beta: 0.5288220551378446
train: step: 2500, loss: 0.6941613554954529, acc: 0.44, recall: 0.44, precision: 0.43912337662337664, f_beta: 0.43797671617824174
train: step: 2501, loss: 0.6929814219474792, acc: 0.53, recall: 0.53, precision: 0.5300120048019208, f_beta: 0.5299529952995299
train: step: 2502, loss: 0.6932289600372314, acc: 0.49, recall: 0.49, precision: 0.4866666666666667, f_beta: 0.45599999999999996
train: step: 2503, loss: 0.6924837231636047, acc: 0.54, recall: 0.54, precision: 0.5445632798573975, f_beta: 0.5279146141215106
train: step: 2504, loss: 0.693162739276886, acc: 0.5, recall: 0.5, precision: 0.5, f_beta: 0.4967793880837359
train: step: 2505, loss: 0.6948760747909546, acc: 0.37, recall: 0.37, precision: 0.35300768882858435, f_beta: 0.35125115848007415
train: step: 2506, loss: 0.6918940544128418, acc: 0.56, recall: 0.56, precision: 0.5615763546798029, f_beta: 0.5571658615136876
train: step: 2507, loss: 0.6932772994041443, acc: 0.48, recall: 0.48, precision: 0.47996794871794873, f_beta: 0.4797919167667067
train: step: 2508, loss: 0.6939064860343933, acc: 0.39, recall: 0.39, precision: 0.3896025692492975, f_beta: 0.3894505054549094
train: step: 2509, loss: 0.6929568648338318, acc: 0.51, recall: 0.51, precision: 0.510334849111203, f_beta: 0.5059985885673959
train: step: 2510, loss: 0.6936032176017761, acc: 0.47, recall: 0.47, precision: 0.4699879951980792, f_beta: 0.46994699469946993
train: step: 2511, loss: 0.6928561329841614, acc: 0.52, recall: 0.52, precision: 0.5200320512820513, f_beta: 0.5198079231692677
train: step: 2512, loss: 0.6935798525810242, acc: 0.47, recall: 0.47000000000000003, precision: 0.4694002447980416, f_beta: 0.467390212038991
train: step: 2513, loss: 0.6929163932800293, acc: 0.47, recall: 0.47, precision: 0.4696969696969697, f_beta: 0.468671679197995
train: step: 2514, loss: 0.6933411955833435, acc: 0.47, recall: 0.47000000000000003, precision: 0.4694002447980416, f_beta: 0.467390212038991
train: step: 2515, loss: 0.6935114860534668, acc: 0.45, recall: 0.45, precision: 0.439290917921321, f_beta: 0.42462600690448793
train: step: 2516, loss: 0.6935661435127258, acc: 0.46, recall: 0.45999999999999996, precision: 0.45993589743589747, f_beta: 0.45978391356542614
train: step: 2517, loss: 0.6931178569793701, acc: 0.49, recall: 0.49, precision: 0.4898000815993472, f_beta: 0.48748869460355737
train: step: 2518, loss: 0.6921323537826538, acc: 0.59, recall: 0.59, precision: 0.6141552511415524, f_beta: 0.5670995670995671
train: step: 2519, loss: 0.693960428237915, acc: 0.42, recall: 0.42000000000000004, precision: 0.39604989604989604, f_beta: 0.38455008488964343
train: step: 2520, loss: 0.6932503581047058, acc: 0.49, recall: 0.49, precision: 0.48831229546517063, f_beta: 0.47089947089947093
train: step: 2521, loss: 0.6938838958740234, acc: 0.45, recall: 0.45, precision: 0.449000407996736, f_beta: 0.4472917294744247
train: step: 2522, loss: 0.6926850080490112, acc: 0.52, recall: 0.52, precision: 0.5201288244766505, f_beta: 0.5192307692307692
train: step: 2523, loss: 0.6924334764480591, acc: 0.45, recall: 0.45, precision: 0.439290917921321, f_beta: 0.42462600690448793
train: step: 2524, loss: 0.6943871974945068, acc: 0.42, recall: 0.42000000000000004, precision: 0.4178981937602627, f_beta: 0.4162640901771337
train: step: 2525, loss: 0.6923437714576721, acc: 0.52, recall: 0.52, precision: 0.5200320512820513, f_beta: 0.5198079231692677
train: step: 2526, loss: 0.6933596730232239, acc: 0.52, recall: 0.52, precision: 0.5201288244766505, f_beta: 0.5192307692307692
train: step: 2527, loss: 0.693712055683136, acc: 0.5, recall: 0.5, precision: 0.5, f_beta: 0.4949494949494949
train: step: 2528, loss: 0.6927594542503357, acc: 0.56, recall: 0.56, precision: 0.5603864734299517, f_beta: 0.5592948717948718
train: step: 2529, loss: 0.6928471326828003, acc: 0.45, recall: 0.45, precision: 0.4494949494949495, f_beta: 0.4486215538847118
train: step: 2530, loss: 0.6928759813308716, acc: 0.52, recall: 0.52, precision: 0.5202922077922079, f_beta: 0.5182657567242072
train: step: 2531, loss: 0.6929901242256165, acc: 0.53, recall: 0.53, precision: 0.5315258511979823, f_beta: 0.5242433444680635
train: step: 2532, loss: 0.6932553648948669, acc: 0.5, recall: 0.5, precision: 0.5, f_beta: 0.4981934965877158
train: step: 2533, loss: 0.6932445764541626, acc: 0.49, recall: 0.49, precision: 0.48869289914066033, f_beta: 0.4748223663886314
train: step: 2534, loss: 0.6923942565917969, acc: 0.59, recall: 0.59, precision: 0.5900360144057624, f_beta: 0.5899589958995899
train: step: 2535, loss: 0.6933133602142334, acc: 0.49, recall: 0.49, precision: 0.48966515088879703, f_beta: 0.48583526565177937
train: step: 2536, loss: 0.6931991577148438, acc: 0.53, recall: 0.53, precision: 0.5315258511979823, f_beta: 0.5242433444680635
train: step: 2537, loss: 0.6924793720245361, acc: 0.55, recall: 0.55, precision: 0.5500200080032013, f_beta: 0.54995499549955
train: step: 2538, loss: 0.6925808191299438, acc: 0.53, recall: 0.53, precision: 0.5315258511979823, f_beta: 0.5242433444680635
train: step: 2539, loss: 0.6921910643577576, acc: 0.51, recall: 0.51, precision: 0.510508617065994, f_beta: 0.5039983804028748
train: step: 2540, loss: 0.6940752267837524, acc: 0.46, recall: 0.46, precision: 0.45833333333333337, f_beta: 0.4545454545454545
train: step: 2541, loss: 0.6943099498748779, acc: 0.46, recall: 0.45999999999999996, precision: 0.4565972222222222, f_beta: 0.44920440636474906
train: step: 2542, loss: 0.6931580305099487, acc: 0.5, recall: 0.5, precision: 0.5, f_beta: 0.4981934965877158
train: step: 2543, loss: 0.6939577460289001, acc: 0.47, recall: 0.47, precision: 0.4699879951980792, f_beta: 0.46994699469946993
train: step: 2544, loss: 0.6925066113471985, acc: 0.54, recall: 0.54, precision: 0.5410509031198687, f_beta: 0.537037037037037
train: step: 2545, loss: 0.692775547504425, acc: 0.57, recall: 0.5700000000000001, precision: 0.5791497060153776, f_beta: 0.557203171661003
train: step: 2546, loss: 0.6930640935897827, acc: 0.46, recall: 0.46, precision: 0.45894909688013136, f_beta: 0.4565217391304348
train: step: 2547, loss: 0.6934637427330017, acc: 0.51, recall: 0.51, precision: 0.51010101010101, f_beta: 0.5087719298245614
train: step: 2548, loss: 0.6934124231338501, acc: 0.51, recall: 0.51, precision: 0.5100361300682457, f_beta: 0.5095586027424682
train: step: 2549, loss: 0.6943154335021973, acc: 0.49, recall: 0.49, precision: 0.4898000815993472, f_beta: 0.48748869460355737
train: step: 2550, loss: 0.6932405233383179, acc: 0.5, recall: 0.5, precision: 0.5, f_beta: 0.49000407996736023
train: step: 2551, loss: 0.6936584711074829, acc: 0.46, recall: 0.45999999999999996, precision: 0.4554367201426025, f_beta: 0.44581280788177335
train: step: 2552, loss: 0.6930989027023315, acc: 0.48, recall: 0.48000000000000004, precision: 0.47947454844006565, f_beta: 0.47665056360708535
train: step: 2553, loss: 0.6929971575737, acc: 0.54, recall: 0.54, precision: 0.5459558823529411, f_beta: 0.524596940884663
train: step: 2554, loss: 0.6926794648170471, acc: 0.51, recall: 0.51, precision: 0.5100361300682457, f_beta: 0.5095586027424682
train: step: 2555, loss: 0.6931237578392029, acc: 0.47, recall: 0.47, precision: 0.46989160979526295, f_beta: 0.46952257031328193
train: step: 2556, loss: 0.6929057240486145, acc: 0.49, recall: 0.49, precision: 0.48949138293400585, f_beta: 0.48375341633768604
train: step: 2557, loss: 0.6939825415611267, acc: 0.46, recall: 0.45999999999999996, precision: 0.4565972222222222, f_beta: 0.44920440636474906
train: step: 2558, loss: 0.693983256816864, acc: 0.46, recall: 0.46, precision: 0.4594155844155844, f_beta: 0.45804897631473307
train: step: 2559, loss: 0.6921248435974121, acc: 0.55, recall: 0.55, precision: 0.5536250536250535, f_beta: 0.5422642660970399
train: step: 2560, loss: 0.6922582387924194, acc: 0.51, recall: 0.51, precision: 0.510508617065994, f_beta: 0.5039983804028748
train: step: 2561, loss: 0.6920830011367798, acc: 0.54, recall: 0.54, precision: 0.5424448217317488, f_beta: 0.5332792207792207
train: step: 2562, loss: 0.6922709941864014, acc: 0.58, recall: 0.58, precision: 0.5848896434634975, f_beta: 0.5738636363636364
train: step: 2563, loss: 0.6928284168243408, acc: 0.55, recall: 0.55, precision: 0.5501806503412284, f_beta: 0.5495946351716544
train: step: 2564, loss: 0.6925906538963318, acc: 0.52, recall: 0.52, precision: 0.5208333333333334, f_beta: 0.5151515151515151
train: step: 2565, loss: 0.6931456923484802, acc: 0.45, recall: 0.45, precision: 0.449000407996736, f_beta: 0.4472917294744247
train: step: 2566, loss: 0.6919209361076355, acc: 0.56, recall: 0.56, precision: 0.56, f_beta: 0.56
train: step: 2567, loss: 0.6935955286026001, acc: 0.48, recall: 0.48, precision: 0.47996794871794873, f_beta: 0.4797919167667067
train: step: 2568, loss: 0.6938217282295227, acc: 0.51, recall: 0.51, precision: 0.5101999184006528, f_beta: 0.5075871771681238
train: step: 2569, loss: 0.6930283308029175, acc: 0.52, recall: 0.52, precision: 0.5201288244766505, f_beta: 0.5192307692307692
train: step: 2570, loss: 0.6936805248260498, acc: 0.49, recall: 0.49, precision: 0.48966515088879703, f_beta: 0.48583526565177937
train: step: 2571, loss: 0.6937119364738464, acc: 0.45, recall: 0.45, precision: 0.4434644957033017, f_beta: 0.4336319637524457
train: step: 2572, loss: 0.6931928396224976, acc: 0.5, recall: 0.5, precision: 0.5, f_beta: 0.46374946374946374
train: step: 2573, loss: 0.6923452019691467, acc: 0.59, recall: 0.5900000000000001, precision: 0.5909090909090908, f_beta: 0.5889724310776943
train: step: 2574, loss: 0.692287266254425, acc: 0.54, recall: 0.54, precision: 0.5405844155844155, f_beta: 0.5383380168606985
train: step: 2575, loss: 0.6935046315193176, acc: 0.45, recall: 0.44999999999999996, precision: 0.4498193496587716, f_beta: 0.44950455409868884
train: step: 2576, loss: 0.6934806704521179, acc: 0.46, recall: 0.46, precision: 0.4523809523809524, f_beta: 0.4375
train: step: 2577, loss: 0.6921871900558472, acc: 0.48, recall: 0.48, precision: 0.46875, f_beta: 0.4285714285714286
train: step: 2578, loss: 0.6938169002532959, acc: 0.44, recall: 0.44, precision: 0.43961352657004826, f_beta: 0.4391025641025641
train: step: 2579, loss: 0.6938741207122803, acc: 0.45, recall: 0.44999999999999996, precision: 0.44637494637494635, f_beta: 0.44054521411860437
train: step: 2580, loss: 0.6931188702583313, acc: 0.55, recall: 0.55, precision: 0.550999592003264, f_beta: 0.5477841422972565
train: step: 2581, loss: 0.6930754780769348, acc: 0.5, recall: 0.5, precision: 0.5, f_beta: 0.49979991996798717
train: step: 2582, loss: 0.6925413012504578, acc: 0.56, recall: 0.56, precision: 0.5608766233766234, f_beta: 0.5584102769971899
train: step: 2583, loss: 0.6932974457740784, acc: 0.46, recall: 0.46, precision: 0.4594155844155844, f_beta: 0.45804897631473307
train: step: 2584, loss: 0.6922706365585327, acc: 0.48, recall: 0.48, precision: 0.4787775891341256, f_beta: 0.4724025974025974
train: step: 2585, loss: 0.693715512752533, acc: 0.46, recall: 0.46, precision: 0.4594155844155844, f_beta: 0.45804897631473307
train: step: 2586, loss: 0.6921078562736511, acc: 0.6, recall: 0.6, precision: 0.6085069444444444, f_beta: 0.5920032639738881
train: step: 2587, loss: 0.6937813758850098, acc: 0.44, recall: 0.44, precision: 0.4331550802139037, f_beta: 0.4252873563218391
train: step: 2588, loss: 0.6930785179138184, acc: 0.47, recall: 0.47, precision: 0.466078697421981, f_beta: 0.4542271650705386
train: step: 2589, loss: 0.6933495998382568, acc: 0.55, recall: 0.55, precision: 0.5525430853299707, f_beta: 0.5444883085332524
train: step: 2590, loss: 0.6942324042320251, acc: 0.44, recall: 0.44, precision: 0.4331550802139037, f_beta: 0.4252873563218391
train: step: 2591, loss: 0.6928593516349792, acc: 0.5, recall: 0.5, precision: 0.5, f_beta: 0.4949494949494949
train: step: 2592, loss: 0.6937108039855957, acc: 0.47, recall: 0.47000000000000003, precision: 0.4689954526663911, f_beta: 0.46567194273616297
train: step: 2593, loss: 0.69366854429245, acc: 0.47, recall: 0.47, precision: 0.4699879951980792, f_beta: 0.46994699469946993
train: step: 2594, loss: 0.6934368014335632, acc: 0.49, recall: 0.49, precision: 0.4899959983993597, f_beta: 0.48994899489948995
train: step: 2595, loss: 0.693877100944519, acc: 0.46, recall: 0.46, precision: 0.4523809523809524, f_beta: 0.4375
train: step: 2596, loss: 0.6921817064285278, acc: 0.53, recall: 0.53, precision: 0.5321750321750323, f_beta: 0.5219204557013528
train: step: 2597, loss: 0.6933032274246216, acc: 0.49, recall: 0.49, precision: 0.48949138293400585, f_beta: 0.48375341633768604
train: step: 2598, loss: 0.6941064596176147, acc: 0.38, recall: 0.38, precision: 0.37684729064039413, f_beta: 0.37600644122383253
train: step: 2599, loss: 0.6941230893135071, acc: 0.39, recall: 0.39, precision: 0.3888888888888889, f_beta: 0.38847117794486213
train: step: 2600, loss: 0.6925216913223267, acc: 0.56, recall: 0.56, precision: 0.5603864734299517, f_beta: 0.5592948717948718
train: step: 2601, loss: 0.6931495070457458, acc: 0.48, recall: 0.48, precision: 0.47702205882352944, f_beta: 0.46258784621744525
train: step: 2602, loss: 0.6936812400817871, acc: 0.44, recall: 0.44, precision: 0.43912337662337664, f_beta: 0.43797671617824174
train: step: 2603, loss: 0.6927307844161987, acc: 0.51, recall: 0.51, precision: 0.5100361300682457, f_beta: 0.5095586027424682
train: step: 2604, loss: 0.6936383247375488, acc: 0.44, recall: 0.44000000000000006, precision: 0.4375, f_beta: 0.43434343434343436
train: step: 2605, loss: 0.6930183172225952, acc: 0.6, recall: 0.6, precision: 0.6001602564102564, f_beta: 0.5998399359743898
train: step: 2606, loss: 0.6930078268051147, acc: 0.49, recall: 0.49, precision: 0.48831229546517063, f_beta: 0.47089947089947093
train: step: 2607, loss: 0.6920398473739624, acc: 0.63, recall: 0.63, precision: 0.6394251394251393, f_beta: 0.6236395076797885
train: step: 2608, loss: 0.6925190687179565, acc: 0.59, recall: 0.59, precision: 0.5965250965250966, f_beta: 0.5829518868884142
train: step: 2609, loss: 0.6929598450660706, acc: 0.49, recall: 0.49, precision: 0.48996386993175434, f_beta: 0.4895405865278751
train: step: 2610, loss: 0.6926634311676025, acc: 0.52, recall: 0.52, precision: 0.5202922077922079, f_beta: 0.5182657567242072
train: step: 2611, loss: 0.6922008395195007, acc: 0.59, recall: 0.5900000000000001, precision: 0.5917992656058751, f_beta: 0.5879811074263892
train: step: 2612, loss: 0.6932714581489563, acc: 0.48, recall: 0.48, precision: 0.4782986111111111, f_beta: 0.46960424316605465
train: step: 2613, loss: 0.6926219463348389, acc: 0.5, recall: 0.5, precision: 0.5, f_beta: 0.49269480519480513
train: step: 2614, loss: 0.6940696239471436, acc: 0.41, recall: 0.41000000000000003, precision: 0.4096748293857888, f_beta: 0.40946852166950254
train: step: 2615, loss: 0.6934240460395813, acc: 0.47, recall: 0.47, precision: 0.46989160979526295, f_beta: 0.46952257031328193
train: step: 2616, loss: 0.693855345249176, acc: 0.48, recall: 0.48, precision: 0.47987117552334946, f_beta: 0.47916666666666663
train: step: 2617, loss: 0.6923048496246338, acc: 0.54, recall: 0.54, precision: 0.5405844155844155, f_beta: 0.5383380168606985
train: step: 2618, loss: 0.6927546858787537, acc: 0.51, recall: 0.51, precision: 0.510508617065994, f_beta: 0.5039983804028748
train: step: 2619, loss: 0.692420244216919, acc: 0.56, recall: 0.56, precision: 0.5603864734299517, f_beta: 0.5592948717948718
train: step: 2620, loss: 0.6918625831604004, acc: 0.61, recall: 0.61, precision: 0.6100440176070427, f_beta: 0.60996099609961
train: step: 2621, loss: 0.6928523182868958, acc: 0.51, recall: 0.51, precision: 0.510508617065994, f_beta: 0.5039983804028748
train: step: 2622, loss: 0.6916564702987671, acc: 0.6, recall: 0.6, precision: 0.6041666666666667, f_beta: 0.595959595959596
train: step: 2623, loss: 0.6923116445541382, acc: 0.57, recall: 0.5700000000000001, precision: 0.5707070707070707, f_beta: 0.5689223057644109
train: step: 2624, loss: 0.6930318474769592, acc: 0.51, recall: 0.51, precision: 0.5101999184006528, f_beta: 0.5075871771681238
train: step: 2625, loss: 0.6909267902374268, acc: 0.61, recall: 0.61, precision: 0.6179751179751181, f_beta: 0.6032956972841013
train: step: 2626, loss: 0.6923397779464722, acc: 0.59, recall: 0.59, precision: 0.5965250965250966, f_beta: 0.5829518868884142
train: step: 2627, loss: 0.6939846873283386, acc: 0.45, recall: 0.45, precision: 0.4246534056660639, f_beta: 0.39949776176438473
train: step: 2628, loss: 0.6924378871917725, acc: 0.55, recall: 0.55, precision: 0.550999592003264, f_beta: 0.5477841422972565
train: step: 2629, loss: 0.6916100978851318, acc: 0.61, recall: 0.6100000000000001, precision: 0.6121991024071807, f_beta: 0.6080795899909557
train: step: 2630, loss: 0.6919152736663818, acc: 0.56, recall: 0.5599999999999999, precision: 0.5651041666666667, f_beta: 0.5512035903712771
train: step: 2631, loss: 0.6932269930839539, acc: 0.5, recall: 0.5, precision: 0.5, f_beta: 0.49919871794871795
train: step: 2632, loss: 0.6944613456726074, acc: 0.42, recall: 0.42, precision: 0.42, f_beta: 0.41999999999999993
train: step: 2633, loss: 0.691450297832489, acc: 0.67, recall: 0.6699999999999999, precision: 0.6706142111601766, f_beta: 0.6697027324592133
train: step: 2634, loss: 0.6931602358818054, acc: 0.51, recall: 0.51, precision: 0.5107250107250108, f_beta: 0.5015766453056658
train: step: 2635, loss: 0.6931381821632385, acc: 0.46, recall: 0.45999999999999996, precision: 0.45974235104669886, f_beta: 0.4591346153846153
train: step: 2636, loss: 0.6915891766548157, acc: 0.54, recall: 0.54, precision: 0.5405844155844155, f_beta: 0.5383380168606985
train: step: 2637, loss: 0.6914410591125488, acc: 0.55, recall: 0.55, precision: 0.5525430853299707, f_beta: 0.5444883085332524
train: step: 2638, loss: 0.6918448805809021, acc: 0.56, recall: 0.56, precision: 0.5689338235294117, f_beta: 0.545266639107069
train: step: 2639, loss: 0.6937165856361389, acc: 0.43, recall: 0.43, precision: 0.42307692307692313, f_beta: 0.41687979539641945
train: step: 2640, loss: 0.6924281120300293, acc: 0.51, recall: 0.51, precision: 0.5100361300682457, f_beta: 0.5095586027424682
train: step: 2641, loss: 0.6928556561470032, acc: 0.54, recall: 0.54, precision: 0.5459558823529411, f_beta: 0.524596940884663
train: step: 2642, loss: 0.6924963593482971, acc: 0.52, recall: 0.52, precision: 0.5212224108658743, f_beta: 0.512987012987013
train: step: 2643, loss: 0.6944745779037476, acc: 0.39, recall: 0.39, precision: 0.3896025692492975, f_beta: 0.3894505054549094
train: step: 2644, loss: 0.6924765110015869, acc: 0.57, recall: 0.5700000000000001, precision: 0.5735603194619587, f_beta: 0.5647332725984411
train: step: 2645, loss: 0.6900811195373535, acc: 0.66, recall: 0.6599999999999999, precision: 0.6642036124794746, f_beta: 0.6578099838969405
train: step: 2646, loss: 0.6917724609375, acc: 0.6, recall: 0.6000000000000001, precision: 0.601461038961039, f_beta: 0.5985547972701726
train: step: 2647, loss: 0.6926775574684143, acc: 0.57, recall: 0.5700000000000001, precision: 0.5735603194619587, f_beta: 0.5647332725984411
train: step: 2648, loss: 0.6925936937332153, acc: 0.56, recall: 0.56, precision: 0.5636672325976231, f_beta: 0.5535714285714286
train: step: 2649, loss: 0.6917498707771301, acc: 0.63, recall: 0.63, precision: 0.6578436134045653, f_beta: 0.6129302228266555
train: step: 2650, loss: 0.6929119229316711, acc: 0.52, recall: 0.52, precision: 0.5202922077922079, f_beta: 0.5182657567242072
train: step: 2651, loss: 0.6926345229148865, acc: 0.51, recall: 0.51, precision: 0.5121418164157359, f_beta: 0.4873940788785438
train: step: 2652, loss: 0.6905118823051453, acc: 0.68, recall: 0.6799999999999999, precision: 0.6802884615384616, f_beta: 0.6798719487795117
train: step: 2653, loss: 0.6932286620140076, acc: 0.48, recall: 0.48, precision: 0.48, f_beta: 0.48
train: step: 2654, loss: 0.6923569440841675, acc: 0.59, recall: 0.5900000000000001, precision: 0.5903251706142112, f_beta: 0.5896306676008407
train: step: 2655, loss: 0.6929869651794434, acc: 0.45, recall: 0.45, precision: 0.4415614773258532, f_beta: 0.4294013901857039
train: step: 2656, loss: 0.6933607459068298, acc: 0.53, recall: 0.53, precision: 0.530108390204737, f_beta: 0.5295766189570613
train: step: 2657, loss: 0.6924161314964294, acc: 0.58, recall: 0.5800000000000001, precision: 0.5801282051282051, f_beta: 0.5798319327731092
train: step: 2658, loss: 0.692852795124054, acc: 0.53, recall: 0.53, precision: 0.530108390204737, f_beta: 0.5295766189570613
train: step: 2659, loss: 0.6906142234802246, acc: 0.62, recall: 0.62, precision: 0.625, f_beta: 0.6161616161616162
train: step: 2660, loss: 0.6930514574050903, acc: 0.56, recall: 0.56, precision: 0.5615763546798029, f_beta: 0.5571658615136876
train: step: 2661, loss: 0.6919670701026917, acc: 0.59, recall: 0.5900000000000001, precision: 0.5903251706142112, f_beta: 0.5896306676008407
train: step: 2662, loss: 0.6931702494621277, acc: 0.53, recall: 0.53, precision: 0.530108390204737, f_beta: 0.5295766189570613
train: step: 2663, loss: 0.6923159956932068, acc: 0.54, recall: 0.54, precision: 0.5400641025641025, f_beta: 0.5398159263705482
train: step: 2664, loss: 0.6921629905700684, acc: 0.58, recall: 0.5800000000000001, precision: 0.5805152979066023, f_beta: 0.5793269230769231
train: step: 2665, loss: 0.6898521184921265, acc: 0.71, recall: 0.71, precision: 0.7121212121212122, f_beta: 0.7092731829573935
train: step: 2666, loss: 0.6931406259536743, acc: 0.49, recall: 0.49, precision: 0.48966515088879703, f_beta: 0.48583526565177937
train: step: 2667, loss: 0.6930559277534485, acc: 0.47, recall: 0.47, precision: 0.46989160979526295, f_beta: 0.46952257031328193
train: step: 2668, loss: 0.6919313669204712, acc: 0.64, recall: 0.6399999999999999, precision: 0.6420454545454546, f_beta: 0.6386993175431553
train: step: 2669, loss: 0.6922395825386047, acc: 0.59, recall: 0.5900000000000001, precision: 0.5909090909090908, f_beta: 0.5889724310776943
train: step: 2670, loss: 0.6910688281059265, acc: 0.54, recall: 0.54, precision: 0.5405844155844155, f_beta: 0.5383380168606985
train: step: 2671, loss: 0.6905497908592224, acc: 0.64, recall: 0.64, precision: 0.640224358974359, f_beta: 0.6398559423769508
train: step: 2672, loss: 0.6887624859809875, acc: 0.7, recall: 0.7, precision: 0.7, f_beta: 0.7
train: step: 2673, loss: 0.6927217245101929, acc: 0.57, recall: 0.5700000000000001, precision: 0.5818139317438056, f_beta: 0.5538956323270049
train: step: 2674, loss: 0.6916311383247375, acc: 0.6, recall: 0.6000000000000001, precision: 0.601461038961039, f_beta: 0.5985547972701726
train: step: 2675, loss: 0.6899771094322205, acc: 0.65, recall: 0.6499999999999999, precision: 0.652998776009792, f_beta: 0.6482765551200884
train: step: 2676, loss: 0.6924384832382202, acc: 0.55, recall: 0.55, precision: 0.5525430853299707, f_beta: 0.5444883085332524
train: step: 2677, loss: 0.6924804449081421, acc: 0.5, recall: 0.5, precision: 0.5, f_beta: 0.47916666666666663
train: step: 2678, loss: 0.6920228600502014, acc: 0.57, recall: 0.5700000000000001, precision: 0.5723439437784208, f_beta: 0.5664885573142453
train: step: 2679, loss: 0.6923820376396179, acc: 0.55, recall: 0.55, precision: 0.5505050505050505, f_beta: 0.5488721804511277
train: step: 2680, loss: 0.6930792331695557, acc: 0.49, recall: 0.49, precision: 0.4898000815993472, f_beta: 0.48748869460355737
train: step: 2681, loss: 0.6931960582733154, acc: 0.5, recall: 0.5, precision: 0.5, f_beta: 0.49919871794871795
train: step: 2682, loss: 0.6913794875144958, acc: 0.61, recall: 0.6100000000000001, precision: 0.6111111111111112, f_beta: 0.6090225563909775
train: step: 2683, loss: 0.693850576877594, acc: 0.44, recall: 0.44, precision: 0.44, f_beta: 0.44
train: step: 2684, loss: 0.6931824684143066, acc: 0.51, recall: 0.51, precision: 0.510989010989011, f_beta: 0.4987212276214833
train: step: 2685, loss: 0.6900079250335693, acc: 0.65, recall: 0.65, precision: 0.6505419510236852, f_beta: 0.6496847162446201
train: step: 2686, loss: 0.6931084990501404, acc: 0.53, recall: 0.53, precision: 0.5300120048019208, f_beta: 0.5299529952995299
train: step: 2687, loss: 0.6932350397109985, acc: 0.51, recall: 0.51, precision: 0.5116877045348294, f_beta: 0.4916485112563544
train: step: 2688, loss: 0.693362832069397, acc: 0.49, recall: 0.49, precision: 0.4899959983993597, f_beta: 0.48994899489948995
train: step: 2689, loss: 0.6931135654449463, acc: 0.54, recall: 0.54, precision: 0.5416666666666667, f_beta: 0.5353535353535354
train: step: 2690, loss: 0.6914150714874268, acc: 0.61, recall: 0.61, precision: 0.6136833402232327, f_beta: 0.6068152031454783
train: step: 2691, loss: 0.6912941932678223, acc: 0.55, recall: 0.55, precision: 0.5536250536250535, f_beta: 0.5422642660970399
train: step: 2692, loss: 0.6910517811775208, acc: 0.64, recall: 0.64, precision: 0.6409017713365539, f_beta: 0.639423076923077
train: step: 2693, loss: 0.6912826299667358, acc: 0.64, recall: 0.64, precision: 0.6409017713365539, f_beta: 0.639423076923077
train: step: 2694, loss: 0.6927925944328308, acc: 0.5, recall: 0.5, precision: 0.5, f_beta: 0.4949494949494949
train: step: 2695, loss: 0.6924735307693481, acc: 0.57, recall: 0.5700000000000001, precision: 0.5702529104777199, f_beta: 0.5696126513862476
train: step: 2696, loss: 0.6911675930023193, acc: 0.63, recall: 0.63, precision: 0.6325989392084863, f_beta: 0.6281780725555222
train: step: 2697, loss: 0.6904482841491699, acc: 0.59, recall: 0.5900000000000001, precision: 0.5909090909090908, f_beta: 0.5889724310776943
train: step: 2698, loss: 0.6912596821784973, acc: 0.63, recall: 0.63, precision: 0.6300520208083233, f_beta: 0.6299629962996299
train: step: 2699, loss: 0.6934020519256592, acc: 0.47, recall: 0.47, precision: 0.46782496782496785, f_beta: 0.4608890245142916
train: step: 2700, loss: 0.6930873990058899, acc: 0.51, recall: 0.51, precision: 0.510989010989011, f_beta: 0.4987212276214833
train: step: 2701, loss: 0.6922086477279663, acc: 0.56, recall: 0.56, precision: 0.5600961538461539, f_beta: 0.5598239295718288
train: step: 2702, loss: 0.6906557679176331, acc: 0.64, recall: 0.64, precision: 0.6485568760611206, f_beta: 0.6347402597402597
train: step: 2703, loss: 0.6914451718330383, acc: 0.57, recall: 0.5700000000000001, precision: 0.5707070707070707, f_beta: 0.5689223057644109
train: step: 2704, loss: 0.6908348202705383, acc: 0.58, recall: 0.5800000000000001, precision: 0.5919117647058824, f_beta: 0.565936337329475
train: step: 2705, loss: 0.6944466233253479, acc: 0.45, recall: 0.45, precision: 0.4494949494949495, f_beta: 0.4486215538847118
train: step: 2706, loss: 0.6924909353256226, acc: 0.53, recall: 0.53, precision: 0.5315258511979823, f_beta: 0.5242433444680635
train: step: 2707, loss: 0.6916775703430176, acc: 0.59, recall: 0.5900000000000001, precision: 0.5909090909090908, f_beta: 0.5889724310776943
train: step: 2708, loss: 0.6921703219413757, acc: 0.6, recall: 0.6000000000000001, precision: 0.601461038961039, f_beta: 0.5985547972701726
train: step: 2709, loss: 0.6917503476142883, acc: 0.59, recall: 0.5900000000000001, precision: 0.5930136420008267, f_beta: 0.5866518802298619
train: step: 2710, loss: 0.6912891864776611, acc: 0.57, recall: 0.5700000000000001, precision: 0.5713994288045696, f_beta: 0.567882624861823
train: step: 2711, loss: 0.6908450126647949, acc: 0.67, recall: 0.6699999999999999, precision: 0.6733986128110976, f_beta: 0.6683750376846548
train: step: 2712, loss: 0.6906220316886902, acc: 0.57, recall: 0.57, precision: 0.5769230769230769, f_beta: 0.5601023017902813
train: step: 2713, loss: 0.6909612417221069, acc: 0.69, recall: 0.69, precision: 0.690686471296668, f_beta: 0.6897207486738064
train: step: 2714, loss: 0.6926825046539307, acc: 0.52, recall: 0.52, precision: 0.5202922077922079, f_beta: 0.5182657567242072
train: step: 2715, loss: 0.6917852759361267, acc: 0.59, recall: 0.5900000000000001, precision: 0.5917992656058751, f_beta: 0.5879811074263892
train: step: 2716, loss: 0.6906924247741699, acc: 0.6, recall: 0.6000000000000001, precision: 0.6006441223832528, f_beta: 0.5993589743589743
train: step: 2717, loss: 0.6932055950164795, acc: 0.43, recall: 0.43, precision: 0.4150072850898494, f_beta: 0.40370331624646927
train: step: 2718, loss: 0.6906591057777405, acc: 0.57, recall: 0.57, precision: 0.5769230769230769, f_beta: 0.5601023017902813
train: step: 2719, loss: 0.6917634010314941, acc: 0.54, recall: 0.54, precision: 0.5410509031198687, f_beta: 0.537037037037037
train: step: 2720, loss: 0.6934942007064819, acc: 0.51, recall: 0.51, precision: 0.5100040016006402, f_beta: 0.50995099509951
train: step: 2721, loss: 0.6921700835227966, acc: 0.61, recall: 0.6100000000000001, precision: 0.6111111111111112, f_beta: 0.6090225563909775
train: step: 2722, loss: 0.6917884945869446, acc: 0.57, recall: 0.5700000000000001, precision: 0.5702529104777199, f_beta: 0.5696126513862476
train: step: 2723, loss: 0.6907686591148376, acc: 0.62, recall: 0.62, precision: 0.6201923076923077, f_beta: 0.6198479391756702
train: step: 2724, loss: 0.6906672716140747, acc: 0.64, recall: 0.64, precision: 0.6458333333333334, f_beta: 0.6363636363636365
train: step: 2725, loss: 0.691285252571106, acc: 0.53, recall: 0.53, precision: 0.530108390204737, f_beta: 0.5295766189570613
train: step: 2726, loss: 0.6936492323875427, acc: 0.46, recall: 0.46, precision: 0.45833333333333337, f_beta: 0.4545454545454545
train: step: 2727, loss: 0.6917473673820496, acc: 0.58, recall: 0.5800000000000001, precision: 0.5801282051282051, f_beta: 0.5798319327731092
train: step: 2728, loss: 0.6900673508644104, acc: 0.56, recall: 0.56, precision: 0.5625, f_beta: 0.5555555555555556
train: step: 2729, loss: 0.6913639307022095, acc: 0.65, recall: 0.6499999999999999, precision: 0.6515151515151515, f_beta: 0.6491228070175439
train: step: 2730, loss: 0.6917054057121277, acc: 0.62, recall: 0.62, precision: 0.6273344651952462, f_beta: 0.614448051948052
train: step: 2731, loss: 0.6905691623687744, acc: 0.66, recall: 0.6599999999999999, precision: 0.6623376623376623, f_beta: 0.6587715776796468
train: step: 2732, loss: 0.6933639645576477, acc: 0.42, recall: 0.42000000000000004, precision: 0.4198717948717949, f_beta: 0.41976790716286516
train: step: 2733, loss: 0.6932352185249329, acc: 0.52, recall: 0.52, precision: 0.5205254515599343, f_beta: 0.5169082125603865
train: step: 2734, loss: 0.6929323673248291, acc: 0.54, recall: 0.54, precision: 0.5459558823529411, f_beta: 0.524596940884663
train: step: 2735, loss: 0.692843496799469, acc: 0.49, recall: 0.49, precision: 0.4898000815993472, f_beta: 0.48748869460355737
train: step: 2736, loss: 0.6913824677467346, acc: 0.63, recall: 0.63, precision: 0.6304696908871938, f_beta: 0.629666700030027
train: step: 2737, loss: 0.6925457119941711, acc: 0.48, recall: 0.48, precision: 0.47996794871794873, f_beta: 0.4797919167667067
train: step: 2738, loss: 0.6927493214607239, acc: 0.48, recall: 0.48, precision: 0.47996794871794873, f_beta: 0.4797919167667067
train: step: 2739, loss: 0.6893572211265564, acc: 0.63, recall: 0.63, precision: 0.6325989392084863, f_beta: 0.6281780725555222
train: step: 2740, loss: 0.6918042898178101, acc: 0.61, recall: 0.6100000000000001, precision: 0.6111111111111112, f_beta: 0.6090225563909775
train: step: 2741, loss: 0.6922606825828552, acc: 0.55, recall: 0.55, precision: 0.550999592003264, f_beta: 0.5477841422972565
train: step: 2742, loss: 0.6925007700920105, acc: 0.63, recall: 0.63, precision: 0.6300520208083233, f_beta: 0.6299629962996299
train: step: 2743, loss: 0.6927691102027893, acc: 0.53, recall: 0.53, precision: 0.532967032967033, f_beta: 0.5191815856777493
train: step: 2744, loss: 0.6929987072944641, acc: 0.5, recall: 0.5, precision: 0.5, f_beta: 0.49979991996798717
train: step: 2745, loss: 0.6881824731826782, acc: 0.65, recall: 0.65, precision: 0.6608751608751608, f_beta: 0.6439833180754755
train: step: 2746, loss: 0.6913056969642639, acc: 0.62, recall: 0.62, precision: 0.6231527093596059, f_beta: 0.6175523349436394
train: step: 2747, loss: 0.6888302564620972, acc: 0.63, recall: 0.63, precision: 0.6300520208083233, f_beta: 0.6299629962996299
train: step: 2748, loss: 0.6924373507499695, acc: 0.52, recall: 0.52, precision: 0.5212224108658743, f_beta: 0.512987012987013
train: step: 2749, loss: 0.6890245079994202, acc: 0.7, recall: 0.7, precision: 0.7083333333333333, f_beta: 0.6969696969696968
train: step: 2750, loss: 0.6881822943687439, acc: 0.7, recall: 0.7, precision: 0.7052545155993433, f_beta: 0.6980676328502415
train: step: 2751, loss: 0.6898338794708252, acc: 0.68, recall: 0.6799999999999999, precision: 0.6826298701298701, f_beta: 0.6788438378161381
train: step: 2752, loss: 0.6910291314125061, acc: 0.63, recall: 0.63, precision: 0.6343530384456387, f_beta: 0.6269785260610949
train: step: 2753, loss: 0.6914163827896118, acc: 0.63, recall: 0.63, precision: 0.6469923111714156, f_beta: 0.6189887756152817
train: step: 2754, loss: 0.6891253590583801, acc: 0.7, recall: 0.7, precision: 0.7083333333333333, f_beta: 0.6969696969696968
train: step: 2755, loss: 0.6905982494354248, acc: 0.66, recall: 0.66, precision: 0.6602564102564102, f_beta: 0.6598639455782312
train: step: 2756, loss: 0.6888173818588257, acc: 0.75, recall: 0.75, precision: 0.750903251706142, f_beta: 0.7497747973175859
train: step: 2757, loss: 0.689857542514801, acc: 0.72, recall: 0.72, precision: 0.7291666666666667, f_beta: 0.7171717171717171
train: step: 2758, loss: 0.6924946308135986, acc: 0.53, recall: 0.53, precision: 0.530108390204737, f_beta: 0.5295766189570613
train: step: 2759, loss: 0.6892988681793213, acc: 0.65, recall: 0.65, precision: 0.6753155680224404, f_beta: 0.636891793754539
train: step: 2760, loss: 0.6912134289741516, acc: 0.59, recall: 0.59, precision: 0.5900360144057624, f_beta: 0.5899589958995899
train: step: 2761, loss: 0.6911534070968628, acc: 0.66, recall: 0.6599999999999999, precision: 0.6623376623376623, f_beta: 0.6587715776796468
train: step: 2762, loss: 0.6893722414970398, acc: 0.71, recall: 0.71, precision: 0.7121212121212122, f_beta: 0.7092731829573935
train: step: 2763, loss: 0.6919983625411987, acc: 0.61, recall: 0.61, precision: 0.6243781094527363, f_beta: 0.5983935742971889
train: step: 2764, loss: 0.6905751824378967, acc: 0.67, recall: 0.6699999999999999, precision: 0.6717171717171717, f_beta: 0.6691729323308271
train: step: 2765, loss: 0.6892448663711548, acc: 0.72, recall: 0.72, precision: 0.7203525641025641, f_beta: 0.7198879551820727
train: step: 2766, loss: 0.6921577453613281, acc: 0.61, recall: 0.6100000000000001, precision: 0.6121991024071807, f_beta: 0.6080795899909557
train: step: 2767, loss: 0.6901299953460693, acc: 0.67, recall: 0.67, precision: 0.6823251823251824, f_beta: 0.6643271284711627
train: step: 2768, loss: 0.6757079362869263, acc: 0.85, recall: 0.85, precision: 0.8501400560224089, f_beta: 0.84998499849985
train: step: 2769, loss: 0.6909770965576172, acc: 0.62, recall: 0.62, precision: 0.6201923076923077, f_beta: 0.6198479391756702
train: step: 2770, loss: 0.6773310303688049, acc: 0.73, recall: 0.73, precision: 0.7345981232150143, f_beta: 0.7286704853783539
train: step: 2771, loss: 0.692513108253479, acc: 0.55, recall: 0.55, precision: 0.550999592003264, f_beta: 0.5477841422972565
train: step: 2772, loss: 0.6880713105201721, acc: 0.63, recall: 0.63, precision: 0.7303330970942594, f_beta: 0.5847828526540231
train: step: 2773, loss: 0.6922938823699951, acc: 0.53, recall: 0.53, precision: 0.530108390204737, f_beta: 0.5295766189570613
train: step: 2774, loss: 0.6922351717948914, acc: 0.55, recall: 0.55, precision: 0.5584385226741468, f_beta: 0.5331465919701215
train: step: 2775, loss: 0.6903955936431885, acc: 0.67, recall: 0.67, precision: 0.7266666666666666, f_beta: 0.648
train: step: 2776, loss: 0.6916373372077942, acc: 0.61, recall: 0.6100000000000001, precision: 0.6121991024071807, f_beta: 0.6080795899909557
train: step: 2777, loss: 0.6927505731582642, acc: 0.53, recall: 0.53, precision: 0.6152073732718895, f_beta: 0.42338363390994965
train: step: 2778, loss: 0.6921910643577576, acc: 0.65, recall: 0.65, precision: 0.7, f_beta: 0.6266666666666667
train: step: 2779, loss: 0.6932827830314636, acc: 0.48, recall: 0.48000000000000004, precision: 0.47947454844006565, f_beta: 0.47665056360708535
train: step: 2780, loss: 0.692069411277771, acc: 0.55, recall: 0.55, precision: 0.5525430853299707, f_beta: 0.5444883085332524
train: step: 2781, loss: 0.6927367448806763, acc: 0.51, recall: 0.51, precision: 0.51010101010101, f_beta: 0.5087719298245614
train: step: 2782, loss: 0.6928604245185852, acc: 0.56, recall: 0.56, precision: 0.5600961538461539, f_beta: 0.5598239295718288
train: step: 2783, loss: 0.6930015683174133, acc: 0.47, recall: 0.47000000000000003, precision: 0.4649368863955119, f_beta: 0.4501504305425874
train: step: 2784, loss: 0.6920735836029053, acc: 0.56, recall: 0.56, precision: 0.5603864734299517, f_beta: 0.5592948717948718
train: step: 2785, loss: 0.6928089261054993, acc: 0.53, recall: 0.53, precision: 0.5487329434697856, f_beta: 0.4800309768779732
train: step: 2786, loss: 0.6933717131614685, acc: 0.5, recall: 0.5, precision: 0.5, f_beta: 0.46374946374946374
train: step: 2787, loss: 0.6927350759506226, acc: 0.51, recall: 0.51, precision: 0.510508617065994, f_beta: 0.5039983804028748
train: step: 2788, loss: 0.6927724480628967, acc: 0.6, recall: 0.6, precision: 0.6456876456876457, f_beta: 0.5659722222222222
train: step: 2789, loss: 0.6932051777839661, acc: 0.51, recall: 0.51, precision: 0.5100361300682457, f_beta: 0.5095586027424682
train: step: 2790, loss: 0.6930950880050659, acc: 0.49, recall: 0.49, precision: 0.4898000815993472, f_beta: 0.48748869460355737
train: step: 2791, loss: 0.6923906803131104, acc: 0.56, recall: 0.56, precision: 0.5714285714285714, f_beta: 0.5416666666666666
train: step: 2792, loss: 0.6924160122871399, acc: 0.54, recall: 0.54, precision: 0.5405844155844155, f_beta: 0.5383380168606985
train: step: 2793, loss: 0.6925615668296814, acc: 0.51, recall: 0.51, precision: 0.5100040016006402, f_beta: 0.50995099509951
train: step: 2794, loss: 0.6929084062576294, acc: 0.48, recall: 0.48, precision: 0.47916666666666663, f_beta: 0.4747474747474748
train: step: 2795, loss: 0.6931757926940918, acc: 0.47, recall: 0.47, precision: 0.46847414880201765, f_beta: 0.46350845227249715
train: step: 2796, loss: 0.6904800534248352, acc: 0.64, recall: 0.64, precision: 0.6608455882352942, f_beta: 0.6279454319966928
train: step: 2797, loss: 0.6917757987976074, acc: 0.6, recall: 0.6000000000000001, precision: 0.6006441223832528, f_beta: 0.5993589743589743
train: step: 2798, loss: 0.6899569630622864, acc: 0.65, recall: 0.65, precision: 0.6576292559899117, f_beta: 0.6457131288591963
train: step: 2799, loss: 0.6926729679107666, acc: 0.47, recall: 0.47, precision: 0.46703296703296704, f_beta: 0.4578005115089514
train: step: 2800, loss: 0.6935638189315796, acc: 0.45, recall: 0.44999999999999996, precision: 0.44505494505494503, f_beta: 0.4373401534526854
train: step: 2801, loss: 0.6922571063041687, acc: 0.59, recall: 0.59, precision: 0.601763907734057, f_beta: 0.5777983729790959
train: step: 2802, loss: 0.6929052472114563, acc: 0.53, recall: 0.53, precision: 0.533921302578019, f_beta: 0.5160127690248172
train: step: 2803, loss: 0.693207859992981, acc: 0.49, recall: 0.49, precision: 0.4898989898989899, f_beta: 0.48872180451127817
train: step: 2804, loss: 0.692472517490387, acc: 0.56, recall: 0.56, precision: 0.5615763546798029, f_beta: 0.5571658615136876
train: step: 2805, loss: 0.6926523447036743, acc: 0.53, recall: 0.53, precision: 0.5315258511979823, f_beta: 0.5242433444680635
train: step: 2806, loss: 0.6926589012145996, acc: 0.57, recall: 0.5700000000000001, precision: 0.5713994288045696, f_beta: 0.567882624861823
train: step: 2807, loss: 0.6918070912361145, acc: 0.62, recall: 0.62, precision: 0.6644736842105263, f_beta: 0.5924495924495925
train: step: 2808, loss: 0.6933672428131104, acc: 0.51, recall: 0.51, precision: 0.5121418164157359, f_beta: 0.4873940788785438
train: step: 2809, loss: 0.6908209919929504, acc: 0.59, recall: 0.59, precision: 0.5989010989010989, f_beta: 0.5805626598465473
train: step: 2810, loss: 0.6903683543205261, acc: 0.62, recall: 0.62, precision: 0.6644736842105263, f_beta: 0.5924495924495925
train: step: 2811, loss: 0.6925190687179565, acc: 0.58, recall: 0.5800000000000001, precision: 0.5833333333333333, f_beta: 0.5757575757575757
train: step: 2812, loss: 0.6907920837402344, acc: 0.65, recall: 0.6499999999999999, precision: 0.6515151515151515, f_beta: 0.6491228070175439
train: step: 2813, loss: 0.6927713751792908, acc: 0.51, recall: 0.51, precision: 0.5121418164157359, f_beta: 0.4873940788785438
train: step: 2814, loss: 0.691942036151886, acc: 0.61, recall: 0.61, precision: 0.6103974307507025, f_beta: 0.6096486838154338
train: step: 2815, loss: 0.6919500231742859, acc: 0.57, recall: 0.57, precision: 0.575075075075075, f_beta: 0.562608076492727
train: step: 2816, loss: 0.6893512606620789, acc: 0.6, recall: 0.6, precision: 0.6085069444444444, f_beta: 0.5920032639738881
train: step: 2817, loss: 0.6922394633293152, acc: 0.6, recall: 0.6000000000000001, precision: 0.6006441223832528, f_beta: 0.5993589743589743
train: step: 2818, loss: 0.6922979950904846, acc: 0.56, recall: 0.56, precision: 0.5615763546798029, f_beta: 0.5571658615136876
train: step: 2819, loss: 0.693316638469696, acc: 0.47, recall: 0.47, precision: 0.45999999999999996, f_beta: 0.43466666666666665
train: step: 2820, loss: 0.6884057521820068, acc: 0.64, recall: 0.64, precision: 0.7604166666666667, f_beta: 0.592944369063772
train: step: 2821, loss: 0.6912757754325867, acc: 0.57, recall: 0.57, precision: 0.5769230769230769, f_beta: 0.5601023017902813
train: step: 2822, loss: 0.6911274194717407, acc: 0.67, recall: 0.6699999999999999, precision: 0.6706142111601766, f_beta: 0.6697027324592133
train: step: 2823, loss: 0.6908693909645081, acc: 0.6, recall: 0.6, precision: 0.6085069444444444, f_beta: 0.5920032639738881
train: step: 2824, loss: 0.6898742914199829, acc: 0.61, recall: 0.61, precision: 0.6179751179751181, f_beta: 0.6032956972841013
train: step: 2825, loss: 0.6906305551528931, acc: 0.63, recall: 0.63, precision: 0.6578436134045653, f_beta: 0.6129302228266555
train: step: 2826, loss: 0.6908040642738342, acc: 0.55, recall: 0.55, precision: 0.6105216622458002, f_beta: 0.4786235662148072
train: step: 2827, loss: 0.6881304979324341, acc: 0.66, recall: 0.66, precision: 0.66, f_beta: 0.66
train: step: 2828, loss: 0.6883193850517273, acc: 0.62, recall: 0.62, precision: 0.6378676470588236, f_beta: 0.6072757337742869
train: step: 2829, loss: 0.692677915096283, acc: 0.5, recall: 0.5, precision: 0.5, f_beta: 0.47916666666666663
train: step: 2830, loss: 0.6895138025283813, acc: 0.61, recall: 0.61, precision: 0.6786874593892138, f_beta: 0.568536342515765
train: step: 2831, loss: 0.6897188425064087, acc: 0.62, recall: 0.62, precision: 0.7491694352159468, f_beta: 0.5634191176470589
train: step: 2832, loss: 0.6903364658355713, acc: 0.6, recall: 0.6000000000000001, precision: 0.6006441223832528, f_beta: 0.5993589743589743
train: step: 2833, loss: 0.6919613480567932, acc: 0.57, recall: 0.5700000000000001, precision: 0.5887874175545409, f_beta: 0.5459824728117411
train: step: 2834, loss: 0.6902329921722412, acc: 0.6, recall: 0.6, precision: 0.7777777777777778, f_beta: 0.5238095238095238
train: step: 2835, loss: 0.6910128593444824, acc: 0.57, recall: 0.5700000000000001, precision: 0.6240255138199858, f_beta: 0.5174503422735944
train: step: 2836, loss: 0.693834662437439, acc: 0.44, recall: 0.44, precision: 0.44, f_beta: 0.44
train: step: 2837, loss: 0.68631911277771, acc: 0.73, recall: 0.73, precision: 0.7416981925178646, f_beta: 0.7266929851199515
train: step: 2838, loss: 0.689852237701416, acc: 0.71, recall: 0.71, precision: 0.7170318313352625, f_beta: 0.7076318177235609
train: step: 2839, loss: 0.6923193335533142, acc: 0.5, recall: 0.5, precision: 0.5, f_beta: 0.4981934965877158
train: step: 2840, loss: 0.6913626194000244, acc: 0.61, recall: 0.61, precision: 0.6179751179751181, f_beta: 0.6032956972841013
train: step: 2841, loss: 0.6919176578521729, acc: 0.54, recall: 0.54, precision: 0.5402576489533011, f_beta: 0.5392628205128205
train: step: 2842, loss: 0.688042402267456, acc: 0.71, recall: 0.71, precision: 0.7121212121212122, f_beta: 0.7092731829573935
train: step: 2843, loss: 0.692008376121521, acc: 0.61, recall: 0.61, precision: 0.6208791208791209, f_beta: 0.6010230179028133
train: step: 2844, loss: 0.693040668964386, acc: 0.48, recall: 0.48, precision: 0.4725877192982456, f_beta: 0.4422994422994423
train: step: 2845, loss: 0.6923732161521912, acc: 0.55, recall: 0.55, precision: 0.550999592003264, f_beta: 0.5477841422972565
train: step: 2846, loss: 0.693250834941864, acc: 0.49, recall: 0.49, precision: 0.4899959983993597, f_beta: 0.48994899489948995
train: step: 2847, loss: 0.6939653754234314, acc: 0.48, recall: 0.48, precision: 0.47970779220779225, f_beta: 0.4781212364512244
train: step: 2848, loss: 0.6917707920074463, acc: 0.59, recall: 0.5900000000000001, precision: 0.5903251706142112, f_beta: 0.5896306676008407
train: step: 2849, loss: 0.694354236125946, acc: 0.46, recall: 0.45999999999999996, precision: 0.4451754385964912, f_beta: 0.42084942084942084
train: step: 2850, loss: 0.6927276849746704, acc: 0.51, recall: 0.51, precision: 0.51010101010101, f_beta: 0.5087719298245614
train: step: 2851, loss: 0.6926140785217285, acc: 0.58, recall: 0.58, precision: 0.58, f_beta: 0.58
train: step: 2852, loss: 0.6928174495697021, acc: 0.53, recall: 0.53, precision: 0.5300120048019208, f_beta: 0.5299529952995299
train: step: 2853, loss: 0.6923478841781616, acc: 0.51, recall: 0.51, precision: 0.51010101010101, f_beta: 0.5087719298245614
train: step: 2854, loss: 0.6925135254859924, acc: 0.54, recall: 0.54, precision: 0.5402576489533011, f_beta: 0.5392628205128205
train: step: 2855, loss: 0.6927564740180969, acc: 0.47, recall: 0.47, precision: 0.46989160979526295, f_beta: 0.46952257031328193
train: step: 2856, loss: 0.6890331506729126, acc: 0.63, recall: 0.63, precision: 0.6313131313131313, f_beta: 0.6290726817042607
train: step: 2857, loss: 0.6888164281845093, acc: 0.78, recall: 0.78, precision: 0.8119429590017826, f_beta: 0.7742200328407225
train: step: 2858, loss: 0.6924968957901001, acc: 0.53, recall: 0.53, precision: 0.5305997552019583, f_beta: 0.5276856597326902
train: step: 2859, loss: 0.6910874843597412, acc: 0.63, recall: 0.63, precision: 0.6325989392084863, f_beta: 0.6281780725555222
train: step: 2860, loss: 0.6891815066337585, acc: 0.66, recall: 0.66, precision: 0.669779286926995, f_beta: 0.6550324675324675
train: step: 2861, loss: 0.6917788982391357, acc: 0.58, recall: 0.5800000000000001, precision: 0.5801282051282051, f_beta: 0.5798319327731092
train: step: 2862, loss: 0.6919395327568054, acc: 0.58, recall: 0.58, precision: 0.6165501165501166, f_beta: 0.5442708333333334
train: step: 2863, loss: 0.692888617515564, acc: 0.55, recall: 0.55, precision: 0.5565355042966984, f_beta: 0.5366079703429101
train: step: 2864, loss: 0.6907370090484619, acc: 0.62, recall: 0.62, precision: 0.6302083333333333, f_beta: 0.6124031007751938
train: step: 2865, loss: 0.6907225847244263, acc: 0.64, recall: 0.64, precision: 0.6519097222222222, f_beta: 0.6328029375764994
train: step: 2866, loss: 0.69230055809021, acc: 0.47, recall: 0.47, precision: 0.4699879951980792, f_beta: 0.46994699469946993
train: step: 2867, loss: 0.6914907097816467, acc: 0.59, recall: 0.5900000000000001, precision: 0.5909090909090908, f_beta: 0.5889724310776943
train: step: 2868, loss: 0.6926510334014893, acc: 0.56, recall: 0.56, precision: 0.5636672325976231, f_beta: 0.5535714285714286
train: step: 2869, loss: 0.6892275214195251, acc: 0.61, recall: 0.61, precision: 0.6155947877259353, f_beta: 0.6052232007288187
train: step: 2870, loss: 0.687908947467804, acc: 0.73, recall: 0.73, precision: 0.7345981232150143, f_beta: 0.7286704853783539
train: step: 2871, loss: 0.6939427256584167, acc: 0.46, recall: 0.46, precision: 0.4255952380952381, f_beta: 0.3894165535956581
train: step: 2872, loss: 0.6902250647544861, acc: 0.59, recall: 0.5900000000000001, precision: 0.6092763477416221, f_beta: 0.5710848415106183
train: step: 2873, loss: 0.6849107146263123, acc: 0.69, recall: 0.69, precision: 0.7533333333333334, f_beta: 0.6693333333333333
train: step: 2874, loss: 0.6916968822479248, acc: 0.54, recall: 0.54, precision: 0.5434027777777778, f_beta: 0.5308037535699714
train: step: 2875, loss: 0.6832467913627625, acc: 0.71, recall: 0.71, precision: 0.7206809583858764, f_beta: 0.7064480210547626
train: step: 2876, loss: 0.6918742656707764, acc: 0.58, recall: 0.58, precision: 0.6096491228070176, f_beta: 0.5495495495495496
train: step: 2877, loss: 0.67839515209198, acc: 0.72, recall: 0.72, precision: 0.7527573529411764, f_beta: 0.7106242248863166
train: step: 2878, loss: 0.686521053314209, acc: 0.7, recall: 0.7, precision: 0.7741228070175439, f_beta: 0.6782496782496782
train: step: 2879, loss: 0.688630998134613, acc: 0.67, recall: 0.6699999999999999, precision: 0.6922207146087743, f_beta: 0.6601791782514674
train: step: 2880, loss: 0.6852755546569824, acc: 0.72, recall: 0.72, precision: 0.7619047619047619, f_beta: 0.7083333333333334
train: step: 2881, loss: 0.6913921236991882, acc: 0.53, recall: 0.53, precision: 0.533921302578019, f_beta: 0.5160127690248172
train: step: 2882, loss: 0.6773849725723267, acc: 0.74, recall: 0.74, precision: 0.7604166666666667, f_beta: 0.7348021215830274
train: step: 2883, loss: 0.6847920417785645, acc: 0.6, recall: 0.6, precision: 0.6860119047619048, f_beta: 0.5477159656264134
train: step: 2884, loss: 0.6756234169006348, acc: 0.69, recall: 0.69, precision: 0.7863170584689572, f_beta: 0.6615351020853805
train: step: 2885, loss: 0.6894748210906982, acc: 0.63, recall: 0.63, precision: 0.6959011452682339, f_beta: 0.5960257670051317
train: step: 2886, loss: 0.6719098091125488, acc: 0.67, recall: 0.6699999999999999, precision: 0.7761533463287849, f_beta: 0.6349153667441089
train: step: 2887, loss: 0.6745868921279907, acc: 0.65, recall: 0.65, precision: 0.7117447769621683, f_beta: 0.6224786970121886
train: step: 2888, loss: 0.6750900149345398, acc: 0.77, recall: 0.77, precision: 0.8155680224403927, f_beta: 0.7613860358958398
train: step: 2889, loss: 0.6653980016708374, acc: 0.69, recall: 0.69, precision: 0.7682100508187464, f_beta: 0.6656239887822242
train: step: 2890, loss: 0.675362229347229, acc: 0.7, recall: 0.7, precision: 0.7913752913752914, f_beta: 0.6744791666666667
train: step: 2891, loss: 0.6931329965591431, acc: 0.52, recall: 0.52, precision: 0.5200320512820513, f_beta: 0.5198079231692677
train: step: 2892, loss: 0.6828721761703491, acc: 0.63, recall: 0.63, precision: 0.7873563218390804, f_beta: 0.5713127099988414
train: step: 2893, loss: 0.6746050119400024, acc: 0.61, recall: 0.61, precision: 0.6948972360028349, f_beta: 0.5623386825272136
train: step: 2894, loss: 0.6793967485427856, acc: 0.66, recall: 0.66, precision: 0.7192982456140351, f_beta: 0.6353496353496353
train: step: 2895, loss: 0.6624311804771423, acc: 0.68, recall: 0.6799999999999999, precision: 0.7622377622377623, f_beta: 0.6527777777777779
train: step: 2896, loss: 0.6762381196022034, acc: 0.76, recall: 0.76, precision: 0.8378378378378378, f_beta: 0.7453310696095077
train: step: 2897, loss: 0.6843040585517883, acc: 0.61, recall: 0.61, precision: 0.7431476569407605, f_beta: 0.5481404240528328
train: step: 2898, loss: 0.6881590485572815, acc: 0.66, recall: 0.66, precision: 0.6736111111111112, f_beta: 0.6532027743778049
train: step: 2899, loss: 0.6882529258728027, acc: 0.55, recall: 0.55, precision: 0.763157894736842, f_beta: 0.4357366771159875
train: step: 2900, loss: 0.6568357348442078, acc: 0.67, recall: 0.6699999999999999, precision: 0.7561784207353828, f_beta: 0.6396986570586309
train: step: 2901, loss: 0.6626192331314087, acc: 0.77, recall: 0.77, precision: 0.8155680224403927, f_beta: 0.7613860358958398
train: step: 2902, loss: 0.6613968014717102, acc: 0.7, recall: 0.7, precision: 0.702922077922078, f_beta: 0.6989160979526294
train: step: 2903, loss: 0.6504990458488464, acc: 0.7, recall: 0.7, precision: 0.8125, f_beta: 0.6703296703296704
train: step: 2904, loss: 0.6809157729148865, acc: 0.59, recall: 0.59, precision: 0.6989389920424403, f_beta: 0.5249681381068243
train: step: 2905, loss: 0.6736536622047424, acc: 0.63, recall: 0.63, precision: 0.6733333333333333, f_beta: 0.6053333333333333
train: step: 2906, loss: 0.6872509121894836, acc: 0.64, recall: 0.64, precision: 0.6608455882352942, f_beta: 0.6279454319966928
train: step: 2907, loss: 0.6512371897697449, acc: 0.71, recall: 0.71, precision: 0.7663622526636225, f_beta: 0.6938021328265231
train: step: 2908, loss: 0.684824526309967, acc: 0.67, recall: 0.67, precision: 0.7266666666666666, f_beta: 0.648
train: step: 2909, loss: 0.6751226186752319, acc: 0.65, recall: 0.65, precision: 0.7436647173489279, f_beta: 0.612789025334661
train: step: 2910, loss: 0.6600107550621033, acc: 0.79, recall: 0.79, precision: 0.7901160464185675, f_beta: 0.78997899789979
train: step: 2911, loss: 0.6838070154190063, acc: 0.58, recall: 0.58, precision: 0.7222222222222222, f_beta: 0.5
train: step: 2912, loss: 0.6610077023506165, acc: 0.68, recall: 0.6799999999999999, precision: 0.6875, f_beta: 0.6767676767676767
train: step: 2913, loss: 0.6560766696929932, acc: 0.7, recall: 0.7, precision: 0.7913752913752914, f_beta: 0.6744791666666667
train: step: 2914, loss: 0.6693477630615234, acc: 0.68, recall: 0.6799999999999999, precision: 0.6953125, f_beta: 0.6736026111791106
train: step: 2915, loss: 0.6790022850036621, acc: 0.66, recall: 0.6599999999999999, precision: 0.7331002331002331, f_beta: 0.6310763888888888
train: step: 2916, loss: 0.681528627872467, acc: 0.57, recall: 0.5700000000000001, precision: 0.6240255138199858, f_beta: 0.5174503422735944
train: step: 2917, loss: 0.6682626605033875, acc: 0.67, recall: 0.67, precision: 0.7064108790675085, f_beta: 0.6547756041426929
train: step: 2918, loss: 0.6925399303436279, acc: 0.63, recall: 0.63, precision: 0.6959011452682339, f_beta: 0.5960257670051317
train: step: 2919, loss: 0.6764528751373291, acc: 0.69, recall: 0.69, precision: 0.7863170584689572, f_beta: 0.6615351020853805
train: step: 2920, loss: 0.6496481895446777, acc: 0.7, recall: 0.7, precision: 0.7003205128205128, f_beta: 0.6998799519807923
train: step: 2921, loss: 0.6406566500663757, acc: 0.73, recall: 0.73, precision: 0.7377015295576685, f_beta: 0.7277951406391774
train: step: 2922, loss: 0.6341903209686279, acc: 0.79, recall: 0.79, precision: 0.8279059249208502, f_beta: 0.7837503861600247
train: step: 2923, loss: 0.6538336873054504, acc: 0.71, recall: 0.71, precision: 0.78, f_beta: 0.6906666666666665
train: step: 2924, loss: 0.645717978477478, acc: 0.68, recall: 0.6799999999999999, precision: 0.6875, f_beta: 0.6767676767676767
train: step: 2925, loss: 0.6578608751296997, acc: 0.7, recall: 0.7, precision: 0.7741228070175439, f_beta: 0.6782496782496782
train: step: 2926, loss: 0.6630614399909973, acc: 0.67, recall: 0.6699999999999999, precision: 0.6868131868131868, f_beta: 0.6624040920716112
train: step: 2927, loss: 0.6546761989593506, acc: 0.75, recall: 0.75, precision: 0.7826775214834916, f_beta: 0.7425599835238389
train: step: 2928, loss: 0.6533491611480713, acc: 0.69, recall: 0.69, precision: 0.6900760304121649, f_beta: 0.68996899689969
train: step: 2929, loss: 0.6648133993148804, acc: 0.7, recall: 0.7, precision: 0.8125, f_beta: 0.6703296703296704
train: step: 2930, loss: 0.6646066308021545, acc: 0.66, recall: 0.66, precision: 0.7192982456140351, f_beta: 0.6353496353496353
train: step: 2931, loss: 0.6496906280517578, acc: 0.71, recall: 0.71, precision: 0.7307692307692308, f_beta: 0.7033248081841432
train: step: 2932, loss: 0.6484178900718689, acc: 0.76, recall: 0.76, precision: 0.7821180555555556, f_beta: 0.755201958384333
train: step: 2933, loss: 0.6436703205108643, acc: 0.69, recall: 0.6900000000000001, precision: 0.7409944190766109, f_beta: 0.6726850385386971
train: step: 2934, loss: 0.6836717128753662, acc: 0.61, recall: 0.61, precision: 0.6657625075346594, f_beta: 0.5741893219783819
train: step: 2935, loss: 0.654506504535675, acc: 0.67, recall: 0.6699999999999999, precision: 0.7399774138904573, f_beta: 0.6440513428972063
train: step: 2936, loss: 0.693347156047821, acc: 0.51, recall: 0.51, precision: 0.5126839167935058, f_beta: 0.48263118994826315
train: step: 2937, loss: 0.6726168990135193, acc: 0.61, recall: 0.61, precision: 0.6243781094527363, f_beta: 0.5983935742971889
train: step: 2938, loss: 0.6613436937332153, acc: 0.67, recall: 0.6699999999999999, precision: 0.6868131868131868, f_beta: 0.6624040920716112
train: step: 2939, loss: 0.6905601620674133, acc: 0.57, recall: 0.5700000000000001, precision: 0.5707070707070707, f_beta: 0.5689223057644109
train: step: 2940, loss: 0.684054970741272, acc: 0.57, recall: 0.5700000000000001, precision: 0.5818139317438056, f_beta: 0.5538956323270049
train: step: 2941, loss: 0.6903231143951416, acc: 0.5, recall: 0.5, precision: 0.5, f_beta: 0.4981934965877158
train: step: 2942, loss: 0.6928696632385254, acc: 0.54, recall: 0.54, precision: 0.5410509031198687, f_beta: 0.537037037037037
train: step: 2943, loss: 0.6929097175598145, acc: 0.52, recall: 0.52, precision: 0.5201288244766505, f_beta: 0.5192307692307692
train: step: 2944, loss: 0.6937294006347656, acc: 0.5, recall: 0.5, precision: 0.5, f_beta: 0.4981934965877158
train: step: 2945, loss: 0.6908428072929382, acc: 0.62, recall: 0.62, precision: 0.6273344651952462, f_beta: 0.614448051948052
train: step: 2946, loss: 0.6919746994972229, acc: 0.55, recall: 0.55, precision: 0.5525430853299707, f_beta: 0.5444883085332524
train: step: 2947, loss: 0.6927124857902527, acc: 0.53, recall: 0.53, precision: 0.530108390204737, f_beta: 0.5295766189570613
train: step: 2948, loss: 0.6921967267990112, acc: 0.52, recall: 0.52, precision: 0.5201288244766505, f_beta: 0.5192307692307692
train: step: 2949, loss: 0.6923826336860657, acc: 0.5, recall: 0.5, precision: 0.5, f_beta: 0.486863711001642
train: step: 2950, loss: 0.6935701966285706, acc: 0.49, recall: 0.49, precision: 0.48966515088879703, f_beta: 0.48583526565177937
train: step: 2951, loss: 0.6921565532684326, acc: 0.58, recall: 0.5800000000000001, precision: 0.5919117647058824, f_beta: 0.565936337329475
train: step: 2952, loss: 0.69146329164505, acc: 0.56, recall: 0.56, precision: 0.5608766233766234, f_beta: 0.5584102769971899
train: step: 2953, loss: 0.6902417540550232, acc: 0.62, recall: 0.62, precision: 0.6207729468599035, f_beta: 0.6193910256410255
train: step: 2954, loss: 0.6941345930099487, acc: 0.43, recall: 0.43, precision: 0.42860057119543044, f_beta: 0.42719324690985827
train: step: 2955, loss: 0.6927480101585388, acc: 0.51, recall: 0.51, precision: 0.51010101010101, f_beta: 0.5087719298245614
train: step: 2956, loss: 0.6920003294944763, acc: 0.51, recall: 0.51, precision: 0.510508617065994, f_beta: 0.5039983804028748
train: step: 2957, loss: 0.6910722851753235, acc: 0.65, recall: 0.65, precision: 0.6505419510236852, f_beta: 0.6496847162446201
train: step: 2958, loss: 0.6922681927680969, acc: 0.57, recall: 0.5700000000000001, precision: 0.5700280112044818, f_beta: 0.56995699569957
train: step: 2959, loss: 0.6930972337722778, acc: 0.54, recall: 0.54, precision: 0.5405844155844155, f_beta: 0.5383380168606985
train: step: 2960, loss: 0.693102240562439, acc: 0.54, recall: 0.54, precision: 0.5459558823529411, f_beta: 0.524596940884663
train: step: 2961, loss: 0.6896240711212158, acc: 0.6, recall: 0.6, precision: 0.6114081996434938, f_beta: 0.5894909688013137
train: step: 2962, loss: 0.6925762891769409, acc: 0.52, recall: 0.52, precision: 0.5202922077922079, f_beta: 0.5182657567242072
train: step: 2963, loss: 0.6901364922523499, acc: 0.59, recall: 0.59, precision: 0.5900360144057624, f_beta: 0.5899589958995899
train: step: 2964, loss: 0.6881906986236572, acc: 0.71, recall: 0.71, precision: 0.7206809583858764, f_beta: 0.7064480210547626
train: step: 2965, loss: 0.6895967721939087, acc: 0.64, recall: 0.6399999999999999, precision: 0.6420454545454546, f_beta: 0.6386993175431553
train: step: 2966, loss: 0.6899111866950989, acc: 0.62, recall: 0.62, precision: 0.6231527093596059, f_beta: 0.6175523349436394
train: step: 2967, loss: 0.6927005052566528, acc: 0.52, recall: 0.52, precision: 0.5201288244766505, f_beta: 0.5192307692307692
train: step: 2968, loss: 0.6920530200004578, acc: 0.53, recall: 0.53, precision: 0.5305997552019583, f_beta: 0.5276856597326902
train: step: 2969, loss: 0.6907209753990173, acc: 0.54, recall: 0.54, precision: 0.5496031746031746, f_beta: 0.5166036149642708
train: step: 2970, loss: 0.693580687046051, acc: 0.5, recall: 0.5, precision: 0.5, f_beta: 0.49919871794871795
train: step: 2971, loss: 0.6911162734031677, acc: 0.63, recall: 0.63, precision: 0.6300520208083233, f_beta: 0.6299629962996299
train: step: 2972, loss: 0.6937952637672424, acc: 0.47, recall: 0.47, precision: 0.4699879951980792, f_beta: 0.46994699469946993
train: step: 2973, loss: 0.6927762031555176, acc: 0.53, recall: 0.53, precision: 0.530108390204737, f_beta: 0.5295766189570613
train: step: 2974, loss: 0.692091703414917, acc: 0.49, recall: 0.49, precision: 0.48996386993175434, f_beta: 0.4895405865278751
train: step: 2975, loss: 0.6912208795547485, acc: 0.65, recall: 0.65, precision: 0.6500600240096038, f_beta: 0.6499649964996499
train: step: 2976, loss: 0.693143904209137, acc: 0.5, recall: 0.5, precision: 0.5, f_beta: 0.4967793880837359
train: step: 2977, loss: 0.6922919750213623, acc: 0.59, recall: 0.59, precision: 0.5900360144057624, f_beta: 0.5899589958995899
train: step: 2978, loss: 0.6925727725028992, acc: 0.46, recall: 0.46, precision: 0.46, f_beta: 0.46
train: step: 2979, loss: 0.6916750073432922, acc: 0.57, recall: 0.5700000000000001, precision: 0.5723439437784208, f_beta: 0.5664885573142453
train: step: 2980, loss: 0.6886270046234131, acc: 0.73, recall: 0.73, precision: 0.7377015295576685, f_beta: 0.7277951406391774
train: step: 2981, loss: 0.6908645033836365, acc: 0.58, recall: 0.5800000000000001, precision: 0.5952380952380952, f_beta: 0.5625
train: step: 2982, loss: 0.6913596987724304, acc: 0.53, recall: 0.53, precision: 0.530108390204737, f_beta: 0.5295766189570613
train: step: 2983, loss: 0.6900390386581421, acc: 0.66, recall: 0.66, precision: 0.6602564102564102, f_beta: 0.6598639455782312
train: step: 2984, loss: 0.6890140771865845, acc: 0.7, recall: 0.7, precision: 0.702922077922078, f_beta: 0.6989160979526294
train: step: 2985, loss: 0.6877549886703491, acc: 0.67, recall: 0.6699999999999999, precision: 0.6717171717171717, f_beta: 0.6691729323308271
train: step: 2986, loss: 0.6909807324409485, acc: 0.63, recall: 0.63, precision: 0.6304696908871938, f_beta: 0.629666700030027
train: step: 2987, loss: 0.6910922527313232, acc: 0.57, recall: 0.5700000000000001, precision: 0.5713994288045696, f_beta: 0.567882624861823
train: step: 2988, loss: 0.692331850528717, acc: 0.54, recall: 0.54, precision: 0.5476190476190477, f_beta: 0.5208333333333333
train: step: 2989, loss: 0.6919865608215332, acc: 0.54, recall: 0.54, precision: 0.5434027777777778, f_beta: 0.5308037535699714
train: step: 2990, loss: 0.6935697197914124, acc: 0.48, recall: 0.48000000000000004, precision: 0.47947454844006565, f_beta: 0.47665056360708535
train: step: 2991, loss: 0.6912026405334473, acc: 0.66, recall: 0.66, precision: 0.66, f_beta: 0.66
train: step: 2992, loss: 0.6915850639343262, acc: 0.63, recall: 0.63, precision: 0.6300520208083233, f_beta: 0.6299629962996299
train: step: 2993, loss: 0.694131076335907, acc: 0.45, recall: 0.45, precision: 0.4499799919967987, f_beta: 0.44994499449944997
train: step: 2994, loss: 0.6922264099121094, acc: 0.51, recall: 0.51, precision: 0.510334849111203, f_beta: 0.5059985885673959
train: step: 2995, loss: 0.6913989186286926, acc: 0.65, recall: 0.65, precision: 0.6500600240096038, f_beta: 0.6499649964996499
train: step: 2996, loss: 0.6907972097396851, acc: 0.6, recall: 0.6000000000000001, precision: 0.601461038961039, f_beta: 0.5985547972701726
train: step: 2997, loss: 0.6922394633293152, acc: 0.52, recall: 0.52, precision: 0.5212224108658743, f_beta: 0.512987012987013
train: step: 2998, loss: 0.6910718679428101, acc: 0.56, recall: 0.56, precision: 0.5603864734299517, f_beta: 0.5592948717948718
train: step: 2999, loss: 0.6926430463790894, acc: 0.56, recall: 0.56, precision: 0.5603864734299517, f_beta: 0.5592948717948718
train: step: 3000, loss: 0.6926405429840088, acc: 0.55, recall: 0.55, precision: 0.5505050505050505, f_beta: 0.5488721804511277
train: step: 3001, loss: 0.6922187209129333, acc: 0.53, recall: 0.53, precision: 0.5310045473336089, f_beta: 0.5261619114830125
train: step: 3002, loss: 0.692165732383728, acc: 0.55, recall: 0.55, precision: 0.5500200080032013, f_beta: 0.54995499549955
train: step: 3003, loss: 0.6932174563407898, acc: 0.49, recall: 0.49, precision: 0.4899959983993597, f_beta: 0.48994899489948995
train: step: 3004, loss: 0.6906564831733704, acc: 0.63, recall: 0.63, precision: 0.6300520208083233, f_beta: 0.6299629962996299
train: step: 3005, loss: 0.6914163827896118, acc: 0.57, recall: 0.5700000000000001, precision: 0.5700280112044818, f_beta: 0.56995699569957
train: step: 3006, loss: 0.6920660138130188, acc: 0.56, recall: 0.56, precision: 0.5608766233766234, f_beta: 0.5584102769971899
train: step: 3007, loss: 0.6938406229019165, acc: 0.48, recall: 0.48, precision: 0.47970779220779225, f_beta: 0.4781212364512244
train: step: 3008, loss: 0.6927549242973328, acc: 0.54, recall: 0.54, precision: 0.5410509031198687, f_beta: 0.537037037037037
train: step: 3009, loss: 0.6916119456291199, acc: 0.57, recall: 0.5700000000000001, precision: 0.5702529104777199, f_beta: 0.5696126513862476
train: step: 3010, loss: 0.6912762522697449, acc: 0.61, recall: 0.61, precision: 0.6208791208791209, f_beta: 0.6010230179028133
train: step: 3011, loss: 0.6919959187507629, acc: 0.6, recall: 0.6, precision: 0.6061120543293719, f_beta: 0.5941558441558441
train: step: 3012, loss: 0.6912235021591187, acc: 0.54, recall: 0.54, precision: 0.5424448217317488, f_beta: 0.5332792207792207
train: step: 3013, loss: 0.6937439441680908, acc: 0.47, recall: 0.47, precision: 0.46847414880201765, f_beta: 0.46350845227249715
train: step: 3014, loss: 0.6916080713272095, acc: 0.58, recall: 0.5800000000000001, precision: 0.5805152979066023, f_beta: 0.5793269230769231
train: step: 3015, loss: 0.6926289200782776, acc: 0.48, recall: 0.48, precision: 0.47970779220779225, f_beta: 0.4781212364512244
train: step: 3016, loss: 0.6929910182952881, acc: 0.55, recall: 0.55, precision: 0.5584385226741468, f_beta: 0.5331465919701215
train: step: 3017, loss: 0.6912112236022949, acc: 0.58, recall: 0.58, precision: 0.5868055555555556, f_beta: 0.5716034271725826
train: step: 3018, loss: 0.6917049288749695, acc: 0.57, recall: 0.57, precision: 0.5769230769230769, f_beta: 0.5601023017902813
train: step: 3019, loss: 0.6910327672958374, acc: 0.57, recall: 0.5700000000000001, precision: 0.5713994288045696, f_beta: 0.567882624861823
train: step: 3020, loss: 0.693789005279541, acc: 0.48, recall: 0.48, precision: 0.4782986111111111, f_beta: 0.46960424316605465
train: step: 3021, loss: 0.6922542452812195, acc: 0.53, recall: 0.53, precision: 0.5300120048019208, f_beta: 0.5299529952995299
train: step: 3022, loss: 0.6918630003929138, acc: 0.57, recall: 0.5700000000000001, precision: 0.5700280112044818, f_beta: 0.56995699569957
train: step: 3023, loss: 0.6927027106285095, acc: 0.54, recall: 0.54, precision: 0.5410509031198687, f_beta: 0.537037037037037
train: step: 3024, loss: 0.6927059292793274, acc: 0.54, recall: 0.54, precision: 0.5410509031198687, f_beta: 0.537037037037037
train: step: 3025, loss: 0.6911097168922424, acc: 0.6, recall: 0.6000000000000001, precision: 0.601461038961039, f_beta: 0.5985547972701726
train: step: 3026, loss: 0.6923765540122986, acc: 0.55, recall: 0.55, precision: 0.550999592003264, f_beta: 0.5477841422972565
train: step: 3027, loss: 0.6928905248641968, acc: 0.55, recall: 0.55, precision: 0.5525430853299707, f_beta: 0.5444883085332524
train: step: 3028, loss: 0.691310703754425, acc: 0.58, recall: 0.58, precision: 0.58, f_beta: 0.58
train: step: 3029, loss: 0.6907910108566284, acc: 0.63, recall: 0.63, precision: 0.6325989392084863, f_beta: 0.6281780725555222
train: step: 3030, loss: 0.6934705972671509, acc: 0.48, recall: 0.48, precision: 0.47702205882352944, f_beta: 0.46258784621744525
train: step: 3031, loss: 0.6927653551101685, acc: 0.6, recall: 0.6, precision: 0.6041666666666667, f_beta: 0.595959595959596
train: step: 3032, loss: 0.689431369304657, acc: 0.64, recall: 0.64, precision: 0.6736111111111112, f_beta: 0.6216897856242118
train: step: 3033, loss: 0.6923487186431885, acc: 0.51, recall: 0.51, precision: 0.51010101010101, f_beta: 0.5087719298245614
train: step: 3034, loss: 0.6933897137641907, acc: 0.48, recall: 0.48, precision: 0.47401247401247404, f_beta: 0.44821731748726656
train: step: 3035, loss: 0.690915048122406, acc: 0.6, recall: 0.6, precision: 0.6085069444444444, f_beta: 0.5920032639738881
train: step: 3036, loss: 0.6913673281669617, acc: 0.59, recall: 0.59, precision: 0.5900360144057624, f_beta: 0.5899589958995899
train: step: 3037, loss: 0.6922821998596191, acc: 0.54, recall: 0.54, precision: 0.5476190476190477, f_beta: 0.5208333333333333
train: step: 3038, loss: 0.6916026473045349, acc: 0.56, recall: 0.56, precision: 0.5600961538461539, f_beta: 0.5598239295718288
train: step: 3039, loss: 0.6905906200408936, acc: 0.64, recall: 0.64, precision: 0.6409017713365539, f_beta: 0.639423076923077
train: step: 3040, loss: 0.6915807127952576, acc: 0.61, recall: 0.6100000000000001, precision: 0.6121991024071807, f_beta: 0.6080795899909557
train: step: 3041, loss: 0.6924842000007629, acc: 0.53, recall: 0.53, precision: 0.5423489553924337, f_beta: 0.49304282170208175
train: step: 3042, loss: 0.6919581890106201, acc: 0.53, recall: 0.53, precision: 0.5321750321750323, f_beta: 0.5219204557013528
train: step: 3043, loss: 0.689494788646698, acc: 0.64, recall: 0.6399999999999999, precision: 0.6420454545454546, f_beta: 0.6386993175431553
train: step: 3044, loss: 0.6911419034004211, acc: 0.58, recall: 0.5800000000000001, precision: 0.5821018062397373, f_beta: 0.5772946859903382
train: step: 3045, loss: 0.6892455816268921, acc: 0.67, recall: 0.67, precision: 0.6700680272108843, f_beta: 0.66996699669967
train: step: 3046, loss: 0.6929142475128174, acc: 0.48, recall: 0.48, precision: 0.47401247401247404, f_beta: 0.44821731748726656
train: step: 3047, loss: 0.6913983821868896, acc: 0.52, recall: 0.52, precision: 0.5238095238095237, f_beta: 0.5
train: step: 3048, loss: 0.6901735067367554, acc: 0.68, recall: 0.6799999999999999, precision: 0.6953125, f_beta: 0.6736026111791106
train: step: 3049, loss: 0.6884893178939819, acc: 0.75, recall: 0.75, precision: 0.7549979600163199, f_beta: 0.7487689679429204
train: step: 3050, loss: 0.6918646097183228, acc: 0.59, recall: 0.59, precision: 0.5989010989010989, f_beta: 0.5805626598465473
train: step: 3051, loss: 0.6896899938583374, acc: 0.67, recall: 0.67, precision: 0.6823251823251824, f_beta: 0.6643271284711627
train: step: 3052, loss: 0.6889280676841736, acc: 0.64, recall: 0.6399999999999999, precision: 0.6420454545454546, f_beta: 0.6386993175431553
train: step: 3053, loss: 0.68869948387146, acc: 0.64, recall: 0.6399999999999999, precision: 0.6420454545454546, f_beta: 0.6386993175431553
train: step: 3054, loss: 0.6911060810089111, acc: 0.57, recall: 0.5700000000000001, precision: 0.5791497060153776, f_beta: 0.557203171661003
train: step: 3055, loss: 0.6870182156562805, acc: 0.74, recall: 0.74, precision: 0.7403846153846154, f_beta: 0.7398959583833533
train: step: 3056, loss: 0.6901072859764099, acc: 0.68, recall: 0.6799999999999999, precision: 0.6826298701298701, f_beta: 0.6788438378161381
train: step: 3057, loss: 0.6926921606063843, acc: 0.49, recall: 0.49, precision: 0.4899959983993597, f_beta: 0.48994899489948995
train: step: 3058, loss: 0.6891770362854004, acc: 0.59, recall: 0.59, precision: 0.5989010989010989, f_beta: 0.5805626598465473
train: step: 3059, loss: 0.6876224279403687, acc: 0.66, recall: 0.6599999999999999, precision: 0.6623376623376623, f_beta: 0.6587715776796468
train: step: 3060, loss: 0.6901907920837402, acc: 0.67, recall: 0.6699999999999999, precision: 0.6733986128110976, f_beta: 0.6683750376846548
train: step: 3061, loss: 0.6878239512443542, acc: 0.63, recall: 0.63, precision: 0.6343530384456387, f_beta: 0.6269785260610949
train: step: 3062, loss: 0.6898229718208313, acc: 0.62, recall: 0.62, precision: 0.6378676470588236, f_beta: 0.6072757337742869
train: step: 3063, loss: 0.6930563449859619, acc: 0.53, recall: 0.53, precision: 0.5423489553924337, f_beta: 0.49304282170208175
train: step: 3064, loss: 0.6873592138290405, acc: 0.67, recall: 0.6699999999999999, precision: 0.6717171717171717, f_beta: 0.6691729323308271
train: step: 3065, loss: 0.6923263072967529, acc: 0.52, recall: 0.52, precision: 0.5217013888888888, f_beta: 0.5104039167686658
train: step: 3066, loss: 0.6895291805267334, acc: 0.68, recall: 0.6799999999999999, precision: 0.6802884615384616, f_beta: 0.6798719487795117
train: step: 3067, loss: 0.6914914846420288, acc: 0.58, recall: 0.5800000000000001, precision: 0.5821018062397373, f_beta: 0.5772946859903382
train: step: 3068, loss: 0.6901792883872986, acc: 0.66, recall: 0.6599999999999999, precision: 0.6610305958132046, f_beta: 0.6594551282051282
train: step: 3069, loss: 0.686880350112915, acc: 0.73, recall: 0.73, precision: 0.7300920368147259, f_beta: 0.72997299729973
train: step: 3070, loss: 0.6900124549865723, acc: 0.63, recall: 0.63, precision: 0.6343530384456387, f_beta: 0.6269785260610949
train: step: 3071, loss: 0.6875256299972534, acc: 0.73, recall: 0.73, precision: 0.7345981232150143, f_beta: 0.7286704853783539
train: step: 3072, loss: 0.686695396900177, acc: 0.72, recall: 0.72, precision: 0.7214170692431562, f_beta: 0.7195512820512822
train: step: 3073, loss: 0.6914556622505188, acc: 0.59, recall: 0.5900000000000001, precision: 0.5909090909090908, f_beta: 0.5889724310776943
train: step: 3074, loss: 0.6916021704673767, acc: 0.6, recall: 0.6000000000000001, precision: 0.6006441223832528, f_beta: 0.5993589743589743
train: step: 3075, loss: 0.6898070573806763, acc: 0.64, recall: 0.64, precision: 0.6409017713365539, f_beta: 0.639423076923077
train: step: 3076, loss: 0.6890910267829895, acc: 0.65, recall: 0.65, precision: 0.6550227366680447, f_beta: 0.6471418489767113
train: step: 3077, loss: 0.6851782202720642, acc: 0.76, recall: 0.76, precision: 0.7668308702791462, f_beta: 0.7584541062801933
train: step: 3078, loss: 0.6829646229743958, acc: 0.79, recall: 0.79, precision: 0.7901160464185675, f_beta: 0.78997899789979
train: step: 3079, loss: 0.6921803951263428, acc: 0.54, recall: 0.54, precision: 0.5402576489533011, f_beta: 0.5392628205128205
train: step: 3080, loss: 0.6898711919784546, acc: 0.67, recall: 0.6699999999999999, precision: 0.6706142111601766, f_beta: 0.6697027324592133
train: step: 3081, loss: 0.6872539520263672, acc: 0.68, recall: 0.68, precision: 0.6910016977928692, f_beta: 0.6753246753246753
train: step: 3082, loss: 0.6882836222648621, acc: 0.67, recall: 0.6699999999999999, precision: 0.6706142111601766, f_beta: 0.6697027324592133
train: step: 3083, loss: 0.6894711852073669, acc: 0.68, recall: 0.6799999999999999, precision: 0.681159420289855, f_beta: 0.6794871794871795
train: step: 3084, loss: 0.6887737512588501, acc: 0.56, recall: 0.56, precision: 0.59375, f_beta: 0.5164835164835164
train: step: 3085, loss: 0.6886218190193176, acc: 0.66, recall: 0.6599999999999999, precision: 0.67825311942959, f_beta: 0.6510673234811166
train: step: 3086, loss: 0.6916092038154602, acc: 0.62, recall: 0.6200000000000001, precision: 0.6217532467532467, f_beta: 0.618627057406664
train: step: 3087, loss: 0.6885005831718445, acc: 0.66, recall: 0.66, precision: 0.66, f_beta: 0.66
train: step: 3088, loss: 0.691506564617157, acc: 0.58, recall: 0.5800000000000001, precision: 0.5801282051282051, f_beta: 0.5798319327731092
train: step: 3089, loss: 0.6918313503265381, acc: 0.55, recall: 0.55, precision: 0.5500200080032013, f_beta: 0.54995499549955
train: step: 3090, loss: 0.686241090297699, acc: 0.73, recall: 0.73, precision: 0.7377015295576685, f_beta: 0.7277951406391774
train: step: 3091, loss: 0.6844571828842163, acc: 0.77, recall: 0.77, precision: 0.7709755118426336, f_beta: 0.769792813532179
train: step: 3092, loss: 0.6901624202728271, acc: 0.62, recall: 0.62, precision: 0.6378676470588236, f_beta: 0.6072757337742869
train: step: 3093, loss: 0.6879993677139282, acc: 0.69, recall: 0.69, precision: 0.690686471296668, f_beta: 0.6897207486738064
train: step: 3094, loss: 0.6900861859321594, acc: 0.64, recall: 0.64, precision: 0.6409017713365539, f_beta: 0.639423076923077
train: step: 3095, loss: 0.689332902431488, acc: 0.62, recall: 0.62, precision: 0.625, f_beta: 0.6161616161616162
train: step: 3096, loss: 0.6856773495674133, acc: 0.71, recall: 0.71, precision: 0.7141982864137086, f_beta: 0.7085720028137876
train: step: 3097, loss: 0.6899960041046143, acc: 0.67, recall: 0.6699999999999999, precision: 0.6868131868131868, f_beta: 0.6624040920716112
train: step: 3098, loss: 0.6846983432769775, acc: 0.76, recall: 0.76, precision: 0.7758913412563667, f_beta: 0.7564935064935063
train: step: 3099, loss: 0.6875855922698975, acc: 0.7, recall: 0.7, precision: 0.7083333333333333, f_beta: 0.6969696969696968
train: step: 3100, loss: 0.690747082233429, acc: 0.66, recall: 0.6599999999999999, precision: 0.6623376623376623, f_beta: 0.6587715776796468
train: step: 3101, loss: 0.6863468885421753, acc: 0.78, recall: 0.78, precision: 0.7840909090909092, f_beta: 0.779205138498595
train: step: 3102, loss: 0.6875448822975159, acc: 0.76, recall: 0.76, precision: 0.7896613190730838, f_beta: 0.7536945812807881
train: step: 3103, loss: 0.6913642883300781, acc: 0.61, recall: 0.61, precision: 0.6335599805730938, f_beta: 0.5920075321686369
train: step: 3104, loss: 0.6905824542045593, acc: 0.67, recall: 0.6699999999999999, precision: 0.6717171717171717, f_beta: 0.6691729323308271
train: step: 3105, loss: 0.6908366680145264, acc: 0.66, recall: 0.66, precision: 0.6602564102564102, f_beta: 0.6598639455782312
train: step: 3106, loss: 0.6918021440505981, acc: 0.53, recall: 0.53, precision: 0.530108390204737, f_beta: 0.5295766189570613
train: step: 3107, loss: 0.687308132648468, acc: 0.77, recall: 0.77, precision: 0.7709755118426336, f_beta: 0.769792813532179
train: step: 3108, loss: 0.6866214275360107, acc: 0.75, recall: 0.75, precision: 0.750903251706142, f_beta: 0.7497747973175859
train: step: 3109, loss: 0.6882511973381042, acc: 0.66, recall: 0.6599999999999999, precision: 0.6642036124794746, f_beta: 0.6578099838969405
train: step: 3110, loss: 0.6892435550689697, acc: 0.71, recall: 0.71, precision: 0.7170318313352625, f_beta: 0.7076318177235609
train: step: 3111, loss: 0.6854548454284668, acc: 0.81, recall: 0.81, precision: 0.810124049619848, f_beta: 0.8099809980998101
train: step: 3112, loss: 0.6898950338363647, acc: 0.69, recall: 0.69, precision: 0.6900760304121649, f_beta: 0.68996899689969
train: step: 3113, loss: 0.6844733357429504, acc: 0.71, recall: 0.71, precision: 0.7100840336134454, f_beta: 0.7099709970997101
train: step: 3114, loss: 0.6839390397071838, acc: 0.78, recall: 0.78, precision: 0.7916666666666666, f_beta: 0.7777777777777779
train: step: 3115, loss: 0.6886627078056335, acc: 0.64, recall: 0.64, precision: 0.640224358974359, f_beta: 0.6398559423769508
train: step: 3116, loss: 0.6931506991386414, acc: 0.47, recall: 0.47000000000000003, precision: 0.4649368863955119, f_beta: 0.4501504305425874
train: step: 3117, loss: 0.6884008049964905, acc: 0.72, recall: 0.72, precision: 0.7257799671592775, f_beta: 0.7181964573268922
train: step: 3118, loss: 0.6898114681243896, acc: 0.56, recall: 0.56, precision: 0.5636672325976231, f_beta: 0.5535714285714286
train: step: 3119, loss: 0.6891375780105591, acc: 0.61, recall: 0.61, precision: 0.6103974307507025, f_beta: 0.6096486838154338
train: step: 3120, loss: 0.6904975771903992, acc: 0.54, recall: 0.54, precision: 0.5400641025641025, f_beta: 0.5398159263705482
train: step: 3121, loss: 0.6904184818267822, acc: 0.65, recall: 0.65, precision: 0.6505419510236852, f_beta: 0.6496847162446201
train: step: 3122, loss: 0.6866310834884644, acc: 0.71, recall: 0.71, precision: 0.7206809583858764, f_beta: 0.7064480210547626
train: step: 3123, loss: 0.6840373873710632, acc: 0.77, recall: 0.77, precision: 0.7709755118426336, f_beta: 0.769792813532179
train: step: 3124, loss: 0.6917911767959595, acc: 0.57, recall: 0.5700000000000001, precision: 0.5791497060153776, f_beta: 0.557203171661003
train: step: 3125, loss: 0.6906788349151611, acc: 0.57, recall: 0.57, precision: 0.5769230769230769, f_beta: 0.5601023017902813
train: step: 3126, loss: 0.6885360479354858, acc: 0.68, recall: 0.6799999999999999, precision: 0.7068014705882353, f_beta: 0.6692848284415047
train: step: 3127, loss: 0.6886768937110901, acc: 0.64, recall: 0.64, precision: 0.6608455882352942, f_beta: 0.6279454319966928
train: step: 3128, loss: 0.6884616613388062, acc: 0.68, recall: 0.6799999999999999, precision: 0.6875, f_beta: 0.6767676767676767
train: step: 3129, loss: 0.6864048838615417, acc: 0.74, recall: 0.74, precision: 0.7415458937198067, f_beta: 0.7395833333333335
train: step: 3130, loss: 0.6892492771148682, acc: 0.68, recall: 0.6799999999999999, precision: 0.6802884615384616, f_beta: 0.6798719487795117
train: step: 3131, loss: 0.6860254406929016, acc: 0.7, recall: 0.7, precision: 0.7003205128205128, f_beta: 0.6998799519807923
train: step: 3132, loss: 0.6906415820121765, acc: 0.6, recall: 0.6000000000000001, precision: 0.6006441223832528, f_beta: 0.5993589743589743
train: step: 3133, loss: 0.6831076741218567, acc: 0.76, recall: 0.76, precision: 0.7896613190730838, f_beta: 0.7536945812807881
train: step: 3134, loss: 0.6892396807670593, acc: 0.68, recall: 0.6799999999999999, precision: 0.681159420289855, f_beta: 0.6794871794871795
train: step: 3135, loss: 0.6898512244224548, acc: 0.58, recall: 0.5800000000000001, precision: 0.5833333333333333, f_beta: 0.5757575757575757
train: step: 3136, loss: 0.6933820843696594, acc: 0.51, recall: 0.51, precision: 0.51010101010101, f_beta: 0.5087719298245614
train: step: 3137, loss: 0.6881376504898071, acc: 0.69, recall: 0.69, precision: 0.693798449612403, f_beta: 0.6884735202492211
train: step: 3138, loss: 0.6846514344215393, acc: 0.76, recall: 0.76, precision: 0.7637987012987013, f_beta: 0.7591328783621035
train: step: 3139, loss: 0.6901122331619263, acc: 0.62, recall: 0.6200000000000001, precision: 0.6217532467532467, f_beta: 0.618627057406664
train: step: 3140, loss: 0.6896580457687378, acc: 0.63, recall: 0.63, precision: 0.6366120218579234, f_beta: 0.6254681647940075
train: step: 3141, loss: 0.6895469427108765, acc: 0.66, recall: 0.66, precision: 0.6666666666666666, f_beta: 0.6565656565656566
train: step: 3142, loss: 0.6885427236557007, acc: 0.62, recall: 0.62, precision: 0.6207729468599035, f_beta: 0.6193910256410255
train: step: 3143, loss: 0.6908317804336548, acc: 0.59, recall: 0.5900000000000001, precision: 0.5909090909090908, f_beta: 0.5889724310776943
train: step: 3144, loss: 0.6906459927558899, acc: 0.56, recall: 0.56, precision: 0.5668449197860963, f_beta: 0.548440065681445
train: step: 3145, loss: 0.6924806833267212, acc: 0.54, recall: 0.54, precision: 0.5400641025641025, f_beta: 0.5398159263705482
train: step: 3146, loss: 0.685482382774353, acc: 0.75, recall: 0.75, precision: 0.7501000400160064, f_beta: 0.7499749974997499
train: step: 3147, loss: 0.6863359212875366, acc: 0.71, recall: 0.71, precision: 0.7100840336134454, f_beta: 0.7099709970997101
train: step: 3148, loss: 0.6885337233543396, acc: 0.73, recall: 0.73, precision: 0.7308309915696507, f_beta: 0.7297567811029926
train: step: 3149, loss: 0.6856338381767273, acc: 0.77, recall: 0.77, precision: 0.7753977968176254, f_beta: 0.7688674505074867
train: step: 3150, loss: 0.6872923970222473, acc: 0.74, recall: 0.74, precision: 0.7415458937198067, f_beta: 0.7395833333333335
train: step: 3151, loss: 0.6916667222976685, acc: 0.58, recall: 0.5800000000000001, precision: 0.5952380952380952, f_beta: 0.5625
train: step: 3152, loss: 0.6865040063858032, acc: 0.73, recall: 0.73, precision: 0.7308309915696507, f_beta: 0.7297567811029926
train: step: 3153, loss: 0.6870579719543457, acc: 0.68, recall: 0.6799999999999999, precision: 0.6802884615384616, f_beta: 0.6798719487795117
train: step: 3154, loss: 0.6897544264793396, acc: 0.62, recall: 0.62, precision: 0.6231527093596059, f_beta: 0.6175523349436394
train: step: 3155, loss: 0.6830366253852844, acc: 0.76, recall: 0.76, precision: 0.7604166666666667, f_beta: 0.7599039615846339
train: step: 3156, loss: 0.6918131113052368, acc: 0.5, recall: 0.5, precision: 0.5, f_beta: 0.4981934965877158
train: step: 3157, loss: 0.6897763609886169, acc: 0.63, recall: 0.63, precision: 0.6300520208083233, f_beta: 0.6299629962996299
train: step: 3158, loss: 0.6907280683517456, acc: 0.63, recall: 0.63, precision: 0.6578436134045653, f_beta: 0.6129302228266555
train: step: 3159, loss: 0.6898918747901917, acc: 0.66, recall: 0.6599999999999999, precision: 0.6642036124794746, f_beta: 0.6578099838969405
train: step: 3160, loss: 0.6919054388999939, acc: 0.55, recall: 0.55, precision: 0.5501806503412284, f_beta: 0.5495946351716544
train: step: 3161, loss: 0.6854296326637268, acc: 0.75, recall: 0.75, precision: 0.7583712277800745, f_beta: 0.7479584635547939
train: step: 3162, loss: 0.6862059235572815, acc: 0.72, recall: 0.72, precision: 0.7203525641025641, f_beta: 0.7198879551820727
train: step: 3163, loss: 0.6880682110786438, acc: 0.73, recall: 0.73, precision: 0.7416981925178646, f_beta: 0.7266929851199515
train: step: 3164, loss: 0.6893577575683594, acc: 0.64, recall: 0.64, precision: 0.6409017713365539, f_beta: 0.639423076923077
train: step: 3165, loss: 0.6894926428794861, acc: 0.62, recall: 0.62, precision: 0.6207729468599035, f_beta: 0.6193910256410255
train: step: 3166, loss: 0.6904237270355225, acc: 0.61, recall: 0.61, precision: 0.6155947877259353, f_beta: 0.6052232007288187
train: step: 3167, loss: 0.689385175704956, acc: 0.68, recall: 0.6799999999999999, precision: 0.681159420289855, f_beta: 0.6794871794871795
train: step: 3168, loss: 0.6801632046699524, acc: 0.84, recall: 0.8400000000000001, precision: 0.8449675324675325, f_beta: 0.839421918908069
train: step: 3169, loss: 0.6832587718963623, acc: 0.73, recall: 0.73, precision: 0.7345981232150143, f_beta: 0.7286704853783539
train: step: 3170, loss: 0.6840342879295349, acc: 0.83, recall: 0.83, precision: 0.8301320528211285, f_beta: 0.8299829982998299
train: step: 3171, loss: 0.6825741529464722, acc: 0.73, recall: 0.73, precision: 0.7300920368147259, f_beta: 0.72997299729973
train: step: 3172, loss: 0.6846197247505188, acc: 0.76, recall: 0.76, precision: 0.7637987012987013, f_beta: 0.7591328783621035
train: step: 3173, loss: 0.689424991607666, acc: 0.72, recall: 0.72, precision: 0.733446519524618, f_beta: 0.7159090909090909
train: step: 3174, loss: 0.6858371496200562, acc: 0.73, recall: 0.73, precision: 0.7466752466752467, f_beta: 0.7253585596582239
train: step: 3175, loss: 0.6876958608627319, acc: 0.72, recall: 0.72, precision: 0.7203525641025641, f_beta: 0.7198879551820727
train: step: 3176, loss: 0.6865289807319641, acc: 0.78, recall: 0.78, precision: 0.78, f_beta: 0.78
train: step: 3177, loss: 0.6838210821151733, acc: 0.75, recall: 0.75, precision: 0.750903251706142, f_beta: 0.7497747973175859
train: step: 3178, loss: 0.6885022521018982, acc: 0.67, recall: 0.6699999999999999, precision: 0.6717171717171717, f_beta: 0.6691729323308271
train: step: 3179, loss: 0.6890060901641846, acc: 0.66, recall: 0.6599999999999999, precision: 0.6623376623376623, f_beta: 0.6587715776796468
train: step: 3180, loss: 0.6862074136734009, acc: 0.8, recall: 0.8, precision: 0.8019323671497585, f_beta: 0.7996794871794872
train: step: 3181, loss: 0.6847605109214783, acc: 0.73, recall: 0.73, precision: 0.7466752466752467, f_beta: 0.7253585596582239
train: step: 3182, loss: 0.6863046884536743, acc: 0.71, recall: 0.71, precision: 0.7252252252252251, f_beta: 0.7050147492625368
train: step: 3183, loss: 0.6872724294662476, acc: 0.63, recall: 0.63, precision: 0.6366120218579234, f_beta: 0.6254681647940075
train: step: 3184, loss: 0.6865645051002502, acc: 0.7, recall: 0.7, precision: 0.702922077922078, f_beta: 0.6989160979526294
train: step: 3185, loss: 0.6836792826652527, acc: 0.8, recall: 0.8, precision: 0.8004807692307692, f_beta: 0.7999199679871949
train: step: 3186, loss: 0.6892939209938049, acc: 0.59, recall: 0.59, precision: 0.5900360144057624, f_beta: 0.5899589958995899
train: step: 3187, loss: 0.6773494482040405, acc: 0.85, recall: 0.8500000000000001, precision: 0.8569971440228478, f_beta: 0.8492613807657521
train: step: 3188, loss: 0.6851041913032532, acc: 0.72, recall: 0.72, precision: 0.733446519524618, f_beta: 0.7159090909090909
train: step: 3189, loss: 0.6842610239982605, acc: 0.74, recall: 0.74, precision: 0.7403846153846154, f_beta: 0.7398959583833533
train: step: 3190, loss: 0.6931036114692688, acc: 0.48, recall: 0.48, precision: 0.4787775891341256, f_beta: 0.4724025974025974
train: step: 3191, loss: 0.6832089424133301, acc: 0.75, recall: 0.75, precision: 0.7747252747252746, f_beta: 0.7442455242966751
train: step: 3192, loss: 0.6790706515312195, acc: 0.78, recall: 0.78, precision: 0.7840909090909092, f_beta: 0.779205138498595
train: step: 3193, loss: 0.6826976537704468, acc: 0.72, recall: 0.72, precision: 0.7387152777777778, f_beta: 0.7144022847817217
train: step: 3194, loss: 0.6847375631332397, acc: 0.74, recall: 0.74, precision: 0.7435064935064934, f_beta: 0.7390606182256123
train: step: 3195, loss: 0.6828932166099548, acc: 0.7, recall: 0.7, precision: 0.7083333333333333, f_beta: 0.6969696969696968
train: step: 3196, loss: 0.6843850612640381, acc: 0.73, recall: 0.73, precision: 0.7308309915696507, f_beta: 0.7297567811029926
train: step: 3197, loss: 0.6847411394119263, acc: 0.71, recall: 0.71, precision: 0.7206809583858764, f_beta: 0.7064480210547626
train: step: 3198, loss: 0.6803829669952393, acc: 0.81, recall: 0.81, precision: 0.810124049619848, f_beta: 0.8099809980998101
train: step: 3199, loss: 0.6828820705413818, acc: 0.77, recall: 0.77, precision: 0.7753977968176254, f_beta: 0.7688674505074867
train: step: 3200, loss: 0.6856961250305176, acc: 0.77, recall: 0.77, precision: 0.7837326607818411, f_beta: 0.767182913250329
train: step: 3201, loss: 0.6792669892311096, acc: 0.81, recall: 0.81, precision: 0.8324753324753325, f_beta: 0.8067338012409724
train: step: 3202, loss: 0.6835215091705322, acc: 0.81, recall: 0.81, precision: 0.8111200321156162, f_beta: 0.8098288459613652
train: step: 3203, loss: 0.6848292350769043, acc: 0.75, recall: 0.75, precision: 0.7583712277800745, f_beta: 0.7479584635547939
train: step: 3204, loss: 0.6882128715515137, acc: 0.64, recall: 0.64, precision: 0.64, f_beta: 0.64
train: step: 3205, loss: 0.6752608418464661, acc: 0.77, recall: 0.77, precision: 0.7790409260024803, f_beta: 0.7681217864704104
train: step: 3206, loss: 0.6846861839294434, acc: 0.71, recall: 0.71, precision: 0.7107587314331594, f_beta: 0.7097387648883996
train: step: 3207, loss: 0.6843150854110718, acc: 0.78, recall: 0.78, precision: 0.780448717948718, f_beta: 0.7799119647859143
train: step: 3208, loss: 0.6848766803741455, acc: 0.71, recall: 0.71, precision: 0.737449118046133, f_beta: 0.7013695808876532
train: step: 3209, loss: 0.6888814568519592, acc: 0.65, recall: 0.65, precision: 0.6550227366680447, f_beta: 0.6471418489767113
train: step: 3210, loss: 0.6825477480888367, acc: 0.73, recall: 0.73, precision: 0.7600633197648123, f_beta: 0.721964782205746
train: step: 3211, loss: 0.6836200952529907, acc: 0.8, recall: 0.8, precision: 0.8004807692307692, f_beta: 0.7999199679871949
train: step: 3212, loss: 0.688508927822113, acc: 0.65, recall: 0.65, precision: 0.6576292559899117, f_beta: 0.6457131288591963
train: step: 3213, loss: 0.6870236992835999, acc: 0.76, recall: 0.76, precision: 0.7616747181964574, f_beta: 0.7596153846153846
train: step: 3214, loss: 0.6884642839431763, acc: 0.57, recall: 0.57, precision: 0.5933333333333333, f_beta: 0.5413333333333333
train: step: 3215, loss: 0.68669593334198, acc: 0.74, recall: 0.74, precision: 0.7403846153846154, f_beta: 0.7398959583833533
train: step: 3216, loss: 0.6781896948814392, acc: 0.87, recall: 0.87, precision: 0.8773969808241534, f_beta: 0.8693598633303186
train: step: 3217, loss: 0.6878440976142883, acc: 0.67, recall: 0.67, precision: 0.6700680272108843, f_beta: 0.66996699669967
train: step: 3218, loss: 0.6880906820297241, acc: 0.68, recall: 0.6799999999999999, precision: 0.6826298701298701, f_beta: 0.6788438378161381
train: step: 3219, loss: 0.6850402355194092, acc: 0.72, recall: 0.72, precision: 0.7291666666666667, f_beta: 0.7171717171717171
train: step: 3220, loss: 0.688617467880249, acc: 0.7, recall: 0.7, precision: 0.7083333333333333, f_beta: 0.6969696969696968
train: step: 3221, loss: 0.6893277168273926, acc: 0.65, recall: 0.6499999999999999, precision: 0.652998776009792, f_beta: 0.6482765551200884
train: step: 3222, loss: 0.6858150362968445, acc: 0.75, recall: 0.75, precision: 0.750903251706142, f_beta: 0.7497747973175859
train: step: 3223, loss: 0.6828482151031494, acc: 0.72, recall: 0.72, precision: 0.7291666666666667, f_beta: 0.7171717171717171
train: step: 3224, loss: 0.6828299164772034, acc: 0.79, recall: 0.79, precision: 0.7957976336189311, f_beta: 0.7889659330720531
train: step: 3225, loss: 0.6828923225402832, acc: 0.8, recall: 0.8, precision: 0.8078817733990147, f_beta: 0.7987117552334944
train: step: 3226, loss: 0.6853758096694946, acc: 0.71, recall: 0.71, precision: 0.7252252252252251, f_beta: 0.7050147492625368
train: step: 3227, loss: 0.6837384700775146, acc: 0.76, recall: 0.76, precision: 0.76, f_beta: 0.76
train: step: 3228, loss: 0.6829913258552551, acc: 0.77, recall: 0.77, precision: 0.7790409260024803, f_beta: 0.7681217864704104
train: step: 3229, loss: 0.6825940012931824, acc: 0.76, recall: 0.76, precision: 0.7668308702791462, f_beta: 0.7584541062801933
train: step: 3230, loss: 0.683102011680603, acc: 0.7, recall: 0.7, precision: 0.702922077922078, f_beta: 0.6989160979526294
train: step: 3231, loss: 0.6833204627037048, acc: 0.74, recall: 0.74, precision: 0.7403846153846154, f_beta: 0.7398959583833533
train: step: 3232, loss: 0.6825428009033203, acc: 0.71, recall: 0.71, precision: 0.7121212121212122, f_beta: 0.7092731829573935
train: step: 3233, loss: 0.6842548251152039, acc: 0.83, recall: 0.8300000000000001, precision: 0.8410500206696983, f_beta: 0.8286117552172598
train: step: 3234, loss: 0.6691725850105286, acc: 0.86, recall: 0.86, precision: 0.8623188405797102, f_beta: 0.859775641025641
train: step: 3235, loss: 0.6843686103820801, acc: 0.75, recall: 0.75, precision: 0.7525252525252526, f_beta: 0.7493734335839599
train: step: 3236, loss: 0.6782726049423218, acc: 0.8, recall: 0.8, precision: 0.8078817733990147, f_beta: 0.7987117552334944
train: step: 3237, loss: 0.6861633062362671, acc: 0.7, recall: 0.7, precision: 0.7170138888888888, f_beta: 0.6940024479804162
train: step: 3238, loss: 0.6841257214546204, acc: 0.74, recall: 0.74, precision: 0.7415458937198067, f_beta: 0.7395833333333335
train: step: 3239, loss: 0.6799257397651672, acc: 0.82, recall: 0.8200000000000001, precision: 0.8246753246753247, f_beta: 0.8193496587715776
train: step: 3240, loss: 0.6656380295753479, acc: 0.89, recall: 0.8899999999999999, precision: 0.8939393939393939, f_beta: 0.8897243107769424
train: step: 3241, loss: 0.6872103810310364, acc: 0.68, recall: 0.6799999999999999, precision: 0.6953125, f_beta: 0.6736026111791106
train: step: 3242, loss: 0.6795705556869507, acc: 0.81, recall: 0.81, precision: 0.8131313131313131, f_beta: 0.8095238095238095
train: step: 3243, loss: 0.6817861795425415, acc: 0.74, recall: 0.74, precision: 0.7604166666666667, f_beta: 0.7348021215830274
train: step: 3244, loss: 0.6926323175430298, acc: 0.55, recall: 0.55, precision: 0.554945054945055, f_beta: 0.5396419437340153
train: step: 3245, loss: 0.6848390102386475, acc: 0.74, recall: 0.74, precision: 0.767379679144385, f_beta: 0.7331691297208538
train: step: 3246, loss: 0.6815903186798096, acc: 0.81, recall: 0.81, precision: 0.8161974704202366, f_beta: 0.8090644156366193
train: step: 3247, loss: 0.685005247592926, acc: 0.69, recall: 0.69, precision: 0.6919191919191919, f_beta: 0.6892230576441103
train: step: 3248, loss: 0.6775728464126587, acc: 0.74, recall: 0.74, precision: 0.7415458937198067, f_beta: 0.7395833333333335
train: step: 3249, loss: 0.6851423382759094, acc: 0.73, recall: 0.73, precision: 0.7300920368147259, f_beta: 0.72997299729973
train: step: 3250, loss: 0.6739609241485596, acc: 0.73, recall: 0.73, precision: 0.7323232323232323, f_beta: 0.7293233082706766
train: step: 3251, loss: 0.6608611941337585, acc: 0.89, recall: 0.89, precision: 0.9030591153369161, f_beta: 0.8891017239641092
train: step: 3252, loss: 0.6737428307533264, acc: 0.75, recall: 0.75, precision: 0.7549979600163199, f_beta: 0.7487689679429204
train: step: 3253, loss: 0.6746971011161804, acc: 0.72, recall: 0.72, precision: 0.72, f_beta: 0.72
train: step: 3254, loss: 0.6678029894828796, acc: 0.79, recall: 0.79, precision: 0.7929292929292929, f_beta: 0.7894736842105263
train: step: 3255, loss: 0.6594453454017639, acc: 0.81, recall: 0.81, precision: 0.8324753324753325, f_beta: 0.8067338012409724
train: step: 3256, loss: 0.6603267788887024, acc: 0.85, recall: 0.85, precision: 0.851264552388599, f_beta: 0.8498648783905516
train: step: 3257, loss: 0.6698469519615173, acc: 0.81, recall: 0.81, precision: 0.8131313131313131, f_beta: 0.8095238095238095
train: step: 3258, loss: 0.6770452260971069, acc: 0.75, recall: 0.75, precision: 0.750903251706142, f_beta: 0.7497747973175859
train: step: 3259, loss: 0.6751231551170349, acc: 0.81, recall: 0.81, precision: 0.8111200321156162, f_beta: 0.8098288459613652
train: step: 3260, loss: 0.6727685332298279, acc: 0.76, recall: 0.76, precision: 0.7637987012987013, f_beta: 0.7591328783621035
train: step: 3261, loss: 0.6725177764892578, acc: 0.76, recall: 0.76, precision: 0.76, f_beta: 0.76
train: step: 3262, loss: 0.6838750243186951, acc: 0.71, recall: 0.71, precision: 0.7121212121212122, f_beta: 0.7092731829573935
train: step: 3263, loss: 0.6865449547767639, acc: 0.58, recall: 0.5800000000000001, precision: 0.5992063492063493, f_beta: 0.5586380832282472
train: step: 3264, loss: 0.6733771562576294, acc: 0.88, recall: 0.88, precision: 0.8824476650563606, f_beta: 0.8798076923076923
train: step: 3265, loss: 0.6705880165100098, acc: 0.77, recall: 0.77, precision: 0.7967032967032968, f_beta: 0.7647058823529411
train: step: 3266, loss: 0.6747027039527893, acc: 0.75, recall: 0.75, precision: 0.8035454103933948, f_beta: 0.7384663667747673
train: step: 3267, loss: 0.6809322237968445, acc: 0.76, recall: 0.76, precision: 0.7896613190730838, f_beta: 0.7536945812807881
train: step: 3268, loss: 0.6863260865211487, acc: 0.75, recall: 0.75, precision: 0.7549979600163199, f_beta: 0.7487689679429204
train: step: 3269, loss: 0.6779951453208923, acc: 0.76, recall: 0.76, precision: 0.7604166666666667, f_beta: 0.7599039615846339
train: step: 3270, loss: 0.6736831665039062, acc: 0.74, recall: 0.74, precision: 0.7546689303904923, f_beta: 0.7362012987012987
train: step: 3271, loss: 0.6852962970733643, acc: 0.76, recall: 0.76, precision: 0.7668308702791462, f_beta: 0.7584541062801933
train: step: 3272, loss: 0.6763169169425964, acc: 0.84, recall: 0.8400000000000001, precision: 0.8421900161030595, f_beta: 0.8397435897435896
train: step: 3273, loss: 0.678870439529419, acc: 0.81, recall: 0.81, precision: 0.8131313131313131, f_beta: 0.8095238095238095
train: step: 3274, loss: 0.6815499067306519, acc: 0.75, recall: 0.75, precision: 0.7583712277800745, f_beta: 0.7479584635547939
train: step: 3275, loss: 0.6880649328231812, acc: 0.64, recall: 0.64, precision: 0.64, f_beta: 0.64
train: step: 3276, loss: 0.6857776641845703, acc: 0.71, recall: 0.71, precision: 0.7121212121212122, f_beta: 0.7092731829573935
train: step: 3277, loss: 0.6763534545898438, acc: 0.84, recall: 0.8400000000000001, precision: 0.8421900161030595, f_beta: 0.8397435897435896
train: step: 3278, loss: 0.6764904856681824, acc: 0.78, recall: 0.78, precision: 0.780448717948718, f_beta: 0.7799119647859143
train: step: 3279, loss: 0.6887990832328796, acc: 0.69, recall: 0.69, precision: 0.693798449612403, f_beta: 0.6884735202492211
train: step: 3280, loss: 0.6905435919761658, acc: 0.64, recall: 0.64, precision: 0.6436781609195402, f_beta: 0.6376811594202898
train: step: 3281, loss: 0.6884827613830566, acc: 0.71, recall: 0.71, precision: 0.7206809583858764, f_beta: 0.7064480210547626
train: step: 3282, loss: 0.6870542168617249, acc: 0.74, recall: 0.74, precision: 0.7403846153846154, f_beta: 0.7398959583833533
train: step: 3283, loss: 0.6891393065452576, acc: 0.67, recall: 0.6699999999999999, precision: 0.6706142111601766, f_beta: 0.6697027324592133
train: step: 3284, loss: 0.6896961331367493, acc: 0.62, recall: 0.6200000000000001, precision: 0.6217532467532467, f_beta: 0.618627057406664
train: step: 3285, loss: 0.6892183423042297, acc: 0.64, recall: 0.64, precision: 0.6559714795008913, f_beta: 0.6305418719211822
train: step: 3286, loss: 0.6841050982475281, acc: 0.75, recall: 0.75, precision: 0.7627154266498528, f_beta: 0.7469379491851402
train: step: 3287, loss: 0.6807894706726074, acc: 0.84, recall: 0.8400000000000001, precision: 0.8489326765188834, f_beta: 0.8389694041867956
train: step: 3288, loss: 0.6861834526062012, acc: 0.72, recall: 0.72, precision: 0.7232142857142857, f_beta: 0.7189883580891208
train: step: 3289, loss: 0.6874431371688843, acc: 0.73, recall: 0.73, precision: 0.7416981925178646, f_beta: 0.7266929851199515
train: step: 3290, loss: 0.6911425590515137, acc: 0.56, recall: 0.56, precision: 0.5603864734299517, f_beta: 0.5592948717948718
train: step: 3291, loss: 0.6937735676765442, acc: 0.51, recall: 0.51, precision: 0.5100361300682457, f_beta: 0.5095586027424682
train: step: 3292, loss: 0.6893735527992249, acc: 0.69, recall: 0.69, precision: 0.693798449612403, f_beta: 0.6884735202492211
train: step: 3293, loss: 0.6797561645507812, acc: 0.78, recall: 0.78, precision: 0.7818035426731079, f_beta: 0.7796474358974359
train: step: 3294, loss: 0.6806514859199524, acc: 0.79, recall: 0.79, precision: 0.7901160464185675, f_beta: 0.78997899789979
train: step: 3295, loss: 0.6899440288543701, acc: 0.65, recall: 0.6499999999999999, precision: 0.6515151515151515, f_beta: 0.6491228070175439
train: step: 3296, loss: 0.6890509128570557, acc: 0.62, recall: 0.6200000000000001, precision: 0.6217532467532467, f_beta: 0.618627057406664
train: step: 3297, loss: 0.6859748959541321, acc: 0.67, recall: 0.67, precision: 0.6786464901219, f_beta: 0.6659580929243851
train: step: 3298, loss: 0.6829487085342407, acc: 0.78, recall: 0.78, precision: 0.780448717948718, f_beta: 0.7799119647859143
train: step: 3299, loss: 0.6817588210105896, acc: 0.79, recall: 0.79, precision: 0.7929292929292929, f_beta: 0.7894736842105263
train: step: 3300, loss: 0.6858869791030884, acc: 0.79, recall: 0.79, precision: 0.7929292929292929, f_beta: 0.7894736842105263
train: step: 3301, loss: 0.6868960857391357, acc: 0.69, recall: 0.69, precision: 0.690686471296668, f_beta: 0.6897207486738064
train: step: 3302, loss: 0.6888682842254639, acc: 0.68, recall: 0.6799999999999999, precision: 0.6847290640394088, f_beta: 0.677938808373591
train: step: 3303, loss: 0.6860193014144897, acc: 0.74, recall: 0.74, precision: 0.7415458937198067, f_beta: 0.7395833333333335
train: step: 3304, loss: 0.6843622326850891, acc: 0.77, recall: 0.77, precision: 0.7709755118426336, f_beta: 0.769792813532179
train: step: 3305, loss: 0.6855529546737671, acc: 0.73, recall: 0.73, precision: 0.7466752466752467, f_beta: 0.7253585596582239
train: step: 3306, loss: 0.6826002597808838, acc: 0.77, recall: 0.77, precision: 0.7701080432172869, f_beta: 0.7699769976997699
train: step: 3307, loss: 0.6838474869728088, acc: 0.74, recall: 0.74, precision: 0.7403846153846154, f_beta: 0.7398959583833533
train: step: 3308, loss: 0.6906991004943848, acc: 0.66, recall: 0.6599999999999999, precision: 0.6642036124794746, f_beta: 0.6578099838969405
train: step: 3309, loss: 0.6894004940986633, acc: 0.63, recall: 0.63, precision: 0.6325989392084863, f_beta: 0.6281780725555222
train: step: 3310, loss: 0.6843538880348206, acc: 0.73, recall: 0.73, precision: 0.7308309915696507, f_beta: 0.7297567811029926
train: step: 3311, loss: 0.6856760382652283, acc: 0.74, recall: 0.74, precision: 0.7463054187192117, f_beta: 0.7383252818035426
train: step: 3312, loss: 0.6845767498016357, acc: 0.77, recall: 0.77, precision: 0.7837326607818411, f_beta: 0.767182913250329
train: step: 3313, loss: 0.6843526363372803, acc: 0.68, recall: 0.68, precision: 0.68, f_beta: 0.68
train: step: 3314, loss: 0.6807432770729065, acc: 0.8, recall: 0.8, precision: 0.8, f_beta: 0.8000000000000002
train: step: 3315, loss: 0.6838749051094055, acc: 0.77, recall: 0.77, precision: 0.7727272727272727, f_beta: 0.7694235588972431
train: step: 3316, loss: 0.6856763958930969, acc: 0.74, recall: 0.74, precision: 0.7435064935064934, f_beta: 0.7390606182256123
train: step: 3317, loss: 0.679574191570282, acc: 0.79, recall: 0.79, precision: 0.7957976336189311, f_beta: 0.7889659330720531
train: step: 3318, loss: 0.6807231903076172, acc: 0.78, recall: 0.78, precision: 0.7873563218390804, f_beta: 0.7785829307568439
train: step: 3319, loss: 0.6860637068748474, acc: 0.69, recall: 0.69, precision: 0.7220663861617578, f_beta: 0.6783898744683057
train: step: 3320, loss: 0.6879249811172485, acc: 0.66, recall: 0.6599999999999999, precision: 0.6623376623376623, f_beta: 0.6587715776796468
train: step: 3321, loss: 0.6741620898246765, acc: 0.83, recall: 0.83, precision: 0.8301320528211285, f_beta: 0.8299829982998299
train: step: 3322, loss: 0.6852872371673584, acc: 0.78, recall: 0.78, precision: 0.780448717948718, f_beta: 0.7799119647859143
train: step: 3323, loss: 0.6841351985931396, acc: 0.75, recall: 0.75, precision: 0.7549979600163199, f_beta: 0.7487689679429204
train: step: 3324, loss: 0.6862033605575562, acc: 0.75, recall: 0.75, precision: 0.7525252525252526, f_beta: 0.7493734335839599
train: step: 3325, loss: 0.6810967326164246, acc: 0.75, recall: 0.75, precision: 0.750903251706142, f_beta: 0.7497747973175859
train: step: 3326, loss: 0.6753550171852112, acc: 0.82, recall: 0.8200000000000001, precision: 0.8205128205128205, f_beta: 0.8199279711884755
train: step: 3327, loss: 0.6843384504318237, acc: 0.72, recall: 0.72, precision: 0.7291666666666667, f_beta: 0.7171717171717171
train: step: 3328, loss: 0.678130030632019, acc: 0.8, recall: 0.8, precision: 0.8043831168831169, f_beta: 0.7992773986350863
train: step: 3329, loss: 0.6793131828308105, acc: 0.82, recall: 0.8200000000000001, precision: 0.822061191626409, f_beta: 0.8197115384615385
train: step: 3330, loss: 0.6697998642921448, acc: 0.88, recall: 0.88, precision: 0.8806089743589745, f_beta: 0.879951980792317
train: step: 3331, loss: 0.6848025321960449, acc: 0.72, recall: 0.72, precision: 0.7203525641025641, f_beta: 0.7198879551820727
train: step: 3332, loss: 0.680422842502594, acc: 0.77, recall: 0.77, precision: 0.7727272727272727, f_beta: 0.7694235588972431
train: step: 3333, loss: 0.6820165514945984, acc: 0.79, recall: 0.79, precision: 0.7957976336189311, f_beta: 0.7889659330720531
train: step: 3334, loss: 0.6673182845115662, acc: 0.84, recall: 0.8400000000000001, precision: 0.8449675324675325, f_beta: 0.839421918908069
train: step: 3335, loss: 0.6678227186203003, acc: 0.81, recall: 0.81, precision: 0.810124049619848, f_beta: 0.8099809980998101
train: step: 3336, loss: 0.6803662776947021, acc: 0.8, recall: 0.8, precision: 0.8004807692307692, f_beta: 0.7999199679871949
train: step: 3337, loss: 0.6795058250427246, acc: 0.8, recall: 0.8, precision: 0.8125, f_beta: 0.797979797979798
train: step: 3338, loss: 0.6842798590660095, acc: 0.73, recall: 0.73, precision: 0.7308309915696507, f_beta: 0.7297567811029926
train: step: 3339, loss: 0.6777027249336243, acc: 0.83, recall: 0.8300000000000001, precision: 0.8311922922521076, f_beta: 0.8298468621759585
train: step: 3340, loss: 0.678708016872406, acc: 0.77, recall: 0.77, precision: 0.7790409260024803, f_beta: 0.7681217864704104
train: step: 3341, loss: 0.6650792956352234, acc: 0.84, recall: 0.8400000000000001, precision: 0.8449675324675325, f_beta: 0.839421918908069
train: step: 3342, loss: 0.6805757880210876, acc: 0.76, recall: 0.76, precision: 0.7637987012987013, f_beta: 0.7591328783621035
train: step: 3343, loss: 0.6820326447486877, acc: 0.76, recall: 0.76, precision: 0.76, f_beta: 0.76
train: step: 3344, loss: 0.6848864555358887, acc: 0.68, recall: 0.6799999999999999, precision: 0.6847290640394088, f_beta: 0.677938808373591
train: step: 3345, loss: 0.6868664622306824, acc: 0.74, recall: 0.74, precision: 0.7435064935064934, f_beta: 0.7390606182256123
train: step: 3346, loss: 0.6615466475486755, acc: 0.85, recall: 0.8500000000000001, precision: 0.8535353535353536, f_beta: 0.849624060150376
train: step: 3347, loss: 0.6703765988349915, acc: 0.79, recall: 0.79, precision: 0.7929292929292929, f_beta: 0.7894736842105263
train: step: 3348, loss: 0.6783664226531982, acc: 0.83, recall: 0.83, precision: 0.8301320528211285, f_beta: 0.8299829982998299
train: step: 3349, loss: 0.6795572638511658, acc: 0.77, recall: 0.77, precision: 0.7727272727272727, f_beta: 0.7694235588972431
train: step: 3350, loss: 0.6740406155586243, acc: 0.76, recall: 0.76, precision: 0.7604166666666667, f_beta: 0.7599039615846339
train: step: 3351, loss: 0.6875662207603455, acc: 0.7, recall: 0.7, precision: 0.7003205128205128, f_beta: 0.6998799519807923
train: step: 3352, loss: 0.6736029982566833, acc: 0.78, recall: 0.78, precision: 0.7818035426731079, f_beta: 0.7796474358974359
train: step: 3353, loss: 0.6895684599876404, acc: 0.61, recall: 0.61, precision: 0.6155947877259353, f_beta: 0.6052232007288187
train: step: 3354, loss: 0.6766350269317627, acc: 0.77, recall: 0.77, precision: 0.7701080432172869, f_beta: 0.7699769976997699
train: step: 3355, loss: 0.6904352307319641, acc: 0.65, recall: 0.6499999999999999, precision: 0.669606512890095, f_beta: 0.6395839769333744
train: step: 3356, loss: 0.6805285811424255, acc: 0.84, recall: 0.84, precision: 0.84, f_beta: 0.8399999999999999
train: step: 3357, loss: 0.6635158658027649, acc: 0.88, recall: 0.88, precision: 0.8899835796387521, f_beta: 0.8792270531400966
train: step: 3358, loss: 0.6771982312202454, acc: 0.79, recall: 0.79, precision: 0.8047498949138294, f_beta: 0.7874278773155178
train: step: 3359, loss: 0.6697449684143066, acc: 0.82, recall: 0.82, precision: 0.82, f_beta: 0.82
train: step: 3360, loss: 0.6691783666610718, acc: 0.8, recall: 0.8, precision: 0.8004807692307692, f_beta: 0.7999199679871949
train: step: 3361, loss: 0.6766864061355591, acc: 0.79, recall: 0.79, precision: 0.8047498949138294, f_beta: 0.7874278773155178
train: step: 3362, loss: 0.6665037274360657, acc: 0.84, recall: 0.8400000000000001, precision: 0.8449675324675325, f_beta: 0.839421918908069
train: step: 3363, loss: 0.6735036969184875, acc: 0.85, recall: 0.85, precision: 0.8501400560224089, f_beta: 0.84998499849985
train: step: 3364, loss: 0.684665858745575, acc: 0.72, recall: 0.72, precision: 0.7214170692431562, f_beta: 0.7195512820512822
train: step: 3365, loss: 0.6878202557563782, acc: 0.66, recall: 0.6599999999999999, precision: 0.6623376623376623, f_beta: 0.6587715776796468
train: step: 3366, loss: 0.6590396165847778, acc: 0.82, recall: 0.8200000000000001, precision: 0.822061191626409, f_beta: 0.8197115384615385
train: step: 3367, loss: 0.6628037095069885, acc: 0.82, recall: 0.8200000000000001, precision: 0.8205128205128205, f_beta: 0.8199279711884755
train: step: 3368, loss: 0.6705954074859619, acc: 0.83, recall: 0.83, precision: 0.8301320528211285, f_beta: 0.8299829982998299
train: step: 3369, loss: 0.6628913879394531, acc: 0.85, recall: 0.85, precision: 0.8501400560224089, f_beta: 0.84998499849985
train: step: 3370, loss: 0.6748272776603699, acc: 0.87, recall: 0.87, precision: 0.8713368125250903, f_beta: 0.8698828946051447
train: step: 3371, loss: 0.6721716523170471, acc: 0.74, recall: 0.74, precision: 0.7403846153846154, f_beta: 0.7398959583833533
train: step: 3372, loss: 0.6779405474662781, acc: 0.82, recall: 0.8200000000000001, precision: 0.822061191626409, f_beta: 0.8197115384615385
train: step: 3373, loss: 0.6764175295829773, acc: 0.77, recall: 0.77, precision: 0.7753977968176254, f_beta: 0.7688674505074867
train: step: 3374, loss: 0.6749501824378967, acc: 0.81, recall: 0.81, precision: 0.8111200321156162, f_beta: 0.8098288459613652
train: step: 3375, loss: 0.6692441701889038, acc: 0.83, recall: 0.8300000000000001, precision: 0.8410500206696983, f_beta: 0.8286117552172598
train: step: 3376, loss: 0.6707049012184143, acc: 0.81, recall: 0.81, precision: 0.8111200321156162, f_beta: 0.8098288459613652
train: step: 3377, loss: 0.679587721824646, acc: 0.74, recall: 0.74, precision: 0.7463054187192117, f_beta: 0.7383252818035426
train: step: 3378, loss: 0.6825029850006104, acc: 0.71, recall: 0.71, precision: 0.7454417952314165, f_beta: 0.6991389148251893
train: step: 3379, loss: 0.6917912364006042, acc: 0.57, recall: 0.5700000000000001, precision: 0.5707070707070707, f_beta: 0.5689223057644109
train: step: 3380, loss: 0.6853047013282776, acc: 0.75, recall: 0.75, precision: 0.7681252681252682, f_beta: 0.7457023700539112
train: step: 3381, loss: 0.6633909344673157, acc: 0.79, recall: 0.79, precision: 0.811025311025311, f_beta: 0.7863899908452854
train: step: 3382, loss: 0.669070303440094, acc: 0.79, recall: 0.79, precision: 0.8047498949138294, f_beta: 0.7874278773155178
train: step: 3383, loss: 0.6555246710777283, acc: 0.84, recall: 0.8400000000000001, precision: 0.8421900161030595, f_beta: 0.8397435897435896
train: step: 3384, loss: 0.6786310076713562, acc: 0.72, recall: 0.72, precision: 0.72, f_beta: 0.72
train: step: 3385, loss: 0.6798596978187561, acc: 0.79, recall: 0.79, precision: 0.8047498949138294, f_beta: 0.7874278773155178
train: step: 3386, loss: 0.657784104347229, acc: 0.85, recall: 0.85, precision: 0.851264552388599, f_beta: 0.8498648783905516
train: step: 3387, loss: 0.663193941116333, acc: 0.85, recall: 0.85, precision: 0.8501400560224089, f_beta: 0.84998499849985
train: step: 3388, loss: 0.6686829328536987, acc: 0.87, recall: 0.87, precision: 0.8713368125250903, f_beta: 0.8698828946051447
train: step: 3389, loss: 0.6715569496154785, acc: 0.84, recall: 0.8400000000000001, precision: 0.8449675324675325, f_beta: 0.839421918908069
train: step: 3390, loss: 0.6727558970451355, acc: 0.81, recall: 0.81, precision: 0.810124049619848, f_beta: 0.8099809980998101
train: step: 3391, loss: 0.6506174206733704, acc: 0.87, recall: 0.87, precision: 0.8737373737373737, f_beta: 0.8696741854636592
train: step: 3392, loss: 0.6676711440086365, acc: 0.84, recall: 0.8400000000000001, precision: 0.8489326765188834, f_beta: 0.8389694041867956
train: step: 3393, loss: 0.6513586640357971, acc: 0.88, recall: 0.88, precision: 0.88, f_beta: 0.88
train: step: 3394, loss: 0.6831901669502258, acc: 0.7, recall: 0.7, precision: 0.702922077922078, f_beta: 0.6989160979526294
train: step: 3395, loss: 0.6677083373069763, acc: 0.76, recall: 0.76, precision: 0.7637987012987013, f_beta: 0.7591328783621035
train: step: 3396, loss: 0.684485912322998, acc: 0.76, recall: 0.76, precision: 0.7637987012987013, f_beta: 0.7591328783621035
train: step: 3397, loss: 0.6641061902046204, acc: 0.78, recall: 0.78, precision: 0.780448717948718, f_beta: 0.7799119647859143
train: step: 3398, loss: 0.6514630913734436, acc: 0.87, recall: 0.87, precision: 0.8737373737373737, f_beta: 0.8696741854636592
train: step: 3399, loss: 0.6707807183265686, acc: 0.74, recall: 0.74, precision: 0.7857142857142857, f_beta: 0.7291666666666666
train: step: 3400, loss: 0.6659689545631409, acc: 0.76, recall: 0.76, precision: 0.7604166666666667, f_beta: 0.7599039615846339
train: step: 3401, loss: 0.6693105101585388, acc: 0.73, recall: 0.73, precision: 0.7416981925178646, f_beta: 0.7266929851199515
train: step: 3402, loss: 0.6760324835777283, acc: 0.82, recall: 0.8200000000000001, precision: 0.822061191626409, f_beta: 0.8197115384615385
train: step: 3403, loss: 0.6729242205619812, acc: 0.78, recall: 0.78, precision: 0.780448717948718, f_beta: 0.7799119647859143
train: step: 3404, loss: 0.6674295663833618, acc: 0.83, recall: 0.8300000000000001, precision: 0.8311922922521076, f_beta: 0.8298468621759585
train: step: 3405, loss: 0.6684272289276123, acc: 0.77, recall: 0.77, precision: 0.7790409260024803, f_beta: 0.7681217864704104
train: step: 3406, loss: 0.6700681447982788, acc: 0.81, recall: 0.81, precision: 0.8131313131313131, f_beta: 0.8095238095238095
train: step: 3407, loss: 0.6629140973091125, acc: 0.8, recall: 0.8, precision: 0.8, f_beta: 0.8000000000000002
train: step: 3408, loss: 0.6621503233909607, acc: 0.86, recall: 0.86, precision: 0.8623188405797102, f_beta: 0.859775641025641
train: step: 3409, loss: 0.6540957093238831, acc: 0.88, recall: 0.88, precision: 0.8824476650563606, f_beta: 0.8798076923076923
train: step: 3410, loss: 0.6689072251319885, acc: 0.81, recall: 0.81, precision: 0.810124049619848, f_beta: 0.8099809980998101
train: step: 3411, loss: 0.659827709197998, acc: 0.84, recall: 0.84, precision: 0.8405448717948718, f_beta: 0.8399359743897559
train: step: 3412, loss: 0.6764689683914185, acc: 0.76, recall: 0.76, precision: 0.7616747181964574, f_beta: 0.7596153846153846
train: step: 3413, loss: 0.6705232858657837, acc: 0.77, recall: 0.77, precision: 0.7701080432172869, f_beta: 0.7699769976997699
train: step: 3414, loss: 0.6778610944747925, acc: 0.76, recall: 0.76, precision: 0.7708333333333333, f_beta: 0.7575757575757576
train: step: 3415, loss: 0.6677932143211365, acc: 0.81, recall: 0.81, precision: 0.8111200321156162, f_beta: 0.8098288459613652
train: step: 3416, loss: 0.6775351166725159, acc: 0.8, recall: 0.8, precision: 0.8019323671497585, f_beta: 0.7996794871794872
train: step: 3417, loss: 0.673906683921814, acc: 0.72, recall: 0.72, precision: 0.7450980392156863, f_beta: 0.7126436781609196
train: step: 3418, loss: 0.6773684620857239, acc: 0.71, recall: 0.71, precision: 0.7107587314331594, f_beta: 0.7097387648883996
train: step: 3419, loss: 0.661105751991272, acc: 0.82, recall: 0.82, precision: 0.8395585738539898, f_beta: 0.8173701298701298
train: step: 3420, loss: 0.6792314648628235, acc: 0.78, recall: 0.78, precision: 0.7818035426731079, f_beta: 0.7796474358974359
train: step: 3421, loss: 0.6785463690757751, acc: 0.8, recall: 0.8, precision: 0.8, f_beta: 0.8000000000000002
train: step: 3422, loss: 0.662569522857666, acc: 0.8, recall: 0.8, precision: 0.8019323671497585, f_beta: 0.7996794871794872
train: step: 3423, loss: 0.655433714389801, acc: 0.81, recall: 0.81, precision: 0.8111200321156162, f_beta: 0.8098288459613652
train: step: 3424, loss: 0.6502789258956909, acc: 0.81, recall: 0.81, precision: 0.810124049619848, f_beta: 0.8099809980998101
train: step: 3425, loss: 0.6557380557060242, acc: 0.81, recall: 0.81, precision: 0.8131313131313131, f_beta: 0.8095238095238095
train: step: 3426, loss: 0.6752705574035645, acc: 0.72, recall: 0.72, precision: 0.72, f_beta: 0.72
train: step: 3427, loss: 0.6646600961685181, acc: 0.82, recall: 0.8200000000000001, precision: 0.8246753246753247, f_beta: 0.8193496587715776
train: step: 3428, loss: 0.6657451391220093, acc: 0.84, recall: 0.8400000000000001, precision: 0.8449675324675325, f_beta: 0.839421918908069
train: step: 3429, loss: 0.6498820781707764, acc: 0.87, recall: 0.87, precision: 0.8701480592236894, f_beta: 0.8699869986998701
train: step: 3430, loss: 0.6751629114151001, acc: 0.78, recall: 0.78, precision: 0.7818035426731079, f_beta: 0.7796474358974359
train: step: 3431, loss: 0.6649008989334106, acc: 0.85, recall: 0.85, precision: 0.8501400560224089, f_beta: 0.84998499849985
train: step: 3432, loss: 0.6611425876617432, acc: 0.8, recall: 0.8, precision: 0.8078817733990147, f_beta: 0.7987117552334944
train: step: 3433, loss: 0.6715510487556458, acc: 0.72, recall: 0.72, precision: 0.7214170692431562, f_beta: 0.7195512820512822
train: step: 3434, loss: 0.6604663133621216, acc: 0.72, recall: 0.72, precision: 0.7232142857142857, f_beta: 0.7189883580891208
train: step: 3435, loss: 0.6661058664321899, acc: 0.89, recall: 0.8899999999999999, precision: 0.8939393939393939, f_beta: 0.8897243107769424
train: step: 3436, loss: 0.6699309349060059, acc: 0.76, recall: 0.76, precision: 0.7616747181964574, f_beta: 0.7596153846153846
train: step: 3437, loss: 0.6684566736221313, acc: 0.83, recall: 0.8300000000000001, precision: 0.8333333333333334, f_beta: 0.8295739348370927
train: step: 3438, loss: 0.662701427936554, acc: 0.83, recall: 0.8300000000000001, precision: 0.8365973072215422, f_beta: 0.8291628982011858
train: step: 3439, loss: 0.653369665145874, acc: 0.8, recall: 0.8, precision: 0.8, f_beta: 0.8000000000000002
train: step: 3440, loss: 0.6684589385986328, acc: 0.78, recall: 0.78, precision: 0.8119429590017826, f_beta: 0.7742200328407225
train: step: 3441, loss: 0.6619500517845154, acc: 0.75, recall: 0.75, precision: 0.750903251706142, f_beta: 0.7497747973175859
train: step: 3442, loss: 0.6576588153839111, acc: 0.81, recall: 0.81, precision: 0.8324753324753325, f_beta: 0.8067338012409724
train: step: 3443, loss: 0.6642981171607971, acc: 0.79, recall: 0.79, precision: 0.811025311025311, f_beta: 0.7863899908452854
train: step: 3444, loss: 0.6602753400802612, acc: 0.81, recall: 0.81, precision: 0.8161974704202366, f_beta: 0.8090644156366193
train: step: 3445, loss: 0.6492446064949036, acc: 0.83, recall: 0.83, precision: 0.853925353925354, f_beta: 0.8270776116366596
train: step: 3446, loss: 0.6617666482925415, acc: 0.84, recall: 0.84, precision: 0.84, f_beta: 0.8399999999999999
train: step: 3447, loss: 0.6454728841781616, acc: 0.82, recall: 0.8200000000000001, precision: 0.8333333333333334, f_beta: 0.8181818181818181
train: step: 3448, loss: 0.660437822341919, acc: 0.81, recall: 0.81, precision: 0.810124049619848, f_beta: 0.8099809980998101
train: step: 3449, loss: 0.6555168032646179, acc: 0.82, recall: 0.8200000000000001, precision: 0.8676470588235294, f_beta: 0.8139727159983464
train: step: 3450, loss: 0.6686611771583557, acc: 0.81, recall: 0.81, precision: 0.810124049619848, f_beta: 0.8099809980998101
train: step: 3451, loss: 0.6700503826141357, acc: 0.82, recall: 0.8200000000000001, precision: 0.8246753246753247, f_beta: 0.8193496587715776
train: step: 3452, loss: 0.6550565361976624, acc: 0.84, recall: 0.84, precision: 0.84, f_beta: 0.8399999999999999
train: step: 3453, loss: 0.6587640643119812, acc: 0.83, recall: 0.8300000000000001, precision: 0.8333333333333334, f_beta: 0.8295739348370927
train: step: 3454, loss: 0.6489419341087341, acc: 0.86, recall: 0.8600000000000001, precision: 0.8652597402597402, f_beta: 0.8594941790445605
train: step: 3455, loss: 0.6477957963943481, acc: 0.81, recall: 0.81, precision: 0.8131313131313131, f_beta: 0.8095238095238095
train: step: 3456, loss: 0.633516252040863, acc: 0.86, recall: 0.8600000000000001, precision: 0.8652597402597402, f_beta: 0.8594941790445605
train: step: 3457, loss: 0.6603888869285583, acc: 0.87, recall: 0.87, precision: 0.8737373737373737, f_beta: 0.8696741854636592
train: step: 3458, loss: 0.65008944272995, acc: 0.84, recall: 0.8400000000000001, precision: 0.8787878787878788, f_beta: 0.8357963875205254
train: step: 3459, loss: 0.669251024723053, acc: 0.72, recall: 0.72, precision: 0.7527573529411764, f_beta: 0.7106242248863166
train: step: 3460, loss: 0.6820037364959717, acc: 0.68, recall: 0.6799999999999999, precision: 0.6875, f_beta: 0.6767676767676767
train: step: 3461, loss: 0.6513824462890625, acc: 0.81, recall: 0.81, precision: 0.8406593406593407, f_beta: 0.8056265984654732
train: step: 3462, loss: 0.6816291213035583, acc: 0.68, recall: 0.6799999999999999, precision: 0.681159420289855, f_beta: 0.6794871794871795
train: step: 3463, loss: 0.6806578636169434, acc: 0.71, recall: 0.71, precision: 0.78, f_beta: 0.6906666666666665
train: step: 3464, loss: 0.6708899140357971, acc: 0.74, recall: 0.74, precision: 0.8118503118503119, f_beta: 0.7241086587436332
train: step: 3465, loss: 0.6642909049987793, acc: 0.72, recall: 0.72, precision: 0.7858627858627859, f_beta: 0.7028862478777589
train: step: 3466, loss: 0.6575968861579895, acc: 0.78, recall: 0.78, precision: 0.8472222222222222, f_beta: 0.7688104245481295
train: step: 3467, loss: 0.6338484287261963, acc: 0.83, recall: 0.8300000000000001, precision: 0.8626373626373627, f_beta: 0.8260869565217391
train: step: 3468, loss: 0.6666235327720642, acc: 0.73, recall: 0.73, precision: 0.8066666666666666, f_beta: 0.712
train: step: 3469, loss: 0.6456118226051331, acc: 0.8, recall: 0.8, precision: 0.8183361629881154, f_beta: 0.797077922077922
train: step: 3470, loss: 0.642860472202301, acc: 0.78, recall: 0.78, precision: 0.7971137521222411, f_beta: 0.7767857142857144
train: step: 3471, loss: 0.6592561602592468, acc: 0.74, recall: 0.74, precision: 0.74, f_beta: 0.74
train: step: 3472, loss: 0.617226779460907, acc: 0.84, recall: 0.8400000000000001, precision: 0.8449675324675325, f_beta: 0.839421918908069
train: step: 3473, loss: 0.6776684522628784, acc: 0.69, recall: 0.69, precision: 0.6996637242538881, f_beta: 0.6862030569895738
train: step: 3474, loss: 0.5841494202613831, acc: 0.86, recall: 0.86, precision: 0.86, f_beta: 0.8599999999999999
train: step: 3475, loss: 0.6450747847557068, acc: 0.78, recall: 0.78, precision: 0.7873563218390804, f_beta: 0.7785829307568439
train: step: 3476, loss: 0.6254740357398987, acc: 0.79, recall: 0.79, precision: 0.8047498949138294, f_beta: 0.7874278773155178
train: step: 3477, loss: 0.6495462656021118, acc: 0.75, recall: 0.75, precision: 0.7501000400160064, f_beta: 0.7499749974997499
train: step: 3478, loss: 0.6686969995498657, acc: 0.78, recall: 0.78, precision: 0.7840909090909092, f_beta: 0.779205138498595
train: step: 3479, loss: 0.6237015128135681, acc: 0.82, recall: 0.8200000000000001, precision: 0.8246753246753247, f_beta: 0.8193496587715776
train: step: 3480, loss: 0.6258535385131836, acc: 0.81, recall: 0.81, precision: 0.8161974704202366, f_beta: 0.8090644156366193
train: step: 3481, loss: 0.6177456378936768, acc: 0.8, recall: 0.8, precision: 0.8, f_beta: 0.8000000000000002
train: step: 3482, loss: 0.6430908441543579, acc: 0.79, recall: 0.79, precision: 0.7901160464185675, f_beta: 0.78997899789979
train: step: 3483, loss: 0.5907724499702454, acc: 0.78, recall: 0.78, precision: 0.7873563218390804, f_beta: 0.7785829307568439
train: step: 3484, loss: 0.5864514112472534, acc: 0.88, recall: 0.88, precision: 0.88, f_beta: 0.88
train: step: 3485, loss: 0.613832950592041, acc: 0.81, recall: 0.81, precision: 0.8161974704202366, f_beta: 0.8090644156366193
train: step: 3486, loss: 0.6650897264480591, acc: 0.64, recall: 0.64, precision: 0.6409017713365539, f_beta: 0.639423076923077
train: step: 3487, loss: 0.5985299944877625, acc: 0.79, recall: 0.79, precision: 0.7901160464185675, f_beta: 0.78997899789979
train: step: 3488, loss: 0.6344459056854248, acc: 0.78, recall: 0.78, precision: 0.7818035426731079, f_beta: 0.7796474358974359
train: step: 3489, loss: 0.611572802066803, acc: 0.79, recall: 0.79, precision: 0.7997106242248864, f_beta: 0.7882851093860268
train: step: 3490, loss: 0.6264311075210571, acc: 0.77, recall: 0.77, precision: 0.7895752895752897, f_beta: 0.7660461804495982
train: step: 3491, loss: 0.612328052520752, acc: 0.84, recall: 0.8400000000000001, precision: 0.8421900161030595, f_beta: 0.8397435897435896
train: step: 3492, loss: 0.6205148100852966, acc: 0.73, recall: 0.73, precision: 0.7300920368147259, f_beta: 0.72997299729973
train: step: 3493, loss: 0.6497966051101685, acc: 0.77, recall: 0.77, precision: 0.7701080432172869, f_beta: 0.7699769976997699
train: step: 3494, loss: 0.5887138247489929, acc: 0.82, recall: 0.8200000000000001, precision: 0.8333333333333334, f_beta: 0.8181818181818181
train: step: 3495, loss: 0.6555092334747314, acc: 0.76, recall: 0.76, precision: 0.7668308702791462, f_beta: 0.7584541062801933
train: step: 3496, loss: 0.6444287896156311, acc: 0.71, recall: 0.71, precision: 0.7252252252252251, f_beta: 0.7050147492625368
train: step: 3497, loss: 0.6582890152931213, acc: 0.79, recall: 0.79, precision: 0.7997106242248864, f_beta: 0.7882851093860268
train: step: 3498, loss: 0.6595256328582764, acc: 0.8, recall: 0.8, precision: 0.8183361629881154, f_beta: 0.797077922077922
train: step: 3499, loss: 0.6693528890609741, acc: 0.69, recall: 0.69, precision: 0.7220663861617578, f_beta: 0.6783898744683057
train: step: 3500, loss: 0.6603562831878662, acc: 0.7, recall: 0.7, precision: 0.7741228070175439, f_beta: 0.6782496782496782
train: step: 3501, loss: 0.6876285076141357, acc: 0.62, recall: 0.62, precision: 0.6488095238095238, f_beta: 0.6006725514922236
train: step: 3502, loss: 0.6875774264335632, acc: 0.64, recall: 0.64, precision: 0.6519097222222222, f_beta: 0.6328029375764994
train: step: 3503, loss: 0.6590060591697693, acc: 0.67, recall: 0.6699999999999999, precision: 0.7399774138904573, f_beta: 0.6440513428972063
train: step: 3504, loss: 0.6605764031410217, acc: 0.71, recall: 0.71, precision: 0.7454417952314165, f_beta: 0.6991389148251893
train: step: 3505, loss: 0.6667049527168274, acc: 0.65, recall: 0.65, precision: 0.7117447769621683, f_beta: 0.6224786970121886
train: step: 3506, loss: 0.6456217169761658, acc: 0.75, recall: 0.75, precision: 0.8333333333333333, f_beta: 0.7333333333333334
train: step: 3507, loss: 0.6795610785484314, acc: 0.63, recall: 0.63, precision: 0.6733333333333333, f_beta: 0.6053333333333333
train: step: 3508, loss: 0.6642813086509705, acc: 0.7, recall: 0.7, precision: 0.7913752913752914, f_beta: 0.6744791666666667
train: step: 3509, loss: 0.6900671124458313, acc: 0.59, recall: 0.5900000000000001, precision: 0.5930136420008267, f_beta: 0.5866518802298619
train: step: 3510, loss: 0.6787034869194031, acc: 0.65, recall: 0.6499999999999999, precision: 0.7260397830018083, f_beta: 0.6178622120318811
train: step: 3511, loss: 0.6334021091461182, acc: 0.8, recall: 0.8, precision: 0.8446691176470589, f_beta: 0.7933030177759404
train: step: 3512, loss: 0.6885198950767517, acc: 0.64, recall: 0.64, precision: 0.6436781609195402, f_beta: 0.6376811594202898
train: step: 3513, loss: 0.6864845752716064, acc: 0.61, recall: 0.61, precision: 0.6948972360028349, f_beta: 0.5623386825272136
train: step: 3514, loss: 0.6821913123130798, acc: 0.61, recall: 0.61, precision: 0.6786874593892138, f_beta: 0.568536342515765
train: step: 3515, loss: 0.6723731756210327, acc: 0.67, recall: 0.6699999999999999, precision: 0.7561784207353828, f_beta: 0.6396986570586309
train: step: 3516, loss: 0.6485396027565002, acc: 0.72, recall: 0.72, precision: 0.7858627858627859, f_beta: 0.7028862478777589
train: step: 3517, loss: 0.6355611681938171, acc: 0.77, recall: 0.77, precision: 0.8155680224403927, f_beta: 0.7613860358958398
train: step: 3518, loss: 0.6484175324440002, acc: 0.75, recall: 0.75, precision: 0.8035454103933948, f_beta: 0.7384663667747673
train: step: 3519, loss: 0.619005024433136, acc: 0.83, recall: 0.8300000000000001, precision: 0.8626373626373627, f_beta: 0.8260869565217391
train: step: 3520, loss: 0.6715670228004456, acc: 0.65, recall: 0.65, precision: 0.6902587519025876, f_beta: 0.630450849963045
train: step: 3521, loss: 0.6544790863990784, acc: 0.69, recall: 0.69, precision: 0.6963621331128566, f_beta: 0.6874684948079444
train: step: 3522, loss: 0.6639361381530762, acc: 0.71, recall: 0.71, precision: 0.7141982864137086, f_beta: 0.7085720028137876
train: step: 3523, loss: 0.6190493106842041, acc: 0.73, recall: 0.73, precision: 0.7792617775619233, f_beta: 0.7175436761167486
train: step: 3524, loss: 0.6583878993988037, acc: 0.75, recall: 0.75, precision: 0.7549979600163199, f_beta: 0.7487689679429204
train: step: 3525, loss: 0.6816216111183167, acc: 0.7, recall: 0.7, precision: 0.702922077922078, f_beta: 0.6989160979526294
train: step: 3526, loss: 0.5978514552116394, acc: 0.8, recall: 0.8, precision: 0.8078817733990147, f_beta: 0.7987117552334944
train: step: 3527, loss: 0.640125036239624, acc: 0.75, recall: 0.75, precision: 0.7549979600163199, f_beta: 0.7487689679429204
train: step: 3528, loss: 0.6298380494117737, acc: 0.74, recall: 0.74, precision: 0.7403846153846154, f_beta: 0.7398959583833533
train: step: 3529, loss: 0.6510878801345825, acc: 0.82, recall: 0.8200000000000001, precision: 0.8246753246753247, f_beta: 0.8193496587715776
train: step: 3530, loss: 0.6271647810935974, acc: 0.77, recall: 0.77, precision: 0.7727272727272727, f_beta: 0.7694235588972431
train: step: 3531, loss: 0.6982209086418152, acc: 0.37, recall: 0.37, precision: 0.36868686868686873, f_beta: 0.368421052631579
train: step: 3532, loss: 0.6504337787628174, acc: 0.76, recall: 0.76, precision: 0.76, f_beta: 0.76
train: step: 3533, loss: 0.6409786939620972, acc: 0.73, recall: 0.73, precision: 0.7300920368147259, f_beta: 0.72997299729973
train: step: 3534, loss: 0.6151372790336609, acc: 0.76, recall: 0.76, precision: 0.76, f_beta: 0.76
train: step: 3535, loss: 0.6345888376235962, acc: 0.76, recall: 0.76, precision: 0.7758913412563667, f_beta: 0.7564935064935063
train: step: 3536, loss: 0.6347610950469971, acc: 0.71, recall: 0.71, precision: 0.7100840336134454, f_beta: 0.7099709970997101
train: step: 3537, loss: 0.629202663898468, acc: 0.76, recall: 0.76, precision: 0.7708333333333333, f_beta: 0.7575757575757576
train: step: 3538, loss: 0.5913282632827759, acc: 0.78, recall: 0.78, precision: 0.7818035426731079, f_beta: 0.7796474358974359
train: step: 3539, loss: 0.6481564044952393, acc: 0.69, recall: 0.69, precision: 0.690686471296668, f_beta: 0.6897207486738064
train: step: 3540, loss: 0.6216578483581543, acc: 0.74, recall: 0.74, precision: 0.7463054187192117, f_beta: 0.7383252818035426
train: step: 3541, loss: 0.6389644145965576, acc: 0.73, recall: 0.73, precision: 0.7323232323232323, f_beta: 0.7293233082706766
train: step: 3542, loss: 0.667161226272583, acc: 0.62, recall: 0.62, precision: 0.6273344651952462, f_beta: 0.614448051948052
train: step: 3543, loss: 0.6284064650535583, acc: 0.77, recall: 0.77, precision: 0.7727272727272727, f_beta: 0.7694235588972431
train: step: 3544, loss: 0.6335370540618896, acc: 0.73, recall: 0.73, precision: 0.7377015295576685, f_beta: 0.7277951406391774
train: step: 3545, loss: 0.6335603594779968, acc: 0.75, recall: 0.75, precision: 0.750903251706142, f_beta: 0.7497747973175859
train: step: 3546, loss: 0.6898853182792664, acc: 0.62, recall: 0.62, precision: 0.6336898395721926, f_beta: 0.6100164203612479
train: step: 3547, loss: 0.6880084276199341, acc: 0.73, recall: 0.73, precision: 0.7600633197648123, f_beta: 0.721964782205746
train: step: 3548, loss: 0.6420168876647949, acc: 0.74, recall: 0.74, precision: 0.7463054187192117, f_beta: 0.7383252818035426
train: step: 3549, loss: 0.6903958916664124, acc: 0.57, recall: 0.5700000000000001, precision: 0.5735603194619587, f_beta: 0.5647332725984411
train: step: 3550, loss: 0.6957843899726868, acc: 0.4, recall: 0.4, precision: 0.398538961038961, f_beta: 0.397832195905259
train: step: 3551, loss: 0.6687004566192627, acc: 0.74, recall: 0.74, precision: 0.7857142857142857, f_beta: 0.7291666666666666
train: step: 3552, loss: 0.559226930141449, acc: 0.85, recall: 0.85, precision: 0.8617197188921042, f_beta: 0.8487750781328762
train: step: 3553, loss: 0.6388522386550903, acc: 0.76, recall: 0.76, precision: 0.7616747181964574, f_beta: 0.7596153846153846
train: step: 3554, loss: 0.6437369585037231, acc: 0.71, recall: 0.71, precision: 0.7307692307692308, f_beta: 0.7033248081841432
train: step: 3555, loss: 0.6834279894828796, acc: 0.67, recall: 0.6699999999999999, precision: 0.6733986128110976, f_beta: 0.6683750376846548
train: step: 3556, loss: 0.689269483089447, acc: 0.54, recall: 0.54, precision: 0.54, f_beta: 0.54
train: step: 3557, loss: 0.63946133852005, acc: 0.74, recall: 0.74, precision: 0.7415458937198067, f_beta: 0.7395833333333335
train: step: 3558, loss: 0.5976337194442749, acc: 0.81, recall: 0.81, precision: 0.8203803224472923, f_beta: 0.8084484323016432
train: step: 3559, loss: 0.6231651306152344, acc: 0.72, recall: 0.72, precision: 0.7257799671592775, f_beta: 0.7181964573268922
train: step: 3560, loss: 0.6586205959320068, acc: 0.67, recall: 0.6699999999999999, precision: 0.6756924348904505, f_beta: 0.667305171892328
train: step: 3561, loss: 0.6502997875213623, acc: 0.72, recall: 0.72, precision: 0.7232142857142857, f_beta: 0.7189883580891208
train: step: 3562, loss: 0.6304635405540466, acc: 0.83, recall: 0.83, precision: 0.8301320528211285, f_beta: 0.8299829982998299
train: step: 3563, loss: 0.6193851828575134, acc: 0.74, recall: 0.74, precision: 0.7403846153846154, f_beta: 0.7398959583833533
train: step: 3564, loss: 0.639890193939209, acc: 0.69, recall: 0.69, precision: 0.7220663861617578, f_beta: 0.6783898744683057
train: step: 3565, loss: 0.6087709665298462, acc: 0.73, recall: 0.73, precision: 0.7300920368147259, f_beta: 0.72997299729973
train: step: 3566, loss: 0.631715714931488, acc: 0.77, recall: 0.77, precision: 0.7701080432172869, f_beta: 0.7699769976997699
train: step: 3567, loss: 0.62720787525177, acc: 0.74, recall: 0.74, precision: 0.75, f_beta: 0.7373737373737373
train: step: 3568, loss: 0.6474364399909973, acc: 0.65, recall: 0.65, precision: 0.7, f_beta: 0.6266666666666667
train: step: 3569, loss: 0.691632091999054, acc: 0.59, recall: 0.59, precision: 0.5989010989010989, f_beta: 0.5805626598465473
train: step: 3570, loss: 0.6534214019775391, acc: 0.68, recall: 0.6799999999999999, precision: 0.7068014705882353, f_beta: 0.6692848284415047
train: step: 3571, loss: 0.6684150695800781, acc: 0.69, recall: 0.69, precision: 0.7087912087912088, f_beta: 0.6828644501278773
train: step: 3572, loss: 0.6391931772232056, acc: 0.7, recall: 0.7, precision: 0.7012882447665056, f_beta: 0.6995192307692307
train: step: 3573, loss: 0.6776668429374695, acc: 0.56, recall: 0.56, precision: 0.6016260162601625, f_beta: 0.5098039215686274
train: step: 3574, loss: 0.6776719093322754, acc: 0.75, recall: 0.75, precision: 0.7501000400160064, f_beta: 0.7499749974997499
train: step: 3575, loss: 0.6766878366470337, acc: 0.67, recall: 0.6699999999999999, precision: 0.6706142111601766, f_beta: 0.6697027324592133
train: step: 3576, loss: 0.6397228240966797, acc: 0.69, recall: 0.69, precision: 0.7037752037752039, f_beta: 0.6846709388668497
train: step: 3577, loss: 0.6220834851264954, acc: 0.74, recall: 0.74, precision: 0.7546689303904923, f_beta: 0.7362012987012987
train: step: 3578, loss: 0.6986801624298096, acc: 0.45, recall: 0.45, precision: 0.439290917921321, f_beta: 0.42462600690448793
train: step: 3579, loss: 0.5721859931945801, acc: 0.79, recall: 0.79, precision: 0.7901160464185675, f_beta: 0.78997899789979
train: step: 3580, loss: 0.6226915121078491, acc: 0.74, recall: 0.74, precision: 0.7415458937198067, f_beta: 0.7395833333333335
train: step: 3581, loss: 0.6186469793319702, acc: 0.8, recall: 0.8, precision: 0.8, f_beta: 0.8000000000000002
train: step: 3582, loss: 0.6267942786216736, acc: 0.72, recall: 0.72, precision: 0.7232142857142857, f_beta: 0.7189883580891208
train: step: 3583, loss: 0.6264872550964355, acc: 0.79, recall: 0.79, precision: 0.7957976336189311, f_beta: 0.7889659330720531
train: step: 3584, loss: 0.6220142245292664, acc: 0.72, recall: 0.72, precision: 0.7232142857142857, f_beta: 0.7189883580891208
train: step: 3585, loss: 0.6108418107032776, acc: 0.77, recall: 0.77, precision: 0.7727272727272727, f_beta: 0.7694235588972431
train: step: 3586, loss: 0.5905177593231201, acc: 0.78, recall: 0.78, precision: 0.780448717948718, f_beta: 0.7799119647859143
train: step: 3587, loss: 0.6605469584465027, acc: 0.73, recall: 0.73, precision: 0.7416981925178646, f_beta: 0.7266929851199515
train: step: 3588, loss: 0.6306769847869873, acc: 0.8, recall: 0.8, precision: 0.8019323671497585, f_beta: 0.7996794871794872
train: step: 3589, loss: 0.6791515350341797, acc: 0.63, recall: 0.63, precision: 0.6428571428571428, f_beta: 0.6214833759590792
train: step: 3590, loss: 0.6649329662322998, acc: 0.71, recall: 0.71, precision: 0.7141982864137086, f_beta: 0.7085720028137876
train: step: 3591, loss: 0.6272702813148499, acc: 0.71, recall: 0.71, precision: 0.7141982864137086, f_beta: 0.7085720028137876
train: step: 3592, loss: 0.6287022829055786, acc: 0.82, recall: 0.82, precision: 0.82, f_beta: 0.82
train: step: 3593, loss: 0.6184331774711609, acc: 0.77, recall: 0.77, precision: 0.7727272727272727, f_beta: 0.7694235588972431
train: step: 3594, loss: 0.6020610332489014, acc: 0.82, recall: 0.82, precision: 0.82, f_beta: 0.82
train: step: 3595, loss: 0.5856233835220337, acc: 0.78, recall: 0.78, precision: 0.7818035426731079, f_beta: 0.7796474358974359
train: step: 3596, loss: 0.630303144454956, acc: 0.73, recall: 0.73, precision: 0.7308309915696507, f_beta: 0.7297567811029926
train: step: 3597, loss: 0.6408107876777649, acc: 0.72, recall: 0.72, precision: 0.733446519524618, f_beta: 0.7159090909090909
train: step: 3598, loss: 0.6297503113746643, acc: 0.81, recall: 0.81, precision: 0.8131313131313131, f_beta: 0.8095238095238095
train: step: 3599, loss: 0.6289467811584473, acc: 0.74, recall: 0.74, precision: 0.767379679144385, f_beta: 0.7331691297208538
train: step: 3600, loss: 0.619453489780426, acc: 0.74, recall: 0.74, precision: 0.7757352941176471, f_beta: 0.7312939231087227
train: step: 3601, loss: 0.5893644690513611, acc: 0.77, recall: 0.77, precision: 0.7727272727272727, f_beta: 0.7694235588972431
train: step: 3602, loss: 0.5981300473213196, acc: 0.79, recall: 0.79, precision: 0.7997106242248864, f_beta: 0.7882851093860268
train: step: 3603, loss: 0.6086864471435547, acc: 0.81, recall: 0.81, precision: 0.810124049619848, f_beta: 0.8099809980998101
train: step: 3604, loss: 0.5717485547065735, acc: 0.85, recall: 0.85, precision: 0.851264552388599, f_beta: 0.8498648783905516
train: step: 3605, loss: 0.6245574951171875, acc: 0.79, recall: 0.79, precision: 0.7929292929292929, f_beta: 0.7894736842105263
train: step: 3606, loss: 0.6455339193344116, acc: 0.77, recall: 0.77, precision: 0.7727272727272727, f_beta: 0.7694235588972431
train: step: 3607, loss: 0.6303855180740356, acc: 0.76, recall: 0.76, precision: 0.7668308702791462, f_beta: 0.7584541062801933
train: step: 3608, loss: 0.6223352551460266, acc: 0.72, recall: 0.72, precision: 0.7257799671592775, f_beta: 0.7181964573268922
train: step: 3609, loss: 0.5703455209732056, acc: 0.76, recall: 0.76, precision: 0.7637987012987013, f_beta: 0.7591328783621035
train: step: 3610, loss: 0.667182981967926, acc: 0.73, recall: 0.73, precision: 0.7466752466752467, f_beta: 0.7253585596582239
train: step: 3611, loss: 0.6063055992126465, acc: 0.8, recall: 0.8, precision: 0.8043831168831169, f_beta: 0.7992773986350863
train: step: 3612, loss: 0.6354996562004089, acc: 0.74, recall: 0.74, precision: 0.7435064935064934, f_beta: 0.7390606182256123
train: step: 3613, loss: 0.6004670858383179, acc: 0.8, recall: 0.8, precision: 0.8078817733990147, f_beta: 0.7987117552334944
train: step: 3614, loss: 0.5847472548484802, acc: 0.82, recall: 0.82, precision: 0.82, f_beta: 0.82
train: step: 3615, loss: 0.5888534188270569, acc: 0.83, recall: 0.8300000000000001, precision: 0.8311922922521076, f_beta: 0.8298468621759585
train: step: 3616, loss: 0.6579306125640869, acc: 0.68, recall: 0.6799999999999999, precision: 0.7005347593582887, f_beta: 0.6715927750410509
train: step: 3617, loss: 0.5539469718933105, acc: 0.78, recall: 0.78, precision: 0.8038194444444444, f_beta: 0.7756017951856385
train: step: 3618, loss: 0.6499015092849731, acc: 0.65, recall: 0.65, precision: 0.6608751608751608, f_beta: 0.6439833180754755
train: step: 3619, loss: 0.5835696458816528, acc: 0.75, recall: 0.75, precision: 0.7681252681252682, f_beta: 0.7457023700539112
train: step: 3620, loss: 0.602338969707489, acc: 0.74, recall: 0.74, precision: 0.7546689303904923, f_beta: 0.7362012987012987
train: step: 3621, loss: 0.6069039702415466, acc: 0.73, recall: 0.73, precision: 0.7345981232150143, f_beta: 0.7286704853783539
train: step: 3622, loss: 0.6501307487487793, acc: 0.77, recall: 0.77, precision: 0.7753977968176254, f_beta: 0.7688674505074867
train: step: 3623, loss: 0.6045644283294678, acc: 0.74, recall: 0.74, precision: 0.7403846153846154, f_beta: 0.7398959583833533
train: step: 3624, loss: 0.5992805361747742, acc: 0.74, recall: 0.74, precision: 0.7546689303904923, f_beta: 0.7362012987012987
train: step: 3625, loss: 0.6600698232650757, acc: 0.72, recall: 0.72, precision: 0.7387152777777778, f_beta: 0.7144022847817217
train: step: 3626, loss: 0.651597797870636, acc: 0.7, recall: 0.7, precision: 0.7012882447665056, f_beta: 0.6995192307692307
train: step: 3627, loss: 0.6437360644340515, acc: 0.67, recall: 0.6699999999999999, precision: 0.6756924348904505, f_beta: 0.667305171892328
train: step: 3628, loss: 0.6709813475608826, acc: 0.66, recall: 0.66, precision: 0.6602564102564102, f_beta: 0.6598639455782312
train: step: 3629, loss: 0.6301532983779907, acc: 0.67, recall: 0.6699999999999999, precision: 0.6756924348904505, f_beta: 0.667305171892328
train: step: 3630, loss: 0.6750731468200684, acc: 0.71, recall: 0.71, precision: 0.7100840336134454, f_beta: 0.7099709970997101
train: step: 3631, loss: 0.5983278155326843, acc: 0.8, recall: 0.8, precision: 0.8043831168831169, f_beta: 0.7992773986350863
train: step: 3632, loss: 0.6111791133880615, acc: 0.77, recall: 0.77, precision: 0.7790409260024803, f_beta: 0.7681217864704104
train: step: 3633, loss: 0.6889444589614868, acc: 0.61, recall: 0.61, precision: 0.6335599805730938, f_beta: 0.5920075321686369
train: step: 3634, loss: 0.574018120765686, acc: 0.83, recall: 0.8300000000000001, precision: 0.8311922922521076, f_beta: 0.8298468621759585
train: step: 3635, loss: 0.6708863973617554, acc: 0.64, recall: 0.64, precision: 0.6736111111111112, f_beta: 0.6216897856242118
train: step: 3636, loss: 0.6790626049041748, acc: 0.74, recall: 0.74, precision: 0.75, f_beta: 0.7373737373737373
train: step: 3637, loss: 0.6701269745826721, acc: 0.77, recall: 0.77, precision: 0.7727272727272727, f_beta: 0.7694235588972431
train: step: 3638, loss: 0.5445297360420227, acc: 0.81, recall: 0.81, precision: 0.8111200321156162, f_beta: 0.8098288459613652
train: step: 3639, loss: 0.6353780627250671, acc: 0.68, recall: 0.6799999999999999, precision: 0.6875, f_beta: 0.6767676767676767
train: step: 3640, loss: 0.6240972280502319, acc: 0.76, recall: 0.76, precision: 0.7637987012987013, f_beta: 0.7591328783621035
train: step: 3641, loss: 0.6534115672111511, acc: 0.75, recall: 0.75, precision: 0.7501000400160064, f_beta: 0.7499749974997499
train: step: 3642, loss: 0.5925912261009216, acc: 0.75, recall: 0.75, precision: 0.7501000400160064, f_beta: 0.7499749974997499
train: step: 3643, loss: 0.6614001989364624, acc: 0.6, recall: 0.6, precision: 0.6148897058823529, f_beta: 0.586606035551881
train: step: 3644, loss: 0.6091745495796204, acc: 0.76, recall: 0.76, precision: 0.7708333333333333, f_beta: 0.7575757575757576
train: step: 3645, loss: 0.6638485789299011, acc: 0.74, recall: 0.74, precision: 0.75, f_beta: 0.7373737373737373
train: step: 3646, loss: 0.6475765705108643, acc: 0.7, recall: 0.7, precision: 0.7052545155993433, f_beta: 0.6980676328502415
train: step: 3647, loss: 0.5882351994514465, acc: 0.78, recall: 0.78, precision: 0.7818035426731079, f_beta: 0.7796474358974359
train: step: 3648, loss: 0.6351423859596252, acc: 0.81, recall: 0.81, precision: 0.8161974704202366, f_beta: 0.8090644156366193
train: step: 3649, loss: 0.6326429843902588, acc: 0.67, recall: 0.67, precision: 0.6823251823251824, f_beta: 0.6643271284711627
train: step: 3650, loss: 0.6504509449005127, acc: 0.73, recall: 0.73, precision: 0.7466752466752467, f_beta: 0.7253585596582239
train: step: 3651, loss: 0.6081697940826416, acc: 0.75, recall: 0.75, precision: 0.750903251706142, f_beta: 0.7497747973175859
train: step: 3652, loss: 0.6652153134346008, acc: 0.71, recall: 0.71, precision: 0.7170318313352625, f_beta: 0.7076318177235609
train: step: 3653, loss: 0.6373084783554077, acc: 0.72, recall: 0.72, precision: 0.7387152777777778, f_beta: 0.7144022847817217
train: step: 3654, loss: 0.6156608462333679, acc: 0.77, recall: 0.77, precision: 0.7701080432172869, f_beta: 0.7699769976997699
train: step: 3655, loss: 0.6143929362297058, acc: 0.8, recall: 0.8, precision: 0.8004807692307692, f_beta: 0.7999199679871949
train: step: 3656, loss: 0.5751192569732666, acc: 0.79, recall: 0.79, precision: 0.7901160464185675, f_beta: 0.78997899789979
train: step: 3657, loss: 0.6045264601707458, acc: 0.78, recall: 0.78, precision: 0.78, f_beta: 0.78
train: step: 3658, loss: 0.613955020904541, acc: 0.81, recall: 0.81, precision: 0.8161974704202366, f_beta: 0.8090644156366193
train: step: 3659, loss: 0.6456878781318665, acc: 0.7, recall: 0.7, precision: 0.702922077922078, f_beta: 0.6989160979526294
train: step: 3660, loss: 0.6181005239486694, acc: 0.78, recall: 0.78, precision: 0.7840909090909092, f_beta: 0.779205138498595
train: step: 3661, loss: 0.6286066174507141, acc: 0.76, recall: 0.76, precision: 0.7616747181964574, f_beta: 0.7596153846153846
train: step: 3662, loss: 0.6524338722229004, acc: 0.68, recall: 0.6799999999999999, precision: 0.6875, f_beta: 0.6767676767676767
train: step: 3663, loss: 0.5837171077728271, acc: 0.77, recall: 0.77, precision: 0.7709755118426336, f_beta: 0.769792813532179
train: step: 3664, loss: 0.6210260987281799, acc: 0.8, recall: 0.8, precision: 0.8019323671497585, f_beta: 0.7996794871794872
train: step: 3665, loss: 0.6078521609306335, acc: 0.84, recall: 0.8400000000000001, precision: 0.8421900161030595, f_beta: 0.8397435897435896
train: step: 3666, loss: 0.6260782480239868, acc: 0.8, recall: 0.8, precision: 0.8, f_beta: 0.8000000000000002
train: step: 3667, loss: 0.5714455842971802, acc: 0.84, recall: 0.8400000000000001, precision: 0.8421900161030595, f_beta: 0.8397435897435896
train: step: 3668, loss: 0.673912763595581, acc: 0.77, recall: 0.77, precision: 0.7701080432172869, f_beta: 0.7699769976997699
train: step: 3669, loss: 0.5953384637832642, acc: 0.79, recall: 0.79, precision: 0.7997106242248864, f_beta: 0.7882851093860268
train: step: 3670, loss: 0.5388019680976868, acc: 0.83, recall: 0.8300000000000001, precision: 0.8410500206696983, f_beta: 0.8286117552172598
train: step: 3671, loss: 0.63785719871521, acc: 0.74, recall: 0.74, precision: 0.7435064935064934, f_beta: 0.7390606182256123
train: step: 3672, loss: 0.6836708188056946, acc: 0.66, recall: 0.66, precision: 0.66, f_beta: 0.66
train: step: 3673, loss: 0.5419170260429382, acc: 0.85, recall: 0.8500000000000001, precision: 0.8569971440228478, f_beta: 0.8492613807657521
train: step: 3674, loss: 0.5940048694610596, acc: 0.77, recall: 0.77, precision: 0.7701080432172869, f_beta: 0.7699769976997699
train: step: 3675, loss: 0.5674182772636414, acc: 0.81, recall: 0.81, precision: 0.8203803224472923, f_beta: 0.8084484323016432
train: step: 3676, loss: 0.5963281393051147, acc: 0.75, recall: 0.75, precision: 0.7549979600163199, f_beta: 0.7487689679429204
train: step: 3677, loss: 0.5960646271705627, acc: 0.81, recall: 0.81, precision: 0.8203803224472923, f_beta: 0.8084484323016432
train: step: 3678, loss: 0.6296965479850769, acc: 0.7, recall: 0.7, precision: 0.702922077922078, f_beta: 0.6989160979526294
train: step: 3679, loss: 0.6180676817893982, acc: 0.72, recall: 0.72, precision: 0.7232142857142857, f_beta: 0.7189883580891208
train: step: 3680, loss: 0.5954700112342834, acc: 0.78, recall: 0.78, precision: 0.780448717948718, f_beta: 0.7799119647859143
train: step: 3681, loss: 0.607103168964386, acc: 0.72, recall: 0.72, precision: 0.7291666666666667, f_beta: 0.7171717171717171
train: step: 3682, loss: 0.6707766652107239, acc: 0.6, recall: 0.6, precision: 0.6370614035087719, f_beta: 0.570999570999571
train: step: 3683, loss: 0.6402986645698547, acc: 0.67, recall: 0.67, precision: 0.6786464901219, f_beta: 0.6659580929243851
train: step: 3684, loss: 0.6209951043128967, acc: 0.74, recall: 0.74, precision: 0.7604166666666667, f_beta: 0.7348021215830274
train: step: 3685, loss: 0.6591649055480957, acc: 0.66, recall: 0.66, precision: 0.6666666666666666, f_beta: 0.6565656565656566
train: step: 3686, loss: 0.6663839221000671, acc: 0.76, recall: 0.76, precision: 0.7616747181964574, f_beta: 0.7596153846153846
train: step: 3687, loss: 0.6343050599098206, acc: 0.71, recall: 0.71, precision: 0.7549781447304517, f_beta: 0.69662098545873
train: step: 3688, loss: 0.6595936417579651, acc: 0.63, recall: 0.63, precision: 0.6428571428571428, f_beta: 0.6214833759590792
train: step: 3689, loss: 0.6541142463684082, acc: 0.65, recall: 0.65, precision: 0.7117447769621683, f_beta: 0.6224786970121886
train: step: 3690, loss: 0.6496477723121643, acc: 0.72, recall: 0.72, precision: 0.7387152777777778, f_beta: 0.7144022847817217
train: step: 3691, loss: 0.6358156204223633, acc: 0.72, recall: 0.72, precision: 0.7858627858627859, f_beta: 0.7028862478777589
train: step: 3692, loss: 0.685624361038208, acc: 0.54, recall: 0.54, precision: 0.5496031746031746, f_beta: 0.5166036149642708
train: step: 3693, loss: 0.6523426175117493, acc: 0.63, recall: 0.63, precision: 0.7303330970942594, f_beta: 0.5847828526540231
train: step: 3694, loss: 0.6389585733413696, acc: 0.62, recall: 0.62, precision: 0.7032520325203252, f_beta: 0.5766488413547237
train: step: 3695, loss: 0.6729015111923218, acc: 0.61, recall: 0.61, precision: 0.6466666666666667, f_beta: 0.584
train: step: 3696, loss: 0.6964483857154846, acc: 0.37, recall: 0.37, precision: 0.31648785996612083, f_beta: 0.3204616546219394
train: step: 3697, loss: 0.6390482187271118, acc: 0.74, recall: 0.74, precision: 0.7857142857142857, f_beta: 0.7291666666666666
train: step: 3698, loss: 0.6803314685821533, acc: 0.69, recall: 0.69, precision: 0.7220663861617578, f_beta: 0.6783898744683057
train: step: 3699, loss: 0.6617022156715393, acc: 0.82, recall: 0.8200000000000001, precision: 0.8205128205128205, f_beta: 0.8199279711884755
train: step: 3700, loss: 0.6838315725326538, acc: 0.66, recall: 0.66, precision: 0.669779286926995, f_beta: 0.6550324675324675
train: step: 3701, loss: 0.6700156927108765, acc: 0.75, recall: 0.75, precision: 0.7681252681252682, f_beta: 0.7457023700539112
train: step: 3702, loss: 0.6377314925193787, acc: 0.67, recall: 0.6699999999999999, precision: 0.7399774138904573, f_beta: 0.6440513428972063
train: step: 3703, loss: 0.6715188026428223, acc: 0.66, recall: 0.66, precision: 0.6984126984126984, f_beta: 0.6427070197562001
train: step: 3704, loss: 0.672101616859436, acc: 0.61, recall: 0.61, precision: 0.6466666666666667, f_beta: 0.584
train: step: 3705, loss: 0.6398533582687378, acc: 0.67, recall: 0.6699999999999999, precision: 0.698690977092099, f_beta: 0.6576408341114224
train: step: 3706, loss: 0.5901548862457275, acc: 0.72, recall: 0.72, precision: 0.7527573529411764, f_beta: 0.7106242248863166
train: step: 3707, loss: 0.6880000233650208, acc: 0.64, recall: 0.64, precision: 0.6436781609195402, f_beta: 0.6376811594202898
train: step: 3708, loss: 0.6209883689880371, acc: 0.67, recall: 0.67, precision: 0.7064108790675085, f_beta: 0.6547756041426929
train: step: 3709, loss: 0.6419413685798645, acc: 0.65, recall: 0.65, precision: 0.7, f_beta: 0.6266666666666667
train: step: 3710, loss: 0.6161292791366577, acc: 0.67, recall: 0.67, precision: 0.6823251823251824, f_beta: 0.6643271284711627
train: step: 3711, loss: 0.6400105953216553, acc: 0.78, recall: 0.78, precision: 0.7818035426731079, f_beta: 0.7796474358974359
train: step: 3712, loss: 0.6671327352523804, acc: 0.75, recall: 0.75, precision: 0.7525252525252526, f_beta: 0.7493734335839599
train: step: 3713, loss: 0.6810604929924011, acc: 0.73, recall: 0.73, precision: 0.7323232323232323, f_beta: 0.7293233082706766
train: step: 3714, loss: 0.6500647664070129, acc: 0.8, recall: 0.8, precision: 0.8004807692307692, f_beta: 0.7999199679871949
train: step: 3715, loss: 0.6571797132492065, acc: 0.88, recall: 0.88, precision: 0.8824476650563606, f_beta: 0.8798076923076923
train: step: 3716, loss: 0.6433811187744141, acc: 0.63, recall: 0.63, precision: 0.6648909183155758, f_beta: 0.6093337556752191
train: step: 3717, loss: 0.6532300710678101, acc: 0.58, recall: 0.5800000000000001, precision: 0.6488095238095238, f_beta: 0.5251017639077341
train: step: 3718, loss: 0.6626971960067749, acc: 0.69, recall: 0.69, precision: 0.7037752037752039, f_beta: 0.6846709388668497
train: step: 3719, loss: 0.6833943128585815, acc: 0.74, recall: 0.74, precision: 0.75, f_beta: 0.7373737373737373
train: step: 3720, loss: 0.677994966506958, acc: 0.79, recall: 0.79, precision: 0.7910477719791249, f_beta: 0.7898108297467722
train: step: 3721, loss: 0.637748658657074, acc: 0.65, recall: 0.65, precision: 0.6902587519025876, f_beta: 0.630450849963045
train: step: 3722, loss: 0.6412144303321838, acc: 0.62, recall: 0.62, precision: 0.6559251559251559, f_beta: 0.596774193548387
train: step: 3723, loss: 0.673947811126709, acc: 0.74, recall: 0.74, precision: 0.7604166666666667, f_beta: 0.7348021215830274
train: step: 3724, loss: 0.6624078154563904, acc: 0.81, recall: 0.81, precision: 0.8257671290458175, f_beta: 0.8076728413807065
train: step: 3725, loss: 0.6884721517562866, acc: 0.6, recall: 0.6, precision: 0.6061120543293719, f_beta: 0.5941558441558441
train: step: 3726, loss: 0.6725267171859741, acc: 0.75, recall: 0.75, precision: 0.7583712277800745, f_beta: 0.7479584635547939
train: step: 3727, loss: 0.6680908203125, acc: 0.7, recall: 0.7, precision: 0.7380952380952381, f_beta: 0.6875
train: step: 3728, loss: 0.6865862011909485, acc: 0.67, recall: 0.6699999999999999, precision: 0.6717171717171717, f_beta: 0.6691729323308271
train: step: 3729, loss: 0.6879204511642456, acc: 0.59, recall: 0.59, precision: 0.5900360144057624, f_beta: 0.5899589958995899
train: step: 3730, loss: 0.6648997664451599, acc: 0.8, recall: 0.8, precision: 0.8125, f_beta: 0.797979797979798
train: step: 3731, loss: 0.6608026027679443, acc: 0.8, recall: 0.8, precision: 0.8043831168831169, f_beta: 0.7992773986350863
train: step: 3732, loss: 0.6801596879959106, acc: 0.61, recall: 0.61, precision: 0.6243781094527363, f_beta: 0.5983935742971889
train: step: 3733, loss: 0.6890155673027039, acc: 0.67, recall: 0.6699999999999999, precision: 0.6733986128110976, f_beta: 0.6683750376846548
train: step: 3734, loss: 0.6712640523910522, acc: 0.74, recall: 0.74, precision: 0.7604166666666667, f_beta: 0.7348021215830274
train: step: 3735, loss: 0.6723741292953491, acc: 0.72, recall: 0.72, precision: 0.7527573529411764, f_beta: 0.7106242248863166
train: step: 3736, loss: 0.672157883644104, acc: 0.67, recall: 0.6699999999999999, precision: 0.698690977092099, f_beta: 0.6576408341114224
train: step: 3737, loss: 0.6482571363449097, acc: 0.82, recall: 0.82, precision: 0.82, f_beta: 0.82
train: step: 3738, loss: 0.672179102897644, acc: 0.62, recall: 0.62, precision: 0.6207729468599035, f_beta: 0.6193910256410255
train: step: 3739, loss: 0.6741921901702881, acc: 0.8, recall: 0.8, precision: 0.8255208333333333, f_beta: 0.7960016319869441
train: step: 3740, loss: 0.6821611523628235, acc: 0.72, recall: 0.72, precision: 0.7203525641025641, f_beta: 0.7198879551820727
train: step: 3741, loss: 0.684506356716156, acc: 0.64, recall: 0.64, precision: 0.640224358974359, f_beta: 0.6398559423769508
train: step: 3742, loss: 0.6676271557807922, acc: 0.77, recall: 0.77, precision: 0.7790409260024803, f_beta: 0.7681217864704104
train: step: 3743, loss: 0.682654857635498, acc: 0.74, recall: 0.74, precision: 0.7435064935064934, f_beta: 0.7390606182256123
train: step: 3744, loss: 0.6630918383598328, acc: 0.79, recall: 0.79, precision: 0.7901160464185675, f_beta: 0.78997899789979
train: step: 3745, loss: 0.6609159708023071, acc: 0.79, recall: 0.79, precision: 0.7910477719791249, f_beta: 0.7898108297467722
train: step: 3746, loss: 0.6718301177024841, acc: 0.68, recall: 0.68, precision: 0.6910016977928692, f_beta: 0.6753246753246753
train: step: 3747, loss: 0.6785026788711548, acc: 0.64, recall: 0.64, precision: 0.6458333333333334, f_beta: 0.6363636363636365
train: step: 3748, loss: 0.6771717667579651, acc: 0.73, recall: 0.73, precision: 0.7308309915696507, f_beta: 0.7297567811029926
train: step: 3749, loss: 0.6857523918151855, acc: 0.63, recall: 0.63, precision: 0.6343530384456387, f_beta: 0.6269785260610949
train: step: 3750, loss: 0.667509913444519, acc: 0.68, recall: 0.68, precision: 0.7338877338877339, f_beta: 0.6604414261460101
train: step: 3751, loss: 0.668468713760376, acc: 0.75, recall: 0.75, precision: 0.8035454103933948, f_beta: 0.7384663667747673
train: step: 3752, loss: 0.6771696209907532, acc: 0.67, recall: 0.67, precision: 0.7156265854895991, f_beta: 0.6515679442508711
train: step: 3753, loss: 0.6727380156517029, acc: 0.76, recall: 0.76, precision: 0.7987132352941176, f_beta: 0.7519636213311285
train: step: 3754, loss: 0.6717780232429504, acc: 0.69, recall: 0.69, precision: 0.6919191919191919, f_beta: 0.6892230576441103
train: step: 3755, loss: 0.6609243750572205, acc: 0.73, recall: 0.73, precision: 0.7600633197648123, f_beta: 0.721964782205746
train: step: 3756, loss: 0.6658212542533875, acc: 0.69, recall: 0.69, precision: 0.7037752037752039, f_beta: 0.6846709388668497
train: step: 3757, loss: 0.6763514876365662, acc: 0.75, recall: 0.75, precision: 0.7583712277800745, f_beta: 0.7479584635547939
train: step: 3758, loss: 0.6703515648841858, acc: 0.76, recall: 0.76, precision: 0.7604166666666667, f_beta: 0.7599039615846339
train: step: 3759, loss: 0.6786844730377197, acc: 0.71, recall: 0.71, precision: 0.7170318313352625, f_beta: 0.7076318177235609
train: step: 3760, loss: 0.6676677465438843, acc: 0.68, recall: 0.6799999999999999, precision: 0.6826298701298701, f_beta: 0.6788438378161381
train: step: 3761, loss: 0.657372236251831, acc: 0.7, recall: 0.7, precision: 0.7741228070175439, f_beta: 0.6782496782496782
train: step: 3762, loss: 0.6470816731452942, acc: 0.77, recall: 0.77, precision: 0.7967032967032968, f_beta: 0.7647058823529411
train: step: 3763, loss: 0.6424443125724792, acc: 0.79, recall: 0.79, precision: 0.8186813186813187, f_beta: 0.7851662404092071
train: step: 3764, loss: 0.6635565757751465, acc: 0.68, recall: 0.6799999999999999, precision: 0.7467105263157895, f_beta: 0.6567996567996568
train: step: 3765, loss: 0.68743896484375, acc: 0.73, recall: 0.73, precision: 0.7300920368147259, f_beta: 0.72997299729973
train: step: 3766, loss: 0.6730417609214783, acc: 0.65, recall: 0.65, precision: 0.6505419510236852, f_beta: 0.6496847162446201
train: step: 3767, loss: 0.6326440572738647, acc: 0.81, recall: 0.81, precision: 0.8324753324753325, f_beta: 0.8067338012409724
train: step: 3768, loss: 0.6515056490898132, acc: 0.77, recall: 0.77, precision: 0.7709755118426336, f_beta: 0.769792813532179
train: step: 3769, loss: 0.6215360164642334, acc: 0.7, recall: 0.7, precision: 0.7083333333333333, f_beta: 0.6969696969696968
train: step: 3770, loss: 0.6215879321098328, acc: 0.78, recall: 0.78, precision: 0.780448717948718, f_beta: 0.7799119647859143
train: step: 3771, loss: 0.6182299256324768, acc: 0.72, recall: 0.72, precision: 0.7214170692431562, f_beta: 0.7195512820512822
train: step: 3772, loss: 0.6315956115722656, acc: 0.78, recall: 0.78, precision: 0.780448717948718, f_beta: 0.7799119647859143
train: step: 3773, loss: 0.6221795678138733, acc: 0.71, recall: 0.71, precision: 0.7170318313352625, f_beta: 0.7076318177235609
train: step: 3774, loss: 0.6595271825790405, acc: 0.75, recall: 0.75, precision: 0.7549979600163199, f_beta: 0.7487689679429204
train: step: 3775, loss: 0.6163171529769897, acc: 0.71, recall: 0.71, precision: 0.7100840336134454, f_beta: 0.7099709970997101
train: step: 3776, loss: 0.6741774082183838, acc: 0.61, recall: 0.61, precision: 0.6136833402232327, f_beta: 0.6068152031454783
train: step: 3777, loss: 0.674562394618988, acc: 0.6, recall: 0.6, precision: 0.6041666666666667, f_beta: 0.595959595959596
train: step: 3778, loss: 0.6176913976669312, acc: 0.78, recall: 0.78, precision: 0.78, f_beta: 0.78
train: step: 3779, loss: 0.5975126028060913, acc: 0.74, recall: 0.74, precision: 0.7857142857142857, f_beta: 0.7291666666666666
train: step: 3780, loss: 0.5913213491439819, acc: 0.76, recall: 0.76, precision: 0.7668308702791462, f_beta: 0.7584541062801933
train: step: 3781, loss: 0.5773840546607971, acc: 0.85, recall: 0.85, precision: 0.851264552388599, f_beta: 0.8498648783905516
train: step: 3782, loss: 0.5640634894371033, acc: 0.79, recall: 0.79, precision: 0.8047498949138294, f_beta: 0.7874278773155178
train: step: 3783, loss: 0.5851066708564758, acc: 0.79, recall: 0.79, precision: 0.8186813186813187, f_beta: 0.7851662404092071
train: step: 3784, loss: 0.608378529548645, acc: 0.76, recall: 0.76, precision: 0.8095238095238095, f_beta: 0.75
train: step: 3785, loss: 0.6249582767486572, acc: 0.74, recall: 0.74, precision: 0.7435064935064934, f_beta: 0.7390606182256123
train: step: 3786, loss: 0.6661787629127502, acc: 0.65, recall: 0.6499999999999999, precision: 0.669606512890095, f_beta: 0.6395839769333744
train: step: 3787, loss: 0.620585024356842, acc: 0.76, recall: 0.76, precision: 0.7758913412563667, f_beta: 0.7564935064935063
train: step: 3788, loss: 0.6655965447425842, acc: 0.58, recall: 0.58, precision: 0.5868055555555556, f_beta: 0.5716034271725826
train: step: 3789, loss: 0.6617374420166016, acc: 0.68, recall: 0.6799999999999999, precision: 0.7142857142857143, f_beta: 0.6666666666666667
train: step: 3790, loss: 0.590156614780426, acc: 0.77, recall: 0.77, precision: 0.7753977968176254, f_beta: 0.7688674505074867
train: step: 3791, loss: 0.6331719756126404, acc: 0.73, recall: 0.73, precision: 0.7416981925178646, f_beta: 0.7266929851199515
train: step: 3792, loss: 0.6581103801727295, acc: 0.73, recall: 0.73, precision: 0.7300920368147259, f_beta: 0.72997299729973
train: step: 3793, loss: 0.6477888226509094, acc: 0.71, recall: 0.71, precision: 0.7206809583858764, f_beta: 0.7064480210547626
train: step: 3794, loss: 0.6034544110298157, acc: 0.74, recall: 0.74, precision: 0.7546689303904923, f_beta: 0.7362012987012987
train: step: 3795, loss: 0.6018784046173096, acc: 0.7, recall: 0.7, precision: 0.7228163992869875, f_beta: 0.6921182266009853
train: step: 3796, loss: 0.6751306653022766, acc: 0.66, recall: 0.66, precision: 0.669779286926995, f_beta: 0.6550324675324675
train: step: 3797, loss: 0.6122244596481323, acc: 0.73, recall: 0.73, precision: 0.7688172043010753, f_beta: 0.7198879551820727
train: step: 3798, loss: 0.6639020442962646, acc: 0.63, recall: 0.63, precision: 0.6313131313131313, f_beta: 0.6290726817042607
train: step: 3799, loss: 0.573548436164856, acc: 0.8, recall: 0.8, precision: 0.8004807692307692, f_beta: 0.7999199679871949
train: step: 3800, loss: 0.6060208082199097, acc: 0.75, recall: 0.75, precision: 0.7525252525252526, f_beta: 0.7493734335839599
train: step: 3801, loss: 0.6441865563392639, acc: 0.65, recall: 0.6499999999999999, precision: 0.652998776009792, f_beta: 0.6482765551200884
train: step: 3802, loss: 0.5401437878608704, acc: 0.82, recall: 0.8200000000000001, precision: 0.8205128205128205, f_beta: 0.8199279711884755
train: step: 3803, loss: 0.6645603179931641, acc: 0.7, recall: 0.7, precision: 0.702922077922078, f_beta: 0.6989160979526294
train: step: 3804, loss: 0.6477087140083313, acc: 0.73, recall: 0.73, precision: 0.7466752466752467, f_beta: 0.7253585596582239
train: step: 3805, loss: 0.5956990122795105, acc: 0.79, recall: 0.79, precision: 0.7901160464185675, f_beta: 0.78997899789979
train: step: 3806, loss: 0.6606913208961487, acc: 0.8, recall: 0.8, precision: 0.8125, f_beta: 0.797979797979798
train: step: 3807, loss: 0.6284155249595642, acc: 0.78, recall: 0.78, precision: 0.780448717948718, f_beta: 0.7799119647859143
train: step: 3808, loss: 0.6290718913078308, acc: 0.67, recall: 0.6699999999999999, precision: 0.7561784207353828, f_beta: 0.6396986570586309
train: step: 3809, loss: 0.6738024950027466, acc: 0.66, recall: 0.66, precision: 0.6736111111111112, f_beta: 0.6532027743778049
train: step: 3810, loss: 0.6233266592025757, acc: 0.73, recall: 0.73, precision: 0.7323232323232323, f_beta: 0.7293233082706766
train: step: 3811, loss: 0.5296758413314819, acc: 0.81, recall: 0.81, precision: 0.8324753324753325, f_beta: 0.8067338012409724
train: step: 3812, loss: 0.6303404569625854, acc: 0.7, recall: 0.7, precision: 0.7170138888888888, f_beta: 0.6940024479804162
train: step: 3813, loss: 0.6430265307426453, acc: 0.74, recall: 0.74, precision: 0.75, f_beta: 0.7373737373737373
train: step: 3814, loss: 0.6198350787162781, acc: 0.71, recall: 0.71, precision: 0.737449118046133, f_beta: 0.7013695808876532
train: step: 3815, loss: 0.558775007724762, acc: 0.77, recall: 0.77, precision: 0.7753977968176254, f_beta: 0.7688674505074867
train: step: 3816, loss: 0.6045148372650146, acc: 0.76, recall: 0.76, precision: 0.7668308702791462, f_beta: 0.7584541062801933
train: step: 3817, loss: 0.6742722988128662, acc: 0.74, recall: 0.74, precision: 0.75, f_beta: 0.7373737373737373
train: step: 3818, loss: 0.6096981763839722, acc: 0.79, recall: 0.79, precision: 0.7901160464185675, f_beta: 0.78997899789979
train: step: 3819, loss: 0.5886492133140564, acc: 0.75, recall: 0.75, precision: 0.7501000400160064, f_beta: 0.7499749974997499
train: step: 3820, loss: 0.5893808603286743, acc: 0.75, recall: 0.75, precision: 0.750903251706142, f_beta: 0.7497747973175859
train: step: 3821, loss: 0.6662126183509827, acc: 0.66, recall: 0.66, precision: 0.6736111111111112, f_beta: 0.6532027743778049
train: step: 3822, loss: 0.591040849685669, acc: 0.77, recall: 0.77, precision: 0.7753977968176254, f_beta: 0.7688674505074867
train: step: 3823, loss: 0.6345199346542358, acc: 0.7, recall: 0.7, precision: 0.7170138888888888, f_beta: 0.6940024479804162
train: step: 3824, loss: 0.558035671710968, acc: 0.78, recall: 0.78, precision: 0.780448717948718, f_beta: 0.7799119647859143
train: step: 3825, loss: 0.5851316452026367, acc: 0.75, recall: 0.75, precision: 0.7501000400160064, f_beta: 0.7499749974997499
train: step: 3826, loss: 0.620130181312561, acc: 0.79, recall: 0.79, precision: 0.7929292929292929, f_beta: 0.7894736842105263
train: step: 3827, loss: 0.5792813301086426, acc: 0.82, recall: 0.8200000000000001, precision: 0.8246753246753247, f_beta: 0.8193496587715776
train: step: 3828, loss: 0.5803285241127014, acc: 0.78, recall: 0.78, precision: 0.78, f_beta: 0.78
train: step: 3829, loss: 0.49706727266311646, acc: 0.86, recall: 0.86, precision: 0.86, f_beta: 0.8599999999999999
train: step: 3830, loss: 0.6387012600898743, acc: 0.77, recall: 0.77, precision: 0.7727272727272727, f_beta: 0.7694235588972431
train: step: 3831, loss: 0.6225273609161377, acc: 0.76, recall: 0.76, precision: 0.7708333333333333, f_beta: 0.7575757575757576
train: step: 3832, loss: 0.6495001912117004, acc: 0.72, recall: 0.72, precision: 0.7214170692431562, f_beta: 0.7195512820512822
train: step: 3833, loss: 0.5677950382232666, acc: 0.85, recall: 0.85, precision: 0.851264552388599, f_beta: 0.8498648783905516
train: step: 3834, loss: 0.641791582107544, acc: 0.77, recall: 0.77, precision: 0.7709755118426336, f_beta: 0.769792813532179
train: step: 3835, loss: 0.571785032749176, acc: 0.78, recall: 0.78, precision: 0.780448717948718, f_beta: 0.7799119647859143
train: step: 3836, loss: 0.5633673667907715, acc: 0.82, recall: 0.8200000000000001, precision: 0.8205128205128205, f_beta: 0.8199279711884755
train: step: 3837, loss: 0.5866224765777588, acc: 0.8, recall: 0.8, precision: 0.8004807692307692, f_beta: 0.7999199679871949
train: step: 3838, loss: 0.6056739091873169, acc: 0.76, recall: 0.76, precision: 0.7708333333333333, f_beta: 0.7575757575757576
train: step: 3839, loss: 0.5750119686126709, acc: 0.78, recall: 0.78, precision: 0.780448717948718, f_beta: 0.7799119647859143
train: step: 3840, loss: 0.6237351298332214, acc: 0.72, recall: 0.72, precision: 0.7387152777777778, f_beta: 0.7144022847817217
train: step: 3841, loss: 0.5707030892372131, acc: 0.85, recall: 0.85, precision: 0.8501400560224089, f_beta: 0.84998499849985
train: step: 3842, loss: 0.5605624914169312, acc: 0.76, recall: 0.76, precision: 0.7616747181964574, f_beta: 0.7596153846153846
train: step: 3843, loss: 0.5713406205177307, acc: 0.79, recall: 0.79, precision: 0.7910477719791249, f_beta: 0.7898108297467722
train: step: 3844, loss: 0.5841591358184814, acc: 0.79, recall: 0.79, precision: 0.7901160464185675, f_beta: 0.78997899789979
train: step: 3845, loss: 0.5616450309753418, acc: 0.81, recall: 0.81, precision: 0.8161974704202366, f_beta: 0.8090644156366193
train: step: 3846, loss: 0.6249948740005493, acc: 0.7, recall: 0.7, precision: 0.7297794117647058, f_beta: 0.6899545266639107
train: step: 3847, loss: 0.5436879992485046, acc: 0.82, recall: 0.82, precision: 0.82, f_beta: 0.82
train: step: 3848, loss: 0.609918475151062, acc: 0.72, recall: 0.72, precision: 0.7203525641025641, f_beta: 0.7198879551820727
train: step: 3849, loss: 0.53677898645401, acc: 0.81, recall: 0.81, precision: 0.8111200321156162, f_beta: 0.8098288459613652
train: step: 3850, loss: 0.5873731374740601, acc: 0.79, recall: 0.79, precision: 0.7910477719791249, f_beta: 0.7898108297467722
train: step: 3851, loss: 0.6396318674087524, acc: 0.79, recall: 0.79, precision: 0.8047498949138294, f_beta: 0.7874278773155178
train: step: 3852, loss: 0.614162266254425, acc: 0.83, recall: 0.8300000000000001, precision: 0.8365973072215422, f_beta: 0.8291628982011858
train: step: 3853, loss: 0.6354076862335205, acc: 0.77, recall: 0.77, precision: 0.7709755118426336, f_beta: 0.769792813532179
train: step: 3854, loss: 0.5865914821624756, acc: 0.83, recall: 0.8300000000000001, precision: 0.8333333333333334, f_beta: 0.8295739348370927
train: step: 3855, loss: 0.6672300100326538, acc: 0.72, recall: 0.72, precision: 0.7203525641025641, f_beta: 0.7198879551820727
train: step: 3856, loss: 0.5876544713973999, acc: 0.77, recall: 0.77, precision: 0.7727272727272727, f_beta: 0.7694235588972431
train: step: 3857, loss: 0.6052321791648865, acc: 0.81, recall: 0.81, precision: 0.810124049619848, f_beta: 0.8099809980998101
train: step: 3858, loss: 0.5742557048797607, acc: 0.85, recall: 0.85, precision: 0.851264552388599, f_beta: 0.8498648783905516
train: step: 3859, loss: 0.5987838506698608, acc: 0.8, recall: 0.8, precision: 0.8, f_beta: 0.8000000000000002
train: step: 3860, loss: 0.5783628821372986, acc: 0.86, recall: 0.86, precision: 0.86, f_beta: 0.8599999999999999
train: step: 3861, loss: 0.5678637027740479, acc: 0.79, recall: 0.79, precision: 0.7910477719791249, f_beta: 0.7898108297467722
train: step: 3862, loss: 0.48162052035331726, acc: 0.9, recall: 0.9, precision: 0.9, f_beta: 0.9
train: step: 3863, loss: 0.5977579355239868, acc: 0.86, recall: 0.86, precision: 0.8605769230769231, f_beta: 0.8599439775910365
train: step: 3864, loss: 0.5937818884849548, acc: 0.8, recall: 0.8, precision: 0.8043831168831169, f_beta: 0.7992773986350863
train: step: 3865, loss: 0.6446695923805237, acc: 0.7, recall: 0.7, precision: 0.7380952380952381, f_beta: 0.6875
train: step: 3866, loss: 0.6130555868148804, acc: 0.8, recall: 0.8, precision: 0.8004807692307692, f_beta: 0.7999199679871949
train: step: 3867, loss: 0.549490213394165, acc: 0.81, recall: 0.81, precision: 0.8161974704202366, f_beta: 0.8090644156366193
train: step: 3868, loss: 0.6038212776184082, acc: 0.77, recall: 0.77, precision: 0.7701080432172869, f_beta: 0.7699769976997699
train: step: 3869, loss: 0.5906792283058167, acc: 0.82, recall: 0.82, precision: 0.8395585738539898, f_beta: 0.8173701298701298
train: step: 3870, loss: 0.6144307851791382, acc: 0.79, recall: 0.79, precision: 0.7910477719791249, f_beta: 0.7898108297467722
train: step: 3871, loss: 0.6464178562164307, acc: 0.78, recall: 0.78, precision: 0.7818035426731079, f_beta: 0.7796474358974359
train: step: 3872, loss: 0.6199869513511658, acc: 0.75, recall: 0.75, precision: 0.7501000400160064, f_beta: 0.7499749974997499
train: step: 3873, loss: 0.6417690515518188, acc: 0.73, recall: 0.73, precision: 0.7300920368147259, f_beta: 0.72997299729973
train: step: 3874, loss: 0.6440818309783936, acc: 0.68, recall: 0.68, precision: 0.6910016977928692, f_beta: 0.6753246753246753
train: step: 3875, loss: 0.6570742130279541, acc: 0.81, recall: 0.81, precision: 0.8111200321156162, f_beta: 0.8098288459613652
train: step: 3876, loss: 0.5807438492774963, acc: 0.75, recall: 0.75, precision: 0.7627154266498528, f_beta: 0.7469379491851402
train: step: 3877, loss: 0.6119372844696045, acc: 0.82, recall: 0.8200000000000001, precision: 0.8246753246753247, f_beta: 0.8193496587715776
train: step: 3878, loss: 0.6249210238456726, acc: 0.72, recall: 0.72, precision: 0.7232142857142857, f_beta: 0.7189883580891208
train: step: 3879, loss: 0.5887342691421509, acc: 0.77, recall: 0.77, precision: 0.7709755118426336, f_beta: 0.769792813532179
train: step: 3880, loss: 0.5980566740036011, acc: 0.74, recall: 0.74, precision: 0.7604166666666667, f_beta: 0.7348021215830274
train: step: 3881, loss: 0.5893219709396362, acc: 0.78, recall: 0.78, precision: 0.7873563218390804, f_beta: 0.7785829307568439
train: step: 3882, loss: 0.6424520611763, acc: 0.74, recall: 0.74, precision: 0.7415458937198067, f_beta: 0.7395833333333335
train: step: 3883, loss: 0.6601921916007996, acc: 0.65, recall: 0.65, precision: 0.6902587519025876, f_beta: 0.630450849963045
train: step: 3884, loss: 0.6722840666770935, acc: 0.65, recall: 0.65, precision: 0.7436647173489279, f_beta: 0.612789025334661
train: step: 3885, loss: 0.6239200830459595, acc: 0.77, recall: 0.77, precision: 0.8155680224403927, f_beta: 0.7613860358958398
train: step: 3886, loss: 0.6174985766410828, acc: 0.72, recall: 0.72, precision: 0.7728174603174603, f_beta: 0.7057587221521648
train: step: 3887, loss: 0.6804900169372559, acc: 0.64, recall: 0.64, precision: 0.6736111111111112, f_beta: 0.6216897856242118
train: step: 3888, loss: 0.657785177230835, acc: 0.63, recall: 0.63, precision: 0.7549019607843137, f_beta: 0.5783475783475783
train: step: 3889, loss: 0.6461276412010193, acc: 0.69, recall: 0.69, precision: 0.7533333333333334, f_beta: 0.6693333333333333
train: step: 3890, loss: 0.5827020406723022, acc: 0.75, recall: 0.75, precision: 0.7681252681252682, f_beta: 0.7457023700539112
train: step: 3891, loss: 0.5995251536369324, acc: 0.72, recall: 0.72, precision: 0.7728174603174603, f_beta: 0.7057587221521648
train: step: 3892, loss: 0.6337052583694458, acc: 0.69, recall: 0.69, precision: 0.7220663861617578, f_beta: 0.6783898744683057
train: step: 3893, loss: 0.6392912864685059, acc: 0.76, recall: 0.76, precision: 0.8095238095238095, f_beta: 0.75
train: step: 3894, loss: 0.6130529642105103, acc: 0.74, recall: 0.74, precision: 0.767379679144385, f_beta: 0.7331691297208538
train: step: 3895, loss: 0.6796810626983643, acc: 0.57, recall: 0.57, precision: 0.5988142292490118, f_beta: 0.5361881134721174
train: step: 3896, loss: 0.6796390414237976, acc: 0.64, recall: 0.64, precision: 0.681912681912682, f_beta: 0.6179966044142614
train: step: 3897, loss: 0.6843828558921814, acc: 0.61, recall: 0.61, precision: 0.6243781094527363, f_beta: 0.5983935742971889
train: step: 3898, loss: 0.5792894959449768, acc: 0.8, recall: 0.8, precision: 0.8255208333333333, f_beta: 0.7960016319869441
train: step: 3899, loss: 0.6118502616882324, acc: 0.77, recall: 0.77, precision: 0.805291723202171, f_beta: 0.7631551848419318
train: step: 3900, loss: 0.6892800331115723, acc: 0.67, recall: 0.67, precision: 0.6700680272108843, f_beta: 0.66996699669967
train: step: 3901, loss: 0.6317195296287537, acc: 0.68, recall: 0.68, precision: 0.7232142857142857, f_beta: 0.6637242538881883
train: step: 3902, loss: 0.6766126155853271, acc: 0.72, recall: 0.72, precision: 0.7387152777777778, f_beta: 0.7144022847817217
train: step: 3903, loss: 0.6281353235244751, acc: 0.74, recall: 0.74, precision: 0.7604166666666667, f_beta: 0.7348021215830274
train: step: 3904, loss: 0.6230233907699585, acc: 0.7, recall: 0.7, precision: 0.7380952380952381, f_beta: 0.6875
train: step: 3905, loss: 0.6199750304222107, acc: 0.72, recall: 0.72, precision: 0.7619047619047619, f_beta: 0.7083333333333334
train: step: 3906, loss: 0.6628069281578064, acc: 0.64, recall: 0.64, precision: 0.71875, f_beta: 0.6043956043956045
train: step: 3907, loss: 0.6067666411399841, acc: 0.76, recall: 0.76, precision: 0.7758913412563667, f_beta: 0.7564935064935063
train: step: 3908, loss: 0.6848580837249756, acc: 0.56, recall: 0.56, precision: 0.59375, f_beta: 0.5164835164835164
train: step: 3909, loss: 0.6298619508743286, acc: 0.68, recall: 0.6799999999999999, precision: 0.6875, f_beta: 0.6767676767676767
train: step: 3910, loss: 0.6754524111747742, acc: 0.72, recall: 0.72, precision: 0.7387152777777778, f_beta: 0.7144022847817217
train: step: 3911, loss: 0.6491356492042542, acc: 0.62, recall: 0.62, precision: 0.6644736842105263, f_beta: 0.5924495924495925
train: step: 3912, loss: 0.6073989272117615, acc: 0.82, recall: 0.8200000000000001, precision: 0.8333333333333334, f_beta: 0.8181818181818181
train: step: 3913, loss: 0.6004782915115356, acc: 0.83, recall: 0.8300000000000001, precision: 0.8311922922521076, f_beta: 0.8298468621759585
train: step: 3914, loss: 0.6402069926261902, acc: 0.79, recall: 0.79, precision: 0.7957976336189311, f_beta: 0.7889659330720531
train: step: 3915, loss: 0.6480361819267273, acc: 0.81, recall: 0.81, precision: 0.810124049619848, f_beta: 0.8099809980998101
train: step: 3916, loss: 0.5429131984710693, acc: 0.85, recall: 0.8500000000000001, precision: 0.8535353535353536, f_beta: 0.849624060150376
train: step: 3917, loss: 0.6165099740028381, acc: 0.71, recall: 0.71, precision: 0.7121212121212122, f_beta: 0.7092731829573935
train: step: 3918, loss: 0.5572389364242554, acc: 0.88, recall: 0.88, precision: 0.8806089743589745, f_beta: 0.879951980792317
train: step: 3919, loss: 0.6180797219276428, acc: 0.8, recall: 0.8, precision: 0.8, f_beta: 0.8000000000000002
train: step: 3920, loss: 0.5853366851806641, acc: 0.75, recall: 0.75, precision: 0.792192613370734, f_beta: 0.7406369955389562
train: step: 3921, loss: 0.6044118404388428, acc: 0.8, recall: 0.8, precision: 0.8078817733990147, f_beta: 0.7987117552334944
train: step: 3922, loss: 0.6324716806411743, acc: 0.77, recall: 0.77, precision: 0.7727272727272727, f_beta: 0.7694235588972431
train: step: 3923, loss: 0.5777491927146912, acc: 0.79, recall: 0.79, precision: 0.7910477719791249, f_beta: 0.7898108297467722
train: step: 3924, loss: 0.6258448362350464, acc: 0.73, recall: 0.73, precision: 0.7300920368147259, f_beta: 0.72997299729973
train: step: 3925, loss: 0.5043177604675293, acc: 0.85, recall: 0.8500000000000001, precision: 0.8535353535353536, f_beta: 0.849624060150376
train: step: 3926, loss: 0.6301854252815247, acc: 0.68, recall: 0.6799999999999999, precision: 0.681159420289855, f_beta: 0.6794871794871795
train: step: 3927, loss: 0.5478156805038452, acc: 0.86, recall: 0.86, precision: 0.86, f_beta: 0.8599999999999999
train: step: 3928, loss: 0.5880029201507568, acc: 0.77, recall: 0.77, precision: 0.7790409260024803, f_beta: 0.7681217864704104
train: step: 3929, loss: 0.5667015314102173, acc: 0.78, recall: 0.78, precision: 0.7818035426731079, f_beta: 0.7796474358974359
train: step: 3930, loss: 0.6052013039588928, acc: 0.72, recall: 0.72, precision: 0.7257799671592775, f_beta: 0.7181964573268922
train: step: 3931, loss: 0.5985293388366699, acc: 0.82, recall: 0.8200000000000001, precision: 0.8246753246753247, f_beta: 0.8193496587715776
train: step: 3932, loss: 0.6009522080421448, acc: 0.73, recall: 0.73, precision: 0.7377015295576685, f_beta: 0.7277951406391774
train: step: 3933, loss: 0.5890466570854187, acc: 0.8, recall: 0.8, precision: 0.8078817733990147, f_beta: 0.7987117552334944
train: step: 3934, loss: 0.6138684749603271, acc: 0.71, recall: 0.71, precision: 0.737449118046133, f_beta: 0.7013695808876532
train: step: 3935, loss: 0.573910117149353, acc: 0.79, recall: 0.79, precision: 0.7910477719791249, f_beta: 0.7898108297467722
train: step: 3936, loss: 0.6335095763206482, acc: 0.67, recall: 0.67, precision: 0.7156265854895991, f_beta: 0.6515679442508711
train: step: 3937, loss: 0.602782130241394, acc: 0.75, recall: 0.75, precision: 0.7549979600163199, f_beta: 0.7487689679429204
train: step: 3938, loss: 0.6555898189544678, acc: 0.7, recall: 0.7, precision: 0.7083333333333333, f_beta: 0.6969696969696968
train: step: 3939, loss: 0.4978204369544983, acc: 0.89, recall: 0.8899999999999999, precision: 0.8977968176254589, f_beta: 0.8894583458948848
train: step: 3940, loss: 0.6022160053253174, acc: 0.76, recall: 0.76, precision: 0.7708333333333333, f_beta: 0.7575757575757576
train: step: 3941, loss: 0.6026669144630432, acc: 0.76, recall: 0.76, precision: 0.7616747181964574, f_beta: 0.7596153846153846
train: step: 3942, loss: 0.584481954574585, acc: 0.77, recall: 0.77, precision: 0.7701080432172869, f_beta: 0.7699769976997699
train: step: 3943, loss: 0.6036784052848816, acc: 0.74, recall: 0.74, precision: 0.7546689303904923, f_beta: 0.7362012987012987
train: step: 3944, loss: 0.6356050968170166, acc: 0.79, recall: 0.79, precision: 0.7957976336189311, f_beta: 0.7889659330720531
train: step: 3945, loss: 0.6357591152191162, acc: 0.67, recall: 0.6699999999999999, precision: 0.6922207146087743, f_beta: 0.6601791782514674
train: step: 3946, loss: 0.640596866607666, acc: 0.73, recall: 0.73, precision: 0.7416981925178646, f_beta: 0.7266929851199515
train: step: 3947, loss: 0.5705491304397583, acc: 0.78, recall: 0.78, precision: 0.7840909090909092, f_beta: 0.779205138498595
train: step: 3948, loss: 0.583895206451416, acc: 0.82, recall: 0.8200000000000001, precision: 0.8246753246753247, f_beta: 0.8193496587715776
train: step: 3949, loss: 0.5948374271392822, acc: 0.74, recall: 0.74, precision: 0.7604166666666667, f_beta: 0.7348021215830274
train: step: 3950, loss: 0.6338313817977905, acc: 0.76, recall: 0.76, precision: 0.7821180555555556, f_beta: 0.755201958384333
train: step: 3951, loss: 0.5761452317237854, acc: 0.79, recall: 0.79, precision: 0.8279059249208502, f_beta: 0.7837503861600247
train: step: 3952, loss: 0.6855732202529907, acc: 0.62, recall: 0.62, precision: 0.6207729468599035, f_beta: 0.6193910256410255
train: step: 3953, loss: 0.615805447101593, acc: 0.77, recall: 0.77, precision: 0.7753977968176254, f_beta: 0.7688674505074867
train: step: 3954, loss: 0.6139031648635864, acc: 0.78, recall: 0.78, precision: 0.7873563218390804, f_beta: 0.7785829307568439
train: step: 3955, loss: 0.6120513677597046, acc: 0.74, recall: 0.74, precision: 0.7463054187192117, f_beta: 0.7383252818035426
train: step: 3956, loss: 0.6032691597938538, acc: 0.76, recall: 0.76, precision: 0.7604166666666667, f_beta: 0.7599039615846339
train: step: 3957, loss: 0.5809752941131592, acc: 0.82, recall: 0.8200000000000001, precision: 0.8205128205128205, f_beta: 0.8199279711884755
train: step: 3958, loss: 0.573006272315979, acc: 0.84, recall: 0.8400000000000001, precision: 0.8489326765188834, f_beta: 0.8389694041867956
train: step: 3959, loss: 0.6171273589134216, acc: 0.74, recall: 0.74, precision: 0.74, f_beta: 0.74
train: step: 3960, loss: 0.5854324698448181, acc: 0.83, recall: 0.8300000000000001, precision: 0.8311922922521076, f_beta: 0.8298468621759585
train: step: 3961, loss: 0.5408537983894348, acc: 0.84, recall: 0.8400000000000001, precision: 0.8421900161030595, f_beta: 0.8397435897435896
train: step: 3962, loss: 0.5182747840881348, acc: 0.89, recall: 0.89, precision: 0.89015606242497, f_beta: 0.88998899889989
train: step: 3963, loss: 0.6438071727752686, acc: 0.66, recall: 0.66, precision: 0.6602564102564102, f_beta: 0.6598639455782312
train: step: 3964, loss: 0.6084226369857788, acc: 0.8, recall: 0.8, precision: 0.8078817733990147, f_beta: 0.7987117552334944
train: step: 3965, loss: 0.567371666431427, acc: 0.85, recall: 0.8500000000000001, precision: 0.8535353535353536, f_beta: 0.849624060150376
train: step: 3966, loss: 0.5243571400642395, acc: 0.81, recall: 0.81, precision: 0.8203803224472923, f_beta: 0.8084484323016432
train: step: 3967, loss: 0.5525694489479065, acc: 0.83, recall: 0.83, precision: 0.8301320528211285, f_beta: 0.8299829982998299
train: step: 3968, loss: 0.5756455659866333, acc: 0.75, recall: 0.75, precision: 0.7583712277800745, f_beta: 0.7479584635547939
train: step: 3969, loss: 0.6232339143753052, acc: 0.77, recall: 0.77, precision: 0.7837326607818411, f_beta: 0.767182913250329
train: step: 3970, loss: 0.586311936378479, acc: 0.74, recall: 0.74, precision: 0.7546689303904923, f_beta: 0.7362012987012987
train: step: 3971, loss: 0.5414613485336304, acc: 0.85, recall: 0.85, precision: 0.851264552388599, f_beta: 0.8498648783905516
train: step: 3972, loss: 0.5861241817474365, acc: 0.78, recall: 0.78, precision: 0.7818035426731079, f_beta: 0.7796474358974359
train: step: 3973, loss: 0.5778587460517883, acc: 0.83, recall: 0.83, precision: 0.853925353925354, f_beta: 0.8270776116366596
train: step: 3974, loss: 0.6501410007476807, acc: 0.8, recall: 0.8, precision: 0.8043831168831169, f_beta: 0.7992773986350863
train: step: 3975, loss: 0.5862439870834351, acc: 0.79, recall: 0.79, precision: 0.8047498949138294, f_beta: 0.7874278773155178
train: step: 3976, loss: 0.5369662642478943, acc: 0.79, recall: 0.79, precision: 0.8047498949138294, f_beta: 0.7874278773155178
train: step: 3977, loss: 0.629097580909729, acc: 0.7, recall: 0.7, precision: 0.7003205128205128, f_beta: 0.6998799519807923
train: step: 3978, loss: 0.601412296295166, acc: 0.72, recall: 0.72, precision: 0.7257799671592775, f_beta: 0.7181964573268922
train: step: 3979, loss: 0.5780790448188782, acc: 0.83, recall: 0.83, precision: 0.8301320528211285, f_beta: 0.8299829982998299
train: step: 3980, loss: 0.5712466239929199, acc: 0.82, recall: 0.82, precision: 0.82, f_beta: 0.82
train: step: 3981, loss: 0.6321502923965454, acc: 0.77, recall: 0.77, precision: 0.7790409260024803, f_beta: 0.7681217864704104
train: step: 3982, loss: 0.6681816577911377, acc: 0.72, recall: 0.72, precision: 0.7214170692431562, f_beta: 0.7195512820512822
train: step: 3983, loss: 0.5524250864982605, acc: 0.83, recall: 0.8300000000000001, precision: 0.8311922922521076, f_beta: 0.8298468621759585
train: step: 3984, loss: 0.5654574036598206, acc: 0.78, recall: 0.78, precision: 0.78, f_beta: 0.78
train: step: 3985, loss: 0.47395041584968567, acc: 0.91, recall: 0.9099999999999999, precision: 0.9114813327980731, f_beta: 0.9099189270343309
train: step: 3986, loss: 0.6432018876075745, acc: 0.68, recall: 0.6799999999999999, precision: 0.681159420289855, f_beta: 0.6794871794871795
train: step: 3987, loss: 0.5386084914207458, acc: 0.8, recall: 0.8, precision: 0.8004807692307692, f_beta: 0.7999199679871949
train: step: 3988, loss: 0.5786961317062378, acc: 0.84, recall: 0.8400000000000001, precision: 0.8449675324675325, f_beta: 0.839421918908069
train: step: 3989, loss: 0.5917713046073914, acc: 0.76, recall: 0.76, precision: 0.7708333333333333, f_beta: 0.7575757575757576
train: step: 3990, loss: 0.5352819561958313, acc: 0.82, recall: 0.8200000000000001, precision: 0.822061191626409, f_beta: 0.8197115384615385
train: step: 3991, loss: 0.5544397830963135, acc: 0.82, recall: 0.8200000000000001, precision: 0.8246753246753247, f_beta: 0.8193496587715776
train: step: 3992, loss: 0.5195810794830322, acc: 0.85, recall: 0.85, precision: 0.851264552388599, f_beta: 0.8498648783905516
train: step: 3993, loss: 0.5916265845298767, acc: 0.79, recall: 0.79, precision: 0.7929292929292929, f_beta: 0.7894736842105263
train: step: 3994, loss: 0.5273585319519043, acc: 0.8, recall: 0.8, precision: 0.8078817733990147, f_beta: 0.7987117552334944
train: step: 3995, loss: 0.6638818979263306, acc: 0.65, recall: 0.65, precision: 0.6500600240096038, f_beta: 0.6499649964996499
train: step: 3996, loss: 0.6616315245628357, acc: 0.79, recall: 0.79, precision: 0.7910477719791249, f_beta: 0.7898108297467722
train: step: 3997, loss: 0.6678740978240967, acc: 0.59, recall: 0.5900000000000001, precision: 0.5930136420008267, f_beta: 0.5866518802298619
train: step: 3998, loss: 0.5964319109916687, acc: 0.76, recall: 0.76, precision: 0.7604166666666667, f_beta: 0.7599039615846339
train: step: 3999, loss: 0.5970562100410461, acc: 0.77, recall: 0.77, precision: 0.7895752895752897, f_beta: 0.7660461804495982
train: step: 4000, loss: 0.5885974168777466, acc: 0.75, recall: 0.75, precision: 0.7525252525252526, f_beta: 0.7493734335839599
train: step: 4001, loss: 0.6176862120628357, acc: 0.73, recall: 0.73, precision: 0.7323232323232323, f_beta: 0.7293233082706766
train: step: 4002, loss: 0.5831356644630432, acc: 0.76, recall: 0.76, precision: 0.7758913412563667, f_beta: 0.7564935064935063
train: step: 4003, loss: 0.5911847949028015, acc: 0.75, recall: 0.75, precision: 0.750903251706142, f_beta: 0.7497747973175859
train: step: 4004, loss: 0.5680831670761108, acc: 0.79, recall: 0.79, precision: 0.7910477719791249, f_beta: 0.7898108297467722
train: step: 4005, loss: 0.6019593477249146, acc: 0.73, recall: 0.73, precision: 0.7300920368147259, f_beta: 0.72997299729973
train: step: 4006, loss: 0.6102054715156555, acc: 0.71, recall: 0.71, precision: 0.7121212121212122, f_beta: 0.7092731829573935
train: step: 4007, loss: 0.6501405239105225, acc: 0.75, recall: 0.75, precision: 0.7583712277800745, f_beta: 0.7479584635547939
train: step: 4008, loss: 0.6583120822906494, acc: 0.75, recall: 0.75, precision: 0.7627154266498528, f_beta: 0.7469379491851402
train: step: 4009, loss: 0.5465747714042664, acc: 0.82, recall: 0.8200000000000001, precision: 0.8246753246753247, f_beta: 0.8193496587715776
train: step: 4010, loss: 0.6146133542060852, acc: 0.86, recall: 0.86, precision: 0.8605769230769231, f_beta: 0.8599439775910365
train: step: 4011, loss: 0.5839354991912842, acc: 0.75, recall: 0.75, precision: 0.7501000400160064, f_beta: 0.7499749974997499
train: step: 4012, loss: 0.611092746257782, acc: 0.74, recall: 0.74, precision: 0.7463054187192117, f_beta: 0.7383252818035426
train: step: 4013, loss: 0.5421515703201294, acc: 0.88, recall: 0.88, precision: 0.8806089743589745, f_beta: 0.879951980792317
train: step: 4014, loss: 0.5497785806655884, acc: 0.8, recall: 0.8, precision: 0.8125, f_beta: 0.797979797979798
train: step: 4015, loss: 0.5234003663063049, acc: 0.85, recall: 0.8500000000000001, precision: 0.8569971440228478, f_beta: 0.8492613807657521
train: step: 4016, loss: 0.543569803237915, acc: 0.8, recall: 0.8, precision: 0.8342245989304813, f_beta: 0.794745484400657
train: step: 4017, loss: 0.5355064868927002, acc: 0.78, recall: 0.78, precision: 0.8038194444444444, f_beta: 0.7756017951856385
train: step: 4018, loss: 0.6171992421150208, acc: 0.74, recall: 0.74, precision: 0.7463054187192117, f_beta: 0.7383252818035426
train: step: 4019, loss: 0.5998517870903015, acc: 0.79, recall: 0.79, precision: 0.7997106242248864, f_beta: 0.7882851093860268
train: step: 4020, loss: 0.6483421325683594, acc: 0.78, recall: 0.78, precision: 0.8472222222222222, f_beta: 0.7688104245481295
train: step: 4021, loss: 0.5332162380218506, acc: 0.82, recall: 0.82, precision: 0.8395585738539898, f_beta: 0.8173701298701298
train: step: 4022, loss: 0.6235740780830383, acc: 0.76, recall: 0.76, precision: 0.7637987012987013, f_beta: 0.7591328783621035
train: step: 4023, loss: 0.5693967342376709, acc: 0.83, recall: 0.8300000000000001, precision: 0.8333333333333334, f_beta: 0.8295739348370927
train: step: 4024, loss: 0.633233904838562, acc: 0.78, recall: 0.78, precision: 0.7840909090909092, f_beta: 0.779205138498595
train: step: 4025, loss: 0.5407627820968628, acc: 0.81, recall: 0.81, precision: 0.8257671290458175, f_beta: 0.8076728413807065
train: step: 4026, loss: 0.5468192100524902, acc: 0.81, recall: 0.81, precision: 0.8131313131313131, f_beta: 0.8095238095238095
train: step: 4027, loss: 0.6081146597862244, acc: 0.79, recall: 0.79, precision: 0.7910477719791249, f_beta: 0.7898108297467722
train: step: 4028, loss: 0.6121677160263062, acc: 0.77, recall: 0.77, precision: 0.7701080432172869, f_beta: 0.7699769976997699
train: step: 4029, loss: 0.558427095413208, acc: 0.86, recall: 0.86, precision: 0.8694581280788177, f_beta: 0.8590982286634461
train: step: 4030, loss: 0.5230518579483032, acc: 0.85, recall: 0.85, precision: 0.851264552388599, f_beta: 0.8498648783905516
train: step: 4031, loss: 0.6292679309844971, acc: 0.74, recall: 0.74, precision: 0.7403846153846154, f_beta: 0.7398959583833533
train: step: 4032, loss: 0.5950922966003418, acc: 0.77, recall: 0.77, precision: 0.7790409260024803, f_beta: 0.7681217864704104
train: step: 4033, loss: 0.5993404984474182, acc: 0.77, recall: 0.77, precision: 0.7967032967032968, f_beta: 0.7647058823529411
train: step: 4034, loss: 0.604236900806427, acc: 0.73, recall: 0.73, precision: 0.7600633197648123, f_beta: 0.721964782205746
train: step: 4035, loss: 0.6093794107437134, acc: 0.74, recall: 0.74, precision: 0.74, f_beta: 0.74
train: step: 4036, loss: 0.571861207485199, acc: 0.75, recall: 0.75, precision: 0.7549979600163199, f_beta: 0.7487689679429204
train: step: 4037, loss: 0.5952733755111694, acc: 0.73, recall: 0.73, precision: 0.7323232323232323, f_beta: 0.7293233082706766
train: step: 4038, loss: 0.6315808296203613, acc: 0.7, recall: 0.7, precision: 0.7052545155993433, f_beta: 0.6980676328502415
train: step: 4039, loss: 0.5917227268218994, acc: 0.81, recall: 0.81, precision: 0.8257671290458175, f_beta: 0.8076728413807065
train: step: 4040, loss: 0.5803577303886414, acc: 0.79, recall: 0.79, precision: 0.7957976336189311, f_beta: 0.7889659330720531
train: step: 4041, loss: 0.6202052235603333, acc: 0.79, recall: 0.79, precision: 0.7997106242248864, f_beta: 0.7882851093860268
train: step: 4042, loss: 0.641247034072876, acc: 0.72, recall: 0.72, precision: 0.7214170692431562, f_beta: 0.7195512820512822
train: step: 4043, loss: 0.5409639477729797, acc: 0.82, recall: 0.8200000000000001, precision: 0.8246753246753247, f_beta: 0.8193496587715776
train: step: 4044, loss: 0.5726891756057739, acc: 0.8, recall: 0.8, precision: 0.8255208333333333, f_beta: 0.7960016319869441
train: step: 4045, loss: 0.6002182364463806, acc: 0.71, recall: 0.71, precision: 0.7252252252252251, f_beta: 0.7050147492625368
train: step: 4046, loss: 0.623603343963623, acc: 0.81, recall: 0.81, precision: 0.8406593406593407, f_beta: 0.8056265984654732
train: step: 4047, loss: 0.5260848999023438, acc: 0.84, recall: 0.8400000000000001, precision: 0.8787878787878788, f_beta: 0.8357963875205254
train: step: 4048, loss: 0.5834989547729492, acc: 0.79, recall: 0.79, precision: 0.7901160464185675, f_beta: 0.78997899789979
train: step: 4049, loss: 0.6718428730964661, acc: 0.85, recall: 0.85, precision: 0.8846153846153846, f_beta: 0.8465473145780051
train: step: 4050, loss: 0.6006015539169312, acc: 0.71, recall: 0.71, precision: 0.78, f_beta: 0.6906666666666665
train: step: 4051, loss: 0.6566987633705139, acc: 0.66, recall: 0.6599999999999999, precision: 0.6623376623376623, f_beta: 0.6587715776796468
train: step: 4052, loss: 0.6323816180229187, acc: 0.74, recall: 0.74, precision: 0.7857142857142857, f_beta: 0.7291666666666666
train: step: 4053, loss: 0.5977270007133484, acc: 0.73, recall: 0.73, precision: 0.7600633197648123, f_beta: 0.721964782205746
train: step: 4054, loss: 0.6451858282089233, acc: 0.65, recall: 0.65, precision: 0.6821272462360368, f_beta: 0.6338529134846741
train: step: 4055, loss: 0.5950186252593994, acc: 0.77, recall: 0.77, precision: 0.7837326607818411, f_beta: 0.767182913250329
train: step: 4056, loss: 0.5981385707855225, acc: 0.77, recall: 0.77, precision: 0.7895752895752897, f_beta: 0.7660461804495982
train: step: 4057, loss: 0.6250996589660645, acc: 0.76, recall: 0.76, precision: 0.7987132352941176, f_beta: 0.7519636213311285
train: step: 4058, loss: 0.6362700462341309, acc: 0.73, recall: 0.73, precision: 0.7792617775619233, f_beta: 0.7175436761167486
train: step: 4059, loss: 0.615490734577179, acc: 0.79, recall: 0.79, precision: 0.811025311025311, f_beta: 0.7863899908452854
train: step: 4060, loss: 0.6387918591499329, acc: 0.67, recall: 0.6699999999999999, precision: 0.7399774138904573, f_beta: 0.6440513428972063
train: step: 4061, loss: 0.613602876663208, acc: 0.71, recall: 0.71, precision: 0.7454417952314165, f_beta: 0.6991389148251893
train: step: 4062, loss: 0.6301922798156738, acc: 0.66, recall: 0.6599999999999999, precision: 0.75, f_beta: 0.6263736263736264
train: step: 4063, loss: 0.5932313799858093, acc: 0.78, recall: 0.78, precision: 0.8119429590017826, f_beta: 0.7742200328407225
train: step: 4064, loss: 0.5929723978042603, acc: 0.72, recall: 0.72, precision: 0.7527573529411764, f_beta: 0.7106242248863166
train: step: 4065, loss: 0.6228070855140686, acc: 0.73, recall: 0.73, precision: 0.7466752466752467, f_beta: 0.7253585596582239
train: step: 4066, loss: 0.580088198184967, acc: 0.74, recall: 0.74, precision: 0.8118503118503119, f_beta: 0.7241086587436332
train: step: 4067, loss: 0.5991695523262024, acc: 0.73, recall: 0.73, precision: 0.7688172043010753, f_beta: 0.7198879551820727
train: step: 4068, loss: 0.5396673083305359, acc: 0.8, recall: 0.8, precision: 0.8183361629881154, f_beta: 0.797077922077922
train: step: 4069, loss: 0.6334245204925537, acc: 0.74, recall: 0.74, precision: 0.7604166666666667, f_beta: 0.7348021215830274
train: step: 4070, loss: 0.5825633406639099, acc: 0.76, recall: 0.76, precision: 0.7604166666666667, f_beta: 0.7599039615846339
train: step: 4071, loss: 0.5613271594047546, acc: 0.85, recall: 0.8500000000000001, precision: 0.8535353535353536, f_beta: 0.849624060150376
train: step: 4072, loss: 0.5918344259262085, acc: 0.77, recall: 0.77, precision: 0.7895752895752897, f_beta: 0.7660461804495982
train: step: 4073, loss: 0.5737576484680176, acc: 0.82, recall: 0.82, precision: 0.8395585738539898, f_beta: 0.8173701298701298
train: step: 4074, loss: 0.5198231339454651, acc: 0.89, recall: 0.89, precision: 0.9098360655737705, f_beta: 0.8886526976414617
train: step: 4075, loss: 0.5730856657028198, acc: 0.77, recall: 0.77, precision: 0.8278290432248665, f_beta: 0.7593890574327858
train: step: 4076, loss: 0.5426061749458313, acc: 0.85, recall: 0.85, precision: 0.8617197188921042, f_beta: 0.8487750781328762
train: step: 4077, loss: 0.6045854091644287, acc: 0.75, recall: 0.75, precision: 0.7826775214834916, f_beta: 0.7425599835238389
train: step: 4078, loss: 0.49270275235176086, acc: 0.86, recall: 0.86, precision: 0.875, f_beta: 0.8585858585858586
train: step: 4079, loss: 0.5388649106025696, acc: 0.83, recall: 0.8300000000000001, precision: 0.8410500206696983, f_beta: 0.8286117552172598
train: step: 4080, loss: 0.5809568166732788, acc: 0.83, recall: 0.83, precision: 0.8301320528211285, f_beta: 0.8299829982998299
train: step: 4081, loss: 0.662943422794342, acc: 0.69, recall: 0.69, precision: 0.693798449612403, f_beta: 0.6884735202492211
train: step: 4082, loss: 0.6061609387397766, acc: 0.79, recall: 0.79, precision: 0.8279059249208502, f_beta: 0.7837503861600247
train: step: 4083, loss: 0.6103954315185547, acc: 0.81, recall: 0.81, precision: 0.810124049619848, f_beta: 0.8099809980998101
train: step: 4084, loss: 0.5987624526023865, acc: 0.77, recall: 0.77, precision: 0.7967032967032968, f_beta: 0.7647058823529411
train: step: 4085, loss: 0.6256622076034546, acc: 0.7, recall: 0.7, precision: 0.748015873015873, f_beta: 0.6847414880201765
train: step: 4086, loss: 0.5487152934074402, acc: 0.84, recall: 0.84, precision: 0.8541666666666667, f_beta: 0.8383838383838385
train: step: 4087, loss: 0.5776363015174866, acc: 0.78, recall: 0.78, precision: 0.780448717948718, f_beta: 0.7799119647859143
train: step: 4088, loss: 0.6127561926841736, acc: 0.77, recall: 0.77, precision: 0.7701080432172869, f_beta: 0.7699769976997699
train: step: 4089, loss: 0.6841144561767578, acc: 0.74, recall: 0.74, precision: 0.75, f_beta: 0.7373737373737373
train: step: 4090, loss: 0.6121422648429871, acc: 0.81, recall: 0.81, precision: 0.8203803224472923, f_beta: 0.8084484323016432
train: step: 4091, loss: 0.5799872875213623, acc: 0.78, recall: 0.78, precision: 0.7873563218390804, f_beta: 0.7785829307568439
train: step: 4092, loss: 0.6015560030937195, acc: 0.75, recall: 0.75, precision: 0.7583712277800745, f_beta: 0.7479584635547939
train: step: 4093, loss: 0.5463157296180725, acc: 0.83, recall: 0.8300000000000001, precision: 0.8365973072215422, f_beta: 0.8291628982011858
train: step: 4094, loss: 0.6306784749031067, acc: 0.78, recall: 0.78, precision: 0.7818035426731079, f_beta: 0.7796474358974359
train: step: 4095, loss: 0.6176804900169373, acc: 0.78, recall: 0.78, precision: 0.780448717948718, f_beta: 0.7799119647859143
train: step: 4096, loss: 0.6318403482437134, acc: 0.8, recall: 0.8, precision: 0.8078817733990147, f_beta: 0.7987117552334944
train: step: 4097, loss: 0.5305280685424805, acc: 0.81, recall: 0.81, precision: 0.8111200321156162, f_beta: 0.8098288459613652
train: step: 4098, loss: 0.5891180038452148, acc: 0.75, recall: 0.75, precision: 0.7627154266498528, f_beta: 0.7469379491851402
train: step: 4099, loss: 0.6108250617980957, acc: 0.74, recall: 0.74, precision: 0.767379679144385, f_beta: 0.7331691297208538
train: step: 4100, loss: 0.6426323056221008, acc: 0.73, recall: 0.73, precision: 0.7300920368147259, f_beta: 0.72997299729973
train: step: 4101, loss: 0.5919056534767151, acc: 0.73, recall: 0.73, precision: 0.7300920368147259, f_beta: 0.72997299729973
train: step: 4102, loss: 0.6121988296508789, acc: 0.74, recall: 0.74, precision: 0.7546689303904923, f_beta: 0.7362012987012987
train: step: 4103, loss: 0.6084088683128357, acc: 0.8, recall: 0.8, precision: 0.8019323671497585, f_beta: 0.7996794871794872
train: step: 4104, loss: 0.5640003085136414, acc: 0.85, recall: 0.8500000000000001, precision: 0.8569971440228478, f_beta: 0.8492613807657521
train: step: 4105, loss: 0.6177962422370911, acc: 0.76, recall: 0.76, precision: 0.7604166666666667, f_beta: 0.7599039615846339
train: step: 4106, loss: 0.5648487210273743, acc: 0.77, recall: 0.77, precision: 0.7701080432172869, f_beta: 0.7699769976997699
train: step: 4107, loss: 0.5656285285949707, acc: 0.76, recall: 0.76, precision: 0.7616747181964574, f_beta: 0.7596153846153846
train: step: 4108, loss: 0.5757300853729248, acc: 0.83, recall: 0.8300000000000001, precision: 0.8333333333333334, f_beta: 0.8295739348370927
train: step: 4109, loss: 0.5802368521690369, acc: 0.83, recall: 0.8300000000000001, precision: 0.8365973072215422, f_beta: 0.8291628982011858
train: step: 4110, loss: 0.5555837154388428, acc: 0.85, recall: 0.8500000000000001, precision: 0.8569971440228478, f_beta: 0.8492613807657521
train: step: 4111, loss: 0.6091051697731018, acc: 0.77, recall: 0.77, precision: 0.7895752895752897, f_beta: 0.7660461804495982
train: step: 4112, loss: 0.6875343322753906, acc: 0.72, recall: 0.72, precision: 0.7232142857142857, f_beta: 0.7189883580891208
train: step: 4113, loss: 0.5696929693222046, acc: 0.77, recall: 0.77, precision: 0.7895752895752897, f_beta: 0.7660461804495982
train: step: 4114, loss: 0.6323248147964478, acc: 0.72, recall: 0.72, precision: 0.7203525641025641, f_beta: 0.7198879551820727
train: step: 4115, loss: 0.4892749786376953, acc: 0.86, recall: 0.86, precision: 0.86, f_beta: 0.8599999999999999
train: step: 4116, loss: 0.5836619734764099, acc: 0.78, recall: 0.78, precision: 0.8038194444444444, f_beta: 0.7756017951856385
train: step: 4117, loss: 0.588360071182251, acc: 0.79, recall: 0.79, precision: 0.7929292929292929, f_beta: 0.7894736842105263
train: step: 4118, loss: 0.5714095234870911, acc: 0.84, recall: 0.84, precision: 0.8405448717948718, f_beta: 0.8399359743897559
train: step: 4119, loss: 0.613720715045929, acc: 0.76, recall: 0.76, precision: 0.7604166666666667, f_beta: 0.7599039615846339
train: step: 4120, loss: 0.553693413734436, acc: 0.84, recall: 0.84, precision: 0.8405448717948718, f_beta: 0.8399359743897559
train: step: 4121, loss: 0.6469228863716125, acc: 0.77, recall: 0.77, precision: 0.7790409260024803, f_beta: 0.7681217864704104
train: step: 4122, loss: 0.5341808199882507, acc: 0.81, recall: 0.81, precision: 0.8161974704202366, f_beta: 0.8090644156366193
train: step: 4123, loss: 0.5090910792350769, acc: 0.86, recall: 0.86, precision: 0.8605769230769231, f_beta: 0.8599439775910365
train: step: 4124, loss: 0.6139647960662842, acc: 0.74, recall: 0.74, precision: 0.7403846153846154, f_beta: 0.7398959583833533
train: step: 4125, loss: 0.608875572681427, acc: 0.72, recall: 0.72, precision: 0.7203525641025641, f_beta: 0.7198879551820727
train: step: 4126, loss: 0.5331485867500305, acc: 0.8, recall: 0.8, precision: 0.8043831168831169, f_beta: 0.7992773986350863
train: step: 4127, loss: 0.6073476672172546, acc: 0.8, recall: 0.8, precision: 0.8183361629881154, f_beta: 0.797077922077922
train: step: 4128, loss: 0.6096920371055603, acc: 0.73, recall: 0.73, precision: 0.7466752466752467, f_beta: 0.7253585596582239
train: step: 4129, loss: 0.5706520080566406, acc: 0.77, recall: 0.77, precision: 0.7895752895752897, f_beta: 0.7660461804495982
train: step: 4130, loss: 0.5433984398841858, acc: 0.8, recall: 0.8, precision: 0.8183361629881154, f_beta: 0.797077922077922
train: step: 4131, loss: 0.6087717413902283, acc: 0.77, recall: 0.77, precision: 0.7753977968176254, f_beta: 0.7688674505074867
train: step: 4132, loss: 0.4733981788158417, acc: 0.86, recall: 0.86, precision: 0.8694581280788177, f_beta: 0.8590982286634461
train: step: 4133, loss: 0.6627110838890076, acc: 0.63, recall: 0.63, precision: 0.6300520208083233, f_beta: 0.6299629962996299
train: step: 4134, loss: 0.6219488978385925, acc: 0.79, recall: 0.79, precision: 0.8186813186813187, f_beta: 0.7851662404092071
train: step: 4135, loss: 0.5731161236763, acc: 0.76, recall: 0.76, precision: 0.7708333333333333, f_beta: 0.7575757575757576
train: step: 4136, loss: 0.5221348404884338, acc: 0.85, recall: 0.85, precision: 0.867801597309794, f_beta: 0.848162769511084
train: step: 4137, loss: 0.6026401519775391, acc: 0.76, recall: 0.76, precision: 0.7708333333333333, f_beta: 0.7575757575757576
train: step: 4138, loss: 0.6183502674102783, acc: 0.75, recall: 0.75, precision: 0.7747252747252746, f_beta: 0.7442455242966751
train: step: 4139, loss: 0.6555748581886292, acc: 0.66, recall: 0.66, precision: 0.6602564102564102, f_beta: 0.6598639455782312
train: step: 4140, loss: 0.6126788258552551, acc: 0.78, recall: 0.78, precision: 0.8119429590017826, f_beta: 0.7742200328407225
train: step: 4141, loss: 0.6035698056221008, acc: 0.77, recall: 0.77, precision: 0.7895752895752897, f_beta: 0.7660461804495982
train: step: 4142, loss: 0.5774160027503967, acc: 0.8, recall: 0.8, precision: 0.8078817733990147, f_beta: 0.7987117552334944
train: step: 4143, loss: 0.5600478649139404, acc: 0.79, recall: 0.79, precision: 0.7929292929292929, f_beta: 0.7894736842105263
train: step: 4144, loss: 0.5375978350639343, acc: 0.85, recall: 0.85, precision: 0.8501400560224089, f_beta: 0.84998499849985
train: step: 4145, loss: 0.5874587893486023, acc: 0.78, recall: 0.78, precision: 0.7916666666666666, f_beta: 0.7777777777777779
train: step: 4146, loss: 0.5902848839759827, acc: 0.76, recall: 0.76, precision: 0.7896613190730838, f_beta: 0.7536945812807881
train: step: 4147, loss: 0.5412854552268982, acc: 0.81, recall: 0.81, precision: 0.8111200321156162, f_beta: 0.8098288459613652
train: step: 4148, loss: 0.5766887068748474, acc: 0.81, recall: 0.81, precision: 0.8111200321156162, f_beta: 0.8098288459613652
train: step: 4149, loss: 0.6057508587837219, acc: 0.78, recall: 0.78, precision: 0.7916666666666666, f_beta: 0.7777777777777779
train: step: 4150, loss: 0.6312059760093689, acc: 0.74, recall: 0.74, precision: 0.7604166666666667, f_beta: 0.7348021215830274
train: step: 4151, loss: 0.5808094143867493, acc: 0.77, recall: 0.77, precision: 0.7790409260024803, f_beta: 0.7681217864704104
train: step: 4152, loss: 0.6335250735282898, acc: 0.76, recall: 0.76, precision: 0.7604166666666667, f_beta: 0.7599039615846339
train: step: 4153, loss: 0.5703712105751038, acc: 0.85, recall: 0.8500000000000001, precision: 0.8569971440228478, f_beta: 0.8492613807657521
train: step: 4154, loss: 0.5947239398956299, acc: 0.71, recall: 0.71, precision: 0.7206809583858764, f_beta: 0.7064480210547626
train: step: 4155, loss: 0.576509952545166, acc: 0.77, recall: 0.77, precision: 0.7709755118426336, f_beta: 0.769792813532179
train: step: 4156, loss: 0.5661831498146057, acc: 0.75, recall: 0.75, precision: 0.7583712277800745, f_beta: 0.7479584635547939
train: step: 4157, loss: 0.6064527034759521, acc: 0.72, recall: 0.72, precision: 0.7387152777777778, f_beta: 0.7144022847817217
train: step: 4158, loss: 0.5177155137062073, acc: 0.82, recall: 0.8200000000000001, precision: 0.8333333333333334, f_beta: 0.8181818181818181
train: step: 4159, loss: 0.5141258835792542, acc: 0.86, recall: 0.86, precision: 0.875, f_beta: 0.8585858585858586
train: step: 4160, loss: 0.5577117204666138, acc: 0.81, recall: 0.81, precision: 0.8161974704202366, f_beta: 0.8090644156366193
train: step: 4161, loss: 0.5163683295249939, acc: 0.85, recall: 0.85, precision: 0.8617197188921042, f_beta: 0.8487750781328762
train: step: 4162, loss: 0.5396583676338196, acc: 0.82, recall: 0.8200000000000001, precision: 0.8333333333333334, f_beta: 0.8181818181818181
train: step: 4163, loss: 0.5944397449493408, acc: 0.78, recall: 0.78, precision: 0.8038194444444444, f_beta: 0.7756017951856385
train: step: 4164, loss: 0.5656365752220154, acc: 0.83, recall: 0.8300000000000001, precision: 0.8333333333333334, f_beta: 0.8295739348370927
train: step: 4165, loss: 0.6705667972564697, acc: 0.65, recall: 0.65, precision: 0.6550227366680447, f_beta: 0.6471418489767113
train: step: 4166, loss: 0.5340538620948792, acc: 0.83, recall: 0.83, precision: 0.8467843631778058, f_beta: 0.8279178054458953
train: step: 4167, loss: 0.6408401727676392, acc: 0.69, recall: 0.69, precision: 0.6900760304121649, f_beta: 0.68996899689969
train: step: 4168, loss: 0.5736463069915771, acc: 0.82, recall: 0.8200000000000001, precision: 0.8284072249589491, f_beta: 0.818840579710145
train: step: 4169, loss: 0.6594755053520203, acc: 0.72, recall: 0.72, precision: 0.7450980392156863, f_beta: 0.7126436781609196
train: step: 4170, loss: 0.5628826022148132, acc: 0.79, recall: 0.79, precision: 0.8186813186813187, f_beta: 0.7851662404092071
train: step: 4171, loss: 0.5637803673744202, acc: 0.82, recall: 0.8200000000000001, precision: 0.8333333333333334, f_beta: 0.8181818181818181
train: step: 4172, loss: 0.6670302748680115, acc: 0.64, recall: 0.64, precision: 0.640224358974359, f_beta: 0.6398559423769508
train: step: 4173, loss: 0.5089403390884399, acc: 0.84, recall: 0.84, precision: 0.8607809847198642, f_beta: 0.8376623376623376
train: step: 4174, loss: 0.6274802684783936, acc: 0.75, recall: 0.75, precision: 0.7826775214834916, f_beta: 0.7425599835238389
train: step: 4175, loss: 0.5420228242874146, acc: 0.8, recall: 0.8, precision: 0.8125, f_beta: 0.797979797979798
train: step: 4176, loss: 0.5817598700523376, acc: 0.77, recall: 0.77, precision: 0.8155680224403927, f_beta: 0.7613860358958398
train: step: 4177, loss: 0.543474018573761, acc: 0.82, recall: 0.8200000000000001, precision: 0.8472222222222222, f_beta: 0.8164014687882497
train: step: 4178, loss: 0.6530543565750122, acc: 0.65, recall: 0.65, precision: 0.6505419510236852, f_beta: 0.6496847162446201
train: step: 4179, loss: 0.6462701559066772, acc: 0.74, recall: 0.74, precision: 0.7604166666666667, f_beta: 0.7348021215830274
train: step: 4180, loss: 0.5965539813041687, acc: 0.79, recall: 0.79, precision: 0.811025311025311, f_beta: 0.7863899908452854
train: step: 4181, loss: 0.5800431966781616, acc: 0.76, recall: 0.76, precision: 0.8095238095238095, f_beta: 0.75
train: step: 4182, loss: 0.6167866587638855, acc: 0.76, recall: 0.76, precision: 0.7987132352941176, f_beta: 0.7519636213311285
train: step: 4183, loss: 0.5191528797149658, acc: 0.82, recall: 0.8200000000000001, precision: 0.8333333333333334, f_beta: 0.8181818181818181
train: step: 4184, loss: 0.5048838257789612, acc: 0.88, recall: 0.88, precision: 0.8958333333333333, f_beta: 0.8787878787878789
train: step: 4185, loss: 0.611584484577179, acc: 0.69, recall: 0.69, precision: 0.693798449612403, f_beta: 0.6884735202492211
train: step: 4186, loss: 0.5506460666656494, acc: 0.79, recall: 0.79, precision: 0.7997106242248864, f_beta: 0.7882851093860268
train: step: 4187, loss: 0.6532954573631287, acc: 0.73, recall: 0.73, precision: 0.7300920368147259, f_beta: 0.72997299729973
train: step: 4188, loss: 0.5624983310699463, acc: 0.82, recall: 0.8200000000000001, precision: 0.8205128205128205, f_beta: 0.8199279711884755
train: step: 4189, loss: 0.5825608372688293, acc: 0.73, recall: 0.73, precision: 0.7527472527472527, f_beta: 0.7237851662404092
train: step: 4190, loss: 0.6415472626686096, acc: 0.66, recall: 0.66, precision: 0.6666666666666666, f_beta: 0.6565656565656566
train: step: 4191, loss: 0.6045606732368469, acc: 0.79, recall: 0.79, precision: 0.7997106242248864, f_beta: 0.7882851093860268
train: step: 4192, loss: 0.6181501150131226, acc: 0.76, recall: 0.76, precision: 0.7758913412563667, f_beta: 0.7564935064935063
train: step: 4193, loss: 0.6119017004966736, acc: 0.8, recall: 0.8, precision: 0.8255208333333333, f_beta: 0.7960016319869441
train: step: 4194, loss: 0.569266676902771, acc: 0.8, recall: 0.8, precision: 0.8078817733990147, f_beta: 0.7987117552334944
train: step: 4195, loss: 0.5142732262611389, acc: 0.87, recall: 0.87, precision: 0.8737373737373737, f_beta: 0.8696741854636592
train: step: 4196, loss: 0.693257212638855, acc: 0.49, recall: 0.49, precision: 0.4873160832064941, f_beta: 0.4615140956604371
train: step: 4197, loss: 0.47622615098953247, acc: 0.89, recall: 0.8899999999999999, precision: 0.8939393939393939, f_beta: 0.8897243107769424
train: step: 4198, loss: 0.5756973028182983, acc: 0.75, recall: 0.75, precision: 0.7826775214834916, f_beta: 0.7425599835238389
train: step: 4199, loss: 0.5300079584121704, acc: 0.82, recall: 0.82, precision: 0.8395585738539898, f_beta: 0.8173701298701298
train: step: 4200, loss: 0.5409878492355347, acc: 0.82, recall: 0.8200000000000001, precision: 0.822061191626409, f_beta: 0.8197115384615385
train: step: 4201, loss: 0.5887932181358337, acc: 0.77, recall: 0.77, precision: 0.7790409260024803, f_beta: 0.7681217864704104
train: step: 4202, loss: 0.6364602446556091, acc: 0.74, recall: 0.74, precision: 0.75, f_beta: 0.7373737373737373
train: step: 4203, loss: 0.5553978085517883, acc: 0.79, recall: 0.79, precision: 0.7997106242248864, f_beta: 0.7882851093860268
train: step: 4204, loss: 0.5315690040588379, acc: 0.85, recall: 0.8500000000000001, precision: 0.8535353535353536, f_beta: 0.849624060150376
train: step: 4205, loss: 0.6084919571876526, acc: 0.76, recall: 0.76, precision: 0.7708333333333333, f_beta: 0.7575757575757576
train: step: 4206, loss: 0.5904351472854614, acc: 0.8, recall: 0.8, precision: 0.8043831168831169, f_beta: 0.7992773986350863
train: step: 4207, loss: 0.496375173330307, acc: 0.88, recall: 0.88, precision: 0.8824476650563606, f_beta: 0.8798076923076923
train: step: 4208, loss: 0.5803739428520203, acc: 0.83, recall: 0.83, precision: 0.8301320528211285, f_beta: 0.8299829982998299
train: step: 4209, loss: 0.5631335377693176, acc: 0.77, recall: 0.77, precision: 0.7753977968176254, f_beta: 0.7688674505074867
train: step: 4210, loss: 0.6581243276596069, acc: 0.71, recall: 0.71, precision: 0.7107587314331594, f_beta: 0.7097387648883996
train: step: 4211, loss: 0.5967181921005249, acc: 0.75, recall: 0.75, precision: 0.7583712277800745, f_beta: 0.7479584635547939
train: step: 4212, loss: 0.5743343830108643, acc: 0.85, recall: 0.8500000000000001, precision: 0.8535353535353536, f_beta: 0.849624060150376
train: step: 4213, loss: 0.6064066886901855, acc: 0.73, recall: 0.73, precision: 0.7416981925178646, f_beta: 0.7266929851199515
train: step: 4214, loss: 0.5232357382774353, acc: 0.81, recall: 0.81, precision: 0.8257671290458175, f_beta: 0.8076728413807065
train: step: 4215, loss: 0.607273519039154, acc: 0.74, recall: 0.74, precision: 0.7857142857142857, f_beta: 0.7291666666666666
train: step: 4216, loss: 0.5618804693222046, acc: 0.79, recall: 0.79, precision: 0.7957976336189311, f_beta: 0.7889659330720531
train: step: 4217, loss: 0.5416867733001709, acc: 0.89, recall: 0.89, precision: 0.9098360655737705, f_beta: 0.8886526976414617
train: step: 4218, loss: 0.6394151449203491, acc: 0.67, recall: 0.6699999999999999, precision: 0.6706142111601766, f_beta: 0.6697027324592133
train: step: 4219, loss: 0.5685529112815857, acc: 0.85, recall: 0.85, precision: 0.867801597309794, f_beta: 0.848162769511084
train: step: 4220, loss: 0.5475658774375916, acc: 0.77, recall: 0.77, precision: 0.7727272727272727, f_beta: 0.7694235588972431
train: step: 4221, loss: 0.6157049536705017, acc: 0.74, recall: 0.74, precision: 0.7463054187192117, f_beta: 0.7383252818035426
train: step: 4222, loss: 0.5330199599266052, acc: 0.85, recall: 0.85, precision: 0.8753753753753755, f_beta: 0.8474214220323468
train: step: 4223, loss: 0.5582320094108582, acc: 0.8, recall: 0.8, precision: 0.8, f_beta: 0.8000000000000002
train: step: 4224, loss: 0.6755498647689819, acc: 0.78, recall: 0.78, precision: 0.780448717948718, f_beta: 0.7799119647859143
train: step: 4225, loss: 0.5621506571769714, acc: 0.8, recall: 0.8, precision: 0.8125, f_beta: 0.797979797979798
train: step: 4226, loss: 0.6044708490371704, acc: 0.75, recall: 0.75, precision: 0.750903251706142, f_beta: 0.7497747973175859
train: step: 4227, loss: 0.541634202003479, acc: 0.84, recall: 0.84, precision: 0.84, f_beta: 0.8399999999999999
train: step: 4228, loss: 0.5785217881202698, acc: 0.81, recall: 0.81, precision: 0.8161974704202366, f_beta: 0.8090644156366193
train: step: 4229, loss: 0.5864650011062622, acc: 0.77, recall: 0.77, precision: 0.7967032967032968, f_beta: 0.7647058823529411
train: step: 4230, loss: 0.6281387209892273, acc: 0.75, recall: 0.75, precision: 0.7501000400160064, f_beta: 0.7499749974997499
train: step: 4231, loss: 0.5598470568656921, acc: 0.86, recall: 0.86, precision: 0.8605769230769231, f_beta: 0.8599439775910365
train: step: 4232, loss: 0.5630922317504883, acc: 0.84, recall: 0.84, precision: 0.8541666666666667, f_beta: 0.8383838383838385
train: step: 4233, loss: 0.6008298397064209, acc: 0.8, recall: 0.8, precision: 0.8, f_beta: 0.8000000000000002
train: step: 4234, loss: 0.5369535684585571, acc: 0.87, recall: 0.87, precision: 0.8701480592236894, f_beta: 0.8699869986998701
train: step: 4235, loss: 0.5691553354263306, acc: 0.78, recall: 0.78, precision: 0.7818035426731079, f_beta: 0.7796474358974359
train: step: 4236, loss: 0.6115299463272095, acc: 0.85, recall: 0.85, precision: 0.851264552388599, f_beta: 0.8498648783905516
train: step: 4237, loss: 0.5813804864883423, acc: 0.79, recall: 0.79, precision: 0.7901160464185675, f_beta: 0.78997899789979
train: step: 4238, loss: 0.5567958950996399, acc: 0.82, recall: 0.8200000000000001, precision: 0.8246753246753247, f_beta: 0.8193496587715776
train: step: 4239, loss: 0.6167858242988586, acc: 0.73, recall: 0.73, precision: 0.7323232323232323, f_beta: 0.7293233082706766
train: step: 4240, loss: 0.5756670236587524, acc: 0.79, recall: 0.79, precision: 0.7997106242248864, f_beta: 0.7882851093860268
train: step: 4241, loss: 0.5498143434524536, acc: 0.83, recall: 0.8300000000000001, precision: 0.8311922922521076, f_beta: 0.8298468621759585
train: step: 4242, loss: 0.571662962436676, acc: 0.76, recall: 0.76, precision: 0.7668308702791462, f_beta: 0.7584541062801933
train: step: 4243, loss: 0.5007991194725037, acc: 0.83, recall: 0.83, precision: 0.8301320528211285, f_beta: 0.8299829982998299
train: step: 4244, loss: 0.5896353721618652, acc: 0.78, recall: 0.78, precision: 0.8038194444444444, f_beta: 0.7756017951856385
train: step: 4245, loss: 0.4938129782676697, acc: 0.85, recall: 0.8500000000000001, precision: 0.8535353535353536, f_beta: 0.849624060150376
train: step: 4246, loss: 0.5628783702850342, acc: 0.76, recall: 0.76, precision: 0.7637987012987013, f_beta: 0.7591328783621035
train: step: 4247, loss: 0.6586711406707764, acc: 0.84, recall: 0.8400000000000001, precision: 0.8421900161030595, f_beta: 0.8397435897435896
train: step: 4248, loss: 0.5826523900032043, acc: 0.82, recall: 0.8200000000000001, precision: 0.822061191626409, f_beta: 0.8197115384615385
train: step: 4249, loss: 0.5555931329727173, acc: 0.81, recall: 0.81, precision: 0.8406593406593407, f_beta: 0.8056265984654732
train: step: 4250, loss: 0.5478950142860413, acc: 0.86, recall: 0.86, precision: 0.86, f_beta: 0.8599999999999999
train: step: 4251, loss: 0.604261577129364, acc: 0.81, recall: 0.81, precision: 0.8111200321156162, f_beta: 0.8098288459613652
train: step: 4252, loss: 0.5017751455307007, acc: 0.83, recall: 0.8300000000000001, precision: 0.8365973072215422, f_beta: 0.8291628982011858
train: step: 4253, loss: 0.5648484230041504, acc: 0.78, recall: 0.78, precision: 0.7840909090909092, f_beta: 0.779205138498595
train: step: 4254, loss: 0.6606537699699402, acc: 0.73, recall: 0.73, precision: 0.7308309915696507, f_beta: 0.7297567811029926
train: step: 4255, loss: 0.5154099464416504, acc: 0.85, recall: 0.8500000000000001, precision: 0.8569971440228478, f_beta: 0.8492613807657521
train: step: 4256, loss: 0.5564762949943542, acc: 0.79, recall: 0.79, precision: 0.7997106242248864, f_beta: 0.7882851093860268
train: step: 4257, loss: 0.6116818785667419, acc: 0.78, recall: 0.78, precision: 0.7840909090909092, f_beta: 0.779205138498595
train: step: 4258, loss: 0.6593735218048096, acc: 0.65, recall: 0.6499999999999999, precision: 0.6515151515151515, f_beta: 0.6491228070175439
train: step: 4259, loss: 0.5294537544250488, acc: 0.81, recall: 0.81, precision: 0.8111200321156162, f_beta: 0.8098288459613652
train: step: 4260, loss: 0.5496184229850769, acc: 0.81, recall: 0.81, precision: 0.8406593406593407, f_beta: 0.8056265984654732
train: step: 4261, loss: 0.5515472888946533, acc: 0.78, recall: 0.78, precision: 0.8119429590017826, f_beta: 0.7742200328407225
train: step: 4262, loss: 0.6605032086372375, acc: 0.78, recall: 0.78, precision: 0.7818035426731079, f_beta: 0.7796474358974359
train: step: 4263, loss: 0.5187453031539917, acc: 0.84, recall: 0.84, precision: 0.84, f_beta: 0.8399999999999999
train: step: 4264, loss: 0.5623006224632263, acc: 0.84, recall: 0.84, precision: 0.8405448717948718, f_beta: 0.8399359743897559
train: step: 4265, loss: 0.6057122349739075, acc: 0.74, recall: 0.74, precision: 0.74, f_beta: 0.74
train: step: 4266, loss: 0.6027815341949463, acc: 0.81, recall: 0.81, precision: 0.810124049619848, f_beta: 0.8099809980998101
train: step: 4267, loss: 0.5589603185653687, acc: 0.8, recall: 0.8, precision: 0.8019323671497585, f_beta: 0.7996794871794872
train: step: 4268, loss: 0.5640344619750977, acc: 0.83, recall: 0.83, precision: 0.8301320528211285, f_beta: 0.8299829982998299
train: step: 4269, loss: 0.5369191765785217, acc: 0.84, recall: 0.84, precision: 0.84, f_beta: 0.8399999999999999
train: step: 4270, loss: 0.5442941188812256, acc: 0.81, recall: 0.81, precision: 0.8111200321156162, f_beta: 0.8098288459613652
train: step: 4271, loss: 0.4957255423069, acc: 0.87, recall: 0.87, precision: 0.8701480592236894, f_beta: 0.8699869986998701
train: step: 4272, loss: 0.4869992733001709, acc: 0.85, recall: 0.85, precision: 0.867801597309794, f_beta: 0.848162769511084
train: step: 4273, loss: 0.6052615642547607, acc: 0.81, recall: 0.81, precision: 0.8203803224472923, f_beta: 0.8084484323016432
train: step: 4274, loss: 0.6147382259368896, acc: 0.8, recall: 0.8, precision: 0.8043831168831169, f_beta: 0.7992773986350863
train: step: 4275, loss: 0.5307455062866211, acc: 0.85, recall: 0.85, precision: 0.8501400560224089, f_beta: 0.84998499849985
train: step: 4276, loss: 0.6538760662078857, acc: 0.75, recall: 0.75, precision: 0.7583712277800745, f_beta: 0.7479584635547939
train: step: 4277, loss: 0.5173677206039429, acc: 0.81, recall: 0.81, precision: 0.8111200321156162, f_beta: 0.8098288459613652
train: step: 4278, loss: 0.5714340209960938, acc: 0.78, recall: 0.78, precision: 0.7818035426731079, f_beta: 0.7796474358974359
train: step: 4279, loss: 0.5858402848243713, acc: 0.77, recall: 0.77, precision: 0.7837326607818411, f_beta: 0.767182913250329
train: step: 4280, loss: 0.5653968453407288, acc: 0.83, recall: 0.8300000000000001, precision: 0.8311922922521076, f_beta: 0.8298468621759585
train: step: 4281, loss: 0.6026888489723206, acc: 0.77, recall: 0.77, precision: 0.7709755118426336, f_beta: 0.769792813532179
train: step: 4282, loss: 0.6233526468276978, acc: 0.7, recall: 0.7, precision: 0.7052545155993433, f_beta: 0.6980676328502415
train: step: 4283, loss: 0.556290864944458, acc: 0.85, recall: 0.85, precision: 0.8501400560224089, f_beta: 0.84998499849985
train: step: 4284, loss: 0.5592144727706909, acc: 0.79, recall: 0.79, precision: 0.7957976336189311, f_beta: 0.7889659330720531
train: step: 4285, loss: 0.4605772793292999, acc: 0.89, recall: 0.89, precision: 0.89015606242497, f_beta: 0.88998899889989
train: step: 4286, loss: 0.5276352167129517, acc: 0.89, recall: 0.89, precision: 0.89015606242497, f_beta: 0.88998899889989
train: step: 4287, loss: 0.580668568611145, acc: 0.78, recall: 0.78, precision: 0.7916666666666666, f_beta: 0.7777777777777779
train: step: 4288, loss: 0.5261327624320984, acc: 0.87, recall: 0.87, precision: 0.8701480592236894, f_beta: 0.8699869986998701
train: step: 4289, loss: 0.48636046051979065, acc: 0.87, recall: 0.87, precision: 0.8823894171145101, f_beta: 0.8689384010484928
train: step: 4290, loss: 0.5635959506034851, acc: 0.77, recall: 0.77, precision: 0.7701080432172869, f_beta: 0.7699769976997699
train: step: 4291, loss: 0.6186777353286743, acc: 0.75, recall: 0.75, precision: 0.7549979600163199, f_beta: 0.7487689679429204
train: step: 4292, loss: 0.5715381503105164, acc: 0.76, recall: 0.76, precision: 0.7758913412563667, f_beta: 0.7564935064935063
train: step: 4293, loss: 0.5505938529968262, acc: 0.83, recall: 0.8300000000000001, precision: 0.8410500206696983, f_beta: 0.8286117552172598
train: step: 4294, loss: 0.5369589924812317, acc: 0.83, recall: 0.83, precision: 0.8301320528211285, f_beta: 0.8299829982998299
train: step: 4295, loss: 0.5697831511497498, acc: 0.79, recall: 0.79, precision: 0.7929292929292929, f_beta: 0.7894736842105263
train: step: 4296, loss: 0.583050549030304, acc: 0.8, recall: 0.8, precision: 0.8019323671497585, f_beta: 0.7996794871794872
train: step: 4297, loss: 0.545600175857544, acc: 0.83, recall: 0.8300000000000001, precision: 0.8410500206696983, f_beta: 0.8286117552172598
train: step: 4298, loss: 0.5721570253372192, acc: 0.82, recall: 0.8200000000000001, precision: 0.85650623885918, f_beta: 0.8152709359605911
train: step: 4299, loss: 0.5951927304267883, acc: 0.8, recall: 0.8, precision: 0.8043831168831169, f_beta: 0.7992773986350863
train: step: 4300, loss: 0.5929561853408813, acc: 0.74, recall: 0.74, precision: 0.7604166666666667, f_beta: 0.7348021215830274
train: step: 4301, loss: 0.6122762560844421, acc: 0.91, recall: 0.9099999999999999, precision: 0.9181966544267646, f_beta: 0.9095568284594513
train: step: 4302, loss: 0.6344854831695557, acc: 0.74, recall: 0.74, precision: 0.74, f_beta: 0.74
train: step: 4303, loss: 0.581718921661377, acc: 0.75, recall: 0.75, precision: 0.7525252525252526, f_beta: 0.7493734335839599
train: step: 4304, loss: 0.5622217655181885, acc: 0.76, recall: 0.76, precision: 0.76, f_beta: 0.76
train: step: 4305, loss: 0.5091189742088318, acc: 0.81, recall: 0.81, precision: 0.8131313131313131, f_beta: 0.8095238095238095
train: step: 4306, loss: 0.5588626861572266, acc: 0.77, recall: 0.77, precision: 0.7709755118426336, f_beta: 0.769792813532179
train: step: 4307, loss: 0.5508105754852295, acc: 0.77, recall: 0.77, precision: 0.7727272727272727, f_beta: 0.7694235588972431
train: step: 4308, loss: 0.5785641670227051, acc: 0.74, recall: 0.74, precision: 0.7546689303904923, f_beta: 0.7362012987012987
train: step: 4309, loss: 0.6245412826538086, acc: 0.73, recall: 0.73, precision: 0.7345981232150143, f_beta: 0.7286704853783539
train: step: 4310, loss: 0.5176779627799988, acc: 0.85, recall: 0.85, precision: 0.8617197188921042, f_beta: 0.8487750781328762
train: step: 4311, loss: 0.4930596649646759, acc: 0.84, recall: 0.8400000000000001, precision: 0.8421900161030595, f_beta: 0.8397435897435896
train: step: 4312, loss: 0.5749814510345459, acc: 0.78, recall: 0.78, precision: 0.780448717948718, f_beta: 0.7799119647859143
train: step: 4313, loss: 0.46403348445892334, acc: 0.87, recall: 0.87, precision: 0.8701480592236894, f_beta: 0.8699869986998701
train: step: 4314, loss: 0.5089762210845947, acc: 0.84, recall: 0.84, precision: 0.8405448717948718, f_beta: 0.8399359743897559
train: step: 4315, loss: 0.5688457489013672, acc: 0.82, recall: 0.8200000000000001, precision: 0.8205128205128205, f_beta: 0.8199279711884755
train: step: 4316, loss: 0.49539798498153687, acc: 0.87, recall: 0.87, precision: 0.8737373737373737, f_beta: 0.8696741854636592
train: step: 4317, loss: 0.5195233821868896, acc: 0.87, recall: 0.87, precision: 0.8701480592236894, f_beta: 0.8699869986998701
train: step: 4318, loss: 0.6525704264640808, acc: 0.69, recall: 0.69, precision: 0.693798449612403, f_beta: 0.6884735202492211
train: step: 4319, loss: 0.5642361640930176, acc: 0.84, recall: 0.8400000000000001, precision: 0.8421900161030595, f_beta: 0.8397435897435896
train: step: 4320, loss: 0.539848268032074, acc: 0.8, recall: 0.8, precision: 0.8125, f_beta: 0.797979797979798
train: step: 4321, loss: 0.5666768550872803, acc: 0.86, recall: 0.86, precision: 0.86, f_beta: 0.8599999999999999
train: step: 4322, loss: 0.5055131316184998, acc: 0.83, recall: 0.8300000000000001, precision: 0.8333333333333334, f_beta: 0.8295739348370927
train: step: 4323, loss: 0.5494640469551086, acc: 0.85, recall: 0.8500000000000001, precision: 0.8535353535353536, f_beta: 0.849624060150376
train: step: 4324, loss: 0.6139817237854004, acc: 0.74, recall: 0.74, precision: 0.7415458937198067, f_beta: 0.7395833333333335
train: step: 4325, loss: 0.5964269042015076, acc: 0.8, recall: 0.8, precision: 0.8019323671497585, f_beta: 0.7996794871794872
train: step: 4326, loss: 0.6225590109825134, acc: 0.83, recall: 0.8300000000000001, precision: 0.8365973072215422, f_beta: 0.8291628982011858
train: step: 4327, loss: 0.4958686828613281, acc: 0.85, recall: 0.85, precision: 0.8501400560224089, f_beta: 0.84998499849985
train: step: 4328, loss: 0.520362913608551, acc: 0.79, recall: 0.79, precision: 0.7957976336189311, f_beta: 0.7889659330720531
train: step: 4329, loss: 0.5813762545585632, acc: 0.75, recall: 0.75, precision: 0.7583712277800745, f_beta: 0.7479584635547939
train: step: 4330, loss: 0.5305067300796509, acc: 0.86, recall: 0.86, precision: 0.8605769230769231, f_beta: 0.8599439775910365
train: step: 4331, loss: 0.5412760376930237, acc: 0.8, recall: 0.8, precision: 0.8125, f_beta: 0.797979797979798
train: step: 4332, loss: 0.6097690463066101, acc: 0.83, recall: 0.8300000000000001, precision: 0.8311922922521076, f_beta: 0.8298468621759585
train: step: 4333, loss: 0.5632188320159912, acc: 0.84, recall: 0.84, precision: 0.84, f_beta: 0.8399999999999999
train: step: 4334, loss: 0.6158633828163147, acc: 0.74, recall: 0.74, precision: 0.75, f_beta: 0.7373737373737373
train: step: 4335, loss: 0.6434283256530762, acc: 0.78, recall: 0.78, precision: 0.7916666666666666, f_beta: 0.7777777777777779
train: step: 4336, loss: 0.5154692530632019, acc: 0.84, recall: 0.8400000000000001, precision: 0.8489326765188834, f_beta: 0.8389694041867956
train: step: 4337, loss: 0.5917017459869385, acc: 0.76, recall: 0.76, precision: 0.7604166666666667, f_beta: 0.7599039615846339
train: step: 4338, loss: 0.5298262238502502, acc: 0.84, recall: 0.8400000000000001, precision: 0.8449675324675325, f_beta: 0.839421918908069
train: step: 4339, loss: 0.5325387120246887, acc: 0.84, recall: 0.8400000000000001, precision: 0.8449675324675325, f_beta: 0.839421918908069
train: step: 4340, loss: 0.5180477499961853, acc: 0.82, recall: 0.8200000000000001, precision: 0.8205128205128205, f_beta: 0.8199279711884755
train: step: 4341, loss: 0.5551706552505493, acc: 0.81, recall: 0.81, precision: 0.810124049619848, f_beta: 0.8099809980998101
train: step: 4342, loss: 0.5799596309661865, acc: 0.8, recall: 0.8, precision: 0.8125, f_beta: 0.797979797979798
train: step: 4343, loss: 0.5471837520599365, acc: 0.78, recall: 0.78, precision: 0.780448717948718, f_beta: 0.7799119647859143
train: step: 4344, loss: 0.5277950167655945, acc: 0.81, recall: 0.81, precision: 0.810124049619848, f_beta: 0.8099809980998101
train: step: 4345, loss: 0.6100817918777466, acc: 0.73, recall: 0.73, precision: 0.7345981232150143, f_beta: 0.7286704853783539
train: step: 4346, loss: 0.5854834914207458, acc: 0.76, recall: 0.76, precision: 0.7821180555555556, f_beta: 0.755201958384333
train: step: 4347, loss: 0.4914875328540802, acc: 0.86, recall: 0.86, precision: 0.875, f_beta: 0.8585858585858586
train: step: 4348, loss: 0.6521728038787842, acc: 0.65, recall: 0.65, precision: 0.6550227366680447, f_beta: 0.6471418489767113
train: step: 4349, loss: 0.636164665222168, acc: 0.67, recall: 0.67, precision: 0.6700680272108843, f_beta: 0.66996699669967
train: step: 4350, loss: 0.6123397946357727, acc: 0.86, recall: 0.86, precision: 0.8623188405797102, f_beta: 0.859775641025641
train: step: 4351, loss: 0.5322588086128235, acc: 0.84, recall: 0.8400000000000001, precision: 0.8449675324675325, f_beta: 0.839421918908069
train: step: 4352, loss: 0.5428539514541626, acc: 0.83, recall: 0.8300000000000001, precision: 0.8333333333333334, f_beta: 0.8295739348370927
train: step: 4353, loss: 0.487909197807312, acc: 0.85, recall: 0.8500000000000001, precision: 0.8535353535353536, f_beta: 0.849624060150376
train: step: 4354, loss: 0.5720584392547607, acc: 0.75, recall: 0.75, precision: 0.7583712277800745, f_beta: 0.7479584635547939
train: step: 4355, loss: 0.5621383786201477, acc: 0.77, recall: 0.77, precision: 0.7967032967032968, f_beta: 0.7647058823529411
train: step: 4356, loss: 0.5470216870307922, acc: 0.82, recall: 0.82, precision: 0.82, f_beta: 0.82
train: step: 4357, loss: 0.6335449814796448, acc: 0.76, recall: 0.76, precision: 0.7821180555555556, f_beta: 0.755201958384333
train: step: 4358, loss: 0.5761411190032959, acc: 0.79, recall: 0.79, precision: 0.7929292929292929, f_beta: 0.7894736842105263
train: step: 4359, loss: 0.5603783130645752, acc: 0.76, recall: 0.76, precision: 0.7668308702791462, f_beta: 0.7584541062801933
train: step: 4360, loss: 0.4947358965873718, acc: 0.88, recall: 0.88, precision: 0.88, f_beta: 0.88
train: step: 4361, loss: 0.5267326831817627, acc: 0.86, recall: 0.8600000000000001, precision: 0.8652597402597402, f_beta: 0.8594941790445605
train: step: 4362, loss: 0.5361988544464111, acc: 0.86, recall: 0.86, precision: 0.8605769230769231, f_beta: 0.8599439775910365
train: step: 4363, loss: 0.6259725093841553, acc: 0.71, recall: 0.71, precision: 0.7252252252252251, f_beta: 0.7050147492625368
train: step: 4364, loss: 0.5223387479782104, acc: 0.87, recall: 0.87, precision: 0.8713368125250903, f_beta: 0.8698828946051447
train: step: 4365, loss: 0.6181584000587463, acc: 0.69, recall: 0.69, precision: 0.6963621331128566, f_beta: 0.6874684948079444
train: step: 4366, loss: 0.5272395610809326, acc: 0.84, recall: 0.84, precision: 0.8607809847198642, f_beta: 0.8376623376623376
train: step: 4367, loss: 0.5873087048530579, acc: 0.77, recall: 0.77, precision: 0.7753977968176254, f_beta: 0.7688674505074867
train: step: 4368, loss: 0.5895119905471802, acc: 0.76, recall: 0.76, precision: 0.7821180555555556, f_beta: 0.755201958384333
train: step: 4369, loss: 0.525569498538971, acc: 0.86, recall: 0.86, precision: 0.8623188405797102, f_beta: 0.859775641025641
train: step: 4370, loss: 0.6966449618339539, acc: 0.33, recall: 0.33, precision: 0.3282828282828283, f_beta: 0.32832080200501257
train: step: 4371, loss: 0.5918864011764526, acc: 0.8, recall: 0.8, precision: 0.8183361629881154, f_beta: 0.797077922077922
train: step: 4372, loss: 0.6123331189155579, acc: 0.81, recall: 0.81, precision: 0.8131313131313131, f_beta: 0.8095238095238095
train: step: 4373, loss: 0.5561580061912537, acc: 0.81, recall: 0.81, precision: 0.8324753324753325, f_beta: 0.8067338012409724
train: step: 4374, loss: 0.5996629595756531, acc: 0.77, recall: 0.77, precision: 0.805291723202171, f_beta: 0.7631551848419318
train: step: 4375, loss: 0.6094615459442139, acc: 0.79, recall: 0.79, precision: 0.7901160464185675, f_beta: 0.78997899789979
train: step: 4376, loss: 0.5097466111183167, acc: 0.88, recall: 0.8799999999999999, precision: 0.885551948051948, f_beta: 0.8795664391810518
train: step: 4377, loss: 0.6067326068878174, acc: 0.78, recall: 0.78, precision: 0.7873563218390804, f_beta: 0.7785829307568439
train: step: 4378, loss: 0.5388174057006836, acc: 0.84, recall: 0.84, precision: 0.8405448717948718, f_beta: 0.8399359743897559
train: step: 4379, loss: 0.4187256693840027, acc: 0.9, recall: 0.8999999999999999, precision: 0.9025764895330113, f_beta: 0.8998397435897436
train: step: 4380, loss: 0.6084085702896118, acc: 0.89, recall: 0.89, precision: 0.89015606242497, f_beta: 0.88998899889989
train: step: 4381, loss: 0.6099753975868225, acc: 0.76, recall: 0.76, precision: 0.7821180555555556, f_beta: 0.755201958384333
train: step: 4382, loss: 0.6031646132469177, acc: 0.72, recall: 0.72, precision: 0.7214170692431562, f_beta: 0.7195512820512822
train: step: 4383, loss: 0.5908836126327515, acc: 0.78, recall: 0.78, precision: 0.7840909090909092, f_beta: 0.779205138498595
train: step: 4384, loss: 0.639632523059845, acc: 0.72, recall: 0.72, precision: 0.7450980392156863, f_beta: 0.7126436781609196
train: step: 4385, loss: 0.6000835299491882, acc: 0.75, recall: 0.75, precision: 0.750903251706142, f_beta: 0.7497747973175859
train: step: 4386, loss: 0.5642823576927185, acc: 0.79, recall: 0.79, precision: 0.7901160464185675, f_beta: 0.78997899789979
train: step: 4387, loss: 0.5220134258270264, acc: 0.85, recall: 0.8500000000000001, precision: 0.8535353535353536, f_beta: 0.849624060150376
train: step: 4388, loss: 0.5701798796653748, acc: 0.83, recall: 0.83, precision: 0.8301320528211285, f_beta: 0.8299829982998299
train: step: 4389, loss: 0.5635144710540771, acc: 0.8, recall: 0.8, precision: 0.8043831168831169, f_beta: 0.7992773986350863
train: step: 4390, loss: 0.6083040833473206, acc: 0.72, recall: 0.72, precision: 0.7232142857142857, f_beta: 0.7189883580891208
train: step: 4391, loss: 0.5304381847381592, acc: 0.85, recall: 0.8500000000000001, precision: 0.8535353535353536, f_beta: 0.849624060150376
train: step: 4392, loss: 0.5734108090400696, acc: 0.77, recall: 0.77, precision: 0.7837326607818411, f_beta: 0.767182913250329
train: step: 4393, loss: 0.5238616466522217, acc: 0.84, recall: 0.84, precision: 0.8405448717948718, f_beta: 0.8399359743897559
train: step: 4394, loss: 0.5385476350784302, acc: 0.82, recall: 0.8200000000000001, precision: 0.822061191626409, f_beta: 0.8197115384615385
train: step: 4395, loss: 0.6078373193740845, acc: 0.76, recall: 0.76, precision: 0.7758913412563667, f_beta: 0.7564935064935063
train: step: 4396, loss: 0.6229159832000732, acc: 0.8, recall: 0.8, precision: 0.8078817733990147, f_beta: 0.7987117552334944
train: step: 4397, loss: 0.5930454134941101, acc: 0.8, recall: 0.8, precision: 0.8125, f_beta: 0.797979797979798
train: step: 4398, loss: 0.5208561420440674, acc: 0.82, recall: 0.8200000000000001, precision: 0.8333333333333334, f_beta: 0.8181818181818181
train: step: 4399, loss: 0.5422705411911011, acc: 0.79, recall: 0.79, precision: 0.7910477719791249, f_beta: 0.7898108297467722
train: step: 4400, loss: 0.6351140141487122, acc: 0.83, recall: 0.8300000000000001, precision: 0.8333333333333334, f_beta: 0.8295739348370927
train: step: 4401, loss: 0.5524393320083618, acc: 0.81, recall: 0.81, precision: 0.810124049619848, f_beta: 0.8099809980998101
train: step: 4402, loss: 0.5581330060958862, acc: 0.82, recall: 0.8200000000000001, precision: 0.8205128205128205, f_beta: 0.8199279711884755
train: step: 4403, loss: 0.48164820671081543, acc: 0.85, recall: 0.85, precision: 0.8501400560224089, f_beta: 0.84998499849985
train: step: 4404, loss: 0.5461305975914001, acc: 0.79, recall: 0.79, precision: 0.7910477719791249, f_beta: 0.7898108297467722
train: step: 4405, loss: 0.5765685439109802, acc: 0.79, recall: 0.79, precision: 0.7957976336189311, f_beta: 0.7889659330720531
train: step: 4406, loss: 0.6830797791481018, acc: 0.72, recall: 0.72, precision: 0.7232142857142857, f_beta: 0.7189883580891208
train: step: 4407, loss: 0.6237928867340088, acc: 0.71, recall: 0.71, precision: 0.7141982864137086, f_beta: 0.7085720028137876
train: step: 4408, loss: 0.5298346877098083, acc: 0.81, recall: 0.81, precision: 0.8161974704202366, f_beta: 0.8090644156366193
train: step: 4409, loss: 0.6081392765045166, acc: 0.77, recall: 0.77, precision: 0.7701080432172869, f_beta: 0.7699769976997699
train: step: 4410, loss: 0.5414318442344666, acc: 0.84, recall: 0.8400000000000001, precision: 0.8489326765188834, f_beta: 0.8389694041867956
train: step: 4411, loss: 0.6344654560089111, acc: 0.71, recall: 0.71, precision: 0.7100840336134454, f_beta: 0.7099709970997101
train: step: 4412, loss: 0.5351132750511169, acc: 0.84, recall: 0.84, precision: 0.8541666666666667, f_beta: 0.8383838383838385
train: step: 4413, loss: 0.5281917452812195, acc: 0.82, recall: 0.8200000000000001, precision: 0.8205128205128205, f_beta: 0.8199279711884755
train: step: 4414, loss: 0.5344671010971069, acc: 0.83, recall: 0.8300000000000001, precision: 0.8365973072215422, f_beta: 0.8291628982011858
train: step: 4415, loss: 0.5970278978347778, acc: 0.72, recall: 0.72, precision: 0.7257799671592775, f_beta: 0.7181964573268922
train: step: 4416, loss: 0.6537382006645203, acc: 0.74, recall: 0.74, precision: 0.7435064935064934, f_beta: 0.7390606182256123
train: step: 4417, loss: 0.4917852282524109, acc: 0.87, recall: 0.87, precision: 0.8773969808241534, f_beta: 0.8693598633303186
train: step: 4418, loss: 0.5874707102775574, acc: 0.76, recall: 0.76, precision: 0.7616747181964574, f_beta: 0.7596153846153846
train: step: 4419, loss: 0.5363868474960327, acc: 0.79, recall: 0.79, precision: 0.7910477719791249, f_beta: 0.7898108297467722
train: step: 4420, loss: 0.5105471014976501, acc: 0.82, recall: 0.8200000000000001, precision: 0.8205128205128205, f_beta: 0.8199279711884755
train: step: 4421, loss: 0.6153551340103149, acc: 0.75, recall: 0.75, precision: 0.7501000400160064, f_beta: 0.7499749974997499
train: step: 4422, loss: 0.6220017075538635, acc: 0.73, recall: 0.73, precision: 0.7600633197648123, f_beta: 0.721964782205746
train: step: 4423, loss: 0.6221467852592468, acc: 0.78, recall: 0.78, precision: 0.7873563218390804, f_beta: 0.7785829307568439
train: step: 4424, loss: 0.4653207063674927, acc: 0.87, recall: 0.87, precision: 0.8701480592236894, f_beta: 0.8699869986998701
train: step: 4425, loss: 0.4912452697753906, acc: 0.84, recall: 0.8400000000000001, precision: 0.8449675324675325, f_beta: 0.839421918908069
train: step: 4426, loss: 0.5582820773124695, acc: 0.83, recall: 0.83, precision: 0.8301320528211285, f_beta: 0.8299829982998299
train: step: 4427, loss: 0.5619152188301086, acc: 0.8, recall: 0.8, precision: 0.8019323671497585, f_beta: 0.7996794871794872
train: step: 4428, loss: 0.5919927954673767, acc: 0.73, recall: 0.73, precision: 0.7308309915696507, f_beta: 0.7297567811029926
train: step: 4429, loss: 0.5201123952865601, acc: 0.82, recall: 0.8200000000000001, precision: 0.8205128205128205, f_beta: 0.8199279711884755
train: step: 4430, loss: 0.48937562108039856, acc: 0.83, recall: 0.8300000000000001, precision: 0.8311922922521076, f_beta: 0.8298468621759585
train: step: 4431, loss: 0.5993441939353943, acc: 0.76, recall: 0.76, precision: 0.7604166666666667, f_beta: 0.7599039615846339
train: step: 4432, loss: 0.5713904500007629, acc: 0.82, recall: 0.8200000000000001, precision: 0.8205128205128205, f_beta: 0.8199279711884755
train: step: 4433, loss: 0.5327210426330566, acc: 0.85, recall: 0.85, precision: 0.8501400560224089, f_beta: 0.84998499849985
train: step: 4434, loss: 0.5482426285743713, acc: 0.78, recall: 0.78, precision: 0.78, f_beta: 0.78
train: step: 4435, loss: 0.5546663403511047, acc: 0.78, recall: 0.78, precision: 0.7916666666666666, f_beta: 0.7777777777777779
train: step: 4436, loss: 0.5802050828933716, acc: 0.83, recall: 0.8300000000000001, precision: 0.8333333333333334, f_beta: 0.8295739348370927
train: step: 4437, loss: 0.5822027921676636, acc: 0.74, recall: 0.74, precision: 0.7415458937198067, f_beta: 0.7395833333333335
train: step: 4438, loss: 0.5857320427894592, acc: 0.87, recall: 0.87, precision: 0.8773969808241534, f_beta: 0.8693598633303186
train: step: 4439, loss: 0.4716728627681732, acc: 0.83, recall: 0.8300000000000001, precision: 0.8333333333333334, f_beta: 0.8295739348370927
train: step: 4440, loss: 0.5047624111175537, acc: 0.83, recall: 0.8300000000000001, precision: 0.8365973072215422, f_beta: 0.8291628982011858
train: step: 4441, loss: 0.6142751574516296, acc: 0.81, recall: 0.81, precision: 0.8131313131313131, f_beta: 0.8095238095238095
train: step: 4442, loss: 0.6079518795013428, acc: 0.83, recall: 0.8300000000000001, precision: 0.8333333333333334, f_beta: 0.8295739348370927
train: step: 4443, loss: 0.596703827381134, acc: 0.73, recall: 0.73, precision: 0.7377015295576685, f_beta: 0.7277951406391774
train: step: 4444, loss: 0.5868505239486694, acc: 0.8, recall: 0.8, precision: 0.8, f_beta: 0.8000000000000002
train: step: 4445, loss: 0.5570412278175354, acc: 0.82, recall: 0.8200000000000001, precision: 0.8284072249589491, f_beta: 0.818840579710145
train: step: 4446, loss: 0.5684507489204407, acc: 0.83, recall: 0.8300000000000001, precision: 0.8365973072215422, f_beta: 0.8291628982011858
train: step: 4447, loss: 0.5120089054107666, acc: 0.84, recall: 0.8400000000000001, precision: 0.8489326765188834, f_beta: 0.8389694041867956
train: step: 4448, loss: 0.5660364627838135, acc: 0.78, recall: 0.78, precision: 0.7916666666666666, f_beta: 0.7777777777777779
train: step: 4449, loss: 0.4493705630302429, acc: 0.87, recall: 0.87, precision: 0.8701480592236894, f_beta: 0.8699869986998701
train: step: 4450, loss: 0.5587389469146729, acc: 0.79, recall: 0.79, precision: 0.7901160464185675, f_beta: 0.78997899789979
train: step: 4451, loss: 0.4651026427745819, acc: 0.87, recall: 0.87, precision: 0.8713368125250903, f_beta: 0.8698828946051447
train: step: 4452, loss: 0.5980204343795776, acc: 0.8, recall: 0.8, precision: 0.8043831168831169, f_beta: 0.7992773986350863
train: step: 4453, loss: 0.5801129341125488, acc: 0.75, recall: 0.75, precision: 0.7583712277800745, f_beta: 0.7479584635547939
train: step: 4454, loss: 0.5168493986129761, acc: 0.86, recall: 0.86, precision: 0.8694581280788177, f_beta: 0.8590982286634461
train: step: 4455, loss: 0.5365716218948364, acc: 0.83, recall: 0.83, precision: 0.8301320528211285, f_beta: 0.8299829982998299
train: step: 4456, loss: 0.5463544130325317, acc: 0.82, recall: 0.8200000000000001, precision: 0.8284072249589491, f_beta: 0.818840579710145
train: step: 4457, loss: 0.6118307113647461, acc: 0.77, recall: 0.77, precision: 0.7753977968176254, f_beta: 0.7688674505074867
train: step: 4458, loss: 0.5297101736068726, acc: 0.82, recall: 0.8200000000000001, precision: 0.822061191626409, f_beta: 0.8197115384615385
train: step: 4459, loss: 0.544797956943512, acc: 0.8, recall: 0.8, precision: 0.8019323671497585, f_beta: 0.7996794871794872
train: step: 4460, loss: 0.6398002505302429, acc: 0.85, recall: 0.85, precision: 0.851264552388599, f_beta: 0.8498648783905516
train: step: 4461, loss: 0.5811837315559387, acc: 0.78, recall: 0.78, precision: 0.780448717948718, f_beta: 0.7799119647859143
train: step: 4462, loss: 0.5892861485481262, acc: 0.78, recall: 0.78, precision: 0.8119429590017826, f_beta: 0.7742200328407225
train: step: 4463, loss: 0.5777009725570679, acc: 0.76, recall: 0.76, precision: 0.7637987012987013, f_beta: 0.7591328783621035
train: step: 4464, loss: 0.5632771849632263, acc: 0.79, recall: 0.79, precision: 0.7929292929292929, f_beta: 0.7894736842105263
train: step: 4465, loss: 0.5203731060028076, acc: 0.84, recall: 0.8400000000000001, precision: 0.8421900161030595, f_beta: 0.8397435897435896
train: step: 4466, loss: 0.5822265148162842, acc: 0.76, recall: 0.76, precision: 0.7604166666666667, f_beta: 0.7599039615846339
train: step: 4467, loss: 0.5625421404838562, acc: 0.76, recall: 0.76, precision: 0.76, f_beta: 0.76
train: step: 4468, loss: 0.6018888354301453, acc: 0.75, recall: 0.75, precision: 0.7525252525252526, f_beta: 0.7493734335839599
train: step: 4469, loss: 0.5094559192657471, acc: 0.83, recall: 0.8300000000000001, precision: 0.8333333333333334, f_beta: 0.8295739348370927
train: step: 4470, loss: 0.6191843152046204, acc: 0.75, recall: 0.75, precision: 0.7627154266498528, f_beta: 0.7469379491851402
train: step: 4471, loss: 0.5614427924156189, acc: 0.83, recall: 0.8300000000000001, precision: 0.8365973072215422, f_beta: 0.8291628982011858
train: step: 4472, loss: 0.5411935448646545, acc: 0.79, recall: 0.79, precision: 0.7957976336189311, f_beta: 0.7889659330720531
train: step: 4473, loss: 0.5399718284606934, acc: 0.84, recall: 0.84, precision: 0.8405448717948718, f_beta: 0.8399359743897559
train: step: 4474, loss: 0.5008108019828796, acc: 0.85, recall: 0.85, precision: 0.8617197188921042, f_beta: 0.8487750781328762
train: step: 4475, loss: 0.5241963863372803, acc: 0.81, recall: 0.81, precision: 0.8161974704202366, f_beta: 0.8090644156366193
train: step: 4476, loss: 0.6740537285804749, acc: 0.68, recall: 0.6799999999999999, precision: 0.6875, f_beta: 0.6767676767676767
train: step: 4477, loss: 0.530714750289917, acc: 0.83, recall: 0.83, precision: 0.8301320528211285, f_beta: 0.8299829982998299
train: step: 4478, loss: 0.5051698088645935, acc: 0.85, recall: 0.8500000000000001, precision: 0.8535353535353536, f_beta: 0.849624060150376
train: step: 4479, loss: 0.6214203238487244, acc: 0.72, recall: 0.72, precision: 0.7214170692431562, f_beta: 0.7195512820512822
train: step: 4480, loss: 0.5351467728614807, acc: 0.84, recall: 0.84, precision: 0.8541666666666667, f_beta: 0.8383838383838385
train: step: 4481, loss: 0.6032145023345947, acc: 0.74, recall: 0.74, precision: 0.74, f_beta: 0.74
train: step: 4482, loss: 0.5344916582107544, acc: 0.83, recall: 0.8300000000000001, precision: 0.8365973072215422, f_beta: 0.8291628982011858
train: step: 4483, loss: 0.553242027759552, acc: 0.8, recall: 0.8, precision: 0.8125, f_beta: 0.797979797979798
train: step: 4484, loss: 0.5871309041976929, acc: 0.74, recall: 0.74, precision: 0.7435064935064934, f_beta: 0.7390606182256123
train: step: 4485, loss: 0.5952651500701904, acc: 0.85, recall: 0.85, precision: 0.851264552388599, f_beta: 0.8498648783905516
train: step: 4486, loss: 0.5716497898101807, acc: 0.79, recall: 0.79, precision: 0.7901160464185675, f_beta: 0.78997899789979
train: step: 4487, loss: 0.5960134863853455, acc: 0.76, recall: 0.76, precision: 0.7604166666666667, f_beta: 0.7599039615846339
train: step: 4488, loss: 0.605478823184967, acc: 0.83, recall: 0.8300000000000001, precision: 0.8333333333333334, f_beta: 0.8295739348370927
train: step: 4489, loss: 0.5819737911224365, acc: 0.75, recall: 0.75, precision: 0.7501000400160064, f_beta: 0.7499749974997499
train: step: 4490, loss: 0.5394496321678162, acc: 0.79, recall: 0.79, precision: 0.7901160464185675, f_beta: 0.78997899789979
train: step: 4491, loss: 0.4618486762046814, acc: 0.93, recall: 0.9299999999999999, precision: 0.9315535929345644, f_beta: 0.9299369432489241
train: step: 4492, loss: 0.516960620880127, acc: 0.87, recall: 0.87, precision: 0.8701480592236894, f_beta: 0.8699869986998701
train: step: 4493, loss: 0.5480018854141235, acc: 0.83, recall: 0.8300000000000001, precision: 0.8333333333333334, f_beta: 0.8295739348370927
train: step: 4494, loss: 0.5337502956390381, acc: 0.8, recall: 0.8, precision: 0.8, f_beta: 0.8000000000000002
train: step: 4495, loss: 0.5061191320419312, acc: 0.81, recall: 0.81, precision: 0.810124049619848, f_beta: 0.8099809980998101
train: step: 4496, loss: 0.5333402156829834, acc: 0.83, recall: 0.8300000000000001, precision: 0.8311922922521076, f_beta: 0.8298468621759585
train: step: 4497, loss: 0.6048218607902527, acc: 0.81, recall: 0.81, precision: 0.8111200321156162, f_beta: 0.8098288459613652
train: step: 4498, loss: 0.589184582233429, acc: 0.77, recall: 0.77, precision: 0.7967032967032968, f_beta: 0.7647058823529411
train: step: 4499, loss: 0.5459855794906616, acc: 0.83, recall: 0.83, precision: 0.8467843631778058, f_beta: 0.8279178054458953
train: step: 4500, loss: 0.48911014199256897, acc: 0.89, recall: 0.89, precision: 0.8914090726615818, f_beta: 0.8899009108197378
train: step: 4501, loss: 0.5189205408096313, acc: 0.8, recall: 0.8, precision: 0.8004807692307692, f_beta: 0.7999199679871949
train: step: 4502, loss: 0.6327880620956421, acc: 0.79, recall: 0.79, precision: 0.7957976336189311, f_beta: 0.7889659330720531
train: step: 4503, loss: 0.5245221257209778, acc: 0.85, recall: 0.8500000000000001, precision: 0.8535353535353536, f_beta: 0.849624060150376
train: step: 4504, loss: 0.5867748260498047, acc: 0.78, recall: 0.78, precision: 0.7840909090909092, f_beta: 0.779205138498595
train: step: 4505, loss: 0.6105632185935974, acc: 0.76, recall: 0.76, precision: 0.7604166666666667, f_beta: 0.7599039615846339
train: step: 4506, loss: 0.4687604010105133, acc: 0.91, recall: 0.9099999999999999, precision: 0.9114813327980731, f_beta: 0.9099189270343309
train: step: 4507, loss: 0.5765366554260254, acc: 0.83, recall: 0.8300000000000001, precision: 0.8311922922521076, f_beta: 0.8298468621759585
train: step: 4508, loss: 0.5906498432159424, acc: 0.71, recall: 0.71, precision: 0.7107587314331594, f_beta: 0.7097387648883996
train: step: 4509, loss: 0.61577308177948, acc: 0.78, recall: 0.78, precision: 0.7873563218390804, f_beta: 0.7785829307568439
train: step: 4510, loss: 0.5043933987617493, acc: 0.88, recall: 0.88, precision: 0.8899835796387521, f_beta: 0.8792270531400966
train: step: 4511, loss: 0.5254903435707092, acc: 0.81, recall: 0.81, precision: 0.8131313131313131, f_beta: 0.8095238095238095
train: step: 4512, loss: 0.5827960968017578, acc: 0.77, recall: 0.77, precision: 0.7709755118426336, f_beta: 0.769792813532179
train: step: 4513, loss: 0.5250903367996216, acc: 0.83, recall: 0.8300000000000001, precision: 0.8333333333333334, f_beta: 0.8295739348370927
train: step: 4514, loss: 0.5337188839912415, acc: 0.84, recall: 0.84, precision: 0.8405448717948718, f_beta: 0.8399359743897559
train: step: 4515, loss: 0.6187112927436829, acc: 0.78, recall: 0.78, precision: 0.78, f_beta: 0.78
train: step: 4516, loss: 0.5355687141418457, acc: 0.81, recall: 0.81, precision: 0.810124049619848, f_beta: 0.8099809980998101
train: step: 4517, loss: 0.5246948003768921, acc: 0.82, recall: 0.82, precision: 0.82, f_beta: 0.82
train: step: 4518, loss: 0.5462497472763062, acc: 0.78, recall: 0.78, precision: 0.7818035426731079, f_beta: 0.7796474358974359
train: step: 4519, loss: 0.5601031184196472, acc: 0.77, recall: 0.77, precision: 0.7837326607818411, f_beta: 0.767182913250329
train: step: 4520, loss: 0.6065623760223389, acc: 0.72, recall: 0.72, precision: 0.7232142857142857, f_beta: 0.7189883580891208
train: step: 4521, loss: 0.5183919072151184, acc: 0.83, recall: 0.8300000000000001, precision: 0.8333333333333334, f_beta: 0.8295739348370927
train: step: 4522, loss: 0.6102985143661499, acc: 0.7, recall: 0.7, precision: 0.7170138888888888, f_beta: 0.6940024479804162
train: step: 4523, loss: 0.5754554271697998, acc: 0.81, recall: 0.81, precision: 0.8131313131313131, f_beta: 0.8095238095238095
train: step: 4524, loss: 0.6629783511161804, acc: 0.75, recall: 0.75, precision: 0.750903251706142, f_beta: 0.7497747973175859
train: step: 4525, loss: 0.6054430603981018, acc: 0.77, recall: 0.77, precision: 0.7727272727272727, f_beta: 0.7694235588972431
train: step: 4526, loss: 0.5799292325973511, acc: 0.78, recall: 0.78, precision: 0.78, f_beta: 0.78
train: step: 4527, loss: 0.6306152939796448, acc: 0.81, recall: 0.81, precision: 0.8111200321156162, f_beta: 0.8098288459613652
train: step: 4528, loss: 0.524164080619812, acc: 0.84, recall: 0.84, precision: 0.8405448717948718, f_beta: 0.8399359743897559
train: step: 4529, loss: 0.590315043926239, acc: 0.81, recall: 0.81, precision: 0.8111200321156162, f_beta: 0.8098288459613652
train: step: 4530, loss: 0.5769990682601929, acc: 0.76, recall: 0.76, precision: 0.7637987012987013, f_beta: 0.7591328783621035
train: step: 4531, loss: 0.5428459048271179, acc: 0.78, recall: 0.78, precision: 0.7818035426731079, f_beta: 0.7796474358974359
train: step: 4532, loss: 0.5574714541435242, acc: 0.78, recall: 0.78, precision: 0.7818035426731079, f_beta: 0.7796474358974359
train: step: 4533, loss: 0.6043167114257812, acc: 0.83, recall: 0.8300000000000001, precision: 0.8365973072215422, f_beta: 0.8291628982011858
train: step: 4534, loss: 0.5684875249862671, acc: 0.79, recall: 0.79, precision: 0.8047498949138294, f_beta: 0.7874278773155178
train: step: 4535, loss: 0.5328588485717773, acc: 0.8, recall: 0.8, precision: 0.8019323671497585, f_beta: 0.7996794871794872
train: step: 4536, loss: 0.5931744575500488, acc: 0.78, recall: 0.78, precision: 0.78, f_beta: 0.78
train: step: 4537, loss: 0.6369352340698242, acc: 0.8, recall: 0.8, precision: 0.8004807692307692, f_beta: 0.7999199679871949
train: step: 4538, loss: 0.6105107069015503, acc: 0.83, recall: 0.8300000000000001, precision: 0.8311922922521076, f_beta: 0.8298468621759585
train: step: 4539, loss: 0.5599908232688904, acc: 0.84, recall: 0.8400000000000001, precision: 0.8421900161030595, f_beta: 0.8397435897435896
train: step: 4540, loss: 0.5712152123451233, acc: 0.78, recall: 0.78, precision: 0.7873563218390804, f_beta: 0.7785829307568439
train: step: 4541, loss: 0.4949333667755127, acc: 0.83, recall: 0.8300000000000001, precision: 0.8333333333333334, f_beta: 0.8295739348370927
train: step: 4542, loss: 0.6145792007446289, acc: 0.73, recall: 0.73, precision: 0.7323232323232323, f_beta: 0.7293233082706766
train: step: 4543, loss: 0.4865370988845825, acc: 0.85, recall: 0.85, precision: 0.8617197188921042, f_beta: 0.8487750781328762
train: step: 4544, loss: 0.5678330063819885, acc: 0.77, recall: 0.77, precision: 0.7727272727272727, f_beta: 0.7694235588972431
train: step: 4545, loss: 0.5534436702728271, acc: 0.85, recall: 0.8500000000000001, precision: 0.8535353535353536, f_beta: 0.849624060150376
train: step: 4546, loss: 0.5365097522735596, acc: 0.81, recall: 0.81, precision: 0.8131313131313131, f_beta: 0.8095238095238095
train: step: 4547, loss: 0.5833666324615479, acc: 0.79, recall: 0.79, precision: 0.7929292929292929, f_beta: 0.7894736842105263
train: step: 4548, loss: 0.6276640892028809, acc: 0.77, recall: 0.77, precision: 0.7753977968176254, f_beta: 0.7688674505074867
train: step: 4549, loss: 0.6269041299819946, acc: 0.7, recall: 0.7, precision: 0.7170138888888888, f_beta: 0.6940024479804162
train: step: 4550, loss: 0.6175330877304077, acc: 0.78, recall: 0.78, precision: 0.7818035426731079, f_beta: 0.7796474358974359
train: step: 4551, loss: 0.5807816982269287, acc: 0.79, recall: 0.79, precision: 0.7957976336189311, f_beta: 0.7889659330720531
train: step: 4552, loss: 0.5795404314994812, acc: 0.76, recall: 0.76, precision: 0.7668308702791462, f_beta: 0.7584541062801933
train: step: 4553, loss: 0.6185068488121033, acc: 0.81, recall: 0.81, precision: 0.8257671290458175, f_beta: 0.8076728413807065
train: step: 4554, loss: 0.5560285449028015, acc: 0.76, recall: 0.76, precision: 0.7896613190730838, f_beta: 0.7536945812807881
train: step: 4555, loss: 0.589026689529419, acc: 0.78, recall: 0.78, precision: 0.780448717948718, f_beta: 0.7799119647859143
train: step: 4556, loss: 0.5942771434783936, acc: 0.84, recall: 0.8400000000000001, precision: 0.8489326765188834, f_beta: 0.8389694041867956
train: step: 4557, loss: 0.5246459245681763, acc: 0.86, recall: 0.8600000000000001, precision: 0.8652597402597402, f_beta: 0.8594941790445605
train: step: 4558, loss: 0.5917364954948425, acc: 0.78, recall: 0.78, precision: 0.780448717948718, f_beta: 0.7799119647859143
train: step: 4559, loss: 0.5181810259819031, acc: 0.83, recall: 0.83, precision: 0.8301320528211285, f_beta: 0.8299829982998299
train: step: 4560, loss: 0.5624769926071167, acc: 0.81, recall: 0.81, precision: 0.8111200321156162, f_beta: 0.8098288459613652
train: step: 4561, loss: 0.5672104954719543, acc: 0.79, recall: 0.79, precision: 0.7997106242248864, f_beta: 0.7882851093860268
train: step: 4562, loss: 0.6171827912330627, acc: 0.83, recall: 0.8300000000000001, precision: 0.8333333333333334, f_beta: 0.8295739348370927
train: step: 4563, loss: 0.48789870738983154, acc: 0.85, recall: 0.85, precision: 0.851264552388599, f_beta: 0.8498648783905516
train: step: 4564, loss: 0.549170970916748, acc: 0.79, recall: 0.79, precision: 0.8047498949138294, f_beta: 0.7874278773155178
train: step: 4565, loss: 0.5310566425323486, acc: 0.8, recall: 0.8, precision: 0.8019323671497585, f_beta: 0.7996794871794872
train: step: 4566, loss: 0.5672571063041687, acc: 0.78, recall: 0.78, precision: 0.7818035426731079, f_beta: 0.7796474358974359
train: step: 4567, loss: 0.5128467082977295, acc: 0.81, recall: 0.81, precision: 0.810124049619848, f_beta: 0.8099809980998101
train: step: 4568, loss: 0.5965158939361572, acc: 0.79, recall: 0.79, precision: 0.7929292929292929, f_beta: 0.7894736842105263
train: step: 4569, loss: 0.6147323250770569, acc: 0.74, recall: 0.74, precision: 0.7463054187192117, f_beta: 0.7383252818035426
train: step: 4570, loss: 0.5625587701797485, acc: 0.79, recall: 0.79, precision: 0.7929292929292929, f_beta: 0.7894736842105263
train: step: 4571, loss: 0.5676841139793396, acc: 0.77, recall: 0.77, precision: 0.7753977968176254, f_beta: 0.7688674505074867
train: step: 4572, loss: 0.5608514547348022, acc: 0.82, recall: 0.8200000000000001, precision: 0.8284072249589491, f_beta: 0.818840579710145
train: step: 4573, loss: 0.5196147561073303, acc: 0.85, recall: 0.8500000000000001, precision: 0.8535353535353536, f_beta: 0.849624060150376
train: step: 4574, loss: 0.5386985540390015, acc: 0.78, recall: 0.78, precision: 0.7971137521222411, f_beta: 0.7767857142857144
train: step: 4575, loss: 0.526440441608429, acc: 0.9, recall: 0.8999999999999999, precision: 0.9058441558441559, f_beta: 0.8996386993175431
train: step: 4576, loss: 0.6051059365272522, acc: 0.76, recall: 0.76, precision: 0.7668308702791462, f_beta: 0.7584541062801933
train: step: 4577, loss: 0.45297595858573914, acc: 0.9, recall: 0.8999999999999999, precision: 0.9058441558441559, f_beta: 0.8996386993175431
train: step: 4578, loss: 0.5193367600440979, acc: 0.88, recall: 0.88, precision: 0.8806089743589745, f_beta: 0.879951980792317
train: step: 4579, loss: 0.4925723671913147, acc: 0.85, recall: 0.85, precision: 0.8501400560224089, f_beta: 0.84998499849985
train: step: 4580, loss: 0.5901976823806763, acc: 0.84, recall: 0.8400000000000001, precision: 0.8421900161030595, f_beta: 0.8397435897435896
train: step: 4581, loss: 0.5956146121025085, acc: 0.8, recall: 0.8, precision: 0.8078817733990147, f_beta: 0.7987117552334944
train: step: 4582, loss: 0.6128761768341064, acc: 0.76, recall: 0.76, precision: 0.7821180555555556, f_beta: 0.755201958384333
train: step: 4583, loss: 0.4818403124809265, acc: 0.84, recall: 0.8400000000000001, precision: 0.8421900161030595, f_beta: 0.8397435897435896
train: step: 4584, loss: 0.5801049470901489, acc: 0.73, recall: 0.73, precision: 0.7377015295576685, f_beta: 0.7277951406391774
train: step: 4585, loss: 0.5689554810523987, acc: 0.8, recall: 0.8, precision: 0.8004807692307692, f_beta: 0.7999199679871949
train: step: 4586, loss: 0.5574533939361572, acc: 0.79, recall: 0.79, precision: 0.8047498949138294, f_beta: 0.7874278773155178
train: step: 4587, loss: 0.5530197620391846, acc: 0.79, recall: 0.79, precision: 0.7901160464185675, f_beta: 0.78997899789979
train: step: 4588, loss: 0.5743038654327393, acc: 0.82, recall: 0.8200000000000001, precision: 0.8333333333333334, f_beta: 0.8181818181818181
train: step: 4589, loss: 0.5426551699638367, acc: 0.85, recall: 0.85, precision: 0.851264552388599, f_beta: 0.8498648783905516
train: step: 4590, loss: 0.4553239345550537, acc: 0.9, recall: 0.8999999999999999, precision: 0.9025764895330113, f_beta: 0.8998397435897436
train: step: 4591, loss: 0.5546518564224243, acc: 0.83, recall: 0.8300000000000001, precision: 0.8311922922521076, f_beta: 0.8298468621759585
train: step: 4592, loss: 0.6233989000320435, acc: 0.75, recall: 0.75, precision: 0.7549979600163199, f_beta: 0.7487689679429204
train: step: 4593, loss: 0.5630679130554199, acc: 0.75, recall: 0.75, precision: 0.7501000400160064, f_beta: 0.7499749974997499
train: step: 4594, loss: 0.5292248129844666, acc: 0.83, recall: 0.8300000000000001, precision: 0.8365973072215422, f_beta: 0.8291628982011858
train: step: 4595, loss: 0.5525878667831421, acc: 0.79, recall: 0.79, precision: 0.7910477719791249, f_beta: 0.7898108297467722
train: step: 4596, loss: 0.5819380283355713, acc: 0.79, recall: 0.79, precision: 0.8047498949138294, f_beta: 0.7874278773155178
train: step: 4597, loss: 0.5960522294044495, acc: 0.81, recall: 0.81, precision: 0.8324753324753325, f_beta: 0.8067338012409724
train: step: 4598, loss: 0.5582355260848999, acc: 0.83, recall: 0.83, precision: 0.8301320528211285, f_beta: 0.8299829982998299
train: step: 4599, loss: 0.5774705410003662, acc: 0.78, recall: 0.78, precision: 0.7840909090909092, f_beta: 0.779205138498595
train: step: 4600, loss: 0.5022545456886292, acc: 0.84, recall: 0.84, precision: 0.8405448717948718, f_beta: 0.8399359743897559
train: step: 4601, loss: 0.5298789143562317, acc: 0.81, recall: 0.81, precision: 0.8131313131313131, f_beta: 0.8095238095238095
train: step: 4602, loss: 0.5346958041191101, acc: 0.84, recall: 0.8400000000000001, precision: 0.8421900161030595, f_beta: 0.8397435897435896
train: step: 4603, loss: 0.4829464852809906, acc: 0.87, recall: 0.87, precision: 0.8713368125250903, f_beta: 0.8698828946051447
train: step: 4604, loss: 0.5851142406463623, acc: 0.77, recall: 0.77, precision: 0.7753977968176254, f_beta: 0.7688674505074867
train: step: 4605, loss: 0.6365162134170532, acc: 0.67, recall: 0.67, precision: 0.6700680272108843, f_beta: 0.66996699669967
train: step: 4606, loss: 0.5846948027610779, acc: 0.75, recall: 0.75, precision: 0.7549979600163199, f_beta: 0.7487689679429204
train: step: 4607, loss: 0.5138823986053467, acc: 0.8, recall: 0.8, precision: 0.8043831168831169, f_beta: 0.7992773986350863
train: step: 4608, loss: 0.5093902349472046, acc: 0.83, recall: 0.8300000000000001, precision: 0.8365973072215422, f_beta: 0.8291628982011858
train: step: 4609, loss: 0.512968122959137, acc: 0.83, recall: 0.83, precision: 0.8301320528211285, f_beta: 0.8299829982998299
train: step: 4610, loss: 0.6091773509979248, acc: 0.73, recall: 0.73, precision: 0.7300920368147259, f_beta: 0.72997299729973
train: step: 4611, loss: 0.6044188141822815, acc: 0.77, recall: 0.77, precision: 0.7837326607818411, f_beta: 0.767182913250329
train: step: 4612, loss: 0.5476070046424866, acc: 0.78, recall: 0.78, precision: 0.8038194444444444, f_beta: 0.7756017951856385
train: step: 4613, loss: 0.5018184185028076, acc: 0.85, recall: 0.85, precision: 0.851264552388599, f_beta: 0.8498648783905516
train: step: 4614, loss: 0.5433405637741089, acc: 0.78, recall: 0.78, precision: 0.8038194444444444, f_beta: 0.7756017951856385
train: step: 4615, loss: 0.5256398916244507, acc: 0.82, recall: 0.8200000000000001, precision: 0.8284072249589491, f_beta: 0.818840579710145
train: step: 4616, loss: 0.5569851994514465, acc: 0.76, recall: 0.76, precision: 0.7708333333333333, f_beta: 0.7575757575757576
train: step: 4617, loss: 0.6039853692054749, acc: 0.73, recall: 0.73, precision: 0.7466752466752467, f_beta: 0.7253585596582239
train: step: 4618, loss: 0.617870032787323, acc: 0.72, recall: 0.72, precision: 0.7257799671592775, f_beta: 0.7181964573268922
train: step: 4619, loss: 0.5586792230606079, acc: 0.8, recall: 0.8, precision: 0.8019323671497585, f_beta: 0.7996794871794872
train: step: 4620, loss: 0.5993191599845886, acc: 0.78, recall: 0.78, precision: 0.7840909090909092, f_beta: 0.779205138498595
train: step: 4621, loss: 0.513621985912323, acc: 0.84, recall: 0.84, precision: 0.8405448717948718, f_beta: 0.8399359743897559
train: step: 4622, loss: 0.6367357969284058, acc: 0.69, recall: 0.69, precision: 0.7533333333333334, f_beta: 0.6693333333333333
train: step: 4623, loss: 0.5436442494392395, acc: 0.82, recall: 0.8200000000000001, precision: 0.8472222222222222, f_beta: 0.8164014687882497
train: step: 4624, loss: 0.6118446588516235, acc: 0.74, recall: 0.74, precision: 0.74, f_beta: 0.74
train: step: 4625, loss: 0.6465505957603455, acc: 0.71, recall: 0.71, precision: 0.7170318313352625, f_beta: 0.7076318177235609
train: step: 4626, loss: 0.5348714590072632, acc: 0.86, recall: 0.86, precision: 0.875, f_beta: 0.8585858585858586
train: step: 4627, loss: 0.6389744281768799, acc: 0.76, recall: 0.76, precision: 0.8095238095238095, f_beta: 0.75
train: step: 4628, loss: 0.5778379440307617, acc: 0.86, recall: 0.8600000000000001, precision: 0.8652597402597402, f_beta: 0.8594941790445605
train: step: 4629, loss: 0.580838143825531, acc: 0.83, recall: 0.8300000000000001, precision: 0.8333333333333334, f_beta: 0.8295739348370927
train: step: 4630, loss: 0.6091356873512268, acc: 0.75, recall: 0.75, precision: 0.750903251706142, f_beta: 0.7497747973175859
train: step: 4631, loss: 0.6103031635284424, acc: 0.76, recall: 0.76, precision: 0.7668308702791462, f_beta: 0.7584541062801933
train: step: 4632, loss: 0.5401413440704346, acc: 0.83, recall: 0.83, precision: 0.8467843631778058, f_beta: 0.8279178054458953
train: step: 4633, loss: 0.5553879737854004, acc: 0.86, recall: 0.86, precision: 0.8605769230769231, f_beta: 0.8599439775910365
train: step: 4634, loss: 0.5650680065155029, acc: 0.8, recall: 0.8, precision: 0.8004807692307692, f_beta: 0.7999199679871949
train: step: 4635, loss: 0.587185263633728, acc: 0.77, recall: 0.77, precision: 0.7709755118426336, f_beta: 0.769792813532179
train: step: 4636, loss: 0.5835724472999573, acc: 0.76, recall: 0.76, precision: 0.7637987012987013, f_beta: 0.7591328783621035
train: step: 4637, loss: 0.557466447353363, acc: 0.76, recall: 0.76, precision: 0.7637987012987013, f_beta: 0.7591328783621035
train: step: 4638, loss: 0.6288967132568359, acc: 0.72, recall: 0.72, precision: 0.7232142857142857, f_beta: 0.7189883580891208
train: step: 4639, loss: 0.5603580474853516, acc: 0.77, recall: 0.77, precision: 0.7837326607818411, f_beta: 0.767182913250329
train: step: 4640, loss: 0.5725971460342407, acc: 0.76, recall: 0.76, precision: 0.7616747181964574, f_beta: 0.7596153846153846
train: step: 4641, loss: 0.5567577481269836, acc: 0.78, recall: 0.78, precision: 0.7916666666666666, f_beta: 0.7777777777777779
train: step: 4642, loss: 0.530788242816925, acc: 0.83, recall: 0.83, precision: 0.8301320528211285, f_beta: 0.8299829982998299
train: step: 4643, loss: 0.6134290099143982, acc: 0.7, recall: 0.7, precision: 0.748015873015873, f_beta: 0.6847414880201765
train: step: 4644, loss: 0.6438511610031128, acc: 0.68, recall: 0.6799999999999999, precision: 0.6826298701298701, f_beta: 0.6788438378161381
train: step: 4645, loss: 0.46728062629699707, acc: 0.87, recall: 0.87, precision: 0.8701480592236894, f_beta: 0.8699869986998701
train: step: 4646, loss: 0.5348858833312988, acc: 0.81, recall: 0.81, precision: 0.8161974704202366, f_beta: 0.8090644156366193
train: step: 4647, loss: 0.5352908968925476, acc: 0.86, recall: 0.86, precision: 0.8820033955857385, f_beta: 0.8579545454545454
train: step: 4648, loss: 0.5736697316169739, acc: 0.73, recall: 0.73, precision: 0.7792617775619233, f_beta: 0.7175436761167486
train: step: 4649, loss: 0.5293766260147095, acc: 0.81, recall: 0.81, precision: 0.8203803224472923, f_beta: 0.8084484323016432
train: step: 4650, loss: 0.5300436615943909, acc: 0.8, recall: 0.8, precision: 0.8183361629881154, f_beta: 0.797077922077922
train: step: 4651, loss: 0.5777071118354797, acc: 0.8, recall: 0.8, precision: 0.8019323671497585, f_beta: 0.7996794871794872
train: step: 4652, loss: 0.5431005358695984, acc: 0.83, recall: 0.8300000000000001, precision: 0.8410500206696983, f_beta: 0.8286117552172598
train: step: 4653, loss: 0.4807547330856323, acc: 0.84, recall: 0.84, precision: 0.8689236111111112, f_beta: 0.8368013055895552
train: step: 4654, loss: 0.5476042628288269, acc: 0.81, recall: 0.81, precision: 0.8203803224472923, f_beta: 0.8084484323016432
train: step: 4655, loss: 0.5711613893508911, acc: 0.81, recall: 0.81, precision: 0.8324753324753325, f_beta: 0.8067338012409724
train: step: 4656, loss: 0.5912667512893677, acc: 0.74, recall: 0.74, precision: 0.7546689303904923, f_beta: 0.7362012987012987
train: step: 4657, loss: 0.5555684566497803, acc: 0.83, recall: 0.8300000000000001, precision: 0.8311922922521076, f_beta: 0.8298468621759585
train: step: 4658, loss: 0.5514657497406006, acc: 0.85, recall: 0.85, precision: 0.851264552388599, f_beta: 0.8498648783905516
train: step: 4659, loss: 0.5562352538108826, acc: 0.8, recall: 0.8, precision: 0.8019323671497585, f_beta: 0.7996794871794872
train: step: 4660, loss: 0.5292232632637024, acc: 0.83, recall: 0.83, precision: 0.853925353925354, f_beta: 0.8270776116366596
train: step: 4661, loss: 0.6394596695899963, acc: 0.68, recall: 0.6799999999999999, precision: 0.7068014705882353, f_beta: 0.6692848284415047
train: step: 4662, loss: 0.4792802035808563, acc: 0.86, recall: 0.86, precision: 0.875, f_beta: 0.8585858585858586
train: step: 4663, loss: 0.5814671516418457, acc: 0.75, recall: 0.75, precision: 0.7681252681252682, f_beta: 0.7457023700539112
train: step: 4664, loss: 0.5231044888496399, acc: 0.85, recall: 0.85, precision: 0.8617197188921042, f_beta: 0.8487750781328762
train: step: 4665, loss: 0.5153148174285889, acc: 0.8, recall: 0.8, precision: 0.8342245989304813, f_beta: 0.794745484400657
train: step: 4666, loss: 0.6407314538955688, acc: 0.65, recall: 0.65, precision: 0.6550227366680447, f_beta: 0.6471418489767113
train: step: 4667, loss: 0.5614967942237854, acc: 0.85, recall: 0.85, precision: 0.851264552388599, f_beta: 0.8498648783905516
train: step: 4668, loss: 0.5567061305046082, acc: 0.83, recall: 0.8300000000000001, precision: 0.8311922922521076, f_beta: 0.8298468621759585
train: step: 4669, loss: 0.5600610971450806, acc: 0.78, recall: 0.78, precision: 0.7873563218390804, f_beta: 0.7785829307568439
train: step: 4670, loss: 0.5298151969909668, acc: 0.82, recall: 0.8200000000000001, precision: 0.8205128205128205, f_beta: 0.8199279711884755
train: step: 4671, loss: 0.6501933336257935, acc: 0.8, recall: 0.8, precision: 0.8183361629881154, f_beta: 0.797077922077922
train: step: 4672, loss: 0.6514031887054443, acc: 0.74, recall: 0.74, precision: 0.7857142857142857, f_beta: 0.7291666666666666
train: step: 4673, loss: 0.5978446006774902, acc: 0.72, recall: 0.72, precision: 0.733446519524618, f_beta: 0.7159090909090909
train: step: 4674, loss: 0.6435379981994629, acc: 0.67, recall: 0.6699999999999999, precision: 0.6756924348904505, f_beta: 0.667305171892328
train: step: 4675, loss: 0.5970498919487, acc: 0.72, recall: 0.72, precision: 0.7257799671592775, f_beta: 0.7181964573268922
train: step: 4676, loss: 0.6138246059417725, acc: 0.73, recall: 0.73, precision: 0.7527472527472527, f_beta: 0.7237851662404092
train: step: 4677, loss: 0.4855232536792755, acc: 0.84, recall: 0.84, precision: 0.8607809847198642, f_beta: 0.8376623376623376
train: step: 4678, loss: 0.6150538921356201, acc: 0.7, recall: 0.7, precision: 0.7741228070175439, f_beta: 0.6782496782496782
train: step: 4679, loss: 0.6124213933944702, acc: 0.71, recall: 0.71, precision: 0.7549781447304517, f_beta: 0.69662098545873
train: step: 4680, loss: 0.588325023651123, acc: 0.76, recall: 0.76, precision: 0.8224206349206349, f_beta: 0.7477931904161412
train: step: 4681, loss: 0.5555185675621033, acc: 0.78, recall: 0.78, precision: 0.7971137521222411, f_beta: 0.7767857142857144
train: step: 4682, loss: 0.5932500958442688, acc: 0.79, recall: 0.79, precision: 0.8047498949138294, f_beta: 0.7874278773155178
train: step: 4683, loss: 0.5648953318595886, acc: 0.78, recall: 0.78, precision: 0.8038194444444444, f_beta: 0.7756017951856385
train: step: 4684, loss: 0.5191167593002319, acc: 0.82, recall: 0.8200000000000001, precision: 0.8246753246753247, f_beta: 0.8193496587715776
train: step: 4685, loss: 0.5942036509513855, acc: 0.82, recall: 0.8200000000000001, precision: 0.8246753246753247, f_beta: 0.8193496587715776
train: step: 4686, loss: 0.5339677333831787, acc: 0.81, recall: 0.81, precision: 0.8406593406593407, f_beta: 0.8056265984654732
train: step: 4687, loss: 0.5217466354370117, acc: 0.83, recall: 0.83, precision: 0.8467843631778058, f_beta: 0.8279178054458953
train: step: 4688, loss: 0.6116831302642822, acc: 0.79, recall: 0.79, precision: 0.7997106242248864, f_beta: 0.7882851093860268
train: step: 4689, loss: 0.5443037748336792, acc: 0.79, recall: 0.79, precision: 0.811025311025311, f_beta: 0.7863899908452854
train: step: 4690, loss: 0.6014024019241333, acc: 0.83, recall: 0.8300000000000001, precision: 0.8311922922521076, f_beta: 0.8298468621759585
train: step: 4691, loss: 0.6350435018539429, acc: 0.72, recall: 0.72, precision: 0.7450980392156863, f_beta: 0.7126436781609196
train: step: 4692, loss: 0.5818659663200378, acc: 0.76, recall: 0.76, precision: 0.7708333333333333, f_beta: 0.7575757575757576
train: step: 4693, loss: 0.5393165946006775, acc: 0.82, recall: 0.8200000000000001, precision: 0.822061191626409, f_beta: 0.8197115384615385
train: step: 4694, loss: 0.5283381342887878, acc: 0.82, recall: 0.8200000000000001, precision: 0.8205128205128205, f_beta: 0.8199279711884755
train: step: 4695, loss: 0.5932962894439697, acc: 0.75, recall: 0.75, precision: 0.750903251706142, f_beta: 0.7497747973175859
train: step: 4696, loss: 0.6298929452896118, acc: 0.75, recall: 0.75, precision: 0.8170979198376458, f_beta: 0.736036321402175
train: step: 4697, loss: 0.539554238319397, acc: 0.84, recall: 0.8400000000000001, precision: 0.8449675324675325, f_beta: 0.839421918908069
train: step: 4698, loss: 0.5072076916694641, acc: 0.83, recall: 0.8300000000000001, precision: 0.8333333333333334, f_beta: 0.8295739348370927
train: step: 4699, loss: 0.5934975147247314, acc: 0.8, recall: 0.8, precision: 0.8183361629881154, f_beta: 0.797077922077922
train: step: 4700, loss: 0.5982003211975098, acc: 0.75, recall: 0.75, precision: 0.7747252747252746, f_beta: 0.7442455242966751
train: step: 4701, loss: 0.6298626661300659, acc: 0.78, recall: 0.78, precision: 0.7840909090909092, f_beta: 0.779205138498595
train: step: 4702, loss: 0.5933451652526855, acc: 0.82, recall: 0.82, precision: 0.8395585738539898, f_beta: 0.8173701298701298
train: step: 4703, loss: 0.4945495128631592, acc: 0.83, recall: 0.8300000000000001, precision: 0.8365973072215422, f_beta: 0.8291628982011858
train: step: 4704, loss: 0.6002993583679199, acc: 0.77, recall: 0.77, precision: 0.7895752895752897, f_beta: 0.7660461804495982
train: step: 4705, loss: 0.587365984916687, acc: 0.74, recall: 0.74, precision: 0.7546689303904923, f_beta: 0.7362012987012987
train: step: 4706, loss: 0.529731810092926, acc: 0.79, recall: 0.79, precision: 0.7957976336189311, f_beta: 0.7889659330720531
train: step: 4707, loss: 0.4595523476600647, acc: 0.87, recall: 0.87, precision: 0.8737373737373737, f_beta: 0.8696741854636592
train: step: 4708, loss: 0.6054954528808594, acc: 0.7, recall: 0.7, precision: 0.7228163992869875, f_beta: 0.6921182266009853
train: step: 4709, loss: 0.6302594542503357, acc: 0.71, recall: 0.71, precision: 0.7206809583858764, f_beta: 0.7064480210547626
train: step: 4710, loss: 0.5186598896980286, acc: 0.89, recall: 0.89, precision: 0.8914090726615818, f_beta: 0.8899009108197378
train: step: 4711, loss: 0.5271110534667969, acc: 0.86, recall: 0.86, precision: 0.8623188405797102, f_beta: 0.859775641025641
train: step: 4712, loss: 0.5611184239387512, acc: 0.8, recall: 0.8, precision: 0.8019323671497585, f_beta: 0.7996794871794872
train: step: 4713, loss: 0.551452100276947, acc: 0.81, recall: 0.81, precision: 0.8161974704202366, f_beta: 0.8090644156366193
train: step: 4714, loss: 0.5673868656158447, acc: 0.77, recall: 0.77, precision: 0.7709755118426336, f_beta: 0.769792813532179
train: step: 4715, loss: 0.5786538124084473, acc: 0.82, recall: 0.8200000000000001, precision: 0.8284072249589491, f_beta: 0.818840579710145
train: step: 4716, loss: 0.528303861618042, acc: 0.79, recall: 0.79, precision: 0.7957976336189311, f_beta: 0.7889659330720531
train: step: 4717, loss: 0.646454393863678, acc: 0.75, recall: 0.75, precision: 0.7826775214834916, f_beta: 0.7425599835238389
train: step: 4718, loss: 0.48537981510162354, acc: 0.85, recall: 0.85, precision: 0.851264552388599, f_beta: 0.8498648783905516
train: step: 4719, loss: 0.640828013420105, acc: 0.66, recall: 0.6599999999999999, precision: 0.6610305958132046, f_beta: 0.6594551282051282
train: step: 4720, loss: 0.5637791156768799, acc: 0.83, recall: 0.8300000000000001, precision: 0.8333333333333334, f_beta: 0.8295739348370927
train: step: 4721, loss: 0.5212051272392273, acc: 0.83, recall: 0.83, precision: 0.8301320528211285, f_beta: 0.8299829982998299
train: step: 4722, loss: 0.580176591873169, acc: 0.84, recall: 0.84, precision: 0.8405448717948718, f_beta: 0.8399359743897559
train: step: 4723, loss: 0.6115651726722717, acc: 0.72, recall: 0.72, precision: 0.733446519524618, f_beta: 0.7159090909090909
train: step: 4724, loss: 0.5293647050857544, acc: 0.81, recall: 0.81, precision: 0.8505201266395297, f_beta: 0.8043455874781176
train: step: 4725, loss: 0.6059777140617371, acc: 0.75, recall: 0.75, precision: 0.7501000400160064, f_beta: 0.7499749974997499
train: step: 4726, loss: 0.4811173975467682, acc: 0.87, recall: 0.87, precision: 0.8823894171145101, f_beta: 0.8689384010484928
train: step: 4727, loss: 0.5553603172302246, acc: 0.78, recall: 0.78, precision: 0.7916666666666666, f_beta: 0.7777777777777779
train: step: 4728, loss: 0.6393411159515381, acc: 0.74, recall: 0.74, precision: 0.75, f_beta: 0.7373737373737373
train: step: 4729, loss: 0.5635450482368469, acc: 0.74, recall: 0.74, precision: 0.75, f_beta: 0.7373737373737373
train: step: 4730, loss: 0.5621371865272522, acc: 0.77, recall: 0.77, precision: 0.7727272727272727, f_beta: 0.7694235588972431
train: step: 4731, loss: 0.47435811161994934, acc: 0.88, recall: 0.88, precision: 0.8806089743589745, f_beta: 0.879951980792317
train: step: 4732, loss: 0.5963525772094727, acc: 0.77, recall: 0.77, precision: 0.7967032967032968, f_beta: 0.7647058823529411
train: step: 4733, loss: 0.5327812433242798, acc: 0.85, recall: 0.8500000000000001, precision: 0.8569971440228478, f_beta: 0.8492613807657521
train: step: 4734, loss: 0.630878746509552, acc: 0.68, recall: 0.6799999999999999, precision: 0.6953125, f_beta: 0.6736026111791106
train: step: 4735, loss: 0.6042568683624268, acc: 0.77, recall: 0.77, precision: 0.7753977968176254, f_beta: 0.7688674505074867
train: step: 4736, loss: 0.5250611305236816, acc: 0.81, recall: 0.81, precision: 0.8161974704202366, f_beta: 0.8090644156366193
train: step: 4737, loss: 0.5836241841316223, acc: 0.79, recall: 0.79, precision: 0.7997106242248864, f_beta: 0.7882851093860268
train: step: 4738, loss: 0.5691624879837036, acc: 0.77, recall: 0.77, precision: 0.7701080432172869, f_beta: 0.7699769976997699
train: step: 4739, loss: 0.5547954440116882, acc: 0.78, recall: 0.78, precision: 0.7873563218390804, f_beta: 0.7785829307568439
train: step: 4740, loss: 0.5119797587394714, acc: 0.85, recall: 0.8500000000000001, precision: 0.8535353535353536, f_beta: 0.849624060150376
train: step: 4741, loss: 0.5517736077308655, acc: 0.84, recall: 0.84, precision: 0.8405448717948718, f_beta: 0.8399359743897559
train: step: 4742, loss: 0.5674337148666382, acc: 0.76, recall: 0.76, precision: 0.7637987012987013, f_beta: 0.7591328783621035
train: step: 4743, loss: 0.5923781991004944, acc: 0.79, recall: 0.79, precision: 0.7997106242248864, f_beta: 0.7882851093860268
train: step: 4744, loss: 0.5924445986747742, acc: 0.76, recall: 0.76, precision: 0.7604166666666667, f_beta: 0.7599039615846339
train: step: 4745, loss: 0.6335737109184265, acc: 0.71, recall: 0.71, precision: 0.7252252252252251, f_beta: 0.7050147492625368
train: step: 4746, loss: 0.590402364730835, acc: 0.81, recall: 0.81, precision: 0.8131313131313131, f_beta: 0.8095238095238095
train: step: 4747, loss: 0.6259458661079407, acc: 0.78, recall: 0.78, precision: 0.7916666666666666, f_beta: 0.7777777777777779
train: step: 4748, loss: 0.644676685333252, acc: 0.63, recall: 0.63, precision: 0.6304696908871938, f_beta: 0.629666700030027
train: step: 4749, loss: 0.5757659673690796, acc: 0.83, recall: 0.83, precision: 0.8467843631778058, f_beta: 0.8279178054458953
train: step: 4750, loss: 0.5552660822868347, acc: 0.79, recall: 0.79, precision: 0.7901160464185675, f_beta: 0.78997899789979
train: step: 4751, loss: 0.5897872447967529, acc: 0.81, recall: 0.81, precision: 0.8131313131313131, f_beta: 0.8095238095238095
train: step: 4752, loss: 0.5523761510848999, acc: 0.83, recall: 0.83, precision: 0.8467843631778058, f_beta: 0.8279178054458953
train: step: 4753, loss: 0.5777482986450195, acc: 0.74, recall: 0.74, precision: 0.7546689303904923, f_beta: 0.7362012987012987
train: step: 4754, loss: 0.596148669719696, acc: 0.78, recall: 0.78, precision: 0.7873563218390804, f_beta: 0.7785829307568439
train: step: 4755, loss: 0.5732311010360718, acc: 0.77, recall: 0.77, precision: 0.7895752895752897, f_beta: 0.7660461804495982
train: step: 4756, loss: 0.5327968597412109, acc: 0.82, recall: 0.8200000000000001, precision: 0.8205128205128205, f_beta: 0.8199279711884755
train: step: 4757, loss: 0.5706776976585388, acc: 0.79, recall: 0.79, precision: 0.7910477719791249, f_beta: 0.7898108297467722
train: step: 4758, loss: 0.5759518146514893, acc: 0.84, recall: 0.8400000000000001, precision: 0.8489326765188834, f_beta: 0.8389694041867956
train: step: 4759, loss: 0.5208286046981812, acc: 0.86, recall: 0.86, precision: 0.875, f_beta: 0.8585858585858586
train: step: 4760, loss: 0.47872230410575867, acc: 0.86, recall: 0.8600000000000001, precision: 0.8652597402597402, f_beta: 0.8594941790445605
train: step: 4761, loss: 0.5366162061691284, acc: 0.83, recall: 0.8300000000000001, precision: 0.8311922922521076, f_beta: 0.8298468621759585
train: step: 4762, loss: 0.5328096151351929, acc: 0.83, recall: 0.8300000000000001, precision: 0.8365973072215422, f_beta: 0.8291628982011858
train: step: 4763, loss: 0.5374717116355896, acc: 0.82, recall: 0.8200000000000001, precision: 0.8246753246753247, f_beta: 0.8193496587715776
train: step: 4764, loss: 0.724733829498291, acc: 0.19, recall: 0.19, precision: 0.16752466752466752, f_beta: 0.17607567897467197
train: step: 4765, loss: 0.6398745179176331, acc: 0.71, recall: 0.71, precision: 0.7170318313352625, f_beta: 0.7076318177235609
train: step: 4766, loss: 0.6254185438156128, acc: 0.77, recall: 0.77, precision: 0.7967032967032968, f_beta: 0.7647058823529411
train: step: 4767, loss: 0.5436645746231079, acc: 0.86, recall: 0.86, precision: 0.8694581280788177, f_beta: 0.8590982286634461
train: step: 4768, loss: 0.5580949783325195, acc: 0.82, recall: 0.8200000000000001, precision: 0.8333333333333334, f_beta: 0.8181818181818181
train: step: 4769, loss: 0.6157753467559814, acc: 0.7, recall: 0.7, precision: 0.7297794117647058, f_beta: 0.6899545266639107
train: step: 4770, loss: 0.4950539469718933, acc: 0.88, recall: 0.88, precision: 0.8824476650563606, f_beta: 0.8798076923076923
train: step: 4771, loss: 0.5821539163589478, acc: 0.79, recall: 0.79, precision: 0.8047498949138294, f_beta: 0.7874278773155178
train: step: 4772, loss: 0.5431892275810242, acc: 0.83, recall: 0.8300000000000001, precision: 0.8410500206696983, f_beta: 0.8286117552172598
train: step: 4773, loss: 0.60382479429245, acc: 0.73, recall: 0.73, precision: 0.7917300862506342, f_beta: 0.7149192271143491
train: step: 4774, loss: 0.6283660531044006, acc: 0.69, recall: 0.69, precision: 0.7037752037752039, f_beta: 0.6846709388668497
train: step: 4775, loss: 0.619371235370636, acc: 0.73, recall: 0.73, precision: 0.7466752466752467, f_beta: 0.7253585596582239
train: step: 4776, loss: 0.5621858239173889, acc: 0.89, recall: 0.8899999999999999, precision: 0.8939393939393939, f_beta: 0.8897243107769424
train: step: 4777, loss: 0.4912479817867279, acc: 0.83, recall: 0.8300000000000001, precision: 0.8365973072215422, f_beta: 0.8291628982011858
train: step: 4778, loss: 0.608947217464447, acc: 0.76, recall: 0.76, precision: 0.7616747181964574, f_beta: 0.7596153846153846
train: step: 4779, loss: 0.5050559043884277, acc: 0.85, recall: 0.85, precision: 0.851264552388599, f_beta: 0.8498648783905516
train: step: 4780, loss: 0.5111005902290344, acc: 0.82, recall: 0.8200000000000001, precision: 0.822061191626409, f_beta: 0.8197115384615385
train: step: 4781, loss: 0.566704273223877, acc: 0.79, recall: 0.79, precision: 0.7910477719791249, f_beta: 0.7898108297467722
train: step: 4782, loss: 0.49661949276924133, acc: 0.87, recall: 0.87, precision: 0.8713368125250903, f_beta: 0.8698828946051447
train: step: 4783, loss: 0.5102847814559937, acc: 0.85, recall: 0.85, precision: 0.851264552388599, f_beta: 0.8498648783905516
train: step: 4784, loss: 0.622348964214325, acc: 0.79, recall: 0.79, precision: 0.7997106242248864, f_beta: 0.7882851093860268
train: step: 4785, loss: 0.5273334980010986, acc: 0.81, recall: 0.81, precision: 0.8131313131313131, f_beta: 0.8095238095238095
train: step: 4786, loss: 0.613407552242279, acc: 0.73, recall: 0.73, precision: 0.7345981232150143, f_beta: 0.7286704853783539
train: step: 4787, loss: 0.5386788249015808, acc: 0.79, recall: 0.79, precision: 0.8047498949138294, f_beta: 0.7874278773155178
train: step: 4788, loss: 0.5680747628211975, acc: 0.79, recall: 0.79, precision: 0.7910477719791249, f_beta: 0.7898108297467722
train: step: 4789, loss: 0.48478278517723083, acc: 0.84, recall: 0.84, precision: 0.8405448717948718, f_beta: 0.8399359743897559
train: step: 4790, loss: 0.5828590393066406, acc: 0.82, recall: 0.8200000000000001, precision: 0.822061191626409, f_beta: 0.8197115384615385
train: step: 4791, loss: 0.6091235876083374, acc: 0.74, recall: 0.74, precision: 0.7415458937198067, f_beta: 0.7395833333333335
train: step: 4792, loss: 0.47364571690559387, acc: 0.85, recall: 0.8500000000000001, precision: 0.8569971440228478, f_beta: 0.8492613807657521
train: step: 4793, loss: 0.601111114025116, acc: 0.82, recall: 0.8200000000000001, precision: 0.8205128205128205, f_beta: 0.8199279711884755
train: step: 4794, loss: 0.6838790774345398, acc: 0.54, recall: 0.54, precision: 0.5548245614035088, f_beta: 0.5066495066495066
train: step: 4795, loss: 0.5916178822517395, acc: 0.72, recall: 0.72, precision: 0.7214170692431562, f_beta: 0.7195512820512822
train: step: 4796, loss: 0.5326265692710876, acc: 0.83, recall: 0.8300000000000001, precision: 0.8333333333333334, f_beta: 0.8295739348370927
train: step: 4797, loss: 0.5140278339385986, acc: 0.82, recall: 0.8200000000000001, precision: 0.8205128205128205, f_beta: 0.8199279711884755
train: step: 4798, loss: 0.49861007928848267, acc: 0.86, recall: 0.86, precision: 0.8605769230769231, f_beta: 0.8599439775910365
train: step: 4799, loss: 0.5918460488319397, acc: 0.75, recall: 0.75, precision: 0.7583712277800745, f_beta: 0.7479584635547939
train: step: 4800, loss: 0.5875507593154907, acc: 0.73, recall: 0.73, precision: 0.7377015295576685, f_beta: 0.7277951406391774
train: step: 4801, loss: 0.5718382000923157, acc: 0.8, recall: 0.8, precision: 0.8183361629881154, f_beta: 0.797077922077922
train: step: 4802, loss: 0.5245173573493958, acc: 0.82, recall: 0.8200000000000001, precision: 0.8246753246753247, f_beta: 0.8193496587715776
train: step: 4803, loss: 0.5957080721855164, acc: 0.73, recall: 0.73, precision: 0.7323232323232323, f_beta: 0.7293233082706766
train: step: 4804, loss: 0.5847690105438232, acc: 0.8, recall: 0.8, precision: 0.8043831168831169, f_beta: 0.7992773986350863
train: step: 4805, loss: 0.5557883977890015, acc: 0.77, recall: 0.77, precision: 0.7709755118426336, f_beta: 0.769792813532179
train: step: 4806, loss: 0.651164174079895, acc: 0.68, recall: 0.6799999999999999, precision: 0.6826298701298701, f_beta: 0.6788438378161381
train: step: 4807, loss: 0.5566478371620178, acc: 0.79, recall: 0.79, precision: 0.7997106242248864, f_beta: 0.7882851093860268
train: step: 4808, loss: 0.6724942922592163, acc: 0.72, recall: 0.72, precision: 0.7257799671592775, f_beta: 0.7181964573268922
train: step: 4809, loss: 0.6016808152198792, acc: 0.78, recall: 0.78, precision: 0.7818035426731079, f_beta: 0.7796474358974359
train: step: 4810, loss: 0.5814186334609985, acc: 0.84, recall: 0.8400000000000001, precision: 0.8421900161030595, f_beta: 0.8397435897435896
train: step: 4811, loss: 0.5925804376602173, acc: 0.77, recall: 0.77, precision: 0.7701080432172869, f_beta: 0.7699769976997699
train: step: 4812, loss: 0.6080734729766846, acc: 0.75, recall: 0.75, precision: 0.7501000400160064, f_beta: 0.7499749974997499
train: step: 4813, loss: 0.5171287655830383, acc: 0.83, recall: 0.83, precision: 0.8301320528211285, f_beta: 0.8299829982998299
train: step: 4814, loss: 0.483296275138855, acc: 0.86, recall: 0.86, precision: 0.8605769230769231, f_beta: 0.8599439775910365
train: step: 4815, loss: 0.5765134692192078, acc: 0.78, recall: 0.78, precision: 0.780448717948718, f_beta: 0.7799119647859143
train: step: 4816, loss: 0.5397846102714539, acc: 0.81, recall: 0.81, precision: 0.8161974704202366, f_beta: 0.8090644156366193
train: step: 4817, loss: 0.5199521780014038, acc: 0.82, recall: 0.8200000000000001, precision: 0.8205128205128205, f_beta: 0.8199279711884755
train: step: 4818, loss: 0.5156640410423279, acc: 0.85, recall: 0.85, precision: 0.851264552388599, f_beta: 0.8498648783905516
train: step: 4819, loss: 0.4864787757396698, acc: 0.83, recall: 0.8300000000000001, precision: 0.8311922922521076, f_beta: 0.8298468621759585
train: step: 4820, loss: 0.49880436062812805, acc: 0.83, recall: 0.8300000000000001, precision: 0.8333333333333334, f_beta: 0.8295739348370927
train: step: 4821, loss: 0.6005434393882751, acc: 0.84, recall: 0.84, precision: 0.8405448717948718, f_beta: 0.8399359743897559
train: step: 4822, loss: 0.5498904585838318, acc: 0.86, recall: 0.8600000000000001, precision: 0.8652597402597402, f_beta: 0.8594941790445605
train: step: 4823, loss: 0.5777878761291504, acc: 0.78, recall: 0.78, precision: 0.8119429590017826, f_beta: 0.7742200328407225
train: step: 4824, loss: 0.6176385283470154, acc: 0.78, recall: 0.78, precision: 0.7818035426731079, f_beta: 0.7796474358974359
train: step: 4825, loss: 0.5342434048652649, acc: 0.82, recall: 0.8200000000000001, precision: 0.8246753246753247, f_beta: 0.8193496587715776
train: step: 4826, loss: 0.5920151472091675, acc: 0.74, recall: 0.74, precision: 0.7435064935064934, f_beta: 0.7390606182256123
train: step: 4827, loss: 0.5439761877059937, acc: 0.83, recall: 0.8300000000000001, precision: 0.8410500206696983, f_beta: 0.8286117552172598
train: step: 4828, loss: 0.511127233505249, acc: 0.85, recall: 0.85, precision: 0.867801597309794, f_beta: 0.848162769511084
train: step: 4829, loss: 0.6241583824157715, acc: 0.76, recall: 0.76, precision: 0.7896613190730838, f_beta: 0.7536945812807881
train: step: 4830, loss: 0.5958439111709595, acc: 0.74, recall: 0.74, precision: 0.7757352941176471, f_beta: 0.7312939231087227
train: step: 4831, loss: 0.6036312580108643, acc: 0.77, recall: 0.77, precision: 0.8155680224403927, f_beta: 0.7613860358958398
train: step: 4832, loss: 0.5565641522407532, acc: 0.82, recall: 0.82, precision: 0.8395585738539898, f_beta: 0.8173701298701298
train: step: 4833, loss: 0.5848121047019958, acc: 0.76, recall: 0.76, precision: 0.7758913412563667, f_beta: 0.7564935064935063
train: step: 4834, loss: 0.5629388093948364, acc: 0.75, recall: 0.75, precision: 0.8170979198376458, f_beta: 0.736036321402175
train: step: 4835, loss: 0.6306093335151672, acc: 0.74, recall: 0.74, precision: 0.7604166666666667, f_beta: 0.7348021215830274
train: step: 4836, loss: 0.580121636390686, acc: 0.79, recall: 0.79, precision: 0.8186813186813187, f_beta: 0.7851662404092071
train: step: 4837, loss: 0.6229990124702454, acc: 0.73, recall: 0.73, precision: 0.7688172043010753, f_beta: 0.7198879551820727
train: step: 4838, loss: 0.6930981278419495, acc: 0.56, recall: 0.56, precision: 0.5603864734299517, f_beta: 0.5592948717948718
train: step: 4839, loss: 0.6035404205322266, acc: 0.72, recall: 0.72, precision: 0.7527573529411764, f_beta: 0.7106242248863166
train: step: 4840, loss: 0.6366631388664246, acc: 0.67, recall: 0.6699999999999999, precision: 0.6922207146087743, f_beta: 0.6601791782514674
train: step: 4841, loss: 0.6426125168800354, acc: 0.75, recall: 0.75, precision: 0.7549979600163199, f_beta: 0.7487689679429204
train: step: 4842, loss: 0.6857060194015503, acc: 0.66, recall: 0.66, precision: 0.669779286926995, f_beta: 0.6550324675324675
train: step: 4843, loss: 0.6461944580078125, acc: 0.65, recall: 0.65, precision: 0.7436647173489279, f_beta: 0.612789025334661
train: step: 4844, loss: 0.5322082042694092, acc: 0.85, recall: 0.85, precision: 0.8753753753753755, f_beta: 0.8474214220323468
train: step: 4845, loss: 0.5255767107009888, acc: 0.79, recall: 0.79, precision: 0.8389434315100515, f_beta: 0.7821350762527233
train: step: 4846, loss: 0.6605050563812256, acc: 0.65, recall: 0.65, precision: 0.6550227366680447, f_beta: 0.6471418489767113
train: step: 4847, loss: 0.5522730946540833, acc: 0.81, recall: 0.81, precision: 0.8505201266395297, f_beta: 0.8043455874781176
train: step: 4848, loss: 0.5374047756195068, acc: 0.78, recall: 0.78, precision: 0.8038194444444444, f_beta: 0.7756017951856385
train: step: 4849, loss: 0.5663301944732666, acc: 0.8, recall: 0.8, precision: 0.8342245989304813, f_beta: 0.794745484400657
train: step: 4850, loss: 0.6075911521911621, acc: 0.71, recall: 0.71, precision: 0.7549781447304517, f_beta: 0.69662098545873
train: step: 4851, loss: 0.5976136922836304, acc: 0.74, recall: 0.74, precision: 0.767379679144385, f_beta: 0.7331691297208538
train: step: 4852, loss: 0.6562212109565735, acc: 0.66, recall: 0.66, precision: 0.7192982456140351, f_beta: 0.6353496353496353
train: step: 4853, loss: 0.57904052734375, acc: 0.76, recall: 0.76, precision: 0.7896613190730838, f_beta: 0.7536945812807881
train: step: 4854, loss: 0.6073776483535767, acc: 0.71, recall: 0.71, precision: 0.7663622526636225, f_beta: 0.6938021328265231
train: step: 4855, loss: 0.5395861268043518, acc: 0.81, recall: 0.81, precision: 0.8324753324753325, f_beta: 0.8067338012409724
train: step: 4856, loss: 0.5315548181533813, acc: 0.84, recall: 0.8400000000000001, precision: 0.8787878787878788, f_beta: 0.8357963875205254
train: step: 4857, loss: 0.6567999124526978, acc: 0.69, recall: 0.69, precision: 0.7863170584689572, f_beta: 0.6615351020853805
train: step: 4858, loss: 0.6111252307891846, acc: 0.73, recall: 0.73, precision: 0.7688172043010753, f_beta: 0.7198879551820727
train: step: 4859, loss: 0.5585300922393799, acc: 0.8, recall: 0.8, precision: 0.8342245989304813, f_beta: 0.794745484400657
train: step: 4860, loss: 0.5524097084999084, acc: 0.81, recall: 0.81, precision: 0.8623188405797102, f_beta: 0.8028841166096068
train: step: 4861, loss: 0.6156735420227051, acc: 0.79, recall: 0.79, precision: 0.8047498949138294, f_beta: 0.7874278773155178
train: step: 4862, loss: 0.5817893743515015, acc: 0.75, recall: 0.75, precision: 0.7747252747252746, f_beta: 0.7442455242966751
train: step: 4863, loss: 0.5973337292671204, acc: 0.8, recall: 0.8, precision: 0.8571428571428572, f_beta: 0.7916666666666665
train: step: 4864, loss: 0.6321746706962585, acc: 0.72, recall: 0.72, precision: 0.7619047619047619, f_beta: 0.7083333333333334
train: step: 4865, loss: 0.5976670980453491, acc: 0.79, recall: 0.79, precision: 0.852112676056338, f_beta: 0.7803117480908044
train: step: 4866, loss: 0.5554558038711548, acc: 0.77, recall: 0.77, precision: 0.8424657534246576, f_beta: 0.757153415690001
train: step: 4867, loss: 0.5896301865577698, acc: 0.76, recall: 0.76, precision: 0.8095238095238095, f_beta: 0.75
train: step: 4868, loss: 0.6657036542892456, acc: 0.75, recall: 0.75, precision: 0.7627154266498528, f_beta: 0.7469379491851402
train: step: 4869, loss: 0.6479780077934265, acc: 0.66, recall: 0.66, precision: 0.6984126984126984, f_beta: 0.6427070197562001
train: step: 4870, loss: 0.5838731527328491, acc: 0.75, recall: 0.75, precision: 0.7826775214834916, f_beta: 0.7425599835238389
train: step: 4871, loss: 0.5441477298736572, acc: 0.79, recall: 0.79, precision: 0.852112676056338, f_beta: 0.7803117480908044
train: step: 4872, loss: 0.5443400740623474, acc: 0.79, recall: 0.79, precision: 0.7957976336189311, f_beta: 0.7889659330720531
train: step: 4873, loss: 0.6517624855041504, acc: 0.71, recall: 0.71, precision: 0.78, f_beta: 0.6906666666666665
train: step: 4874, loss: 0.512137770652771, acc: 0.79, recall: 0.79, precision: 0.8186813186813187, f_beta: 0.7851662404092071
train: step: 4875, loss: 0.6182212829589844, acc: 0.78, recall: 0.78, precision: 0.8472222222222222, f_beta: 0.7688104245481295
train: step: 4876, loss: 0.5899938344955444, acc: 0.83, recall: 0.83, precision: 0.853925353925354, f_beta: 0.8270776116366596
train: step: 4877, loss: 0.596811056137085, acc: 0.77, recall: 0.77, precision: 0.8155680224403927, f_beta: 0.7613860358958398
train: step: 4878, loss: 0.6368321776390076, acc: 0.73, recall: 0.73, precision: 0.8066666666666666, f_beta: 0.712
train: step: 4879, loss: 0.6866920590400696, acc: 0.69, recall: 0.69, precision: 0.6900760304121649, f_beta: 0.68996899689969
train: step: 4880, loss: 0.580101728439331, acc: 0.75, recall: 0.75, precision: 0.7826775214834916, f_beta: 0.7425599835238389
train: step: 4881, loss: 0.6974179148674011, acc: 0.32, recall: 0.32, precision: 0.3046875, f_beta: 0.30640554875561
train: step: 4882, loss: 0.6316474080085754, acc: 0.72, recall: 0.72, precision: 0.733446519524618, f_beta: 0.7159090909090909
train: step: 4883, loss: 0.5233083367347717, acc: 0.81, recall: 0.81, precision: 0.8505201266395297, f_beta: 0.8043455874781176
train: step: 4884, loss: 0.550982654094696, acc: 0.82, recall: 0.82, precision: 0.8395585738539898, f_beta: 0.8173701298701298
train: step: 4885, loss: 0.5308798551559448, acc: 0.84, recall: 0.84, precision: 0.8541666666666667, f_beta: 0.8383838383838385
train: step: 4886, loss: 0.5760238170623779, acc: 0.78, recall: 0.78, precision: 0.8038194444444444, f_beta: 0.7756017951856385
train: step: 4887, loss: 0.6121921539306641, acc: 0.69, recall: 0.69, precision: 0.7682100508187464, f_beta: 0.6656239887822242
train: step: 4888, loss: 0.5485600829124451, acc: 0.85, recall: 0.85, precision: 0.867801597309794, f_beta: 0.848162769511084
train: step: 4889, loss: 0.5547915101051331, acc: 0.79, recall: 0.79, precision: 0.7997106242248864, f_beta: 0.7882851093860268
train: step: 4890, loss: 0.6343473196029663, acc: 0.84, recall: 0.8400000000000001, precision: 0.8421900161030595, f_beta: 0.8397435897435896
train: step: 4891, loss: 0.5632192492485046, acc: 0.77, recall: 0.77, precision: 0.8424657534246576, f_beta: 0.757153415690001
train: step: 4892, loss: 0.6183618307113647, acc: 0.7, recall: 0.7, precision: 0.7741228070175439, f_beta: 0.6782496782496782
train: step: 4893, loss: 0.6001182794570923, acc: 0.78, recall: 0.78, precision: 0.8333333333333333, f_beta: 0.7708333333333333
train: step: 4894, loss: 0.5503520369529724, acc: 0.78, recall: 0.78, precision: 0.8038194444444444, f_beta: 0.7756017951856385
train: step: 4895, loss: 0.5763004422187805, acc: 0.8, recall: 0.8, precision: 0.8255208333333333, f_beta: 0.7960016319869441
train: step: 4896, loss: 0.5569595098495483, acc: 0.83, recall: 0.8300000000000001, precision: 0.8626373626373627, f_beta: 0.8260869565217391
train: step: 4897, loss: 0.6672676801681519, acc: 0.67, recall: 0.6699999999999999, precision: 0.7399774138904573, f_beta: 0.6440513428972063
train: step: 4898, loss: 0.6575961112976074, acc: 0.73, recall: 0.73, precision: 0.7917300862506342, f_beta: 0.7149192271143491
train: step: 4899, loss: 0.489515483379364, acc: 0.86, recall: 0.86, precision: 0.8694581280788177, f_beta: 0.8590982286634461
train: step: 4900, loss: 0.6124703288078308, acc: 0.72, recall: 0.72, precision: 0.7728174603174603, f_beta: 0.7057587221521648
train: step: 4901, loss: 0.6569341421127319, acc: 0.67, recall: 0.67, precision: 0.6700680272108843, f_beta: 0.66996699669967
train: step: 4902, loss: 0.5636023879051208, acc: 0.75, recall: 0.75, precision: 0.8333333333333333, f_beta: 0.7333333333333334
train: step: 4903, loss: 0.610009491443634, acc: 0.85, recall: 0.85, precision: 0.8753753753753755, f_beta: 0.8474214220323468
train: step: 4904, loss: 0.6162903904914856, acc: 0.71, recall: 0.71, precision: 0.78, f_beta: 0.6906666666666665
train: step: 4905, loss: 0.5252642631530762, acc: 0.84, recall: 0.84, precision: 0.8541666666666667, f_beta: 0.8383838383838385
train: step: 4906, loss: 0.511424720287323, acc: 0.83, recall: 0.8300000000000001, precision: 0.8365973072215422, f_beta: 0.8291628982011858
train: step: 4907, loss: 0.6387636065483093, acc: 0.69, recall: 0.69, precision: 0.6919191919191919, f_beta: 0.6892230576441103
train: step: 4908, loss: 0.5482648611068726, acc: 0.77, recall: 0.77, precision: 0.7837326607818411, f_beta: 0.767182913250329
train: step: 4909, loss: 0.5138540863990784, acc: 0.82, recall: 0.8200000000000001, precision: 0.8333333333333334, f_beta: 0.8181818181818181
train: step: 4910, loss: 0.5563324689865112, acc: 0.84, recall: 0.8400000000000001, precision: 0.8489326765188834, f_beta: 0.8389694041867956
train: step: 4911, loss: 0.5758012533187866, acc: 0.85, recall: 0.85, precision: 0.8501400560224089, f_beta: 0.84998499849985
train: step: 4912, loss: 0.5144160389900208, acc: 0.88, recall: 0.88, precision: 0.9032258064516129, f_beta: 0.8782467532467533
train: step: 4913, loss: 0.6262507438659668, acc: 0.76, recall: 0.76, precision: 0.7616747181964574, f_beta: 0.7596153846153846
train: step: 4914, loss: 0.5090827941894531, acc: 0.84, recall: 0.8400000000000001, precision: 0.8421900161030595, f_beta: 0.8397435897435896
train: step: 4915, loss: 0.568267822265625, acc: 0.79, recall: 0.79, precision: 0.7957976336189311, f_beta: 0.7889659330720531
train: step: 4916, loss: 0.6147897243499756, acc: 0.77, recall: 0.77, precision: 0.7727272727272727, f_beta: 0.7694235588972431
train: step: 4917, loss: 0.5704574584960938, acc: 0.87, recall: 0.87, precision: 0.8701480592236894, f_beta: 0.8699869986998701
train: step: 4918, loss: 0.5506433248519897, acc: 0.78, recall: 0.78, precision: 0.7971137521222411, f_beta: 0.7767857142857144
train: step: 4919, loss: 0.5954809784889221, acc: 0.78, recall: 0.78, precision: 0.7840909090909092, f_beta: 0.779205138498595
train: step: 4920, loss: 0.5572412610054016, acc: 0.79, recall: 0.79, precision: 0.7957976336189311, f_beta: 0.7889659330720531
train: step: 4921, loss: 0.5244485139846802, acc: 0.81, recall: 0.81, precision: 0.8161974704202366, f_beta: 0.8090644156366193
train: step: 4922, loss: 0.5039684772491455, acc: 0.85, recall: 0.8500000000000001, precision: 0.8569971440228478, f_beta: 0.8492613807657521
train: step: 4923, loss: 0.6057118773460388, acc: 0.79, recall: 0.79, precision: 0.8047498949138294, f_beta: 0.7874278773155178
train: step: 4924, loss: 0.5698630809783936, acc: 0.77, recall: 0.77, precision: 0.7727272727272727, f_beta: 0.7694235588972431
train: step: 4925, loss: 0.5710469484329224, acc: 0.82, recall: 0.8200000000000001, precision: 0.8284072249589491, f_beta: 0.818840579710145
train: step: 4926, loss: 0.5469381213188171, acc: 0.8, recall: 0.8, precision: 0.8019323671497585, f_beta: 0.7996794871794872
train: step: 4927, loss: 0.5118024349212646, acc: 0.86, recall: 0.8600000000000001, precision: 0.8652597402597402, f_beta: 0.8594941790445605
train: step: 4928, loss: 0.46833091974258423, acc: 0.89, recall: 0.89, precision: 0.89015606242497, f_beta: 0.88998899889989
train: step: 4929, loss: 0.607563316822052, acc: 0.74, recall: 0.74, precision: 0.75, f_beta: 0.7373737373737373
train: step: 4930, loss: 0.5908476114273071, acc: 0.73, recall: 0.73, precision: 0.7416981925178646, f_beta: 0.7266929851199515
train: step: 4931, loss: 0.46651649475097656, acc: 0.85, recall: 0.85, precision: 0.851264552388599, f_beta: 0.8498648783905516
train: step: 4932, loss: 0.5348531007766724, acc: 0.81, recall: 0.81, precision: 0.8131313131313131, f_beta: 0.8095238095238095
train: step: 4933, loss: 0.5915076732635498, acc: 0.79, recall: 0.79, precision: 0.8047498949138294, f_beta: 0.7874278773155178
train: step: 4934, loss: 0.542095959186554, acc: 0.79, recall: 0.79, precision: 0.7957976336189311, f_beta: 0.7889659330720531
train: step: 4935, loss: 0.5729960799217224, acc: 0.8, recall: 0.8, precision: 0.8125, f_beta: 0.797979797979798
train: step: 4936, loss: 0.6016546487808228, acc: 0.76, recall: 0.76, precision: 0.7604166666666667, f_beta: 0.7599039615846339
train: step: 4937, loss: 0.6056931614875793, acc: 0.71, recall: 0.71, precision: 0.7141982864137086, f_beta: 0.7085720028137876
train: step: 4938, loss: 0.5105463266372681, acc: 0.85, recall: 0.8500000000000001, precision: 0.8569971440228478, f_beta: 0.8492613807657521
train: step: 4939, loss: 0.5836940407752991, acc: 0.76, recall: 0.76, precision: 0.7821180555555556, f_beta: 0.755201958384333
train: step: 4940, loss: 0.5804955959320068, acc: 0.79, recall: 0.79, precision: 0.7957976336189311, f_beta: 0.7889659330720531
train: step: 4941, loss: 0.5247225761413574, acc: 0.81, recall: 0.81, precision: 0.8203803224472923, f_beta: 0.8084484323016432
train: step: 4942, loss: 0.5913203954696655, acc: 0.81, recall: 0.81, precision: 0.8324753324753325, f_beta: 0.8067338012409724
train: step: 4943, loss: 0.5814276337623596, acc: 0.78, recall: 0.78, precision: 0.7971137521222411, f_beta: 0.7767857142857144
train: step: 4944, loss: 0.5692291259765625, acc: 0.87, recall: 0.87, precision: 0.8888188314417822, f_beta: 0.868407733576273
train: step: 4945, loss: 0.5621816515922546, acc: 0.8, recall: 0.8, precision: 0.8043831168831169, f_beta: 0.7992773986350863
train: step: 4946, loss: 0.5345420837402344, acc: 0.83, recall: 0.8300000000000001, precision: 0.8365973072215422, f_beta: 0.8291628982011858
train: step: 4947, loss: 0.6474133133888245, acc: 0.7, recall: 0.7, precision: 0.7052545155993433, f_beta: 0.6980676328502415
train: step: 4948, loss: 0.5348957180976868, acc: 0.85, recall: 0.85, precision: 0.851264552388599, f_beta: 0.8498648783905516
train: step: 4949, loss: 0.6118620038032532, acc: 0.75, recall: 0.75, precision: 0.7525252525252526, f_beta: 0.7493734335839599
train: step: 4950, loss: 0.5797699689865112, acc: 0.78, recall: 0.78, precision: 0.7873563218390804, f_beta: 0.7785829307568439
train: step: 4951, loss: 0.54978346824646, acc: 0.77, recall: 0.77, precision: 0.7727272727272727, f_beta: 0.7694235588972431
train: step: 4952, loss: 0.5144261717796326, acc: 0.86, recall: 0.8600000000000001, precision: 0.8652597402597402, f_beta: 0.8594941790445605
train: step: 4953, loss: 0.6134690642356873, acc: 0.77, recall: 0.77, precision: 0.7895752895752897, f_beta: 0.7660461804495982
train: step: 4954, loss: 0.4724748730659485, acc: 0.87, recall: 0.87, precision: 0.8888188314417822, f_beta: 0.868407733576273
train: step: 4955, loss: 0.5083416104316711, acc: 0.85, recall: 0.8500000000000001, precision: 0.8535353535353536, f_beta: 0.849624060150376
train: step: 4956, loss: 0.5332204699516296, acc: 0.82, recall: 0.8200000000000001, precision: 0.8246753246753247, f_beta: 0.8193496587715776
train: step: 4957, loss: 0.5881319642066956, acc: 0.78, recall: 0.78, precision: 0.7971137521222411, f_beta: 0.7767857142857144
train: step: 4958, loss: 0.6032901406288147, acc: 0.71, recall: 0.71, precision: 0.7307692307692308, f_beta: 0.7033248081841432
train: step: 4959, loss: 0.5398861169815063, acc: 0.82, recall: 0.8200000000000001, precision: 0.822061191626409, f_beta: 0.8197115384615385
train: step: 4960, loss: 0.6012415885925293, acc: 0.79, recall: 0.79, precision: 0.7929292929292929, f_beta: 0.7894736842105263
train: step: 4961, loss: 0.5820850133895874, acc: 0.8, recall: 0.8, precision: 0.8, f_beta: 0.8000000000000002
train: step: 4962, loss: 0.5456758141517639, acc: 0.81, recall: 0.81, precision: 0.8131313131313131, f_beta: 0.8095238095238095
train: step: 4963, loss: 0.571315586566925, acc: 0.77, recall: 0.77, precision: 0.7753977968176254, f_beta: 0.7688674505074867
train: step: 4964, loss: 0.6354629993438721, acc: 0.72, recall: 0.72, precision: 0.7232142857142857, f_beta: 0.7189883580891208
train: step: 4965, loss: 0.48271238803863525, acc: 0.84, recall: 0.84, precision: 0.8541666666666667, f_beta: 0.8383838383838385
train: step: 4966, loss: 0.5810253620147705, acc: 0.79, recall: 0.79, precision: 0.7957976336189311, f_beta: 0.7889659330720531
train: step: 4967, loss: 0.5235932469367981, acc: 0.83, recall: 0.8300000000000001, precision: 0.8333333333333334, f_beta: 0.8295739348370927
train: step: 4968, loss: 0.5437371730804443, acc: 0.77, recall: 0.77, precision: 0.7709755118426336, f_beta: 0.769792813532179
train: step: 4969, loss: 0.4621591866016388, acc: 0.91, recall: 0.9099999999999999, precision: 0.9114813327980731, f_beta: 0.9099189270343309
train: step: 4970, loss: 0.5094110369682312, acc: 0.87, recall: 0.87, precision: 0.8701480592236894, f_beta: 0.8699869986998701
train: step: 4971, loss: 0.4427243173122406, acc: 0.9, recall: 0.9, precision: 0.9, f_beta: 0.9
train: step: 4972, loss: 0.5036665201187134, acc: 0.82, recall: 0.8200000000000001, precision: 0.8205128205128205, f_beta: 0.8199279711884755
train: step: 4973, loss: 0.568459689617157, acc: 0.79, recall: 0.79, precision: 0.7901160464185675, f_beta: 0.78997899789979
train: step: 4974, loss: 0.49845272302627563, acc: 0.85, recall: 0.85, precision: 0.8501400560224089, f_beta: 0.84998499849985
train: step: 4975, loss: 0.6340556740760803, acc: 0.69, recall: 0.69, precision: 0.6919191919191919, f_beta: 0.6892230576441103
train: step: 4976, loss: 0.6271382570266724, acc: 0.83, recall: 0.8300000000000001, precision: 0.8365973072215422, f_beta: 0.8291628982011858
train: step: 4977, loss: 0.471258282661438, acc: 0.89, recall: 0.89, precision: 0.8914090726615818, f_beta: 0.8899009108197378
train: step: 4978, loss: 0.551032304763794, acc: 0.84, recall: 0.8400000000000001, precision: 0.8449675324675325, f_beta: 0.839421918908069
train: step: 4979, loss: 0.6374732851982117, acc: 0.78, recall: 0.78, precision: 0.7971137521222411, f_beta: 0.7767857142857144
train: step: 4980, loss: 0.579704225063324, acc: 0.75, recall: 0.75, precision: 0.7501000400160064, f_beta: 0.7499749974997499
train: step: 4981, loss: 0.4952806234359741, acc: 0.88, recall: 0.88, precision: 0.8806089743589745, f_beta: 0.879951980792317
train: step: 4982, loss: 0.5558426380157471, acc: 0.8, recall: 0.8, precision: 0.8019323671497585, f_beta: 0.7996794871794872
train: step: 4983, loss: 0.6045916080474854, acc: 0.73, recall: 0.73, precision: 0.7527472527472527, f_beta: 0.7237851662404092
train: step: 4984, loss: 0.4965556859970093, acc: 0.86, recall: 0.86, precision: 0.8605769230769231, f_beta: 0.8599439775910365
train: step: 4985, loss: 0.5508754849433899, acc: 0.8, recall: 0.8, precision: 0.8019323671497585, f_beta: 0.7996794871794872
train: step: 4986, loss: 0.4928953945636749, acc: 0.85, recall: 0.85, precision: 0.851264552388599, f_beta: 0.8498648783905516
train: step: 4987, loss: 0.5632763504981995, acc: 0.8, recall: 0.8, precision: 0.8043831168831169, f_beta: 0.7992773986350863
train: step: 4988, loss: 0.5608939528465271, acc: 0.77, recall: 0.77, precision: 0.7895752895752897, f_beta: 0.7660461804495982
train: step: 4989, loss: 0.5676618218421936, acc: 0.8, recall: 0.8, precision: 0.8125, f_beta: 0.797979797979798
train: step: 4990, loss: 0.5348199605941772, acc: 0.8, recall: 0.8, precision: 0.8078817733990147, f_beta: 0.7987117552334944
train: step: 4991, loss: 0.49721482396125793, acc: 0.83, recall: 0.8300000000000001, precision: 0.8365973072215422, f_beta: 0.8291628982011858
train: step: 4992, loss: 0.5739652514457703, acc: 0.78, recall: 0.78, precision: 0.78, f_beta: 0.78
train: step: 4993, loss: 0.5977250337600708, acc: 0.75, recall: 0.75, precision: 0.7627154266498528, f_beta: 0.7469379491851402
train: step: 4994, loss: 0.5454549193382263, acc: 0.85, recall: 0.8500000000000001, precision: 0.8535353535353536, f_beta: 0.849624060150376
train: step: 4995, loss: 0.5505488514900208, acc: 0.78, recall: 0.78, precision: 0.7916666666666666, f_beta: 0.7777777777777779
train: step: 4996, loss: 0.49390047788619995, acc: 0.86, recall: 0.86, precision: 0.8694581280788177, f_beta: 0.8590982286634461
train: step: 4997, loss: 0.47343289852142334, acc: 0.84, recall: 0.84, precision: 0.8405448717948718, f_beta: 0.8399359743897559
train: step: 4998, loss: 0.5797168016433716, acc: 0.82, recall: 0.8200000000000001, precision: 0.8284072249589491, f_beta: 0.818840579710145
train: step: 4999, loss: 0.518673300743103, acc: 0.92, recall: 0.92, precision: 0.92, f_beta: 0.92
train: step: 5000, loss: 0.5607819557189941, acc: 0.78, recall: 0.78, precision: 0.7873563218390804, f_beta: 0.7785829307568439
train: step: 5001, loss: 0.6223918795585632, acc: 0.78, recall: 0.78, precision: 0.7971137521222411, f_beta: 0.7767857142857144
train: step: 5002, loss: 0.4830101728439331, acc: 0.89, recall: 0.8899999999999999, precision: 0.8939393939393939, f_beta: 0.8897243107769424
train: step: 5003, loss: 0.5733571648597717, acc: 0.82, recall: 0.82, precision: 0.8395585738539898, f_beta: 0.8173701298701298
train: step: 5004, loss: 0.5716751217842102, acc: 0.8, recall: 0.8, precision: 0.8183361629881154, f_beta: 0.797077922077922
train: step: 5005, loss: 0.4791942536830902, acc: 0.83, recall: 0.8300000000000001, precision: 0.8365973072215422, f_beta: 0.8291628982011858
train: step: 5006, loss: 0.6158983111381531, acc: 0.71, recall: 0.71, precision: 0.7121212121212122, f_beta: 0.7092731829573935
train: step: 5007, loss: 0.5657034516334534, acc: 0.75, recall: 0.75, precision: 0.7681252681252682, f_beta: 0.7457023700539112
train: step: 5008, loss: 0.4967641830444336, acc: 0.81, recall: 0.81, precision: 0.8161974704202366, f_beta: 0.8090644156366193
train: step: 5009, loss: 0.5857372879981995, acc: 0.75, recall: 0.75, precision: 0.7525252525252526, f_beta: 0.7493734335839599
train: step: 5010, loss: 0.6001022458076477, acc: 0.79, recall: 0.79, precision: 0.7957976336189311, f_beta: 0.7889659330720531
train: step: 5011, loss: 0.5070545673370361, acc: 0.83, recall: 0.8300000000000001, precision: 0.8311922922521076, f_beta: 0.8298468621759585
train: step: 5012, loss: 0.5907121896743774, acc: 0.81, recall: 0.81, precision: 0.8203803224472923, f_beta: 0.8084484323016432
train: step: 5013, loss: 0.5371235609054565, acc: 0.82, recall: 0.8200000000000001, precision: 0.8246753246753247, f_beta: 0.8193496587715776
train: step: 5014, loss: 0.4971906244754791, acc: 0.84, recall: 0.84, precision: 0.8541666666666667, f_beta: 0.8383838383838385
train: step: 5015, loss: 0.5055666565895081, acc: 0.83, recall: 0.83, precision: 0.8301320528211285, f_beta: 0.8299829982998299
train: step: 5016, loss: 0.5460278391838074, acc: 0.82, recall: 0.82, precision: 0.82, f_beta: 0.82
train: step: 5017, loss: 0.5803652405738831, acc: 0.75, recall: 0.75, precision: 0.7583712277800745, f_beta: 0.7479584635547939
train: step: 5018, loss: 0.5299161672592163, acc: 0.8, recall: 0.8, precision: 0.8078817733990147, f_beta: 0.7987117552334944
train: step: 5019, loss: 0.598213791847229, acc: 0.77, recall: 0.77, precision: 0.7709755118426336, f_beta: 0.769792813532179
train: step: 5020, loss: 0.5607370138168335, acc: 0.82, recall: 0.8200000000000001, precision: 0.8333333333333334, f_beta: 0.8181818181818181
train: step: 5021, loss: 0.6125012040138245, acc: 0.69, recall: 0.69, precision: 0.6963621331128566, f_beta: 0.6874684948079444
train: step: 5022, loss: 0.5665699243545532, acc: 0.82, recall: 0.8200000000000001, precision: 0.8246753246753247, f_beta: 0.8193496587715776
train: step: 5023, loss: 0.5122941732406616, acc: 0.82, recall: 0.82, precision: 0.8395585738539898, f_beta: 0.8173701298701298
train: step: 5024, loss: 0.5866266489028931, acc: 0.78, recall: 0.78, precision: 0.7916666666666666, f_beta: 0.7777777777777779
train: step: 5025, loss: 0.5701930522918701, acc: 0.75, recall: 0.75, precision: 0.7549979600163199, f_beta: 0.7487689679429204
train: step: 5026, loss: 0.5136033892631531, acc: 0.83, recall: 0.8300000000000001, precision: 0.8365973072215422, f_beta: 0.8291628982011858
train: step: 5027, loss: 0.448303759098053, acc: 0.9, recall: 0.8999999999999999, precision: 0.9025764895330113, f_beta: 0.8998397435897436
train: step: 5028, loss: 0.5209737420082092, acc: 0.83, recall: 0.8300000000000001, precision: 0.8311922922521076, f_beta: 0.8298468621759585
train: step: 5029, loss: 0.5895363092422485, acc: 0.72, recall: 0.72, precision: 0.7203525641025641, f_beta: 0.7198879551820727
train: step: 5030, loss: 0.5016070008277893, acc: 0.85, recall: 0.85, precision: 0.8501400560224089, f_beta: 0.84998499849985
train: step: 5031, loss: 0.5479497313499451, acc: 0.78, recall: 0.78, precision: 0.7916666666666666, f_beta: 0.7777777777777779
train: step: 5032, loss: 0.4687844216823578, acc: 0.83, recall: 0.8300000000000001, precision: 0.8311922922521076, f_beta: 0.8298468621759585
train: step: 5033, loss: 0.5775105357170105, acc: 0.76, recall: 0.76, precision: 0.7668308702791462, f_beta: 0.7584541062801933
train: step: 5034, loss: 0.6135567426681519, acc: 0.68, recall: 0.6799999999999999, precision: 0.6953125, f_beta: 0.6736026111791106
train: step: 5035, loss: 0.6044452786445618, acc: 0.74, recall: 0.74, precision: 0.74, f_beta: 0.74
train: step: 5036, loss: 0.5793333053588867, acc: 0.76, recall: 0.76, precision: 0.7604166666666667, f_beta: 0.7599039615846339
train: step: 5037, loss: 0.5785272717475891, acc: 0.79, recall: 0.79, precision: 0.7910477719791249, f_beta: 0.7898108297467722
train: step: 5038, loss: 0.6069136261940002, acc: 0.76, recall: 0.76, precision: 0.76, f_beta: 0.76
train: step: 5039, loss: 0.5629366040229797, acc: 0.8, recall: 0.8, precision: 0.8019323671497585, f_beta: 0.7996794871794872
train: step: 5040, loss: 0.5689601302146912, acc: 0.84, recall: 0.84, precision: 0.8405448717948718, f_beta: 0.8399359743897559
train: step: 5041, loss: 0.5963776111602783, acc: 0.78, recall: 0.78, precision: 0.7818035426731079, f_beta: 0.7796474358974359
train: step: 5042, loss: 0.5307706594467163, acc: 0.85, recall: 0.85, precision: 0.867801597309794, f_beta: 0.848162769511084
train: step: 5043, loss: 0.5213202834129333, acc: 0.79, recall: 0.79, precision: 0.7929292929292929, f_beta: 0.7894736842105263
train: step: 5044, loss: 0.5728922486305237, acc: 0.85, recall: 0.8500000000000001, precision: 0.8535353535353536, f_beta: 0.849624060150376
train: step: 5045, loss: 0.5603379011154175, acc: 0.77, recall: 0.77, precision: 0.7753977968176254, f_beta: 0.7688674505074867
train: step: 5046, loss: 0.6071592569351196, acc: 0.74, recall: 0.74, precision: 0.7415458937198067, f_beta: 0.7395833333333335
train: step: 5047, loss: 0.5964999198913574, acc: 0.72, recall: 0.72, precision: 0.7203525641025641, f_beta: 0.7198879551820727
train: step: 5048, loss: 0.5618005990982056, acc: 0.79, recall: 0.79, precision: 0.7929292929292929, f_beta: 0.7894736842105263
train: step: 5049, loss: 0.5275514721870422, acc: 0.82, recall: 0.8200000000000001, precision: 0.8333333333333334, f_beta: 0.8181818181818181
train: step: 5050, loss: 0.5745584964752197, acc: 0.81, recall: 0.81, precision: 0.8111200321156162, f_beta: 0.8098288459613652
train: step: 5051, loss: 0.5518003106117249, acc: 0.77, recall: 0.77, precision: 0.7709755118426336, f_beta: 0.769792813532179
train: step: 5052, loss: 0.5880725383758545, acc: 0.71, recall: 0.71, precision: 0.7454417952314165, f_beta: 0.6991389148251893
train: step: 5053, loss: 0.5656535625457764, acc: 0.77, recall: 0.77, precision: 0.7709755118426336, f_beta: 0.769792813532179
train: step: 5054, loss: 0.5420008897781372, acc: 0.85, recall: 0.85, precision: 0.8617197188921042, f_beta: 0.8487750781328762
train: step: 5055, loss: 0.6145338416099548, acc: 0.79, recall: 0.79, precision: 0.7910477719791249, f_beta: 0.7898108297467722
train: step: 5056, loss: 0.5966739654541016, acc: 0.77, recall: 0.77, precision: 0.7790409260024803, f_beta: 0.7681217864704104
train: step: 5057, loss: 0.5348000526428223, acc: 0.81, recall: 0.81, precision: 0.8131313131313131, f_beta: 0.8095238095238095
train: step: 5058, loss: 0.5980157852172852, acc: 0.72, recall: 0.72, precision: 0.7232142857142857, f_beta: 0.7189883580891208
train: step: 5059, loss: 0.551797091960907, acc: 0.77, recall: 0.77, precision: 0.7727272727272727, f_beta: 0.7694235588972431
train: step: 5060, loss: 0.5277935266494751, acc: 0.83, recall: 0.8300000000000001, precision: 0.8333333333333334, f_beta: 0.8295739348370927
train: step: 5061, loss: 0.5748968124389648, acc: 0.78, recall: 0.78, precision: 0.780448717948718, f_beta: 0.7799119647859143
train: step: 5062, loss: 0.5575732588768005, acc: 0.78, recall: 0.78, precision: 0.78, f_beta: 0.78
train: step: 5063, loss: 0.5443099141120911, acc: 0.81, recall: 0.81, precision: 0.8131313131313131, f_beta: 0.8095238095238095
train: step: 5064, loss: 0.4339839220046997, acc: 0.92, recall: 0.9199999999999999, precision: 0.9206730769230769, f_beta: 0.919967987194878
train: step: 5065, loss: 0.6139405965805054, acc: 0.76, recall: 0.76, precision: 0.7708333333333333, f_beta: 0.7575757575757576
train: step: 5066, loss: 0.5596873164176941, acc: 0.79, recall: 0.79, precision: 0.7901160464185675, f_beta: 0.78997899789979
train: step: 5067, loss: 0.6065355539321899, acc: 0.72, recall: 0.72, precision: 0.7291666666666667, f_beta: 0.7171717171717171
train: step: 5068, loss: 0.588763415813446, acc: 0.75, recall: 0.75, precision: 0.750903251706142, f_beta: 0.7497747973175859
train: step: 5069, loss: 0.49776431918144226, acc: 0.85, recall: 0.85, precision: 0.8617197188921042, f_beta: 0.8487750781328762
train: step: 5070, loss: 0.543687105178833, acc: 0.78, recall: 0.78, precision: 0.780448717948718, f_beta: 0.7799119647859143
train: step: 5071, loss: 0.6137816309928894, acc: 0.72, recall: 0.72, precision: 0.7203525641025641, f_beta: 0.7198879551820727
train: step: 5072, loss: 0.577355146408081, acc: 0.73, recall: 0.73, precision: 0.7345981232150143, f_beta: 0.7286704853783539
train: step: 5073, loss: 0.5804272294044495, acc: 0.78, recall: 0.78, precision: 0.7818035426731079, f_beta: 0.7796474358974359
train: step: 5074, loss: 0.5662094950675964, acc: 0.8, recall: 0.8, precision: 0.8183361629881154, f_beta: 0.797077922077922
train: step: 5075, loss: 0.6130611300468445, acc: 0.75, recall: 0.75, precision: 0.7583712277800745, f_beta: 0.7479584635547939
train: step: 5076, loss: 0.5671000480651855, acc: 0.85, recall: 0.85, precision: 0.8501400560224089, f_beta: 0.84998499849985
train: step: 5077, loss: 0.5657399296760559, acc: 0.8, recall: 0.8, precision: 0.8019323671497585, f_beta: 0.7996794871794872
train: step: 5078, loss: 0.5572763681411743, acc: 0.79, recall: 0.79, precision: 0.811025311025311, f_beta: 0.7863899908452854
train: step: 5079, loss: 0.5511970520019531, acc: 0.8, recall: 0.8, precision: 0.8004807692307692, f_beta: 0.7999199679871949
train: step: 5080, loss: 0.5375773906707764, acc: 0.79, recall: 0.79, precision: 0.7957976336189311, f_beta: 0.7889659330720531
train: step: 5081, loss: 0.5692195296287537, acc: 0.79, recall: 0.79, precision: 0.7929292929292929, f_beta: 0.7894736842105263
train: step: 5082, loss: 0.5205696225166321, acc: 0.8, recall: 0.8, precision: 0.8125, f_beta: 0.797979797979798
train: step: 5083, loss: 0.5667357444763184, acc: 0.8, recall: 0.8, precision: 0.8078817733990147, f_beta: 0.7987117552334944
train: step: 5084, loss: 0.6222435832023621, acc: 0.7, recall: 0.7, precision: 0.7380952380952381, f_beta: 0.6875
train: step: 5085, loss: 0.5441158413887024, acc: 0.82, recall: 0.8200000000000001, precision: 0.8246753246753247, f_beta: 0.8193496587715776
train: step: 5086, loss: 0.5146287679672241, acc: 0.8, recall: 0.8, precision: 0.8125, f_beta: 0.797979797979798
train: step: 5087, loss: 0.5785942077636719, acc: 0.79, recall: 0.79, precision: 0.7901160464185675, f_beta: 0.78997899789979
train: step: 5088, loss: 0.596828281879425, acc: 0.76, recall: 0.76, precision: 0.7616747181964574, f_beta: 0.7596153846153846
train: step: 5089, loss: 0.5545211434364319, acc: 0.75, recall: 0.75, precision: 0.750903251706142, f_beta: 0.7497747973175859
train: step: 5090, loss: 0.6140363812446594, acc: 0.72, recall: 0.72, precision: 0.7214170692431562, f_beta: 0.7195512820512822
train: step: 5091, loss: 0.5389223694801331, acc: 0.85, recall: 0.85, precision: 0.8501400560224089, f_beta: 0.84998499849985
train: step: 5092, loss: 0.6132465600967407, acc: 0.78, recall: 0.78, precision: 0.7916666666666666, f_beta: 0.7777777777777779
train: step: 5093, loss: 0.5681178569793701, acc: 0.76, recall: 0.76, precision: 0.76, f_beta: 0.76
train: step: 5094, loss: 0.6044878959655762, acc: 0.78, recall: 0.78, precision: 0.780448717948718, f_beta: 0.7799119647859143
train: step: 5095, loss: 0.5518237948417664, acc: 0.79, recall: 0.79, precision: 0.7929292929292929, f_beta: 0.7894736842105263
train: step: 5096, loss: 0.5477306246757507, acc: 0.79, recall: 0.79, precision: 0.7957976336189311, f_beta: 0.7889659330720531
train: step: 5097, loss: 0.6034185290336609, acc: 0.82, recall: 0.8200000000000001, precision: 0.822061191626409, f_beta: 0.8197115384615385
train: step: 5098, loss: 0.6427444219589233, acc: 0.73, recall: 0.73, precision: 0.7416981925178646, f_beta: 0.7266929851199515
train: step: 5099, loss: 0.5975065231323242, acc: 0.85, recall: 0.8500000000000001, precision: 0.8569971440228478, f_beta: 0.8492613807657521
train: step: 5100, loss: 0.6073437333106995, acc: 0.76, recall: 0.76, precision: 0.7668308702791462, f_beta: 0.7584541062801933
train: step: 5101, loss: 0.5257747173309326, acc: 0.81, recall: 0.81, precision: 0.8203803224472923, f_beta: 0.8084484323016432
train: step: 5102, loss: 0.5288368463516235, acc: 0.79, recall: 0.79, precision: 0.7997106242248864, f_beta: 0.7882851093860268
train: step: 5103, loss: 0.5883395075798035, acc: 0.8, recall: 0.8, precision: 0.8078817733990147, f_beta: 0.7987117552334944
train: step: 5104, loss: 0.5608277320861816, acc: 0.77, recall: 0.77, precision: 0.7790409260024803, f_beta: 0.7681217864704104
train: step: 5105, loss: 0.5951260328292847, acc: 0.77, recall: 0.77, precision: 0.7709755118426336, f_beta: 0.769792813532179
train: step: 5106, loss: 0.5695618987083435, acc: 0.78, recall: 0.78, precision: 0.7818035426731079, f_beta: 0.7796474358974359
train: step: 5107, loss: 0.6336877346038818, acc: 0.77, recall: 0.77, precision: 0.7790409260024803, f_beta: 0.7681217864704104
train: step: 5108, loss: 0.5539907217025757, acc: 0.79, recall: 0.79, precision: 0.8047498949138294, f_beta: 0.7874278773155178
train: step: 5109, loss: 0.5458245277404785, acc: 0.84, recall: 0.84, precision: 0.8405448717948718, f_beta: 0.8399359743897559
train: step: 5110, loss: 0.5948699116706848, acc: 0.8, recall: 0.8, precision: 0.8043831168831169, f_beta: 0.7992773986350863
train: step: 5111, loss: 0.5667654871940613, acc: 0.85, recall: 0.85, precision: 0.8617197188921042, f_beta: 0.8487750781328762
train: step: 5112, loss: 0.6388116478919983, acc: 0.76, recall: 0.76, precision: 0.7708333333333333, f_beta: 0.7575757575757576
train: step: 5113, loss: 0.547549307346344, acc: 0.82, recall: 0.8200000000000001, precision: 0.8205128205128205, f_beta: 0.8199279711884755
train: step: 5114, loss: 0.5900831818580627, acc: 0.75, recall: 0.75, precision: 0.7681252681252682, f_beta: 0.7457023700539112
train: step: 5115, loss: 0.5742954015731812, acc: 0.77, recall: 0.77, precision: 0.7790409260024803, f_beta: 0.7681217864704104
train: step: 5116, loss: 0.5490947961807251, acc: 0.79, recall: 0.79, precision: 0.7957976336189311, f_beta: 0.7889659330720531
train: step: 5117, loss: 0.6745166182518005, acc: 0.71, recall: 0.71, precision: 0.7170318313352625, f_beta: 0.7076318177235609
train: step: 5118, loss: 0.5491054058074951, acc: 0.83, recall: 0.8300000000000001, precision: 0.8626373626373627, f_beta: 0.8260869565217391
train: step: 5119, loss: 0.5365685820579529, acc: 0.8, recall: 0.8, precision: 0.8125, f_beta: 0.797979797979798
train: step: 5120, loss: 0.6002888083457947, acc: 0.76, recall: 0.76, precision: 0.7896613190730838, f_beta: 0.7536945812807881
train: step: 5121, loss: 0.5893361568450928, acc: 0.74, recall: 0.74, precision: 0.7403846153846154, f_beta: 0.7398959583833533
train: step: 5122, loss: 0.5355465412139893, acc: 0.87, recall: 0.87, precision: 0.8713368125250903, f_beta: 0.8698828946051447
train: step: 5123, loss: 0.5114423632621765, acc: 0.84, recall: 0.8400000000000001, precision: 0.8489326765188834, f_beta: 0.8389694041867956
train: step: 5124, loss: 0.5810806751251221, acc: 0.79, recall: 0.79, precision: 0.7901160464185675, f_beta: 0.78997899789979
train: step: 5125, loss: 0.577911376953125, acc: 0.76, recall: 0.76, precision: 0.7637987012987013, f_beta: 0.7591328783621035
train: step: 5126, loss: 0.6399816274642944, acc: 0.75, recall: 0.75, precision: 0.750903251706142, f_beta: 0.7497747973175859
train: step: 5127, loss: 0.5614916086196899, acc: 0.78, recall: 0.78, precision: 0.7873563218390804, f_beta: 0.7785829307568439
train: step: 5128, loss: 0.5952660441398621, acc: 0.77, recall: 0.77, precision: 0.7709755118426336, f_beta: 0.769792813532179
train: step: 5129, loss: 0.6014308929443359, acc: 0.76, recall: 0.76, precision: 0.7668308702791462, f_beta: 0.7584541062801933
train: step: 5130, loss: 0.5500122308731079, acc: 0.82, recall: 0.8200000000000001, precision: 0.822061191626409, f_beta: 0.8197115384615385
train: step: 5131, loss: 0.6045767068862915, acc: 0.83, recall: 0.8300000000000001, precision: 0.8410500206696983, f_beta: 0.8286117552172598
train: step: 5132, loss: 0.6427178978919983, acc: 0.7, recall: 0.7, precision: 0.7003205128205128, f_beta: 0.6998799519807923
train: step: 5133, loss: 0.6163544654846191, acc: 0.79, recall: 0.79, precision: 0.7929292929292929, f_beta: 0.7894736842105263
train: step: 5134, loss: 0.5525023341178894, acc: 0.78, recall: 0.78, precision: 0.78, f_beta: 0.78
train: step: 5135, loss: 0.5081642270088196, acc: 0.84, recall: 0.8400000000000001, precision: 0.8489326765188834, f_beta: 0.8389694041867956
train: step: 5136, loss: 0.5667980909347534, acc: 0.77, recall: 0.77, precision: 0.7701080432172869, f_beta: 0.7699769976997699
train: step: 5137, loss: 0.573310136795044, acc: 0.77, recall: 0.77, precision: 0.7753977968176254, f_beta: 0.7688674505074867
train: step: 5138, loss: 0.567665696144104, acc: 0.78, recall: 0.78, precision: 0.7840909090909092, f_beta: 0.779205138498595
train: step: 5139, loss: 0.5043423175811768, acc: 0.81, recall: 0.81, precision: 0.810124049619848, f_beta: 0.8099809980998101
train: step: 5140, loss: 0.6100841760635376, acc: 0.73, recall: 0.73, precision: 0.7300920368147259, f_beta: 0.72997299729973
train: step: 5141, loss: 0.537581205368042, acc: 0.83, recall: 0.8300000000000001, precision: 0.8365973072215422, f_beta: 0.8291628982011858
train: step: 5142, loss: 0.6676412224769592, acc: 0.82, recall: 0.8200000000000001, precision: 0.8333333333333334, f_beta: 0.8181818181818181
train: step: 5143, loss: 0.5801205635070801, acc: 0.78, recall: 0.78, precision: 0.78, f_beta: 0.78
train: step: 5144, loss: 0.5918282270431519, acc: 0.72, recall: 0.72, precision: 0.733446519524618, f_beta: 0.7159090909090909
train: step: 5145, loss: 0.532416582107544, acc: 0.82, recall: 0.8200000000000001, precision: 0.8333333333333334, f_beta: 0.8181818181818181
train: step: 5146, loss: 0.5398638844490051, acc: 0.83, recall: 0.83, precision: 0.8301320528211285, f_beta: 0.8299829982998299
train: step: 5147, loss: 0.47706276178359985, acc: 0.86, recall: 0.8600000000000001, precision: 0.8652597402597402, f_beta: 0.8594941790445605
train: step: 5148, loss: 0.5872777700424194, acc: 0.77, recall: 0.77, precision: 0.7790409260024803, f_beta: 0.7681217864704104
train: step: 5149, loss: 0.5589416027069092, acc: 0.78, recall: 0.78, precision: 0.7818035426731079, f_beta: 0.7796474358974359
train: step: 5150, loss: 0.6074212789535522, acc: 0.73, recall: 0.73, precision: 0.7416981925178646, f_beta: 0.7266929851199515
train: step: 5151, loss: 0.5354174971580505, acc: 0.82, recall: 0.8200000000000001, precision: 0.8333333333333334, f_beta: 0.8181818181818181
train: step: 5152, loss: 0.6043199300765991, acc: 0.82, recall: 0.8200000000000001, precision: 0.822061191626409, f_beta: 0.8197115384615385
train: step: 5153, loss: 0.5395693778991699, acc: 0.82, recall: 0.8200000000000001, precision: 0.8246753246753247, f_beta: 0.8193496587715776
train: step: 5154, loss: 0.5508420467376709, acc: 0.8, recall: 0.8, precision: 0.8125, f_beta: 0.797979797979798
train: step: 5155, loss: 0.5534318089485168, acc: 0.79, recall: 0.79, precision: 0.7901160464185675, f_beta: 0.78997899789979
train: step: 5156, loss: 0.6059781908988953, acc: 0.73, recall: 0.73, precision: 0.7377015295576685, f_beta: 0.7277951406391774
train: step: 5157, loss: 0.5644164085388184, acc: 0.78, recall: 0.78, precision: 0.780448717948718, f_beta: 0.7799119647859143
train: step: 5158, loss: 0.516040563583374, acc: 0.84, recall: 0.84, precision: 0.8405448717948718, f_beta: 0.8399359743897559
train: step: 5159, loss: 0.6310068368911743, acc: 0.73, recall: 0.73, precision: 0.7377015295576685, f_beta: 0.7277951406391774
train: step: 5160, loss: 0.6072402000427246, acc: 0.76, recall: 0.76, precision: 0.7668308702791462, f_beta: 0.7584541062801933
train: step: 5161, loss: 0.5697752237319946, acc: 0.77, recall: 0.77, precision: 0.7837326607818411, f_beta: 0.767182913250329
train: step: 5162, loss: 0.5948358774185181, acc: 0.76, recall: 0.76, precision: 0.7708333333333333, f_beta: 0.7575757575757576
train: step: 5163, loss: 0.5152180790901184, acc: 0.81, recall: 0.81, precision: 0.8111200321156162, f_beta: 0.8098288459613652
train: step: 5164, loss: 0.6375476717948914, acc: 0.77, recall: 0.77, precision: 0.7727272727272727, f_beta: 0.7694235588972431
train: step: 5165, loss: 0.5262676477432251, acc: 0.85, recall: 0.85, precision: 0.8617197188921042, f_beta: 0.8487750781328762
train: step: 5166, loss: 0.539423406124115, acc: 0.78, recall: 0.78, precision: 0.7873563218390804, f_beta: 0.7785829307568439
train: step: 5167, loss: 0.5292145609855652, acc: 0.81, recall: 0.81, precision: 0.8257671290458175, f_beta: 0.8076728413807065
train: step: 5168, loss: 0.5969842672348022, acc: 0.81, recall: 0.81, precision: 0.8257671290458175, f_beta: 0.8076728413807065
train: step: 5169, loss: 0.581787645816803, acc: 0.82, recall: 0.8200000000000001, precision: 0.8205128205128205, f_beta: 0.8199279711884755
train: step: 5170, loss: 0.5284364819526672, acc: 0.83, recall: 0.8300000000000001, precision: 0.8410500206696983, f_beta: 0.8286117552172598
train: step: 5171, loss: 0.5603170990943909, acc: 0.79, recall: 0.79, precision: 0.8047498949138294, f_beta: 0.7874278773155178
train: step: 5172, loss: 0.5324350595474243, acc: 0.81, recall: 0.81, precision: 0.8203803224472923, f_beta: 0.8084484323016432
train: step: 5173, loss: 0.531536877155304, acc: 0.82, recall: 0.8200000000000001, precision: 0.8284072249589491, f_beta: 0.818840579710145
train: step: 5174, loss: 0.5551086664199829, acc: 0.8, recall: 0.8, precision: 0.8, f_beta: 0.8000000000000002
train: step: 5175, loss: 0.5466126203536987, acc: 0.81, recall: 0.81, precision: 0.810124049619848, f_beta: 0.8099809980998101
train: step: 5176, loss: 0.5935945510864258, acc: 0.85, recall: 0.85, precision: 0.867801597309794, f_beta: 0.848162769511084
train: step: 5177, loss: 0.5418639183044434, acc: 0.79, recall: 0.79, precision: 0.7910477719791249, f_beta: 0.7898108297467722
train: step: 5178, loss: 0.5571251511573792, acc: 0.83, recall: 0.8300000000000001, precision: 0.8410500206696983, f_beta: 0.8286117552172598
train: step: 5179, loss: 0.521005392074585, acc: 0.84, recall: 0.84, precision: 0.8541666666666667, f_beta: 0.8383838383838385
train: step: 5180, loss: 0.5309345722198486, acc: 0.82, recall: 0.8200000000000001, precision: 0.8333333333333334, f_beta: 0.8181818181818181
train: step: 5181, loss: 0.5311844944953918, acc: 0.83, recall: 0.83, precision: 0.8467843631778058, f_beta: 0.8279178054458953
train: step: 5182, loss: 0.5117061734199524, acc: 0.82, recall: 0.82, precision: 0.8395585738539898, f_beta: 0.8173701298701298
train: step: 5183, loss: 0.6100021600723267, acc: 0.76, recall: 0.76, precision: 0.7708333333333333, f_beta: 0.7575757575757576
train: step: 5184, loss: 0.5850237607955933, acc: 0.72, recall: 0.72, precision: 0.7527573529411764, f_beta: 0.7106242248863166
train: step: 5185, loss: 0.606540322303772, acc: 0.72, recall: 0.72, precision: 0.7203525641025641, f_beta: 0.7198879551820727
train: step: 5186, loss: 0.6198146939277649, acc: 0.74, recall: 0.74, precision: 0.75, f_beta: 0.7373737373737373
train: step: 5187, loss: 0.5535413026809692, acc: 0.84, recall: 0.8400000000000001, precision: 0.8421900161030595, f_beta: 0.8397435897435896
train: step: 5188, loss: 0.6072930693626404, acc: 0.76, recall: 0.76, precision: 0.7616747181964574, f_beta: 0.7596153846153846
train: step: 5189, loss: 0.4795307219028473, acc: 0.85, recall: 0.85, precision: 0.8501400560224089, f_beta: 0.84998499849985
train: step: 5190, loss: 0.6280514597892761, acc: 0.74, recall: 0.74, precision: 0.7463054187192117, f_beta: 0.7383252818035426
train: step: 5191, loss: 0.639167845249176, acc: 0.71, recall: 0.71, precision: 0.7206809583858764, f_beta: 0.7064480210547626
train: step: 5192, loss: 0.5980416536331177, acc: 0.72, recall: 0.72, precision: 0.7232142857142857, f_beta: 0.7189883580891208
train: step: 5193, loss: 0.5623010396957397, acc: 0.8, recall: 0.8, precision: 0.8004807692307692, f_beta: 0.7999199679871949
train: step: 5194, loss: 0.5922514796257019, acc: 0.79, recall: 0.79, precision: 0.7957976336189311, f_beta: 0.7889659330720531
train: step: 5195, loss: 0.53370201587677, acc: 0.81, recall: 0.81, precision: 0.810124049619848, f_beta: 0.8099809980998101
train: step: 5196, loss: 0.5439803600311279, acc: 0.82, recall: 0.8200000000000001, precision: 0.8246753246753247, f_beta: 0.8193496587715776
train: step: 5197, loss: 0.552985429763794, acc: 0.78, recall: 0.78, precision: 0.78, f_beta: 0.78
train: step: 5198, loss: 0.5121965408325195, acc: 0.86, recall: 0.86, precision: 0.8694581280788177, f_beta: 0.8590982286634461
train: step: 5199, loss: 0.6137341856956482, acc: 0.78, recall: 0.78, precision: 0.7840909090909092, f_beta: 0.779205138498595
train: step: 5200, loss: 0.6178605556488037, acc: 0.74, recall: 0.74, precision: 0.7435064935064934, f_beta: 0.7390606182256123
train: step: 5201, loss: 0.5700308680534363, acc: 0.76, recall: 0.76, precision: 0.7758913412563667, f_beta: 0.7564935064935063
train: step: 5202, loss: 0.5266265869140625, acc: 0.8, recall: 0.8, precision: 0.8019323671497585, f_beta: 0.7996794871794872
train: step: 5203, loss: 0.6002374887466431, acc: 0.78, recall: 0.78, precision: 0.780448717948718, f_beta: 0.7799119647859143
train: step: 5204, loss: 0.4811941087245941, acc: 0.88, recall: 0.88, precision: 0.88, f_beta: 0.88
train: step: 5205, loss: 0.5275880098342896, acc: 0.83, recall: 0.83, precision: 0.8467843631778058, f_beta: 0.8279178054458953
train: step: 5206, loss: 0.5183915495872498, acc: 0.8, recall: 0.8, precision: 0.8183361629881154, f_beta: 0.797077922077922
train: step: 5207, loss: 0.6359642744064331, acc: 0.78, recall: 0.78, precision: 0.7840909090909092, f_beta: 0.779205138498595
train: step: 5208, loss: 0.6202877759933472, acc: 0.7, recall: 0.7, precision: 0.7, f_beta: 0.7
train: step: 5209, loss: 0.5074794292449951, acc: 0.82, recall: 0.8200000000000001, precision: 0.822061191626409, f_beta: 0.8197115384615385
train: step: 5210, loss: 0.5754280686378479, acc: 0.78, recall: 0.78, precision: 0.8038194444444444, f_beta: 0.7756017951856385
train: step: 5211, loss: 0.5917341113090515, acc: 0.75, recall: 0.75, precision: 0.7583712277800745, f_beta: 0.7479584635547939
train: step: 5212, loss: 0.5692031979560852, acc: 0.77, recall: 0.77, precision: 0.7709755118426336, f_beta: 0.769792813532179
train: step: 5213, loss: 0.5316762328147888, acc: 0.82, recall: 0.8200000000000001, precision: 0.8472222222222222, f_beta: 0.8164014687882497
train: step: 5214, loss: 0.6166129112243652, acc: 0.82, recall: 0.82, precision: 0.8395585738539898, f_beta: 0.8173701298701298
train: step: 5215, loss: 0.46951282024383545, acc: 0.86, recall: 0.86, precision: 0.8605769230769231, f_beta: 0.8599439775910365
train: step: 5216, loss: 0.5743982791900635, acc: 0.75, recall: 0.75, precision: 0.7501000400160064, f_beta: 0.7499749974997499
train: step: 5217, loss: 0.5488690733909607, acc: 0.87, recall: 0.87, precision: 0.8888188314417822, f_beta: 0.868407733576273
train: step: 5218, loss: 0.6565950512886047, acc: 0.66, recall: 0.66, precision: 0.6736111111111112, f_beta: 0.6532027743778049
train: step: 5219, loss: 0.6068079471588135, acc: 0.76, recall: 0.76, precision: 0.7708333333333333, f_beta: 0.7575757575757576
train: step: 5220, loss: 0.5701488256454468, acc: 0.75, recall: 0.75, precision: 0.7583712277800745, f_beta: 0.7479584635547939
train: step: 5221, loss: 0.5455746054649353, acc: 0.8, recall: 0.8, precision: 0.8183361629881154, f_beta: 0.797077922077922
train: step: 5222, loss: 0.6167352199554443, acc: 0.69, recall: 0.69, precision: 0.7148349163274537, f_beta: 0.6807743795695602
train: step: 5223, loss: 0.5388981699943542, acc: 0.85, recall: 0.85, precision: 0.8617197188921042, f_beta: 0.8487750781328762
train: step: 5224, loss: 0.6238836050033569, acc: 0.74, recall: 0.74, precision: 0.7546689303904923, f_beta: 0.7362012987012987
train: step: 5225, loss: 0.5733270049095154, acc: 0.75, recall: 0.75, precision: 0.7525252525252526, f_beta: 0.7493734335839599
train: step: 5226, loss: 0.6175492405891418, acc: 0.68, recall: 0.6799999999999999, precision: 0.7068014705882353, f_beta: 0.6692848284415047
train: step: 5227, loss: 0.5787899494171143, acc: 0.76, recall: 0.76, precision: 0.7896613190730838, f_beta: 0.7536945812807881
train: step: 5228, loss: 0.5561867952346802, acc: 0.79, recall: 0.79, precision: 0.811025311025311, f_beta: 0.7863899908452854
train: step: 5229, loss: 0.4896843433380127, acc: 0.84, recall: 0.84, precision: 0.8607809847198642, f_beta: 0.8376623376623376
train: step: 5230, loss: 0.5987988114356995, acc: 0.81, recall: 0.81, precision: 0.8406593406593407, f_beta: 0.8056265984654732
train: step: 5231, loss: 0.5401021838188171, acc: 0.8, recall: 0.8, precision: 0.8183361629881154, f_beta: 0.797077922077922
train: step: 5232, loss: 0.5401232242584229, acc: 0.79, recall: 0.79, precision: 0.8047498949138294, f_beta: 0.7874278773155178
train: step: 5233, loss: 0.5746002197265625, acc: 0.79, recall: 0.79, precision: 0.8186813186813187, f_beta: 0.7851662404092071
train: step: 5234, loss: 0.5658392906188965, acc: 0.81, recall: 0.81, precision: 0.8324753324753325, f_beta: 0.8067338012409724
train: step: 5235, loss: 0.5403774976730347, acc: 0.79, recall: 0.79, precision: 0.7929292929292929, f_beta: 0.7894736842105263
train: step: 5236, loss: 0.6200563311576843, acc: 0.69, recall: 0.69, precision: 0.7037752037752039, f_beta: 0.6846709388668497
train: step: 5237, loss: 0.5299776196479797, acc: 0.85, recall: 0.85, precision: 0.851264552388599, f_beta: 0.8498648783905516
train: step: 5238, loss: 0.5424292087554932, acc: 0.78, recall: 0.78, precision: 0.8038194444444444, f_beta: 0.7756017951856385
train: step: 5239, loss: 0.5799622535705566, acc: 0.79, recall: 0.79, precision: 0.7901160464185675, f_beta: 0.78997899789979
train: step: 5240, loss: 0.5715938806533813, acc: 0.78, recall: 0.78, precision: 0.7873563218390804, f_beta: 0.7785829307568439
train: step: 5241, loss: 0.5506927371025085, acc: 0.78, recall: 0.78, precision: 0.780448717948718, f_beta: 0.7799119647859143
train: step: 5242, loss: 0.5655760765075684, acc: 0.77, recall: 0.77, precision: 0.7790409260024803, f_beta: 0.7681217864704104
train: step: 5243, loss: 0.575546383857727, acc: 0.79, recall: 0.79, precision: 0.8047498949138294, f_beta: 0.7874278773155178
train: step: 5244, loss: 0.5847290754318237, acc: 0.76, recall: 0.76, precision: 0.7987132352941176, f_beta: 0.7519636213311285
train: step: 5245, loss: 0.5387590527534485, acc: 0.78, recall: 0.78, precision: 0.7873563218390804, f_beta: 0.7785829307568439
train: step: 5246, loss: 0.5697193145751953, acc: 0.75, recall: 0.75, precision: 0.7747252747252746, f_beta: 0.7442455242966751
train: step: 5247, loss: 0.6727825403213501, acc: 0.74, recall: 0.74, precision: 0.7546689303904923, f_beta: 0.7362012987012987
train: step: 5248, loss: 0.5585623383522034, acc: 0.81, recall: 0.81, precision: 0.8161974704202366, f_beta: 0.8090644156366193
train: step: 5249, loss: 0.5178282856941223, acc: 0.82, recall: 0.8200000000000001, precision: 0.822061191626409, f_beta: 0.8197115384615385
train: step: 5250, loss: 0.437531054019928, acc: 0.88, recall: 0.8799999999999999, precision: 0.885551948051948, f_beta: 0.8795664391810518
train: step: 5251, loss: 0.6237789988517761, acc: 0.76, recall: 0.76, precision: 0.7758913412563667, f_beta: 0.7564935064935063
train: step: 5252, loss: 0.5950586795806885, acc: 0.8, recall: 0.8, precision: 0.8342245989304813, f_beta: 0.794745484400657
train: step: 5253, loss: 0.5479382872581482, acc: 0.8, recall: 0.8, precision: 0.8, f_beta: 0.8000000000000002
train: step: 5254, loss: 0.5859563946723938, acc: 0.72, recall: 0.72, precision: 0.733446519524618, f_beta: 0.7159090909090909
train: step: 5255, loss: 0.6018229722976685, acc: 0.76, recall: 0.76, precision: 0.7896613190730838, f_beta: 0.7536945812807881
train: step: 5256, loss: 0.5326639413833618, acc: 0.82, recall: 0.8200000000000001, precision: 0.822061191626409, f_beta: 0.8197115384615385
train: step: 5257, loss: 0.5361939072608948, acc: 0.81, recall: 0.81, precision: 0.8131313131313131, f_beta: 0.8095238095238095
train: step: 5258, loss: 0.6164324283599854, acc: 0.7, recall: 0.7, precision: 0.7228163992869875, f_beta: 0.6921182266009853
train: step: 5259, loss: 0.5950519442558289, acc: 0.76, recall: 0.76, precision: 0.7987132352941176, f_beta: 0.7519636213311285
train: step: 5260, loss: 0.5519714951515198, acc: 0.82, recall: 0.8200000000000001, precision: 0.8284072249589491, f_beta: 0.818840579710145
train: step: 5261, loss: 0.5203633308410645, acc: 0.83, recall: 0.83, precision: 0.8301320528211285, f_beta: 0.8299829982998299
train: step: 5262, loss: 0.5911557674407959, acc: 0.81, recall: 0.81, precision: 0.810124049619848, f_beta: 0.8099809980998101
train: step: 5263, loss: 0.5314804315567017, acc: 0.87, recall: 0.87, precision: 0.8968253968253967, f_beta: 0.8677652324280338
train: step: 5264, loss: 0.542773962020874, acc: 0.8, recall: 0.8, precision: 0.8078817733990147, f_beta: 0.7987117552334944
train: step: 5265, loss: 0.6351168155670166, acc: 0.73, recall: 0.73, precision: 0.7416981925178646, f_beta: 0.7266929851199515
train: step: 5266, loss: 0.5953727960586548, acc: 0.79, recall: 0.79, precision: 0.7957976336189311, f_beta: 0.7889659330720531
train: step: 5267, loss: 0.5315492749214172, acc: 0.81, recall: 0.81, precision: 0.8111200321156162, f_beta: 0.8098288459613652
train: step: 5268, loss: 0.5826740860939026, acc: 0.74, recall: 0.74, precision: 0.7463054187192117, f_beta: 0.7383252818035426
train: step: 5269, loss: 0.5883265137672424, acc: 0.8, recall: 0.8, precision: 0.8255208333333333, f_beta: 0.7960016319869441
train: step: 5270, loss: 0.5580844283103943, acc: 0.81, recall: 0.81, precision: 0.810124049619848, f_beta: 0.8099809980998101
train: step: 5271, loss: 0.4967713952064514, acc: 0.86, recall: 0.86, precision: 0.8605769230769231, f_beta: 0.8599439775910365
train: step: 5272, loss: 0.6342024207115173, acc: 0.82, recall: 0.8200000000000001, precision: 0.8205128205128205, f_beta: 0.8199279711884755
train: step: 5273, loss: 0.5312032699584961, acc: 0.83, recall: 0.8300000000000001, precision: 0.8365973072215422, f_beta: 0.8291628982011858
train: step: 5274, loss: 0.5801782011985779, acc: 0.83, recall: 0.8300000000000001, precision: 0.8333333333333334, f_beta: 0.8295739348370927
train: step: 5275, loss: 0.5723010897636414, acc: 0.74, recall: 0.74, precision: 0.7463054187192117, f_beta: 0.7383252818035426
train: step: 5276, loss: 0.49514493346214294, acc: 0.85, recall: 0.85, precision: 0.8617197188921042, f_beta: 0.8487750781328762
train: step: 5277, loss: 0.5014824867248535, acc: 0.86, recall: 0.86, precision: 0.8605769230769231, f_beta: 0.8599439775910365
train: step: 5278, loss: 0.6137024164199829, acc: 0.72, recall: 0.72, precision: 0.7232142857142857, f_beta: 0.7189883580891208
train: step: 5279, loss: 0.5842054486274719, acc: 0.74, recall: 0.74, precision: 0.75, f_beta: 0.7373737373737373
train: step: 5280, loss: 0.5413598418235779, acc: 0.88, recall: 0.8799999999999999, precision: 0.885551948051948, f_beta: 0.8795664391810518
train: step: 5281, loss: 0.527472972869873, acc: 0.87, recall: 0.87, precision: 0.8773969808241534, f_beta: 0.8693598633303186
train: step: 5282, loss: 0.6110593676567078, acc: 0.73, recall: 0.73, precision: 0.7466752466752467, f_beta: 0.7253585596582239
train: step: 5283, loss: 0.5002540349960327, acc: 0.85, recall: 0.85, precision: 0.8617197188921042, f_beta: 0.8487750781328762
train: step: 5284, loss: 0.5490526556968689, acc: 0.81, recall: 0.81, precision: 0.8257671290458175, f_beta: 0.8076728413807065
train: step: 5285, loss: 0.5944117307662964, acc: 0.73, recall: 0.73, precision: 0.7377015295576685, f_beta: 0.7277951406391774
train: step: 5286, loss: 0.5320580005645752, acc: 0.78, recall: 0.78, precision: 0.8119429590017826, f_beta: 0.7742200328407225
train: step: 5287, loss: 0.5366030931472778, acc: 0.82, recall: 0.8200000000000001, precision: 0.8246753246753247, f_beta: 0.8193496587715776
train: step: 5288, loss: 0.5509734153747559, acc: 0.8, recall: 0.8, precision: 0.8043831168831169, f_beta: 0.7992773986350863
train: step: 5289, loss: 0.5491921305656433, acc: 0.84, recall: 0.8400000000000001, precision: 0.8489326765188834, f_beta: 0.8389694041867956
train: step: 5290, loss: 0.5074020028114319, acc: 0.84, recall: 0.84, precision: 0.8541666666666667, f_beta: 0.8383838383838385
train: step: 5291, loss: 0.5994009375572205, acc: 0.74, recall: 0.74, precision: 0.7546689303904923, f_beta: 0.7362012987012987
train: step: 5292, loss: 0.5462679862976074, acc: 0.78, recall: 0.78, precision: 0.8038194444444444, f_beta: 0.7756017951856385
train: step: 5293, loss: 0.525148868560791, acc: 0.79, recall: 0.79, precision: 0.7997106242248864, f_beta: 0.7882851093860268
train: step: 5294, loss: 0.5516360998153687, acc: 0.77, recall: 0.77, precision: 0.7790409260024803, f_beta: 0.7681217864704104
train: step: 5295, loss: 0.5034410357475281, acc: 0.89, recall: 0.89, precision: 0.8914090726615818, f_beta: 0.8899009108197378
train: step: 5296, loss: 0.5414612293243408, acc: 0.81, recall: 0.81, precision: 0.8257671290458175, f_beta: 0.8076728413807065
train: step: 5297, loss: 0.5318939089775085, acc: 0.81, recall: 0.81, precision: 0.810124049619848, f_beta: 0.8099809980998101
train: step: 5298, loss: 0.6321443915367126, acc: 0.72, recall: 0.72, precision: 0.733446519524618, f_beta: 0.7159090909090909
train: step: 5299, loss: 0.5486240386962891, acc: 0.8, recall: 0.8, precision: 0.8019323671497585, f_beta: 0.7996794871794872
train: step: 5300, loss: 0.5644346475601196, acc: 0.78, recall: 0.78, precision: 0.7971137521222411, f_beta: 0.7767857142857144
train: step: 5301, loss: 0.5813878774642944, acc: 0.81, recall: 0.81, precision: 0.8111200321156162, f_beta: 0.8098288459613652
train: step: 5302, loss: 0.612112283706665, acc: 0.74, recall: 0.74, precision: 0.7857142857142857, f_beta: 0.7291666666666666
train: step: 5303, loss: 0.6444510817527771, acc: 0.8, recall: 0.8, precision: 0.8004807692307692, f_beta: 0.7999199679871949
train: step: 5304, loss: 0.5212934613227844, acc: 0.8, recall: 0.8, precision: 0.8004807692307692, f_beta: 0.7999199679871949
train: step: 5305, loss: 0.576447606086731, acc: 0.8, recall: 0.8, precision: 0.8078817733990147, f_beta: 0.7987117552334944
train: step: 5306, loss: 0.5431934595108032, acc: 0.81, recall: 0.81, precision: 0.8324753324753325, f_beta: 0.8067338012409724
train: step: 5307, loss: 0.559734582901001, acc: 0.77, recall: 0.77, precision: 0.7895752895752897, f_beta: 0.7660461804495982
train: step: 5308, loss: 0.6132024526596069, acc: 0.72, recall: 0.72, precision: 0.7291666666666667, f_beta: 0.7171717171717171
train: step: 5309, loss: 0.5731015205383301, acc: 0.77, recall: 0.77, precision: 0.7727272727272727, f_beta: 0.7694235588972431
train: step: 5310, loss: 0.5643417239189148, acc: 0.76, recall: 0.76, precision: 0.7616747181964574, f_beta: 0.7596153846153846
train: step: 5311, loss: 0.6332854628562927, acc: 0.66, recall: 0.6599999999999999, precision: 0.6838235294117647, f_beta: 0.6486151302190988
train: step: 5312, loss: 0.5708518028259277, acc: 0.75, recall: 0.75, precision: 0.7583712277800745, f_beta: 0.7479584635547939
train: step: 5313, loss: 0.510336697101593, acc: 0.82, recall: 0.8200000000000001, precision: 0.8284072249589491, f_beta: 0.818840579710145
train: step: 5314, loss: 0.45761221647262573, acc: 0.86, recall: 0.8600000000000001, precision: 0.8652597402597402, f_beta: 0.8594941790445605
train: step: 5315, loss: 0.5614535808563232, acc: 0.8, recall: 0.8, precision: 0.8078817733990147, f_beta: 0.7987117552334944
train: step: 5316, loss: 0.5716656446456909, acc: 0.8, recall: 0.8, precision: 0.8, f_beta: 0.8000000000000002
train: step: 5317, loss: 0.5133439898490906, acc: 0.82, recall: 0.82, precision: 0.82, f_beta: 0.82
train: step: 5318, loss: 0.4884037375450134, acc: 0.82, recall: 0.8200000000000001, precision: 0.822061191626409, f_beta: 0.8197115384615385
train: step: 5319, loss: 0.5108041167259216, acc: 0.82, recall: 0.8200000000000001, precision: 0.8246753246753247, f_beta: 0.8193496587715776
train: step: 5320, loss: 0.5378740429878235, acc: 0.83, recall: 0.83, precision: 0.8467843631778058, f_beta: 0.8279178054458953
train: step: 5321, loss: 0.5286051034927368, acc: 0.81, recall: 0.81, precision: 0.8111200321156162, f_beta: 0.8098288459613652
train: step: 5322, loss: 0.631770670413971, acc: 0.81, recall: 0.81, precision: 0.8324753324753325, f_beta: 0.8067338012409724
train: step: 5323, loss: 0.5319130420684814, acc: 0.83, recall: 0.8300000000000001, precision: 0.8365973072215422, f_beta: 0.8291628982011858
train: step: 5324, loss: 0.5931020975112915, acc: 0.8, recall: 0.8, precision: 0.8125, f_beta: 0.797979797979798
train: step: 5325, loss: 0.46596428751945496, acc: 0.87, recall: 0.87, precision: 0.8737373737373737, f_beta: 0.8696741854636592
train: step: 5326, loss: 0.5113846063613892, acc: 0.83, recall: 0.8300000000000001, precision: 0.8333333333333334, f_beta: 0.8295739348370927
train: step: 5327, loss: 0.4827266335487366, acc: 0.88, recall: 0.88, precision: 0.8824476650563606, f_beta: 0.8798076923076923
train: step: 5328, loss: 0.567099928855896, acc: 0.75, recall: 0.75, precision: 0.792192613370734, f_beta: 0.7406369955389562
train: step: 5329, loss: 0.6277293562889099, acc: 0.83, recall: 0.8300000000000001, precision: 0.8410500206696983, f_beta: 0.8286117552172598
train: step: 5330, loss: 0.5252041816711426, acc: 0.81, recall: 0.81, precision: 0.8111200321156162, f_beta: 0.8098288459613652
train: step: 5331, loss: 0.5758659243583679, acc: 0.74, recall: 0.74, precision: 0.767379679144385, f_beta: 0.7331691297208538
train: step: 5332, loss: 0.5046349763870239, acc: 0.84, recall: 0.8400000000000001, precision: 0.8489326765188834, f_beta: 0.8389694041867956
train: step: 5333, loss: 0.5752160549163818, acc: 0.78, recall: 0.78, precision: 0.7916666666666666, f_beta: 0.7777777777777779
train: step: 5334, loss: 0.5251623392105103, acc: 0.79, recall: 0.79, precision: 0.8279059249208502, f_beta: 0.7837503861600247
train: step: 5335, loss: 0.5242860913276672, acc: 0.87, recall: 0.87, precision: 0.8713368125250903, f_beta: 0.8698828946051447
train: step: 5336, loss: 0.6047958135604858, acc: 0.7, recall: 0.7, precision: 0.7122241086587436, f_beta: 0.6956168831168832
train: step: 5337, loss: 0.5460290908813477, acc: 0.84, recall: 0.8400000000000001, precision: 0.8449675324675325, f_beta: 0.839421918908069
train: step: 5338, loss: 0.6067075133323669, acc: 0.71, recall: 0.71, precision: 0.7170318313352625, f_beta: 0.7076318177235609
train: step: 5339, loss: 0.5837032794952393, acc: 0.76, recall: 0.76, precision: 0.7668308702791462, f_beta: 0.7584541062801933
train: step: 5340, loss: 0.538963794708252, acc: 0.79, recall: 0.79, precision: 0.8186813186813187, f_beta: 0.7851662404092071
train: step: 5341, loss: 0.6357078552246094, acc: 0.85, recall: 0.8500000000000001, precision: 0.8535353535353536, f_beta: 0.849624060150376
train: step: 5342, loss: 0.6465979218482971, acc: 0.76, recall: 0.76, precision: 0.7758913412563667, f_beta: 0.7564935064935063
train: step: 5343, loss: 0.5954242944717407, acc: 0.73, recall: 0.73, precision: 0.7688172043010753, f_beta: 0.7198879551820727
train: step: 5344, loss: 0.6202718615531921, acc: 0.82, recall: 0.82, precision: 0.8395585738539898, f_beta: 0.8173701298701298
train: step: 5345, loss: 0.6098929047584534, acc: 0.8, recall: 0.8, precision: 0.8019323671497585, f_beta: 0.7996794871794872
train: step: 5346, loss: 0.5323190689086914, acc: 0.78, recall: 0.78, precision: 0.78, f_beta: 0.78
train: step: 5347, loss: 0.5332404971122742, acc: 0.83, recall: 0.83, precision: 0.8467843631778058, f_beta: 0.8279178054458953
train: step: 5348, loss: 0.492203950881958, acc: 0.85, recall: 0.85, precision: 0.8753753753753755, f_beta: 0.8474214220323468
train: step: 5349, loss: 0.5403565764427185, acc: 0.8, recall: 0.8, precision: 0.8255208333333333, f_beta: 0.7960016319869441
train: step: 5350, loss: 0.5705299973487854, acc: 0.77, recall: 0.77, precision: 0.7709755118426336, f_beta: 0.769792813532179
train: step: 5351, loss: 0.5348178148269653, acc: 0.82, recall: 0.8200000000000001, precision: 0.822061191626409, f_beta: 0.8197115384615385
train: step: 5352, loss: 0.5189212560653687, acc: 0.84, recall: 0.84, precision: 0.8405448717948718, f_beta: 0.8399359743897559
train: step: 5353, loss: 0.5407467484474182, acc: 0.83, recall: 0.8300000000000001, precision: 0.8410500206696983, f_beta: 0.8286117552172598
train: step: 5354, loss: 0.5801578164100647, acc: 0.89, recall: 0.89, precision: 0.9098360655737705, f_beta: 0.8886526976414617
train: step: 5355, loss: 0.6230206489562988, acc: 0.77, recall: 0.77, precision: 0.7967032967032968, f_beta: 0.7647058823529411
train: step: 5356, loss: 0.6024768352508545, acc: 0.72, recall: 0.72, precision: 0.7257799671592775, f_beta: 0.7181964573268922
train: step: 5357, loss: 0.527656614780426, acc: 0.8, recall: 0.8, precision: 0.8183361629881154, f_beta: 0.797077922077922
train: step: 5358, loss: 0.5087637901306152, acc: 0.84, recall: 0.8400000000000001, precision: 0.8421900161030595, f_beta: 0.8397435897435896
train: step: 5359, loss: 0.6058862209320068, acc: 0.73, recall: 0.73, precision: 0.7345981232150143, f_beta: 0.7286704853783539
train: step: 5360, loss: 0.5348777174949646, acc: 0.85, recall: 0.85, precision: 0.8617197188921042, f_beta: 0.8487750781328762
train: step: 5361, loss: 0.5680480003356934, acc: 0.79, recall: 0.79, precision: 0.8186813186813187, f_beta: 0.7851662404092071
train: step: 5362, loss: 0.5775895118713379, acc: 0.78, recall: 0.78, precision: 0.7916666666666666, f_beta: 0.7777777777777779
train: step: 5363, loss: 0.5527873635292053, acc: 0.76, recall: 0.76, precision: 0.7668308702791462, f_beta: 0.7584541062801933
train: step: 5364, loss: 0.5431778430938721, acc: 0.8, recall: 0.8, precision: 0.8183361629881154, f_beta: 0.797077922077922
train: step: 5365, loss: 0.5601813793182373, acc: 0.79, recall: 0.79, precision: 0.811025311025311, f_beta: 0.7863899908452854
train: step: 5366, loss: 0.669097363948822, acc: 0.67, recall: 0.67, precision: 0.7156265854895991, f_beta: 0.6515679442508711
train: step: 5367, loss: 0.5678200721740723, acc: 0.79, recall: 0.79, precision: 0.7997106242248864, f_beta: 0.7882851093860268
train: step: 5368, loss: 0.5054645538330078, acc: 0.83, recall: 0.83, precision: 0.853925353925354, f_beta: 0.8270776116366596
train: step: 5369, loss: 0.5276626944541931, acc: 0.81, recall: 0.81, precision: 0.8324753324753325, f_beta: 0.8067338012409724
train: step: 5370, loss: 0.5629652738571167, acc: 0.75, recall: 0.75, precision: 0.792192613370734, f_beta: 0.7406369955389562
train: step: 5371, loss: 0.5991360545158386, acc: 0.78, recall: 0.78, precision: 0.7916666666666666, f_beta: 0.7777777777777779
train: step: 5372, loss: 0.6360995769500732, acc: 0.67, recall: 0.67, precision: 0.6823251823251824, f_beta: 0.6643271284711627
train: step: 5373, loss: 0.615002453327179, acc: 0.78, recall: 0.78, precision: 0.7916666666666666, f_beta: 0.7777777777777779
train: step: 5374, loss: 0.5891265869140625, acc: 0.73, recall: 0.73, precision: 0.7600633197648123, f_beta: 0.721964782205746
train: step: 5375, loss: 0.5767645835876465, acc: 0.74, recall: 0.74, precision: 0.7757352941176471, f_beta: 0.7312939231087227
train: step: 5376, loss: 0.5397752523422241, acc: 0.8, recall: 0.8, precision: 0.8183361629881154, f_beta: 0.797077922077922
train: step: 5377, loss: 0.6159493923187256, acc: 0.76, recall: 0.76, precision: 0.7616747181964574, f_beta: 0.7596153846153846
train: step: 5378, loss: 0.5565230846405029, acc: 0.77, recall: 0.77, precision: 0.7967032967032968, f_beta: 0.7647058823529411
train: step: 5379, loss: 0.5361692309379578, acc: 0.83, recall: 0.8300000000000001, precision: 0.8311922922521076, f_beta: 0.8298468621759585
train: step: 5380, loss: 0.5759208798408508, acc: 0.79, recall: 0.79, precision: 0.7957976336189311, f_beta: 0.7889659330720531
train: step: 5381, loss: 0.6316477060317993, acc: 0.69, recall: 0.69, precision: 0.690686471296668, f_beta: 0.6897207486738064
train: step: 5382, loss: 0.6119046211242676, acc: 0.7, recall: 0.7, precision: 0.7380952380952381, f_beta: 0.6875
train: step: 5383, loss: 0.5598536133766174, acc: 0.79, recall: 0.79, precision: 0.7957976336189311, f_beta: 0.7889659330720531
train: step: 5384, loss: 0.5841771960258484, acc: 0.8, recall: 0.8, precision: 0.8446691176470589, f_beta: 0.7933030177759404
train: step: 5385, loss: 0.5755409002304077, acc: 0.79, recall: 0.79, precision: 0.8047498949138294, f_beta: 0.7874278773155178
train: step: 5386, loss: 0.5496240258216858, acc: 0.83, recall: 0.8300000000000001, precision: 0.8311922922521076, f_beta: 0.8298468621759585
train: step: 5387, loss: 0.579139232635498, acc: 0.75, recall: 0.75, precision: 0.7747252747252746, f_beta: 0.7442455242966751
train: step: 5388, loss: 0.5460841655731201, acc: 0.77, recall: 0.77, precision: 0.7790409260024803, f_beta: 0.7681217864704104
train: step: 5389, loss: 0.5558382868766785, acc: 0.82, recall: 0.8200000000000001, precision: 0.8205128205128205, f_beta: 0.8199279711884755
train: step: 5390, loss: 0.5116308927536011, acc: 0.84, recall: 0.84, precision: 0.8541666666666667, f_beta: 0.8383838383838385
train: step: 5391, loss: 0.48901885747909546, acc: 0.86, recall: 0.86, precision: 0.8623188405797102, f_beta: 0.859775641025641
train: step: 5392, loss: 0.5515176653862, acc: 0.8, recall: 0.8, precision: 0.8078817733990147, f_beta: 0.7987117552334944
train: step: 5393, loss: 0.5938747525215149, acc: 0.75, recall: 0.75, precision: 0.7681252681252682, f_beta: 0.7457023700539112
train: step: 5394, loss: 0.5771908164024353, acc: 0.76, recall: 0.76, precision: 0.7758913412563667, f_beta: 0.7564935064935063
train: step: 5395, loss: 0.5254532098770142, acc: 0.81, recall: 0.81, precision: 0.8131313131313131, f_beta: 0.8095238095238095
train: step: 5396, loss: 0.5435360670089722, acc: 0.81, recall: 0.81, precision: 0.8324753324753325, f_beta: 0.8067338012409724
train: step: 5397, loss: 0.6186992526054382, acc: 0.76, recall: 0.76, precision: 0.7987132352941176, f_beta: 0.7519636213311285
train: step: 5398, loss: 0.5881662964820862, acc: 0.73, recall: 0.73, precision: 0.7792617775619233, f_beta: 0.7175436761167486
train: step: 5399, loss: 0.5505708456039429, acc: 0.83, recall: 0.83, precision: 0.8467843631778058, f_beta: 0.8279178054458953
train: step: 5400, loss: 0.5042169094085693, acc: 0.8, recall: 0.8, precision: 0.8183361629881154, f_beta: 0.797077922077922
train: step: 5401, loss: 0.5945740342140198, acc: 0.71, recall: 0.71, precision: 0.7549781447304517, f_beta: 0.69662098545873
train: step: 5402, loss: 0.5934628248214722, acc: 0.75, recall: 0.75, precision: 0.7826775214834916, f_beta: 0.7425599835238389
train: step: 5403, loss: 0.5558266639709473, acc: 0.78, recall: 0.78, precision: 0.8119429590017826, f_beta: 0.7742200328407225
train: step: 5404, loss: 0.5723682641983032, acc: 0.77, recall: 0.77, precision: 0.7753977968176254, f_beta: 0.7688674505074867
train: step: 5405, loss: 0.5622864365577698, acc: 0.79, recall: 0.79, precision: 0.811025311025311, f_beta: 0.7863899908452854
train: step: 5406, loss: 0.52414870262146, acc: 0.82, recall: 0.82, precision: 0.8395585738539898, f_beta: 0.8173701298701298
train: step: 5407, loss: 0.5719387531280518, acc: 0.76, recall: 0.76, precision: 0.7604166666666667, f_beta: 0.7599039615846339
train: step: 5408, loss: 0.643093466758728, acc: 0.64, recall: 0.64, precision: 0.640224358974359, f_beta: 0.6398559423769508
train: step: 5409, loss: 0.49533236026763916, acc: 0.83, recall: 0.8300000000000001, precision: 0.8311922922521076, f_beta: 0.8298468621759585
train: step: 5410, loss: 0.5467196702957153, acc: 0.78, recall: 0.78, precision: 0.7971137521222411, f_beta: 0.7767857142857144
train: step: 5411, loss: 0.521643877029419, acc: 0.86, recall: 0.86, precision: 0.86, f_beta: 0.8599999999999999
train: step: 5412, loss: 0.5170688033103943, acc: 0.84, recall: 0.8400000000000001, precision: 0.8421900161030595, f_beta: 0.8397435897435896
train: step: 5413, loss: 0.5840380191802979, acc: 0.8, recall: 0.8, precision: 0.8078817733990147, f_beta: 0.7987117552334944
train: step: 5414, loss: 0.5427247881889343, acc: 0.78, recall: 0.78, precision: 0.780448717948718, f_beta: 0.7799119647859143
train: step: 5415, loss: 0.5291145443916321, acc: 0.84, recall: 0.84, precision: 0.8689236111111112, f_beta: 0.8368013055895552
train: step: 5416, loss: 0.4940396547317505, acc: 0.84, recall: 0.84, precision: 0.84, f_beta: 0.8399999999999999
train: step: 5417, loss: 0.5781459212303162, acc: 0.74, recall: 0.74, precision: 0.767379679144385, f_beta: 0.7331691297208538
train: step: 5418, loss: 0.5862157940864563, acc: 0.82, recall: 0.8200000000000001, precision: 0.8333333333333334, f_beta: 0.8181818181818181
train: step: 5419, loss: 0.5772596597671509, acc: 0.83, recall: 0.8300000000000001, precision: 0.8365973072215422, f_beta: 0.8291628982011858
train: step: 5420, loss: 0.5688921809196472, acc: 0.75, recall: 0.75, precision: 0.7681252681252682, f_beta: 0.7457023700539112
train: step: 5421, loss: 0.5963433384895325, acc: 0.77, recall: 0.77, precision: 0.7967032967032968, f_beta: 0.7647058823529411
train: step: 5422, loss: 0.6142951250076294, acc: 0.76, recall: 0.76, precision: 0.7616747181964574, f_beta: 0.7596153846153846
train: step: 5423, loss: 0.43490925431251526, acc: 0.89, recall: 0.89, precision: 0.8914090726615818, f_beta: 0.8899009108197378
train: step: 5424, loss: 0.4435266852378845, acc: 0.92, recall: 0.9199999999999999, precision: 0.9206730769230769, f_beta: 0.919967987194878
train: step: 5425, loss: 0.5466923117637634, acc: 0.76, recall: 0.76, precision: 0.76, f_beta: 0.76
train: step: 5426, loss: 0.5329583883285522, acc: 0.86, recall: 0.86, precision: 0.8605769230769231, f_beta: 0.8599439775910365
train: step: 5427, loss: 0.6638361215591431, acc: 0.84, recall: 0.84, precision: 0.8405448717948718, f_beta: 0.8399359743897559
train: step: 5428, loss: 0.48756131529808044, acc: 0.84, recall: 0.8400000000000001, precision: 0.8449675324675325, f_beta: 0.839421918908069
train: step: 5429, loss: 0.6237202286720276, acc: 0.72, recall: 0.72, precision: 0.733446519524618, f_beta: 0.7159090909090909
train: step: 5430, loss: 0.5270419120788574, acc: 0.8, recall: 0.8, precision: 0.8342245989304813, f_beta: 0.794745484400657
train: step: 5431, loss: 0.5614421367645264, acc: 0.8, recall: 0.8, precision: 0.8342245989304813, f_beta: 0.794745484400657
train: step: 5432, loss: 0.5797549486160278, acc: 0.8, recall: 0.8, precision: 0.8125, f_beta: 0.797979797979798
train: step: 5433, loss: 0.5571300387382507, acc: 0.75, recall: 0.75, precision: 0.7681252681252682, f_beta: 0.7457023700539112
train: step: 5434, loss: 0.5960781574249268, acc: 0.71, recall: 0.71, precision: 0.7121212121212122, f_beta: 0.7092731829573935
train: step: 5435, loss: 0.6038761734962463, acc: 0.74, recall: 0.74, precision: 0.7546689303904923, f_beta: 0.7362012987012987
train: step: 5436, loss: 0.5942133069038391, acc: 0.72, recall: 0.72, precision: 0.7619047619047619, f_beta: 0.7083333333333334
train: step: 5437, loss: 0.6135448813438416, acc: 0.79, recall: 0.79, precision: 0.7901160464185675, f_beta: 0.78997899789979
train: step: 5438, loss: 0.5858152508735657, acc: 0.77, recall: 0.77, precision: 0.7701080432172869, f_beta: 0.7699769976997699
train: step: 5439, loss: 0.5505754947662354, acc: 0.82, recall: 0.8200000000000001, precision: 0.8333333333333334, f_beta: 0.8181818181818181
train: step: 5440, loss: 0.5774915814399719, acc: 0.77, recall: 0.77, precision: 0.7790409260024803, f_beta: 0.7681217864704104
train: step: 5441, loss: 0.5932479500770569, acc: 0.73, recall: 0.73, precision: 0.7917300862506342, f_beta: 0.7149192271143491
train: step: 5442, loss: 0.5097812414169312, acc: 0.83, recall: 0.8300000000000001, precision: 0.8410500206696983, f_beta: 0.8286117552172598
train: step: 5443, loss: 0.5185916423797607, acc: 0.8, recall: 0.8, precision: 0.8125, f_beta: 0.797979797979798
train: step: 5444, loss: 0.511796772480011, acc: 0.81, recall: 0.81, precision: 0.8406593406593407, f_beta: 0.8056265984654732
train: step: 5445, loss: 0.5854805707931519, acc: 0.73, recall: 0.73, precision: 0.7527472527472527, f_beta: 0.7237851662404092
train: step: 5446, loss: 0.5426890254020691, acc: 0.78, recall: 0.78, precision: 0.7916666666666666, f_beta: 0.7777777777777779
train: step: 5447, loss: 0.5792510509490967, acc: 0.8, recall: 0.8, precision: 0.8255208333333333, f_beta: 0.7960016319869441
train: step: 5448, loss: 0.5989168882369995, acc: 0.82, recall: 0.82, precision: 0.8395585738539898, f_beta: 0.8173701298701298
train: step: 5449, loss: 0.6019657850265503, acc: 0.71, recall: 0.71, precision: 0.7549781447304517, f_beta: 0.69662098545873
train: step: 5450, loss: 0.5683556199073792, acc: 0.8, recall: 0.8, precision: 0.8043831168831169, f_beta: 0.7992773986350863
train: step: 5451, loss: 0.5963237285614014, acc: 0.78, recall: 0.78, precision: 0.8333333333333333, f_beta: 0.7708333333333333
train: step: 5452, loss: 0.5802262425422668, acc: 0.72, recall: 0.72, precision: 0.7387152777777778, f_beta: 0.7144022847817217
train: step: 5453, loss: 0.6116692423820496, acc: 0.79, recall: 0.79, precision: 0.7910477719791249, f_beta: 0.7898108297467722
train: step: 5454, loss: 0.528087854385376, acc: 0.8, recall: 0.8, precision: 0.8078817733990147, f_beta: 0.7987117552334944
train: step: 5455, loss: 0.5637269020080566, acc: 0.77, recall: 0.77, precision: 0.805291723202171, f_beta: 0.7631551848419318
train: step: 5456, loss: 0.5912635326385498, acc: 0.81, recall: 0.81, precision: 0.8161974704202366, f_beta: 0.8090644156366193
train: step: 5457, loss: 0.5485997200012207, acc: 0.83, recall: 0.83, precision: 0.853925353925354, f_beta: 0.8270776116366596
train: step: 5458, loss: 0.5269919037818909, acc: 0.86, recall: 0.86, precision: 0.8694581280788177, f_beta: 0.8590982286634461
train: step: 5459, loss: 0.5712926983833313, acc: 0.81, recall: 0.81, precision: 0.8203803224472923, f_beta: 0.8084484323016432
train: step: 5460, loss: 0.5752586722373962, acc: 0.75, recall: 0.75, precision: 0.7681252681252682, f_beta: 0.7457023700539112
train: step: 5461, loss: 0.5365594625473022, acc: 0.82, recall: 0.8200000000000001, precision: 0.8205128205128205, f_beta: 0.8199279711884755
train: step: 5462, loss: 0.59185791015625, acc: 0.77, recall: 0.77, precision: 0.7967032967032968, f_beta: 0.7647058823529411
train: step: 5463, loss: 0.5704121589660645, acc: 0.74, recall: 0.74, precision: 0.7757352941176471, f_beta: 0.7312939231087227
train: step: 5464, loss: 0.5609237551689148, acc: 0.77, recall: 0.77, precision: 0.7895752895752897, f_beta: 0.7660461804495982
train: step: 5465, loss: 0.5855088233947754, acc: 0.79, recall: 0.79, precision: 0.8186813186813187, f_beta: 0.7851662404092071
train: step: 5466, loss: 0.5568418502807617, acc: 0.82, recall: 0.8200000000000001, precision: 0.822061191626409, f_beta: 0.8197115384615385
train: step: 5467, loss: 0.6162981986999512, acc: 0.76, recall: 0.76, precision: 0.7708333333333333, f_beta: 0.7575757575757576
train: step: 5468, loss: 0.5442959666252136, acc: 0.81, recall: 0.81, precision: 0.8203803224472923, f_beta: 0.8084484323016432
train: step: 5469, loss: 0.544653058052063, acc: 0.77, recall: 0.77, precision: 0.7967032967032968, f_beta: 0.7647058823529411
train: step: 5470, loss: 0.5363619327545166, acc: 0.8, recall: 0.8, precision: 0.8183361629881154, f_beta: 0.797077922077922
train: step: 5471, loss: 0.5967805981636047, acc: 0.78, recall: 0.78, precision: 0.7873563218390804, f_beta: 0.7785829307568439
train: step: 5472, loss: 0.6078438758850098, acc: 0.68, recall: 0.6799999999999999, precision: 0.7068014705882353, f_beta: 0.6692848284415047
train: step: 5473, loss: 0.531685471534729, acc: 0.82, recall: 0.82, precision: 0.8395585738539898, f_beta: 0.8173701298701298
train: step: 5474, loss: 0.6141676902770996, acc: 0.7, recall: 0.7, precision: 0.7228163992869875, f_beta: 0.6921182266009853
train: step: 5475, loss: 0.5982707738876343, acc: 0.8, recall: 0.8, precision: 0.8446691176470589, f_beta: 0.7933030177759404
train: step: 5476, loss: 0.41826340556144714, acc: 0.91, recall: 0.9099999999999999, precision: 0.9141414141414141, f_beta: 0.9097744360902256
train: step: 5477, loss: 0.579113781452179, acc: 0.77, recall: 0.77, precision: 0.805291723202171, f_beta: 0.7631551848419318
train: step: 5478, loss: 0.5530219674110413, acc: 0.82, recall: 0.82, precision: 0.8395585738539898, f_beta: 0.8173701298701298
train: step: 5479, loss: 0.5607345700263977, acc: 0.81, recall: 0.81, precision: 0.810124049619848, f_beta: 0.8099809980998101
train: step: 5480, loss: 0.5450757741928101, acc: 0.82, recall: 0.8200000000000001, precision: 0.8284072249589491, f_beta: 0.818840579710145
train: step: 5481, loss: 0.5697512030601501, acc: 0.81, recall: 0.81, precision: 0.8131313131313131, f_beta: 0.8095238095238095
train: step: 5482, loss: 0.5548051595687866, acc: 0.8, recall: 0.8, precision: 0.8043831168831169, f_beta: 0.7992773986350863
train: step: 5483, loss: 0.5660287737846375, acc: 0.75, recall: 0.75, precision: 0.7525252525252526, f_beta: 0.7493734335839599
train: step: 5484, loss: 0.5526185631752014, acc: 0.77, recall: 0.77, precision: 0.7709755118426336, f_beta: 0.769792813532179
train: step: 5485, loss: 0.41417914628982544, acc: 0.88, recall: 0.88, precision: 0.8899835796387521, f_beta: 0.8792270531400966
train: step: 5486, loss: 0.4872004985809326, acc: 0.83, recall: 0.8300000000000001, precision: 0.8333333333333334, f_beta: 0.8295739348370927
train: step: 5487, loss: 0.6502735018730164, acc: 0.74, recall: 0.74, precision: 0.7403846153846154, f_beta: 0.7398959583833533
train: step: 5488, loss: 0.5841884016990662, acc: 0.73, recall: 0.73, precision: 0.7345981232150143, f_beta: 0.7286704853783539
train: step: 5489, loss: 0.5431413054466248, acc: 0.8, recall: 0.8, precision: 0.8004807692307692, f_beta: 0.7999199679871949
train: step: 5490, loss: 0.5851395130157471, acc: 0.73, recall: 0.73, precision: 0.7527472527472527, f_beta: 0.7237851662404092
train: step: 5491, loss: 0.5305471420288086, acc: 0.83, recall: 0.8300000000000001, precision: 0.8365973072215422, f_beta: 0.8291628982011858
train: step: 5492, loss: 0.5990747809410095, acc: 0.72, recall: 0.72, precision: 0.7232142857142857, f_beta: 0.7189883580891208
train: step: 5493, loss: 0.5271826386451721, acc: 0.78, recall: 0.78, precision: 0.7818035426731079, f_beta: 0.7796474358974359
train: step: 5494, loss: 0.5594229698181152, acc: 0.79, recall: 0.79, precision: 0.7997106242248864, f_beta: 0.7882851093860268
train: step: 5495, loss: 0.5986831784248352, acc: 0.74, recall: 0.74, precision: 0.7463054187192117, f_beta: 0.7383252818035426
train: step: 5496, loss: 0.5694823265075684, acc: 0.8, recall: 0.8, precision: 0.8004807692307692, f_beta: 0.7999199679871949
train: step: 5497, loss: 0.5018400549888611, acc: 0.81, recall: 0.81, precision: 0.8406593406593407, f_beta: 0.8056265984654732
train: step: 5498, loss: 0.591238796710968, acc: 0.76, recall: 0.76, precision: 0.7896613190730838, f_beta: 0.7536945812807881
train: step: 5499, loss: 0.6204833984375, acc: 0.77, recall: 0.77, precision: 0.7753977968176254, f_beta: 0.7688674505074867
train: step: 5500, loss: 0.530148983001709, acc: 0.8, recall: 0.8, precision: 0.8183361629881154, f_beta: 0.797077922077922
train: step: 5501, loss: 0.6068372130393982, acc: 0.76, recall: 0.76, precision: 0.7668308702791462, f_beta: 0.7584541062801933
train: step: 5502, loss: 0.6097156405448914, acc: 0.79, recall: 0.79, precision: 0.8047498949138294, f_beta: 0.7874278773155178
train: step: 5503, loss: 0.6026695370674133, acc: 0.76, recall: 0.76, precision: 0.7616747181964574, f_beta: 0.7596153846153846
train: step: 5504, loss: 0.42828741669654846, acc: 0.88, recall: 0.88, precision: 0.8824476650563606, f_beta: 0.8798076923076923
train: step: 5505, loss: 0.5478107929229736, acc: 0.78, recall: 0.78, precision: 0.7873563218390804, f_beta: 0.7785829307568439
train: step: 5506, loss: 0.5746380090713501, acc: 0.75, recall: 0.75, precision: 0.7826775214834916, f_beta: 0.7425599835238389
train: step: 5507, loss: 0.5734591484069824, acc: 0.77, recall: 0.77, precision: 0.7895752895752897, f_beta: 0.7660461804495982
train: step: 5508, loss: 0.5738347768783569, acc: 0.81, recall: 0.81, precision: 0.8111200321156162, f_beta: 0.8098288459613652
train: step: 5509, loss: 0.5796552896499634, acc: 0.73, recall: 0.73, precision: 0.7416981925178646, f_beta: 0.7266929851199515
train: step: 5510, loss: 0.5175434350967407, acc: 0.84, recall: 0.84, precision: 0.8405448717948718, f_beta: 0.8399359743897559
train: step: 5511, loss: 0.6543232202529907, acc: 0.7, recall: 0.7, precision: 0.7003205128205128, f_beta: 0.6998799519807923
train: step: 5512, loss: 0.5318874716758728, acc: 0.85, recall: 0.85, precision: 0.867801597309794, f_beta: 0.848162769511084
train: step: 5513, loss: 0.6033859848976135, acc: 0.76, recall: 0.76, precision: 0.7637987012987013, f_beta: 0.7591328783621035
train: step: 5514, loss: 0.5071660876274109, acc: 0.83, recall: 0.8300000000000001, precision: 0.8365973072215422, f_beta: 0.8291628982011858
train: step: 5515, loss: 0.4946168065071106, acc: 0.85, recall: 0.8500000000000001, precision: 0.8535353535353536, f_beta: 0.849624060150376
train: step: 5516, loss: 0.614371657371521, acc: 0.67, recall: 0.6699999999999999, precision: 0.6756924348904505, f_beta: 0.667305171892328
train: step: 5517, loss: 0.6310194134712219, acc: 0.68, recall: 0.6799999999999999, precision: 0.6826298701298701, f_beta: 0.6788438378161381
train: step: 5518, loss: 0.596345067024231, acc: 0.73, recall: 0.73, precision: 0.7345981232150143, f_beta: 0.7286704853783539
train: step: 5519, loss: 0.5685114860534668, acc: 0.8, recall: 0.8, precision: 0.8004807692307692, f_beta: 0.7999199679871949
train: step: 5520, loss: 0.6219006776809692, acc: 0.75, recall: 0.75, precision: 0.7747252747252746, f_beta: 0.7442455242966751
train: step: 5521, loss: 0.5929569005966187, acc: 0.74, recall: 0.74, precision: 0.7757352941176471, f_beta: 0.7312939231087227
train: step: 5522, loss: 0.5911552906036377, acc: 0.78, recall: 0.78, precision: 0.7916666666666666, f_beta: 0.7777777777777779
train: step: 5523, loss: 0.647423267364502, acc: 0.67, recall: 0.6699999999999999, precision: 0.7561784207353828, f_beta: 0.6396986570586309
train: step: 5524, loss: 0.5607138872146606, acc: 0.83, recall: 0.8300000000000001, precision: 0.8333333333333334, f_beta: 0.8295739348370927
train: step: 5525, loss: 0.5561962723731995, acc: 0.77, recall: 0.77, precision: 0.7967032967032968, f_beta: 0.7647058823529411
train: step: 5526, loss: 0.5466578602790833, acc: 0.83, recall: 0.8300000000000001, precision: 0.8365973072215422, f_beta: 0.8291628982011858
train: step: 5527, loss: 0.5976590514183044, acc: 0.74, recall: 0.74, precision: 0.7415458937198067, f_beta: 0.7395833333333335
train: step: 5528, loss: 0.5848264098167419, acc: 0.76, recall: 0.76, precision: 0.7821180555555556, f_beta: 0.755201958384333
train: step: 5529, loss: 0.4866501986980438, acc: 0.81, recall: 0.81, precision: 0.8131313131313131, f_beta: 0.8095238095238095
train: step: 5530, loss: 0.5425439476966858, acc: 0.78, recall: 0.78, precision: 0.7818035426731079, f_beta: 0.7796474358974359
train: step: 5531, loss: 0.5944815278053284, acc: 0.75, recall: 0.75, precision: 0.7627154266498528, f_beta: 0.7469379491851402
train: step: 5532, loss: 0.5898501873016357, acc: 0.72, recall: 0.72, precision: 0.7527573529411764, f_beta: 0.7106242248863166
train: step: 5533, loss: 0.47321537137031555, acc: 0.84, recall: 0.84, precision: 0.8405448717948718, f_beta: 0.8399359743897559
train: step: 5534, loss: 0.5163103342056274, acc: 0.82, recall: 0.8200000000000001, precision: 0.8284072249589491, f_beta: 0.818840579710145
train: step: 5535, loss: 0.5616014003753662, acc: 0.78, recall: 0.78, precision: 0.7873563218390804, f_beta: 0.7785829307568439
train: step: 5536, loss: 0.4682571291923523, acc: 0.86, recall: 0.86, precision: 0.8623188405797102, f_beta: 0.859775641025641
train: step: 5537, loss: 0.596435010433197, acc: 0.73, recall: 0.73, precision: 0.7308309915696507, f_beta: 0.7297567811029926
train: step: 5538, loss: 0.5548499822616577, acc: 0.76, recall: 0.76, precision: 0.7604166666666667, f_beta: 0.7599039615846339
train: step: 5539, loss: 0.5365333557128906, acc: 0.79, recall: 0.79, precision: 0.7957976336189311, f_beta: 0.7889659330720531
train: step: 5540, loss: 0.5832768082618713, acc: 0.76, recall: 0.76, precision: 0.7616747181964574, f_beta: 0.7596153846153846
train: step: 5541, loss: 0.6045046448707581, acc: 0.81, recall: 0.81, precision: 0.810124049619848, f_beta: 0.8099809980998101
train: step: 5542, loss: 0.605210542678833, acc: 0.71, recall: 0.71, precision: 0.7100840336134454, f_beta: 0.7099709970997101
train: step: 5543, loss: 0.510247528553009, acc: 0.82, recall: 0.8200000000000001, precision: 0.822061191626409, f_beta: 0.8197115384615385
train: step: 5544, loss: 0.5236788988113403, acc: 0.78, recall: 0.78, precision: 0.7873563218390804, f_beta: 0.7785829307568439
train: step: 5545, loss: 0.6571853160858154, acc: 0.63, recall: 0.63, precision: 0.6366120218579234, f_beta: 0.6254681647940075
train: step: 5546, loss: 0.5334184765815735, acc: 0.8, recall: 0.8, precision: 0.8078817733990147, f_beta: 0.7987117552334944
train: step: 5547, loss: 0.5546571612358093, acc: 0.79, recall: 0.79, precision: 0.8047498949138294, f_beta: 0.7874278773155178
train: step: 5548, loss: 0.5468155145645142, acc: 0.81, recall: 0.81, precision: 0.8111200321156162, f_beta: 0.8098288459613652
train: step: 5549, loss: 0.5564334392547607, acc: 0.84, recall: 0.8400000000000001, precision: 0.8489326765188834, f_beta: 0.8389694041867956
train: step: 5550, loss: 0.5516818165779114, acc: 0.77, recall: 0.77, precision: 0.7727272727272727, f_beta: 0.7694235588972431
train: step: 5551, loss: 0.5435431003570557, acc: 0.79, recall: 0.79, precision: 0.8279059249208502, f_beta: 0.7837503861600247
train: step: 5552, loss: 0.5407230854034424, acc: 0.8, recall: 0.8, precision: 0.8043831168831169, f_beta: 0.7992773986350863
train: step: 5553, loss: 0.561517596244812, acc: 0.76, recall: 0.76, precision: 0.7668308702791462, f_beta: 0.7584541062801933
train: step: 5554, loss: 0.5457385778427124, acc: 0.76, recall: 0.76, precision: 0.7708333333333333, f_beta: 0.7575757575757576
train: step: 5555, loss: 0.5497139692306519, acc: 0.78, recall: 0.78, precision: 0.7971137521222411, f_beta: 0.7767857142857144
train: step: 5556, loss: 0.5168337225914001, acc: 0.83, recall: 0.8300000000000001, precision: 0.8311922922521076, f_beta: 0.8298468621759585
train: step: 5557, loss: 0.47910651564598083, acc: 0.83, recall: 0.8300000000000001, precision: 0.8333333333333334, f_beta: 0.8295739348370927
train: step: 5558, loss: 0.6131633520126343, acc: 0.75, recall: 0.75, precision: 0.7627154266498528, f_beta: 0.7469379491851402
train: step: 5559, loss: 0.5924025774002075, acc: 0.72, recall: 0.72, precision: 0.7257799671592775, f_beta: 0.7181964573268922
train: step: 5560, loss: 0.6097837090492249, acc: 0.71, recall: 0.71, precision: 0.7454417952314165, f_beta: 0.6991389148251893
train: step: 5561, loss: 0.6132062077522278, acc: 0.74, recall: 0.74, precision: 0.7463054187192117, f_beta: 0.7383252818035426
train: step: 5562, loss: 0.6142743229866028, acc: 0.76, recall: 0.76, precision: 0.7616747181964574, f_beta: 0.7596153846153846
train: step: 5563, loss: 0.5769204497337341, acc: 0.77, recall: 0.77, precision: 0.7753977968176254, f_beta: 0.7688674505074867
train: step: 5564, loss: 0.5020076632499695, acc: 0.82, recall: 0.8200000000000001, precision: 0.8246753246753247, f_beta: 0.8193496587715776
train: step: 5565, loss: 0.5746318697929382, acc: 0.76, recall: 0.76, precision: 0.7637987012987013, f_beta: 0.7591328783621035
train: step: 5566, loss: 0.5547176599502563, acc: 0.78, recall: 0.78, precision: 0.7873563218390804, f_beta: 0.7785829307568439
train: step: 5567, loss: 0.5896729826927185, acc: 0.77, recall: 0.77, precision: 0.7837326607818411, f_beta: 0.767182913250329
train: step: 5568, loss: 0.5149027109146118, acc: 0.81, recall: 0.81, precision: 0.8111200321156162, f_beta: 0.8098288459613652
train: step: 5569, loss: 0.5382074117660522, acc: 0.82, recall: 0.82, precision: 0.82, f_beta: 0.82
train: step: 5570, loss: 0.5629163384437561, acc: 0.78, recall: 0.78, precision: 0.7916666666666666, f_beta: 0.7777777777777779
train: step: 5571, loss: 0.5669471025466919, acc: 0.76, recall: 0.76, precision: 0.7604166666666667, f_beta: 0.7599039615846339
train: step: 5572, loss: 0.44934412837028503, acc: 0.87, recall: 0.87, precision: 0.8713368125250903, f_beta: 0.8698828946051447
train: step: 5573, loss: 0.5842952728271484, acc: 0.77, recall: 0.77, precision: 0.7895752895752897, f_beta: 0.7660461804495982
train: step: 5574, loss: 0.5602930188179016, acc: 0.81, recall: 0.81, precision: 0.8111200321156162, f_beta: 0.8098288459613652
train: step: 5575, loss: 0.5828830599784851, acc: 0.75, recall: 0.75, precision: 0.750903251706142, f_beta: 0.7497747973175859
train: step: 5576, loss: 0.4480971097946167, acc: 0.9, recall: 0.9, precision: 0.9006410256410255, f_beta: 0.8999599839935974
train: step: 5577, loss: 0.5343655943870544, acc: 0.79, recall: 0.79, precision: 0.7997106242248864, f_beta: 0.7882851093860268
train: step: 5578, loss: 0.6268692016601562, acc: 0.7, recall: 0.7, precision: 0.7297794117647058, f_beta: 0.6899545266639107
train: step: 5579, loss: 0.6009333729743958, acc: 0.71, recall: 0.71, precision: 0.7100840336134454, f_beta: 0.7099709970997101
train: step: 5580, loss: 0.5361723899841309, acc: 0.82, recall: 0.8200000000000001, precision: 0.822061191626409, f_beta: 0.8197115384615385
train: step: 5581, loss: 0.574466347694397, acc: 0.82, recall: 0.8200000000000001, precision: 0.8205128205128205, f_beta: 0.8199279711884755
train: step: 5582, loss: 0.5227656960487366, acc: 0.84, recall: 0.84, precision: 0.8405448717948718, f_beta: 0.8399359743897559
train: step: 5583, loss: 0.5303710699081421, acc: 0.82, recall: 0.8200000000000001, precision: 0.8205128205128205, f_beta: 0.8199279711884755
train: step: 5584, loss: 0.569832444190979, acc: 0.81, recall: 0.81, precision: 0.810124049619848, f_beta: 0.8099809980998101
train: step: 5585, loss: 0.60053551197052, acc: 0.71, recall: 0.71, precision: 0.7170318313352625, f_beta: 0.7076318177235609
train: step: 5586, loss: 0.6015363931655884, acc: 0.75, recall: 0.75, precision: 0.7549979600163199, f_beta: 0.7487689679429204
train: step: 5587, loss: 0.5109624266624451, acc: 0.8, recall: 0.8, precision: 0.8004807692307692, f_beta: 0.7999199679871949
train: step: 5588, loss: 0.5600885152816772, acc: 0.77, recall: 0.77, precision: 0.7753977968176254, f_beta: 0.7688674505074867
train: step: 5589, loss: 0.5000268816947937, acc: 0.83, recall: 0.83, precision: 0.8301320528211285, f_beta: 0.8299829982998299
train: step: 5590, loss: 0.5103814005851746, acc: 0.9, recall: 0.8999999999999999, precision: 0.9025764895330113, f_beta: 0.8998397435897436
train: step: 5591, loss: 0.5024102330207825, acc: 0.83, recall: 0.83, precision: 0.8301320528211285, f_beta: 0.8299829982998299
train: step: 5592, loss: 0.5577592849731445, acc: 0.76, recall: 0.76, precision: 0.7616747181964574, f_beta: 0.7596153846153846
train: step: 5593, loss: 0.5284532308578491, acc: 0.86, recall: 0.86, precision: 0.8605769230769231, f_beta: 0.8599439775910365
train: step: 5594, loss: 0.5728115439414978, acc: 0.74, recall: 0.74, precision: 0.7463054187192117, f_beta: 0.7383252818035426
train: step: 5595, loss: 0.530575156211853, acc: 0.86, recall: 0.86, precision: 0.8694581280788177, f_beta: 0.8590982286634461
train: step: 5596, loss: 0.5394737720489502, acc: 0.8, recall: 0.8, precision: 0.8043831168831169, f_beta: 0.7992773986350863
train: step: 5597, loss: 0.5584571361541748, acc: 0.76, recall: 0.76, precision: 0.7708333333333333, f_beta: 0.7575757575757576
train: step: 5598, loss: 0.5133844614028931, acc: 0.84, recall: 0.8400000000000001, precision: 0.8421900161030595, f_beta: 0.8397435897435896
train: step: 5599, loss: 0.582059383392334, acc: 0.76, recall: 0.76, precision: 0.7604166666666667, f_beta: 0.7599039615846339
train: step: 5600, loss: 0.5237177610397339, acc: 0.87, recall: 0.87, precision: 0.8737373737373737, f_beta: 0.8696741854636592
train: step: 5601, loss: 0.5765163898468018, acc: 0.76, recall: 0.76, precision: 0.7708333333333333, f_beta: 0.7575757575757576
train: step: 5602, loss: 0.5545629262924194, acc: 0.77, recall: 0.77, precision: 0.7837326607818411, f_beta: 0.767182913250329
train: step: 5603, loss: 0.47506004571914673, acc: 0.87, recall: 0.87, precision: 0.8701480592236894, f_beta: 0.8699869986998701
train: step: 5604, loss: 0.5550807118415833, acc: 0.82, recall: 0.8200000000000001, precision: 0.822061191626409, f_beta: 0.8197115384615385
train: step: 5605, loss: 0.5365378856658936, acc: 0.83, recall: 0.8300000000000001, precision: 0.8410500206696983, f_beta: 0.8286117552172598
train: step: 5606, loss: 0.5862449407577515, acc: 0.77, recall: 0.77, precision: 0.7753977968176254, f_beta: 0.7688674505074867
train: step: 5607, loss: 0.4940700829029083, acc: 0.82, recall: 0.8200000000000001, precision: 0.8333333333333334, f_beta: 0.8181818181818181
train: step: 5608, loss: 0.5662329196929932, acc: 0.78, recall: 0.78, precision: 0.7818035426731079, f_beta: 0.7796474358974359
train: step: 5609, loss: 0.5480233430862427, acc: 0.78, recall: 0.78, precision: 0.78, f_beta: 0.78
train: step: 5610, loss: 0.6183527112007141, acc: 0.69, recall: 0.69, precision: 0.6919191919191919, f_beta: 0.6892230576441103
train: step: 5611, loss: 0.5254372358322144, acc: 0.85, recall: 0.85, precision: 0.8501400560224089, f_beta: 0.84998499849985
train: step: 5612, loss: 0.5209699869155884, acc: 0.82, recall: 0.82, precision: 0.8395585738539898, f_beta: 0.8173701298701298
train: step: 5613, loss: 0.6218015551567078, acc: 0.73, recall: 0.73, precision: 0.7416981925178646, f_beta: 0.7266929851199515
train: step: 5614, loss: 0.5605586767196655, acc: 0.75, recall: 0.75, precision: 0.7681252681252682, f_beta: 0.7457023700539112
train: step: 5615, loss: 0.510129988193512, acc: 0.83, recall: 0.83, precision: 0.8301320528211285, f_beta: 0.8299829982998299
train: step: 5616, loss: 0.5103287696838379, acc: 0.8, recall: 0.8, precision: 0.8043831168831169, f_beta: 0.7992773986350863
train: step: 5617, loss: 0.5283777117729187, acc: 0.83, recall: 0.8300000000000001, precision: 0.8333333333333334, f_beta: 0.8295739348370927
train: step: 5618, loss: 0.46573105454444885, acc: 0.86, recall: 0.86, precision: 0.8623188405797102, f_beta: 0.859775641025641
train: step: 5619, loss: 0.442164808511734, acc: 0.87, recall: 0.87, precision: 0.8737373737373737, f_beta: 0.8696741854636592
train: step: 5620, loss: 0.48378920555114746, acc: 0.83, recall: 0.8300000000000001, precision: 0.8410500206696983, f_beta: 0.8286117552172598
train: step: 5621, loss: 0.6831933856010437, acc: 0.67, recall: 0.67, precision: 0.7266666666666666, f_beta: 0.648
train: step: 5622, loss: 0.5592391490936279, acc: 0.79, recall: 0.79, precision: 0.8186813186813187, f_beta: 0.7851662404092071
train: step: 5623, loss: 0.5449129343032837, acc: 0.79, recall: 0.79, precision: 0.7997106242248864, f_beta: 0.7882851093860268
train: step: 5624, loss: 0.6570145487785339, acc: 0.74, recall: 0.74, precision: 0.7415458937198067, f_beta: 0.7395833333333335
train: step: 5625, loss: 0.5474730730056763, acc: 0.78, recall: 0.78, precision: 0.8038194444444444, f_beta: 0.7756017951856385
train: step: 5626, loss: 0.5645058751106262, acc: 0.76, recall: 0.76, precision: 0.7987132352941176, f_beta: 0.7519636213311285
train: step: 5627, loss: 0.5905426144599915, acc: 0.74, recall: 0.74, precision: 0.7463054187192117, f_beta: 0.7383252818035426
train: step: 5628, loss: 0.5284916162490845, acc: 0.82, recall: 0.8200000000000001, precision: 0.822061191626409, f_beta: 0.8197115384615385
train: step: 5629, loss: 0.5387969017028809, acc: 0.85, recall: 0.85, precision: 0.851264552388599, f_beta: 0.8498648783905516
train: step: 5630, loss: 0.5827864408493042, acc: 0.74, recall: 0.74, precision: 0.7463054187192117, f_beta: 0.7383252818035426
train: step: 5631, loss: 0.5554056167602539, acc: 0.78, recall: 0.78, precision: 0.7873563218390804, f_beta: 0.7785829307568439
train: step: 5632, loss: 0.5690094232559204, acc: 0.81, recall: 0.81, precision: 0.8131313131313131, f_beta: 0.8095238095238095
train: step: 5633, loss: 0.620944619178772, acc: 0.76, recall: 0.76, precision: 0.7987132352941176, f_beta: 0.7519636213311285
train: step: 5634, loss: 0.5695223808288574, acc: 0.84, recall: 0.84, precision: 0.8541666666666667, f_beta: 0.8383838383838385
train: step: 5635, loss: 0.5396886467933655, acc: 0.78, recall: 0.78, precision: 0.78, f_beta: 0.78
train: step: 5636, loss: 0.6194478869438171, acc: 0.74, recall: 0.74, precision: 0.7604166666666667, f_beta: 0.7348021215830274
train: step: 5637, loss: 0.5785559415817261, acc: 0.81, recall: 0.81, precision: 0.8161974704202366, f_beta: 0.8090644156366193
train: step: 5638, loss: 0.5936989784240723, acc: 0.84, recall: 0.8400000000000001, precision: 0.8449675324675325, f_beta: 0.839421918908069
train: step: 5639, loss: 0.46139100193977356, acc: 0.86, recall: 0.86, precision: 0.8605769230769231, f_beta: 0.8599439775910365
train: step: 5640, loss: 0.5762268304824829, acc: 0.75, recall: 0.75, precision: 0.792192613370734, f_beta: 0.7406369955389562
train: step: 5641, loss: 0.5494213700294495, acc: 0.81, recall: 0.81, precision: 0.8111200321156162, f_beta: 0.8098288459613652
train: step: 5642, loss: 0.614097535610199, acc: 0.71, recall: 0.71, precision: 0.7141982864137086, f_beta: 0.7085720028137876
train: step: 5643, loss: 0.5709893703460693, acc: 0.77, recall: 0.77, precision: 0.7967032967032968, f_beta: 0.7647058823529411
train: step: 5644, loss: 0.4586637616157532, acc: 0.85, recall: 0.85, precision: 0.8501400560224089, f_beta: 0.84998499849985
train: step: 5645, loss: 0.5195378661155701, acc: 0.84, recall: 0.8400000000000001, precision: 0.8449675324675325, f_beta: 0.839421918908069
train: step: 5646, loss: 0.6585853695869446, acc: 0.82, recall: 0.8200000000000001, precision: 0.8472222222222222, f_beta: 0.8164014687882497
train: step: 5647, loss: 0.526382565498352, acc: 0.86, recall: 0.86, precision: 0.86, f_beta: 0.8599999999999999
train: step: 5648, loss: 0.5616187453269958, acc: 0.9, recall: 0.8999999999999999, precision: 0.9105090311986863, f_beta: 0.8993558776167472
train: step: 5649, loss: 0.6134597659111023, acc: 0.73, recall: 0.73, precision: 0.7300920368147259, f_beta: 0.72997299729973
train: step: 5650, loss: 0.4767875373363495, acc: 0.83, recall: 0.83, precision: 0.8301320528211285, f_beta: 0.8299829982998299
train: step: 5651, loss: 0.5754420757293701, acc: 0.79, recall: 0.79, precision: 0.7901160464185675, f_beta: 0.78997899789979
train: step: 5652, loss: 0.6108759045600891, acc: 0.76, recall: 0.76, precision: 0.7604166666666667, f_beta: 0.7599039615846339
train: step: 5653, loss: 0.6325509548187256, acc: 0.71, recall: 0.71, precision: 0.7141982864137086, f_beta: 0.7085720028137876
train: step: 5654, loss: 0.5407915115356445, acc: 0.76, recall: 0.76, precision: 0.7758913412563667, f_beta: 0.7564935064935063
train: step: 5655, loss: 0.527584433555603, acc: 0.84, recall: 0.84, precision: 0.8405448717948718, f_beta: 0.8399359743897559
train: step: 5656, loss: 0.5756077766418457, acc: 0.77, recall: 0.77, precision: 0.7727272727272727, f_beta: 0.7694235588972431
train: step: 5657, loss: 0.47188079357147217, acc: 0.87, recall: 0.87, precision: 0.8701480592236894, f_beta: 0.8699869986998701
train: step: 5658, loss: 0.5728749632835388, acc: 0.79, recall: 0.79, precision: 0.8047498949138294, f_beta: 0.7874278773155178
train: step: 5659, loss: 0.612357497215271, acc: 0.69, recall: 0.69, precision: 0.7148349163274537, f_beta: 0.6807743795695602
train: step: 5660, loss: 0.5666562914848328, acc: 0.79, recall: 0.79, precision: 0.7901160464185675, f_beta: 0.78997899789979
train: step: 5661, loss: 0.5659703612327576, acc: 0.8, recall: 0.8, precision: 0.8255208333333333, f_beta: 0.7960016319869441
train: step: 5662, loss: 0.5811035633087158, acc: 0.78, recall: 0.78, precision: 0.7840909090909092, f_beta: 0.779205138498595
train: step: 5663, loss: 0.6153442859649658, acc: 0.71, recall: 0.71, precision: 0.7121212121212122, f_beta: 0.7092731829573935
train: step: 5664, loss: 0.5407922863960266, acc: 0.8, recall: 0.8, precision: 0.8004807692307692, f_beta: 0.7999199679871949
train: step: 5665, loss: 0.5568631291389465, acc: 0.78, recall: 0.78, precision: 0.7840909090909092, f_beta: 0.779205138498595
train: step: 5666, loss: 0.5147041082382202, acc: 0.84, recall: 0.84, precision: 0.8405448717948718, f_beta: 0.8399359743897559
train: step: 5667, loss: 0.5453052520751953, acc: 0.81, recall: 0.81, precision: 0.8406593406593407, f_beta: 0.8056265984654732
train: step: 5668, loss: 0.6419844031333923, acc: 0.71, recall: 0.71, precision: 0.7121212121212122, f_beta: 0.7092731829573935
train: step: 5669, loss: 0.4882308542728424, acc: 0.83, recall: 0.83, precision: 0.8301320528211285, f_beta: 0.8299829982998299
train: step: 5670, loss: 0.5089313387870789, acc: 0.83, recall: 0.83, precision: 0.8301320528211285, f_beta: 0.8299829982998299
train: step: 5671, loss: 0.4984404444694519, acc: 0.83, recall: 0.83, precision: 0.8301320528211285, f_beta: 0.8299829982998299
train: step: 5672, loss: 0.5424423813819885, acc: 0.79, recall: 0.79, precision: 0.7957976336189311, f_beta: 0.7889659330720531
train: step: 5673, loss: 0.5594526529312134, acc: 0.76, recall: 0.76, precision: 0.7616747181964574, f_beta: 0.7596153846153846
train: step: 5674, loss: 0.5198506712913513, acc: 0.8, recall: 0.8, precision: 0.8078817733990147, f_beta: 0.7987117552334944
train: step: 5675, loss: 0.5058135390281677, acc: 0.8, recall: 0.8, precision: 0.8125, f_beta: 0.797979797979798
train: step: 5676, loss: 0.46025997400283813, acc: 0.91, recall: 0.91, precision: 0.9101640656262505, f_beta: 0.90999099909991
train: step: 5677, loss: 0.5733280181884766, acc: 0.74, recall: 0.74, precision: 0.7415458937198067, f_beta: 0.7395833333333335
train: step: 5678, loss: 0.5771724581718445, acc: 0.81, recall: 0.81, precision: 0.8111200321156162, f_beta: 0.8098288459613652
train: step: 5679, loss: 0.5655913949012756, acc: 0.83, recall: 0.8300000000000001, precision: 0.8365973072215422, f_beta: 0.8291628982011858
train: step: 5680, loss: 0.6321761608123779, acc: 0.79, recall: 0.79, precision: 0.7997106242248864, f_beta: 0.7882851093860268
train: step: 5681, loss: 0.5754789113998413, acc: 0.81, recall: 0.81, precision: 0.8111200321156162, f_beta: 0.8098288459613652
train: step: 5682, loss: 0.6148908734321594, acc: 0.71, recall: 0.71, precision: 0.7100840336134454, f_beta: 0.7099709970997101
train: step: 5683, loss: 0.5549890398979187, acc: 0.76, recall: 0.76, precision: 0.7604166666666667, f_beta: 0.7599039615846339
train: step: 5684, loss: 0.5364390015602112, acc: 0.81, recall: 0.81, precision: 0.8111200321156162, f_beta: 0.8098288459613652
train: step: 5685, loss: 0.5488772392272949, acc: 0.78, recall: 0.78, precision: 0.7916666666666666, f_beta: 0.7777777777777779
train: step: 5686, loss: 0.4723314046859741, acc: 0.87, recall: 0.87, precision: 0.8737373737373737, f_beta: 0.8696741854636592
train: step: 5687, loss: 0.5221470594406128, acc: 0.83, recall: 0.8300000000000001, precision: 0.8311922922521076, f_beta: 0.8298468621759585
train: step: 5688, loss: 0.543499767780304, acc: 0.8, recall: 0.8, precision: 0.8004807692307692, f_beta: 0.7999199679871949
train: step: 5689, loss: 0.560986340045929, acc: 0.8, recall: 0.8, precision: 0.8078817733990147, f_beta: 0.7987117552334944
train: step: 5690, loss: 0.6131799817085266, acc: 0.7, recall: 0.7, precision: 0.7003205128205128, f_beta: 0.6998799519807923
train: step: 5691, loss: 0.5270822048187256, acc: 0.85, recall: 0.85, precision: 0.851264552388599, f_beta: 0.8498648783905516
train: step: 5692, loss: 0.6279483437538147, acc: 0.72, recall: 0.72, precision: 0.733446519524618, f_beta: 0.7159090909090909
train: step: 5693, loss: 0.5853322744369507, acc: 0.82, recall: 0.8200000000000001, precision: 0.8246753246753247, f_beta: 0.8193496587715776
train: step: 5694, loss: 0.49167105555534363, acc: 0.87, recall: 0.87, precision: 0.8701480592236894, f_beta: 0.8699869986998701
train: step: 5695, loss: 0.5888009667396545, acc: 0.76, recall: 0.76, precision: 0.7604166666666667, f_beta: 0.7599039615846339
train: step: 5696, loss: 0.591655969619751, acc: 0.73, recall: 0.73, precision: 0.7300920368147259, f_beta: 0.72997299729973
train: step: 5697, loss: 0.5531704425811768, acc: 0.81, recall: 0.81, precision: 0.810124049619848, f_beta: 0.8099809980998101
train: step: 5698, loss: 0.5362090468406677, acc: 0.79, recall: 0.79, precision: 0.811025311025311, f_beta: 0.7863899908452854
train: step: 5699, loss: 0.4760891795158386, acc: 0.85, recall: 0.85, precision: 0.8501400560224089, f_beta: 0.84998499849985
train: step: 5700, loss: 0.5849229693412781, acc: 0.72, recall: 0.72, precision: 0.7257799671592775, f_beta: 0.7181964573268922
train: step: 5701, loss: 0.5916004776954651, acc: 0.73, recall: 0.73, precision: 0.7416981925178646, f_beta: 0.7266929851199515
train: step: 5702, loss: 0.6080856919288635, acc: 0.7, recall: 0.7, precision: 0.7083333333333333, f_beta: 0.6969696969696968
train: step: 5703, loss: 0.6010280847549438, acc: 0.69, recall: 0.6900000000000001, precision: 0.7409944190766109, f_beta: 0.6726850385386971
train: step: 5704, loss: 0.4789239168167114, acc: 0.85, recall: 0.85, precision: 0.8501400560224089, f_beta: 0.84998499849985
train: step: 5705, loss: 0.5362331867218018, acc: 0.77, recall: 0.77, precision: 0.7837326607818411, f_beta: 0.767182913250329
train: step: 5706, loss: 0.5830070376396179, acc: 0.76, recall: 0.76, precision: 0.7708333333333333, f_beta: 0.7575757575757576
train: step: 5707, loss: 0.5683801174163818, acc: 0.76, recall: 0.76, precision: 0.7637987012987013, f_beta: 0.7591328783621035
train: step: 5708, loss: 0.5831701755523682, acc: 0.85, recall: 0.85, precision: 0.851264552388599, f_beta: 0.8498648783905516
train: step: 5709, loss: 0.6388037204742432, acc: 0.69, recall: 0.69, precision: 0.6996637242538881, f_beta: 0.6862030569895738
train: step: 5710, loss: 0.5883415341377258, acc: 0.8, recall: 0.8, precision: 0.8078817733990147, f_beta: 0.7987117552334944
train: step: 5711, loss: 0.5698621869087219, acc: 0.81, recall: 0.81, precision: 0.8257671290458175, f_beta: 0.8076728413807065
train: step: 5712, loss: 0.6479322910308838, acc: 0.81, recall: 0.81, precision: 0.8257671290458175, f_beta: 0.8076728413807065
train: step: 5713, loss: 0.6423710584640503, acc: 0.75, recall: 0.75, precision: 0.7583712277800745, f_beta: 0.7479584635547939
train: step: 5714, loss: 0.6715202331542969, acc: 0.64, recall: 0.64, precision: 0.681912681912682, f_beta: 0.6179966044142614
train: step: 5715, loss: 0.5728005766868591, acc: 0.75, recall: 0.75, precision: 0.7501000400160064, f_beta: 0.7499749974997499
train: step: 5716, loss: 0.5707724094390869, acc: 0.76, recall: 0.76, precision: 0.7758913412563667, f_beta: 0.7564935064935063
train: step: 5717, loss: 0.6304299831390381, acc: 0.77, recall: 0.77, precision: 0.7709755118426336, f_beta: 0.769792813532179
train: step: 5718, loss: 0.5894957780838013, acc: 0.74, recall: 0.74, precision: 0.7546689303904923, f_beta: 0.7362012987012987
train: step: 5719, loss: 0.6012098789215088, acc: 0.72, recall: 0.72, precision: 0.7450980392156863, f_beta: 0.7126436781609196
train: step: 5720, loss: 0.6095465421676636, acc: 0.73, recall: 0.73, precision: 0.7323232323232323, f_beta: 0.7293233082706766
train: step: 5721, loss: 0.6178247332572937, acc: 0.69, recall: 0.69, precision: 0.7682100508187464, f_beta: 0.6656239887822242
train: step: 5722, loss: 0.5340837240219116, acc: 0.78, recall: 0.78, precision: 0.7840909090909092, f_beta: 0.779205138498595
train: step: 5723, loss: 0.5975011587142944, acc: 0.7, recall: 0.7, precision: 0.7297794117647058, f_beta: 0.6899545266639107
train: step: 5724, loss: 0.5572389364242554, acc: 0.84, recall: 0.84, precision: 0.8541666666666667, f_beta: 0.8383838383838385
train: step: 5725, loss: 0.6591646075248718, acc: 0.61, recall: 0.61, precision: 0.6948972360028349, f_beta: 0.5623386825272136
train: step: 5726, loss: 0.6177806258201599, acc: 0.69, recall: 0.69, precision: 0.690686471296668, f_beta: 0.6897207486738064
train: step: 5727, loss: 0.5789834856987, acc: 0.74, recall: 0.74, precision: 0.75, f_beta: 0.7373737373737373
train: step: 5728, loss: 0.6077188849449158, acc: 0.75, recall: 0.75, precision: 0.7525252525252526, f_beta: 0.7493734335839599
train: step: 5729, loss: 0.47249892354011536, acc: 0.82, recall: 0.8200000000000001, precision: 0.8284072249589491, f_beta: 0.818840579710145
train: step: 5730, loss: 0.653085470199585, acc: 0.76, recall: 0.76, precision: 0.7604166666666667, f_beta: 0.7599039615846339
train: step: 5731, loss: 0.5976824164390564, acc: 0.77, recall: 0.77, precision: 0.7837326607818411, f_beta: 0.767182913250329
train: step: 5732, loss: 0.5648040175437927, acc: 0.76, recall: 0.76, precision: 0.7758913412563667, f_beta: 0.7564935064935063
train: step: 5733, loss: 0.5456086993217468, acc: 0.8, recall: 0.8, precision: 0.8004807692307692, f_beta: 0.7999199679871949
train: step: 5734, loss: 0.547598123550415, acc: 0.83, recall: 0.8300000000000001, precision: 0.8365973072215422, f_beta: 0.8291628982011858
train: step: 5735, loss: 0.6209197640419006, acc: 0.69, recall: 0.69, precision: 0.7037752037752039, f_beta: 0.6846709388668497
train: step: 5736, loss: 0.668455183506012, acc: 0.71, recall: 0.71, precision: 0.7252252252252251, f_beta: 0.7050147492625368
train: step: 5737, loss: 0.6185850501060486, acc: 0.71, recall: 0.71, precision: 0.7206809583858764, f_beta: 0.7064480210547626
train: step: 5738, loss: 0.6208376288414001, acc: 0.73, recall: 0.73, precision: 0.7300920368147259, f_beta: 0.72997299729973
train: step: 5739, loss: 0.49342823028564453, acc: 0.86, recall: 0.86, precision: 0.8605769230769231, f_beta: 0.8599439775910365
train: step: 5740, loss: 0.520938515663147, acc: 0.84, recall: 0.84, precision: 0.84, f_beta: 0.8399999999999999
train: step: 5741, loss: 0.5523354411125183, acc: 0.78, recall: 0.78, precision: 0.7873563218390804, f_beta: 0.7785829307568439
train: step: 5742, loss: 0.5819438695907593, acc: 0.82, recall: 0.8200000000000001, precision: 0.8246753246753247, f_beta: 0.8193496587715776
train: step: 5743, loss: 0.510892391204834, acc: 0.85, recall: 0.85, precision: 0.867801597309794, f_beta: 0.848162769511084
train: step: 5744, loss: 0.6021667122840881, acc: 0.71, recall: 0.71, precision: 0.7307692307692308, f_beta: 0.7033248081841432
train: step: 5745, loss: 0.5312723517417908, acc: 0.79, recall: 0.79, precision: 0.7910477719791249, f_beta: 0.7898108297467722
train: step: 5746, loss: 0.6194371581077576, acc: 0.74, recall: 0.74, precision: 0.7757352941176471, f_beta: 0.7312939231087227
train: step: 5747, loss: 0.6024078130722046, acc: 0.78, recall: 0.78, precision: 0.7840909090909092, f_beta: 0.779205138498595
train: step: 5748, loss: 0.5959941744804382, acc: 0.74, recall: 0.74, precision: 0.7435064935064934, f_beta: 0.7390606182256123
train: step: 5749, loss: 0.5438541769981384, acc: 0.76, recall: 0.76, precision: 0.7637987012987013, f_beta: 0.7591328783621035
train: step: 5750, loss: 0.5529828071594238, acc: 0.76, recall: 0.76, precision: 0.7604166666666667, f_beta: 0.7599039615846339
train: step: 5751, loss: 0.6214340925216675, acc: 0.7, recall: 0.7, precision: 0.7003205128205128, f_beta: 0.6998799519807923
train: step: 5752, loss: 0.5397192239761353, acc: 0.79, recall: 0.79, precision: 0.7910477719791249, f_beta: 0.7898108297467722
train: step: 5753, loss: 0.5202763676643372, acc: 0.79, recall: 0.79, precision: 0.7910477719791249, f_beta: 0.7898108297467722
train: step: 5754, loss: 0.5309506058692932, acc: 0.82, recall: 0.8200000000000001, precision: 0.8284072249589491, f_beta: 0.818840579710145
train: step: 5755, loss: 0.5393373966217041, acc: 0.77, recall: 0.77, precision: 0.7727272727272727, f_beta: 0.7694235588972431
train: step: 5756, loss: 0.5708101987838745, acc: 0.75, recall: 0.75, precision: 0.7501000400160064, f_beta: 0.7499749974997499
train: step: 5757, loss: 0.5568172335624695, acc: 0.78, recall: 0.78, precision: 0.780448717948718, f_beta: 0.7799119647859143
train: step: 5758, loss: 0.5814302563667297, acc: 0.78, recall: 0.78, precision: 0.7971137521222411, f_beta: 0.7767857142857144
train: step: 5759, loss: 0.4986799955368042, acc: 0.87, recall: 0.87, precision: 0.8701480592236894, f_beta: 0.8699869986998701
train: step: 5760, loss: 0.48397067189216614, acc: 0.9, recall: 0.8999999999999999, precision: 0.9025764895330113, f_beta: 0.8998397435897436
train: step: 5761, loss: 0.5901100635528564, acc: 0.72, recall: 0.72, precision: 0.7291666666666667, f_beta: 0.7171717171717171
train: step: 5762, loss: 0.5311221480369568, acc: 0.81, recall: 0.81, precision: 0.8131313131313131, f_beta: 0.8095238095238095
train: step: 5763, loss: 0.5263350605964661, acc: 0.81, recall: 0.81, precision: 0.8161974704202366, f_beta: 0.8090644156366193
train: step: 5764, loss: 0.5292453169822693, acc: 0.8, recall: 0.8, precision: 0.8004807692307692, f_beta: 0.7999199679871949
train: step: 5765, loss: 0.6116350293159485, acc: 0.8, recall: 0.8, precision: 0.8004807692307692, f_beta: 0.7999199679871949
train: step: 5766, loss: 0.5708785057067871, acc: 0.78, recall: 0.78, precision: 0.7916666666666666, f_beta: 0.7777777777777779
train: step: 5767, loss: 0.5715481042861938, acc: 0.79, recall: 0.79, precision: 0.7901160464185675, f_beta: 0.78997899789979
train: step: 5768, loss: 0.5411429405212402, acc: 0.77, recall: 0.77, precision: 0.7790409260024803, f_beta: 0.7681217864704104
train: step: 5769, loss: 0.5566654205322266, acc: 0.81, recall: 0.81, precision: 0.8161974704202366, f_beta: 0.8090644156366193
train: step: 5770, loss: 0.543073832988739, acc: 0.81, recall: 0.81, precision: 0.8111200321156162, f_beta: 0.8098288459613652
train: step: 5771, loss: 0.5571485757827759, acc: 0.81, recall: 0.81, precision: 0.8161974704202366, f_beta: 0.8090644156366193
train: step: 5772, loss: 0.6456805467605591, acc: 0.77, recall: 0.77, precision: 0.7727272727272727, f_beta: 0.7694235588972431
train: step: 5773, loss: 0.43156155943870544, acc: 0.88, recall: 0.88, precision: 0.8824476650563606, f_beta: 0.8798076923076923
train: step: 5774, loss: 0.6158201098442078, acc: 0.75, recall: 0.75, precision: 0.7549979600163199, f_beta: 0.7487689679429204
train: step: 5775, loss: 0.46028560400009155, acc: 0.84, recall: 0.8400000000000001, precision: 0.8489326765188834, f_beta: 0.8389694041867956
train: step: 5776, loss: 0.6049116253852844, acc: 0.76, recall: 0.76, precision: 0.7604166666666667, f_beta: 0.7599039615846339
train: step: 5777, loss: 0.5198684930801392, acc: 0.81, recall: 0.81, precision: 0.8203803224472923, f_beta: 0.8084484323016432
train: step: 5778, loss: 0.5829278826713562, acc: 0.74, recall: 0.74, precision: 0.7435064935064934, f_beta: 0.7390606182256123
train: step: 5779, loss: 0.5359560251235962, acc: 0.82, recall: 0.82, precision: 0.82, f_beta: 0.82
train: step: 5780, loss: 0.5393507480621338, acc: 0.82, recall: 0.8200000000000001, precision: 0.822061191626409, f_beta: 0.8197115384615385
train: step: 5781, loss: 0.682532787322998, acc: 0.77, recall: 0.77, precision: 0.7727272727272727, f_beta: 0.7694235588972431
train: step: 5782, loss: 0.5674657225608826, acc: 0.74, recall: 0.74, precision: 0.7415458937198067, f_beta: 0.7395833333333335
train: step: 5783, loss: 0.5412675738334656, acc: 0.8, recall: 0.8, precision: 0.8004807692307692, f_beta: 0.7999199679871949
train: step: 5784, loss: 0.5672939419746399, acc: 0.75, recall: 0.75, precision: 0.7747252747252746, f_beta: 0.7442455242966751
train: step: 5785, loss: 0.5763123035430908, acc: 0.81, recall: 0.81, precision: 0.8324753324753325, f_beta: 0.8067338012409724
train: step: 5786, loss: 0.6019908785820007, acc: 0.71, recall: 0.71, precision: 0.7206809583858764, f_beta: 0.7064480210547626
train: step: 5787, loss: 0.5273540019989014, acc: 0.79, recall: 0.79, precision: 0.7910477719791249, f_beta: 0.7898108297467722
train: step: 5788, loss: 0.6168802976608276, acc: 0.69, recall: 0.69, precision: 0.7087912087912088, f_beta: 0.6828644501278773
train: step: 5789, loss: 0.5416607856750488, acc: 0.79, recall: 0.79, precision: 0.7910477719791249, f_beta: 0.7898108297467722
train: step: 5790, loss: 0.5338636636734009, acc: 0.87, recall: 0.87, precision: 0.8713368125250903, f_beta: 0.8698828946051447
train: step: 5791, loss: 0.5240382552146912, acc: 0.79, recall: 0.79, precision: 0.8186813186813187, f_beta: 0.7851662404092071
train: step: 5792, loss: 0.6056126952171326, acc: 0.73, recall: 0.73, precision: 0.7300920368147259, f_beta: 0.72997299729973
train: step: 5793, loss: 0.5487485527992249, acc: 0.78, recall: 0.78, precision: 0.78, f_beta: 0.78
train: step: 5794, loss: 0.5780887603759766, acc: 0.74, recall: 0.74, precision: 0.7415458937198067, f_beta: 0.7395833333333335
train: step: 5795, loss: 0.5575488209724426, acc: 0.91, recall: 0.9099999999999999, precision: 0.9114813327980731, f_beta: 0.9099189270343309
train: step: 5796, loss: 0.5639224648475647, acc: 0.77, recall: 0.77, precision: 0.7753977968176254, f_beta: 0.7688674505074867
train: step: 5797, loss: 0.5951380133628845, acc: 0.76, recall: 0.76, precision: 0.7616747181964574, f_beta: 0.7596153846153846
train: step: 5798, loss: 0.5895024538040161, acc: 0.74, recall: 0.74, precision: 0.7435064935064934, f_beta: 0.7390606182256123
train: step: 5799, loss: 0.541235089302063, acc: 0.78, recall: 0.78, precision: 0.7818035426731079, f_beta: 0.7796474358974359
train: step: 5800, loss: 0.5497746467590332, acc: 0.79, recall: 0.79, precision: 0.7901160464185675, f_beta: 0.78997899789979
train: step: 5801, loss: 0.4937528669834137, acc: 0.83, recall: 0.83, precision: 0.8301320528211285, f_beta: 0.8299829982998299
train: step: 5802, loss: 0.5168247818946838, acc: 0.79, recall: 0.79, precision: 0.8047498949138294, f_beta: 0.7874278773155178
train: step: 5803, loss: 0.6730164885520935, acc: 0.86, recall: 0.8600000000000001, precision: 0.8652597402597402, f_beta: 0.8594941790445605
train: step: 5804, loss: 0.5384277701377869, acc: 0.79, recall: 0.79, precision: 0.7910477719791249, f_beta: 0.7898108297467722
train: step: 5805, loss: 0.6795760989189148, acc: 0.62, recall: 0.62, precision: 0.6231527093596059, f_beta: 0.6175523349436394
train: step: 5806, loss: 0.5939182043075562, acc: 0.87, recall: 0.87, precision: 0.8701480592236894, f_beta: 0.8699869986998701
train: step: 5807, loss: 0.6126337051391602, acc: 0.79, recall: 0.79, precision: 0.7997106242248864, f_beta: 0.7882851093860268
train: step: 5808, loss: 0.6127971410751343, acc: 0.71, recall: 0.71, precision: 0.7252252252252251, f_beta: 0.7050147492625368
train: step: 5809, loss: 0.5301307439804077, acc: 0.79, recall: 0.79, precision: 0.8186813186813187, f_beta: 0.7851662404092071
train: step: 5810, loss: 0.6449369192123413, acc: 0.69, recall: 0.69, precision: 0.6900760304121649, f_beta: 0.68996899689969
train: step: 5811, loss: 0.5784348845481873, acc: 0.74, recall: 0.74, precision: 0.7463054187192117, f_beta: 0.7383252818035426
train: step: 5812, loss: 0.5333389639854431, acc: 0.78, recall: 0.78, precision: 0.8119429590017826, f_beta: 0.7742200328407225
train: step: 5813, loss: 0.5351564288139343, acc: 0.79, recall: 0.79, precision: 0.7910477719791249, f_beta: 0.7898108297467722
train: step: 5814, loss: 0.4894241988658905, acc: 0.85, recall: 0.8500000000000001, precision: 0.8535353535353536, f_beta: 0.849624060150376
train: step: 5815, loss: 0.5973849892616272, acc: 0.74, recall: 0.74, precision: 0.7604166666666667, f_beta: 0.7348021215830274
train: step: 5816, loss: 0.5444235801696777, acc: 0.78, recall: 0.78, precision: 0.7971137521222411, f_beta: 0.7767857142857144
train: step: 5817, loss: 0.43227535486221313, acc: 0.9, recall: 0.8999999999999999, precision: 0.9058441558441559, f_beta: 0.8996386993175431
train: step: 5818, loss: 0.5083185434341431, acc: 0.83, recall: 0.8300000000000001, precision: 0.8333333333333334, f_beta: 0.8295739348370927
train: step: 5819, loss: 0.5889307260513306, acc: 0.8, recall: 0.8, precision: 0.8125, f_beta: 0.797979797979798
train: step: 5820, loss: 0.5160683393478394, acc: 0.83, recall: 0.8300000000000001, precision: 0.8333333333333334, f_beta: 0.8295739348370927
train: step: 5821, loss: 0.6248807311058044, acc: 0.7, recall: 0.7, precision: 0.702922077922078, f_beta: 0.6989160979526294
train: step: 5822, loss: 0.6040945649147034, acc: 0.72, recall: 0.72, precision: 0.7291666666666667, f_beta: 0.7171717171717171
train: step: 5823, loss: 0.5206688046455383, acc: 0.81, recall: 0.81, precision: 0.8111200321156162, f_beta: 0.8098288459613652
train: step: 5824, loss: 0.5486955046653748, acc: 0.77, recall: 0.77, precision: 0.7895752895752897, f_beta: 0.7660461804495982
train: step: 5825, loss: 0.5041610598564148, acc: 0.82, recall: 0.8200000000000001, precision: 0.8246753246753247, f_beta: 0.8193496587715776
train: step: 5826, loss: 0.5356374979019165, acc: 0.81, recall: 0.81, precision: 0.8161974704202366, f_beta: 0.8090644156366193
train: step: 5827, loss: 0.5546818375587463, acc: 0.78, recall: 0.78, precision: 0.780448717948718, f_beta: 0.7799119647859143
train: step: 5828, loss: 0.602532684803009, acc: 0.74, recall: 0.74, precision: 0.7463054187192117, f_beta: 0.7383252818035426
train: step: 5829, loss: 0.5416133403778076, acc: 0.78, recall: 0.78, precision: 0.78, f_beta: 0.78
train: step: 5830, loss: 0.5607868432998657, acc: 0.77, recall: 0.77, precision: 0.7709755118426336, f_beta: 0.769792813532179
train: step: 5831, loss: 0.6505154371261597, acc: 0.69, recall: 0.69, precision: 0.6900760304121649, f_beta: 0.68996899689969
train: step: 5832, loss: 0.5643137097358704, acc: 0.79, recall: 0.79, precision: 0.811025311025311, f_beta: 0.7863899908452854
train: step: 5833, loss: 0.504092276096344, acc: 0.82, recall: 0.82, precision: 0.82, f_beta: 0.82
train: step: 5834, loss: 0.6056445837020874, acc: 0.72, recall: 0.72, precision: 0.7257799671592775, f_beta: 0.7181964573268922
train: step: 5835, loss: 0.5495383739471436, acc: 0.86, recall: 0.86, precision: 0.8623188405797102, f_beta: 0.859775641025641
train: step: 5836, loss: 0.5387572646141052, acc: 0.89, recall: 0.89, precision: 0.8914090726615818, f_beta: 0.8899009108197378
train: step: 5837, loss: 0.5952489972114563, acc: 0.78, recall: 0.78, precision: 0.7818035426731079, f_beta: 0.7796474358974359
train: step: 5838, loss: 0.6372055411338806, acc: 0.75, recall: 0.75, precision: 0.7525252525252526, f_beta: 0.7493734335839599
train: step: 5839, loss: 0.5434989333152771, acc: 0.76, recall: 0.76, precision: 0.7616747181964574, f_beta: 0.7596153846153846
train: step: 5840, loss: 0.5811907052993774, acc: 0.73, recall: 0.73, precision: 0.7466752466752467, f_beta: 0.7253585596582239
train: step: 5841, loss: 0.5956003665924072, acc: 0.74, recall: 0.74, precision: 0.7604166666666667, f_beta: 0.7348021215830274
train: step: 5842, loss: 0.6401457190513611, acc: 0.78, recall: 0.78, precision: 0.78, f_beta: 0.78
train: step: 5843, loss: 0.5099480152130127, acc: 0.81, recall: 0.81, precision: 0.8161974704202366, f_beta: 0.8090644156366193
train: step: 5844, loss: 0.5302297472953796, acc: 0.79, recall: 0.79, precision: 0.7901160464185675, f_beta: 0.78997899789979
train: step: 5845, loss: 0.5490295886993408, acc: 0.79, recall: 0.79, precision: 0.7929292929292929, f_beta: 0.7894736842105263
train: step: 5846, loss: 0.5725328326225281, acc: 0.75, recall: 0.75, precision: 0.7525252525252526, f_beta: 0.7493734335839599
train: step: 5847, loss: 0.5346220135688782, acc: 0.84, recall: 0.8400000000000001, precision: 0.8489326765188834, f_beta: 0.8389694041867956
train: step: 5848, loss: 0.5579084157943726, acc: 0.77, recall: 0.77, precision: 0.7727272727272727, f_beta: 0.7694235588972431
train: step: 5849, loss: 0.5838625431060791, acc: 0.73, recall: 0.73, precision: 0.7300920368147259, f_beta: 0.72997299729973
train: step: 5850, loss: 0.5556632876396179, acc: 0.75, recall: 0.75, precision: 0.7525252525252526, f_beta: 0.7493734335839599
train: step: 5851, loss: 0.5522143840789795, acc: 0.77, recall: 0.77, precision: 0.7753977968176254, f_beta: 0.7688674505074867
train: step: 5852, loss: 0.6016373634338379, acc: 0.77, recall: 0.77, precision: 0.7837326607818411, f_beta: 0.767182913250329
train: step: 5853, loss: 0.6225355267524719, acc: 0.7, recall: 0.7, precision: 0.7003205128205128, f_beta: 0.6998799519807923
train: step: 5854, loss: 0.5710899829864502, acc: 0.84, recall: 0.84, precision: 0.8607809847198642, f_beta: 0.8376623376623376
train: step: 5855, loss: 0.5876134634017944, acc: 0.79, recall: 0.79, precision: 0.7901160464185675, f_beta: 0.78997899789979
train: step: 5856, loss: 0.5349805355072021, acc: 0.85, recall: 0.85, precision: 0.851264552388599, f_beta: 0.8498648783905516
train: step: 5857, loss: 0.5946310758590698, acc: 0.79, recall: 0.79, precision: 0.7929292929292929, f_beta: 0.7894736842105263
train: step: 5858, loss: 0.6316943764686584, acc: 0.74, recall: 0.74, precision: 0.7463054187192117, f_beta: 0.7383252818035426
train: step: 5859, loss: 0.504885733127594, acc: 0.82, recall: 0.8200000000000001, precision: 0.8246753246753247, f_beta: 0.8193496587715776
train: step: 5860, loss: 0.5594033002853394, acc: 0.79, recall: 0.79, precision: 0.811025311025311, f_beta: 0.7863899908452854
train: step: 5861, loss: 0.5583093762397766, acc: 0.8, recall: 0.8, precision: 0.8004807692307692, f_beta: 0.7999199679871949
train: step: 5862, loss: 0.5568432211875916, acc: 0.81, recall: 0.81, precision: 0.8203803224472923, f_beta: 0.8084484323016432
train: step: 5863, loss: 0.5960256457328796, acc: 0.73, recall: 0.73, precision: 0.7323232323232323, f_beta: 0.7293233082706766
train: step: 5864, loss: 0.5942394733428955, acc: 0.79, recall: 0.79, precision: 0.7901160464185675, f_beta: 0.78997899789979
train: step: 5865, loss: 0.5367541313171387, acc: 0.8, recall: 0.8, precision: 0.8004807692307692, f_beta: 0.7999199679871949
train: step: 5866, loss: 0.5375083684921265, acc: 0.8, recall: 0.8, precision: 0.8004807692307692, f_beta: 0.7999199679871949
train: step: 5867, loss: 0.4951471984386444, acc: 0.86, recall: 0.86, precision: 0.8605769230769231, f_beta: 0.8599439775910365
train: step: 5868, loss: 0.5789927840232849, acc: 0.73, recall: 0.73, precision: 0.7300920368147259, f_beta: 0.72997299729973
train: step: 5869, loss: 0.5619558095932007, acc: 0.83, recall: 0.8300000000000001, precision: 0.8333333333333334, f_beta: 0.8295739348370927
train: step: 5870, loss: 0.5287343859672546, acc: 0.82, recall: 0.8200000000000001, precision: 0.822061191626409, f_beta: 0.8197115384615385
train: step: 5871, loss: 0.6056975722312927, acc: 0.72, recall: 0.72, precision: 0.7232142857142857, f_beta: 0.7189883580891208
train: step: 5872, loss: 0.5848913788795471, acc: 0.78, recall: 0.78, precision: 0.7971137521222411, f_beta: 0.7767857142857144
train: step: 5873, loss: 0.5729445815086365, acc: 0.79, recall: 0.79, precision: 0.7910477719791249, f_beta: 0.7898108297467722
train: step: 5874, loss: 0.5988102555274963, acc: 0.75, recall: 0.75, precision: 0.7525252525252526, f_beta: 0.7493734335839599
train: step: 5875, loss: 0.553191602230072, acc: 0.8, recall: 0.8, precision: 0.8, f_beta: 0.8000000000000002
train: step: 5876, loss: 0.5402050614356995, acc: 0.8, recall: 0.8, precision: 0.8043831168831169, f_beta: 0.7992773986350863
train: step: 5877, loss: 0.5633465647697449, acc: 0.76, recall: 0.76, precision: 0.7637987012987013, f_beta: 0.7591328783621035
train: step: 5878, loss: 0.5433077216148376, acc: 0.75, recall: 0.75, precision: 0.7627154266498528, f_beta: 0.7469379491851402
train: step: 5879, loss: 0.506796658039093, acc: 0.82, recall: 0.8200000000000001, precision: 0.8205128205128205, f_beta: 0.8199279711884755
train: step: 5880, loss: 0.5349273085594177, acc: 0.78, recall: 0.78, precision: 0.780448717948718, f_beta: 0.7799119647859143
train: step: 5881, loss: 0.5709880590438843, acc: 0.74, recall: 0.74, precision: 0.7435064935064934, f_beta: 0.7390606182256123
train: step: 5882, loss: 0.4840502142906189, acc: 0.88, recall: 0.88, precision: 0.88, f_beta: 0.88
train: step: 5883, loss: 0.5678767561912537, acc: 0.84, recall: 0.84, precision: 0.8405448717948718, f_beta: 0.8399359743897559
train: step: 5884, loss: 0.6308571100234985, acc: 0.73, recall: 0.73, precision: 0.7323232323232323, f_beta: 0.7293233082706766
train: step: 5885, loss: 0.5414777398109436, acc: 0.79, recall: 0.79, precision: 0.7901160464185675, f_beta: 0.78997899789979
train: step: 5886, loss: 0.5806442499160767, acc: 0.73, recall: 0.73, precision: 0.7345981232150143, f_beta: 0.7286704853783539
train: step: 5887, loss: 0.5620996952056885, acc: 0.77, recall: 0.77, precision: 0.7709755118426336, f_beta: 0.769792813532179
train: step: 5888, loss: 0.5886590480804443, acc: 0.74, recall: 0.74, precision: 0.7415458937198067, f_beta: 0.7395833333333335
train: step: 5889, loss: 0.5324162840843201, acc: 0.84, recall: 0.8400000000000001, precision: 0.8449675324675325, f_beta: 0.839421918908069
train: step: 5890, loss: 0.4940429627895355, acc: 0.89, recall: 0.89, precision: 0.8914090726615818, f_beta: 0.8899009108197378
train: step: 5891, loss: 0.5896853804588318, acc: 0.83, recall: 0.8300000000000001, precision: 0.8311922922521076, f_beta: 0.8298468621759585
train: step: 5892, loss: 0.503303050994873, acc: 0.83, recall: 0.83, precision: 0.8301320528211285, f_beta: 0.8299829982998299
train: step: 5893, loss: 0.5368479490280151, acc: 0.84, recall: 0.84, precision: 0.8405448717948718, f_beta: 0.8399359743897559
train: step: 5894, loss: 0.5352823138237, acc: 0.8, recall: 0.8, precision: 0.8078817733990147, f_beta: 0.7987117552334944
train: step: 5895, loss: 0.5938208103179932, acc: 0.74, recall: 0.74, precision: 0.7403846153846154, f_beta: 0.7398959583833533
train: step: 5896, loss: 0.5774135589599609, acc: 0.81, recall: 0.81, precision: 0.8203803224472923, f_beta: 0.8084484323016432
train: step: 5897, loss: 0.49947401881217957, acc: 0.85, recall: 0.85, precision: 0.851264552388599, f_beta: 0.8498648783905516
train: step: 5898, loss: 0.6204583644866943, acc: 0.7, recall: 0.7, precision: 0.702922077922078, f_beta: 0.6989160979526294
train: step: 5899, loss: 0.5534572601318359, acc: 0.83, recall: 0.83, precision: 0.8301320528211285, f_beta: 0.8299829982998299
train: step: 5900, loss: 0.6492184400558472, acc: 0.76, recall: 0.76, precision: 0.7604166666666667, f_beta: 0.7599039615846339
train: step: 5901, loss: 0.5183942317962646, acc: 0.84, recall: 0.84, precision: 0.8405448717948718, f_beta: 0.8399359743897559
train: step: 5902, loss: 0.5745851993560791, acc: 0.77, recall: 0.77, precision: 0.7701080432172869, f_beta: 0.7699769976997699
train: step: 5903, loss: 0.4825064539909363, acc: 0.89, recall: 0.89, precision: 0.89015606242497, f_beta: 0.88998899889989
train: step: 5904, loss: 0.514326810836792, acc: 0.82, recall: 0.8200000000000001, precision: 0.8246753246753247, f_beta: 0.8193496587715776
train: step: 5905, loss: 0.5258175134658813, acc: 0.85, recall: 0.8500000000000001, precision: 0.8569971440228478, f_beta: 0.8492613807657521
train: step: 5906, loss: 0.5775159597396851, acc: 0.73, recall: 0.73, precision: 0.7300920368147259, f_beta: 0.72997299729973
train: step: 5907, loss: 0.583402156829834, acc: 0.85, recall: 0.85, precision: 0.8501400560224089, f_beta: 0.84998499849985
train: step: 5908, loss: 0.5000966787338257, acc: 0.86, recall: 0.86, precision: 0.8605769230769231, f_beta: 0.8599439775910365
train: step: 5909, loss: 0.5292415022850037, acc: 0.82, recall: 0.8200000000000001, precision: 0.8333333333333334, f_beta: 0.8181818181818181
train: step: 5910, loss: 0.5548025369644165, acc: 0.79, recall: 0.79, precision: 0.7901160464185675, f_beta: 0.78997899789979
train: step: 5911, loss: 0.4973943829536438, acc: 0.85, recall: 0.85, precision: 0.8617197188921042, f_beta: 0.8487750781328762
train: step: 5912, loss: 0.632144570350647, acc: 0.68, recall: 0.6799999999999999, precision: 0.6847290640394088, f_beta: 0.677938808373591
train: step: 5913, loss: 0.5455055236816406, acc: 0.81, recall: 0.81, precision: 0.8161974704202366, f_beta: 0.8090644156366193
train: step: 5914, loss: 0.5086929202079773, acc: 0.84, recall: 0.84, precision: 0.8607809847198642, f_beta: 0.8376623376623376
train: step: 5915, loss: 0.5939459204673767, acc: 0.81, recall: 0.81, precision: 0.8111200321156162, f_beta: 0.8098288459613652
train: step: 5916, loss: 0.5032508373260498, acc: 0.83, recall: 0.8300000000000001, precision: 0.8365973072215422, f_beta: 0.8291628982011858
train: step: 5917, loss: 0.5156354308128357, acc: 0.83, recall: 0.83, precision: 0.8301320528211285, f_beta: 0.8299829982998299
train: step: 5918, loss: 0.5419817566871643, acc: 0.8, recall: 0.8, precision: 0.8004807692307692, f_beta: 0.7999199679871949
train: step: 5919, loss: 0.5174558162689209, acc: 0.87, recall: 0.87, precision: 0.8713368125250903, f_beta: 0.8698828946051447
train: step: 5920, loss: 0.5898883938789368, acc: 0.7, recall: 0.7, precision: 0.7052545155993433, f_beta: 0.6980676328502415
train: step: 5921, loss: 0.6065788269042969, acc: 0.82, recall: 0.82, precision: 0.82, f_beta: 0.82
train: step: 5922, loss: 0.5860143303871155, acc: 0.78, recall: 0.78, precision: 0.78, f_beta: 0.78
train: step: 5923, loss: 0.6168970465660095, acc: 0.75, recall: 0.75, precision: 0.7583712277800745, f_beta: 0.7479584635547939
train: step: 5924, loss: 0.555503249168396, acc: 0.79, recall: 0.79, precision: 0.7910477719791249, f_beta: 0.7898108297467722
train: step: 5925, loss: 0.5846191644668579, acc: 0.75, recall: 0.75, precision: 0.7549979600163199, f_beta: 0.7487689679429204
train: step: 5926, loss: 0.4904433488845825, acc: 0.85, recall: 0.85, precision: 0.8501400560224089, f_beta: 0.84998499849985
train: step: 5927, loss: 0.5782373547554016, acc: 0.75, recall: 0.75, precision: 0.7549979600163199, f_beta: 0.7487689679429204
train: step: 5928, loss: 0.5543100237846375, acc: 0.81, recall: 0.81, precision: 0.8324753324753325, f_beta: 0.8067338012409724
train: step: 5929, loss: 0.42671671509742737, acc: 0.87, recall: 0.87, precision: 0.8737373737373737, f_beta: 0.8696741854636592
train: step: 5930, loss: 0.6095743179321289, acc: 0.82, recall: 0.8200000000000001, precision: 0.822061191626409, f_beta: 0.8197115384615385
train: step: 5931, loss: 0.5069707632064819, acc: 0.83, recall: 0.8300000000000001, precision: 0.8311922922521076, f_beta: 0.8298468621759585
train: step: 5932, loss: 0.5491620898246765, acc: 0.76, recall: 0.76, precision: 0.7616747181964574, f_beta: 0.7596153846153846
train: step: 5933, loss: 0.5979409217834473, acc: 0.76, recall: 0.76, precision: 0.7616747181964574, f_beta: 0.7596153846153846
train: step: 5934, loss: 0.5426315069198608, acc: 0.87, recall: 0.87, precision: 0.8773969808241534, f_beta: 0.8693598633303186
train: step: 5935, loss: 0.552452564239502, acc: 0.79, recall: 0.79, precision: 0.7910477719791249, f_beta: 0.7898108297467722
train: step: 5936, loss: 0.4624543786048889, acc: 0.86, recall: 0.8600000000000001, precision: 0.8652597402597402, f_beta: 0.8594941790445605
train: step: 5937, loss: 0.5864930152893066, acc: 0.75, recall: 0.75, precision: 0.7501000400160064, f_beta: 0.7499749974997499
train: step: 5938, loss: 0.5305256843566895, acc: 0.8, recall: 0.8, precision: 0.8043831168831169, f_beta: 0.7992773986350863
train: step: 5939, loss: 0.5445080399513245, acc: 0.78, recall: 0.78, precision: 0.7840909090909092, f_beta: 0.779205138498595
train: step: 5940, loss: 0.5387926697731018, acc: 0.79, recall: 0.79, precision: 0.7910477719791249, f_beta: 0.7898108297467722
train: step: 5941, loss: 0.5441142916679382, acc: 0.81, recall: 0.81, precision: 0.8161974704202366, f_beta: 0.8090644156366193
train: step: 5942, loss: 0.5268340706825256, acc: 0.79, recall: 0.79, precision: 0.7929292929292929, f_beta: 0.7894736842105263
train: step: 5943, loss: 0.4721421003341675, acc: 0.91, recall: 0.9099999999999999, precision: 0.9114813327980731, f_beta: 0.9099189270343309
train: step: 5944, loss: 0.5275882482528687, acc: 0.81, recall: 0.81, precision: 0.8203803224472923, f_beta: 0.8084484323016432
train: step: 5945, loss: 0.5861055850982666, acc: 0.74, recall: 0.74, precision: 0.7403846153846154, f_beta: 0.7398959583833533
train: step: 5946, loss: 0.562353253364563, acc: 0.76, recall: 0.76, precision: 0.7604166666666667, f_beta: 0.7599039615846339
train: step: 5947, loss: 0.6200241446495056, acc: 0.68, recall: 0.6799999999999999, precision: 0.6802884615384616, f_beta: 0.6798719487795117
train: step: 5948, loss: 0.5700604915618896, acc: 0.76, recall: 0.76, precision: 0.76, f_beta: 0.76
train: step: 5949, loss: 0.5700668096542358, acc: 0.75, recall: 0.75, precision: 0.7826775214834916, f_beta: 0.7425599835238389
train: step: 5950, loss: 0.5885124802589417, acc: 0.73, recall: 0.73, precision: 0.7416981925178646, f_beta: 0.7266929851199515
train: step: 5951, loss: 0.6132288575172424, acc: 0.71, recall: 0.71, precision: 0.7100840336134454, f_beta: 0.7099709970997101
train: step: 5952, loss: 0.52718585729599, acc: 0.81, recall: 0.81, precision: 0.810124049619848, f_beta: 0.8099809980998101
train: step: 5953, loss: 0.6045432090759277, acc: 0.79, recall: 0.79, precision: 0.7957976336189311, f_beta: 0.7889659330720531
train: step: 5954, loss: 0.5875526666641235, acc: 0.77, recall: 0.77, precision: 0.7967032967032968, f_beta: 0.7647058823529411
train: step: 5955, loss: 0.6358234882354736, acc: 0.77, recall: 0.77, precision: 0.7895752895752897, f_beta: 0.7660461804495982
train: step: 5956, loss: 0.6038793921470642, acc: 0.71, recall: 0.71, precision: 0.7100840336134454, f_beta: 0.7099709970997101
train: step: 5957, loss: 0.6350299119949341, acc: 0.76, recall: 0.76, precision: 0.7637987012987013, f_beta: 0.7591328783621035
train: step: 5958, loss: 0.6358151435852051, acc: 0.66, recall: 0.66, precision: 0.6602564102564102, f_beta: 0.6598639455782312
train: step: 5959, loss: 0.5658732056617737, acc: 0.78, recall: 0.78, precision: 0.7818035426731079, f_beta: 0.7796474358974359
train: step: 5960, loss: 0.5737453103065491, acc: 0.74, recall: 0.74, precision: 0.7403846153846154, f_beta: 0.7398959583833533
train: step: 5961, loss: 0.46704646944999695, acc: 0.87, recall: 0.87, precision: 0.8701480592236894, f_beta: 0.8699869986998701
train: step: 5962, loss: 0.5757286548614502, acc: 0.76, recall: 0.76, precision: 0.7668308702791462, f_beta: 0.7584541062801933
train: step: 5963, loss: 0.584878146648407, acc: 0.77, recall: 0.77, precision: 0.7790409260024803, f_beta: 0.7681217864704104
train: step: 5964, loss: 0.4986690282821655, acc: 0.85, recall: 0.85, precision: 0.867801597309794, f_beta: 0.848162769511084
train: step: 5965, loss: 0.5554038882255554, acc: 0.8, recall: 0.8, precision: 0.8078817733990147, f_beta: 0.7987117552334944
train: step: 5966, loss: 0.506067156791687, acc: 0.86, recall: 0.86, precision: 0.8605769230769231, f_beta: 0.8599439775910365
train: step: 5967, loss: 0.536653459072113, acc: 0.77, recall: 0.77, precision: 0.7753977968176254, f_beta: 0.7688674505074867
train: step: 5968, loss: 0.5217152833938599, acc: 0.78, recall: 0.78, precision: 0.7916666666666666, f_beta: 0.7777777777777779
train: step: 5969, loss: 0.5633658766746521, acc: 0.8, recall: 0.8, precision: 0.8004807692307692, f_beta: 0.7999199679871949
train: step: 5970, loss: 0.5092964172363281, acc: 0.8, recall: 0.8, precision: 0.8078817733990147, f_beta: 0.7987117552334944
train: step: 5971, loss: 0.5716259479522705, acc: 0.76, recall: 0.76, precision: 0.76, f_beta: 0.76
train: step: 5972, loss: 0.6187486052513123, acc: 0.81, recall: 0.81, precision: 0.810124049619848, f_beta: 0.8099809980998101
train: step: 5973, loss: 0.51711505651474, acc: 0.8, recall: 0.8, precision: 0.8004807692307692, f_beta: 0.7999199679871949
train: step: 5974, loss: 0.611686646938324, acc: 0.71, recall: 0.71, precision: 0.7107587314331594, f_beta: 0.7097387648883996
train: step: 5975, loss: 0.6054805517196655, acc: 0.78, recall: 0.78, precision: 0.7818035426731079, f_beta: 0.7796474358974359
train: step: 5976, loss: 0.6695850491523743, acc: 0.59, recall: 0.5900000000000001, precision: 0.5909090909090908, f_beta: 0.5889724310776943
train: step: 5977, loss: 0.5297776460647583, acc: 0.82, recall: 0.8200000000000001, precision: 0.8246753246753247, f_beta: 0.8193496587715776
train: step: 5978, loss: 0.5249244570732117, acc: 0.79, recall: 0.79, precision: 0.7910477719791249, f_beta: 0.7898108297467722
train: step: 5979, loss: 0.4169289767742157, acc: 0.87, recall: 0.87, precision: 0.8713368125250903, f_beta: 0.8698828946051447
train: step: 5980, loss: 0.4758341610431671, acc: 0.85, recall: 0.85, precision: 0.851264552388599, f_beta: 0.8498648783905516
train: step: 5981, loss: 0.54954594373703, acc: 0.78, recall: 0.78, precision: 0.780448717948718, f_beta: 0.7799119647859143
train: step: 5982, loss: 0.471439927816391, acc: 0.83, recall: 0.8300000000000001, precision: 0.8365973072215422, f_beta: 0.8291628982011858
train: step: 5983, loss: 0.5746462941169739, acc: 0.76, recall: 0.76, precision: 0.7708333333333333, f_beta: 0.7575757575757576
train: step: 5984, loss: 0.5412470102310181, acc: 0.85, recall: 0.85, precision: 0.8501400560224089, f_beta: 0.84998499849985
train: step: 5985, loss: 0.5586932897567749, acc: 0.77, recall: 0.77, precision: 0.7837326607818411, f_beta: 0.767182913250329
train: step: 5986, loss: 0.5220108032226562, acc: 0.78, recall: 0.78, precision: 0.7818035426731079, f_beta: 0.7796474358974359
train: step: 5987, loss: 0.48208609223365784, acc: 0.86, recall: 0.86, precision: 0.8694581280788177, f_beta: 0.8590982286634461
train: step: 5988, loss: 0.6075334548950195, acc: 0.77, recall: 0.77, precision: 0.7967032967032968, f_beta: 0.7647058823529411
train: step: 5989, loss: 0.4318532645702362, acc: 0.88, recall: 0.88, precision: 0.88, f_beta: 0.88
train: step: 5990, loss: 0.6142898797988892, acc: 0.7, recall: 0.7, precision: 0.7228163992869875, f_beta: 0.6921182266009853
train: step: 5991, loss: 0.6439110040664673, acc: 0.82, recall: 0.82, precision: 0.8395585738539898, f_beta: 0.8173701298701298
train: step: 5992, loss: 0.5265043377876282, acc: 0.82, recall: 0.8200000000000001, precision: 0.8333333333333334, f_beta: 0.8181818181818181
train: step: 5993, loss: 0.5187263488769531, acc: 0.82, recall: 0.8200000000000001, precision: 0.8284072249589491, f_beta: 0.818840579710145
train: step: 5994, loss: 0.5700109601020813, acc: 0.77, recall: 0.77, precision: 0.7837326607818411, f_beta: 0.767182913250329
train: step: 5995, loss: 0.5507625937461853, acc: 0.79, recall: 0.79, precision: 0.7910477719791249, f_beta: 0.7898108297467722
train: step: 5996, loss: 0.5469295382499695, acc: 0.78, recall: 0.78, precision: 0.7840909090909092, f_beta: 0.779205138498595
train: step: 5997, loss: 0.6355991959571838, acc: 0.7, recall: 0.7, precision: 0.7380952380952381, f_beta: 0.6875
train: step: 5998, loss: 0.4882442355155945, acc: 0.81, recall: 0.81, precision: 0.8257671290458175, f_beta: 0.8076728413807065
train: step: 5999, loss: 0.6278151869773865, acc: 0.71, recall: 0.71, precision: 0.7141982864137086, f_beta: 0.7085720028137876
