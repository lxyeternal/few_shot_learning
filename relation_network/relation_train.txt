train: step: 0, loss: 0.40534114837646484, acc: 0.475, recall: 0.475, precision: 0.47333333333333333, f_beta: 0.4666666666666666
train: step: 1, loss: 0.40337318181991577, acc: 0.4, recall: 0.4, precision: 0.39583333333333337, f_beta: 0.3939393939393939
train: step: 2, loss: 0.39881882071495056, acc: 0.5, recall: 0.5, precision: 0.25, f_beta: 0.3333333333333333
train: step: 3, loss: 0.38781657814979553, acc: 0.5, recall: 0.5, precision: 0.25, f_beta: 0.3333333333333333
train: step: 4, loss: 0.34865158796310425, acc: 0.5, recall: 0.5, precision: 0.25, f_beta: 0.3333333333333333
train: step: 5, loss: 0.32633382081985474, acc: 0.5, recall: 0.5, precision: 0.25, f_beta: 0.3333333333333333
train: step: 6, loss: 0.31299299001693726, acc: 0.5, recall: 0.5, precision: 0.25, f_beta: 0.3333333333333333
train: step: 7, loss: 0.3172544538974762, acc: 0.5, recall: 0.5, precision: 0.25, f_beta: 0.3333333333333333
train: step: 8, loss: 0.29914453625679016, acc: 0.5, recall: 0.5, precision: 0.25, f_beta: 0.3333333333333333
train: step: 9, loss: 0.31211552023887634, acc: 0.5, recall: 0.5, precision: 0.25, f_beta: 0.3333333333333333
train: step: 10, loss: 0.2985328137874603, acc: 0.5, recall: 0.5, precision: 0.25, f_beta: 0.3333333333333333
train: step: 11, loss: 0.29270657896995544, acc: 0.475, recall: 0.47500000000000003, precision: 0.40990990990990994, f_beta: 0.35926773455377575
train: step: 12, loss: 0.29742231965065, acc: 0.5, recall: 0.5, precision: 0.25, f_beta: 0.3333333333333333
train: step: 13, loss: 0.3361676037311554, acc: 0.425, recall: 0.425, precision: 0.3924731182795699, f_beta: 0.37795807978363755
train: step: 14, loss: 0.301321804523468, acc: 0.5, recall: 0.5, precision: 0.25, f_beta: 0.3333333333333333
train: step: 15, loss: 0.28848594427108765, acc: 0.5, recall: 0.5, precision: 0.25, f_beta: 0.3333333333333333
train: step: 16, loss: 0.27282339334487915, acc: 0.5, recall: 0.5, precision: 0.25, f_beta: 0.3333333333333333
train: step: 17, loss: 0.33799225091934204, acc: 0.5, recall: 0.5, precision: 0.25, f_beta: 0.3333333333333333
train: step: 18, loss: 0.2671886086463928, acc: 0.45, recall: 0.45, precision: 0.3611111111111111, f_beta: 0.34523809523809523
train: step: 19, loss: 0.26788800954818726, acc: 0.5, recall: 0.5, precision: 0.25, f_beta: 0.3333333333333333
train: step: 20, loss: 0.30275845527648926, acc: 0.5, recall: 0.5, precision: 0.25, f_beta: 0.3333333333333333
train: step: 21, loss: 0.2604622542858124, acc: 0.5, recall: 0.5, precision: 0.25, f_beta: 0.3333333333333333
train: step: 22, loss: 0.27483507990837097, acc: 0.5, recall: 0.5, precision: 0.25, f_beta: 0.3333333333333333
train: step: 23, loss: 0.27106401324272156, acc: 0.5, recall: 0.5, precision: 0.25, f_beta: 0.3333333333333333
train: step: 24, loss: 0.27963581681251526, acc: 0.5, recall: 0.5, precision: 0.25, f_beta: 0.3333333333333333
train: step: 25, loss: 0.2657322287559509, acc: 0.475, recall: 0.475, precision: 0.24358974358974358, f_beta: 0.3220338983050847
train: step: 26, loss: 0.25730428099632263, acc: 0.5, recall: 0.5, precision: 0.25, f_beta: 0.3333333333333333
train: step: 27, loss: 0.26286354660987854, acc: 0.5, recall: 0.5, precision: 0.25, f_beta: 0.3333333333333333
train: step: 28, loss: 0.26192760467529297, acc: 0.5, recall: 0.5, precision: 0.25, f_beta: 0.3333333333333333
train: step: 29, loss: 0.2554381787776947, acc: 0.5, recall: 0.5, precision: 0.25, f_beta: 0.3333333333333333
train: step: 30, loss: 0.251702219247818, acc: 0.5, recall: 0.5, precision: 0.25, f_beta: 0.3333333333333333
train: step: 31, loss: 0.25556451082229614, acc: 0.5, recall: 0.5, precision: 0.25, f_beta: 0.3333333333333333
train: step: 32, loss: 0.2555016279220581, acc: 0.5, recall: 0.5, precision: 0.25, f_beta: 0.3333333333333333
train: step: 33, loss: 0.25509753823280334, acc: 0.5, recall: 0.5, precision: 0.25, f_beta: 0.3333333333333333
train: step: 34, loss: 0.2514859735965729, acc: 0.5, recall: 0.5, precision: 0.25, f_beta: 0.3333333333333333
train: step: 35, loss: 0.2503940463066101, acc: 0.5, recall: 0.5, precision: 0.25, f_beta: 0.3333333333333333
train: step: 36, loss: 0.27135607600212097, acc: 0.5, recall: 0.5, precision: 0.25, f_beta: 0.3333333333333333
train: step: 37, loss: 0.252838134765625, acc: 0.5, recall: 0.5, precision: 0.25, f_beta: 0.3333333333333333
train: step: 38, loss: 0.2525981068611145, acc: 0.5, recall: 0.5, precision: 0.25, f_beta: 0.3333333333333333
train: step: 39, loss: 0.2507404088973999, acc: 0.5, recall: 0.5, precision: 0.25, f_beta: 0.3333333333333333
train: step: 40, loss: 0.2617050111293793, acc: 0.5, recall: 0.5, precision: 0.25, f_beta: 0.3333333333333333
train: step: 41, loss: 0.251352995634079, acc: 0.5, recall: 0.5, precision: 0.25, f_beta: 0.3333333333333333
train: step: 42, loss: 0.25286298990249634, acc: 0.5, recall: 0.5, precision: 0.25, f_beta: 0.3333333333333333
train: step: 43, loss: 0.253106027841568, acc: 0.575, recall: 0.575, precision: 0.6714285714285715, f_beta: 0.5054545454545454
train: step: 44, loss: 0.25925886631011963, acc: 0.5, recall: 0.5, precision: 0.25, f_beta: 0.3333333333333333
train: step: 45, loss: 0.26008325815200806, acc: 0.5, recall: 0.5, precision: 0.25, f_beta: 0.3333333333333333
train: step: 46, loss: 0.2579171657562256, acc: 0.5, recall: 0.5, precision: 0.25, f_beta: 0.3333333333333333
train: step: 47, loss: 0.25283652544021606, acc: 0.525, recall: 0.525, precision: 0.7564102564102564, f_beta: 0.38660209846650523
train: step: 48, loss: 0.2509516775608063, acc: 0.5, recall: 0.5, precision: 0.25, f_beta: 0.3333333333333333
train: step: 49, loss: 0.25119107961654663, acc: 0.525, recall: 0.525, precision: 0.5900900900900901, f_beta: 0.4202898550724638
train: step: 50, loss: 0.2504541575908661, acc: 0.5, recall: 0.5, precision: 0.25, f_beta: 0.3333333333333333
train: step: 51, loss: 0.25703686475753784, acc: 0.5, recall: 0.5, precision: 0.25, f_beta: 0.3333333333333333
train: step: 52, loss: 0.25775855779647827, acc: 0.5, recall: 0.5, precision: 0.25, f_beta: 0.3333333333333333
train: step: 53, loss: 0.2508848309516907, acc: 0.625, recall: 0.625, precision: 0.6567398119122256, f_beta: 0.6050032916392363
train: step: 54, loss: 0.25919511914253235, acc: 0.5, recall: 0.5, precision: 0.25, f_beta: 0.3333333333333333
train: step: 55, loss: 0.25187939405441284, acc: 0.5, recall: 0.5, precision: 0.25, f_beta: 0.3333333333333333
train: step: 56, loss: 0.25072604417800903, acc: 0.6, recall: 0.6, precision: 0.65625, f_beta: 0.5604395604395604
train: step: 57, loss: 0.2508002817630768, acc: 0.525, recall: 0.525, precision: 0.7564102564102564, f_beta: 0.38660209846650523
train: step: 58, loss: 0.2505398690700531, acc: 0.5, recall: 0.5, precision: 0.25, f_beta: 0.3333333333333333
train: step: 59, loss: 0.25110626220703125, acc: 0.525, recall: 0.525, precision: 0.7564102564102564, f_beta: 0.38660209846650523
train: step: 60, loss: 0.2501506507396698, acc: 0.5, recall: 0.5, precision: 0.25, f_beta: 0.3333333333333333
train: step: 61, loss: 0.25031962990760803, acc: 0.525, recall: 0.525, precision: 0.5900900900900901, f_beta: 0.4202898550724638
train: step: 62, loss: 0.2506600320339203, acc: 0.5, recall: 0.5, precision: 0.25, f_beta: 0.3333333333333333
train: step: 63, loss: 0.25055283308029175, acc: 0.5, recall: 0.5, precision: 0.25, f_beta: 0.3333333333333333
train: step: 64, loss: 0.2503141462802887, acc: 0.5, recall: 0.5, precision: 0.25, f_beta: 0.3333333333333333
train: step: 65, loss: 0.2511734366416931, acc: 0.5, recall: 0.5, precision: 0.5, f_beta: 0.3730407523510972
train: step: 66, loss: 0.2512137293815613, acc: 0.55, recall: 0.5499999999999999, precision: 0.6388888888888888, f_beta: 0.46428571428571425
train: step: 67, loss: 0.25097131729125977, acc: 0.5, recall: 0.5, precision: 0.25, f_beta: 0.3333333333333333
train: step: 68, loss: 0.2525938153266907, acc: 0.65, recall: 0.6499999999999999, precision: 0.6515151515151515, f_beta: 0.6491228070175439
train: step: 69, loss: 0.2507160007953644, acc: 0.525, recall: 0.525, precision: 0.7564102564102564, f_beta: 0.38660209846650523
train: step: 70, loss: 0.2511923313140869, acc: 0.5, recall: 0.5, precision: 0.25, f_beta: 0.3333333333333333
train: step: 71, loss: 0.2505452632904053, acc: 0.5, recall: 0.5, precision: 0.25, f_beta: 0.3333333333333333
train: step: 72, loss: 0.25057920813560486, acc: 0.5, recall: 0.5, precision: 0.25, f_beta: 0.3333333333333333
train: step: 73, loss: 0.25033074617385864, acc: 0.5, recall: 0.5, precision: 0.25, f_beta: 0.3333333333333333
train: step: 74, loss: 0.25078320503234863, acc: 0.5, recall: 0.5, precision: 0.25, f_beta: 0.3333333333333333
train: step: 75, loss: 0.2502698004245758, acc: 0.5, recall: 0.5, precision: 0.25, f_beta: 0.3333333333333333
train: step: 76, loss: 0.25135546922683716, acc: 0.575, recall: 0.575, precision: 0.6298701298701299, f_beta: 0.5248078266946192
train: step: 77, loss: 0.25377970933914185, acc: 0.5, recall: 0.5, precision: 0.25, f_beta: 0.3333333333333333
train: step: 78, loss: 0.24985122680664062, acc: 0.525, recall: 0.525, precision: 0.7564102564102564, f_beta: 0.38660209846650523
train: step: 79, loss: 0.2505744397640228, acc: 0.5, recall: 0.5, precision: 0.25, f_beta: 0.3333333333333333
train: step: 80, loss: 0.2602781057357788, acc: 0.5, recall: 0.5, precision: 0.25, f_beta: 0.3333333333333333
train: step: 81, loss: 0.25093191862106323, acc: 0.5, recall: 0.5, precision: 0.25, f_beta: 0.3333333333333333
train: step: 82, loss: 0.2498960942029953, acc: 0.5, recall: 0.5, precision: 0.25, f_beta: 0.3333333333333333
train: step: 83, loss: 0.25022992491722107, acc: 0.5, recall: 0.5, precision: 0.25, f_beta: 0.3333333333333333
train: step: 84, loss: 0.25014498829841614, acc: 0.525, recall: 0.525, precision: 0.7564102564102564, f_beta: 0.38660209846650523
train: step: 85, loss: 0.25121617317199707, acc: 0.575, recall: 0.575, precision: 0.7702702702702703, f_beta: 0.4813119755911518
train: step: 86, loss: 0.25085869431495667, acc: 0.625, recall: 0.625, precision: 0.6792114695340502, f_beta: 0.5943204868154157
train: step: 87, loss: 0.24896033108234406, acc: 0.575, recall: 0.575, precision: 0.6714285714285715, f_beta: 0.5054545454545454
train: step: 88, loss: 0.2567993998527527, acc: 0.3, recall: 0.3, precision: 0.1875, f_beta: 0.23076923076923075
train: step: 89, loss: 0.2500401735305786, acc: 0.6, recall: 0.6, precision: 0.7777777777777778, f_beta: 0.5238095238095238
train: step: 90, loss: 0.2506740987300873, acc: 0.575, recall: 0.575, precision: 0.6714285714285715, f_beta: 0.5054545454545454
train: step: 91, loss: 0.2495175153017044, acc: 0.5, recall: 0.5, precision: 0.25, f_beta: 0.3333333333333333
train: step: 92, loss: 0.2508660852909088, acc: 0.5, recall: 0.5, precision: 0.25, f_beta: 0.3333333333333333
train: step: 93, loss: 0.24934343993663788, acc: 0.55, recall: 0.5499999999999999, precision: 0.6388888888888888, f_beta: 0.46428571428571425
train: step: 94, loss: 0.2516385614871979, acc: 0.525, recall: 0.525, precision: 0.7564102564102564, f_beta: 0.38660209846650523
train: step: 95, loss: 0.25072193145751953, acc: 0.5, recall: 0.5, precision: 0.25, f_beta: 0.3333333333333333
train: step: 96, loss: 0.2498798370361328, acc: 0.525, recall: 0.525, precision: 0.7564102564102564, f_beta: 0.38660209846650523
train: step: 97, loss: 0.2510848641395569, acc: 0.5, recall: 0.5, precision: 0.25, f_beta: 0.3333333333333333
train: step: 98, loss: 0.2554197311401367, acc: 0.5, recall: 0.5, precision: 0.25, f_beta: 0.3333333333333333
train: step: 99, loss: 0.25112637877464294, acc: 0.525, recall: 0.525, precision: 0.7564102564102564, f_beta: 0.38660209846650523
train: step: 100, loss: 0.247270867228508, acc: 0.5, recall: 0.5, precision: 0.25, f_beta: 0.3333333333333333
train: step: 101, loss: 0.2503086030483246, acc: 0.5, recall: 0.5, precision: 0.25, f_beta: 0.3333333333333333
train: step: 102, loss: 0.24957668781280518, acc: 0.7, recall: 0.7, precision: 0.7380952380952381, f_beta: 0.6875
train: step: 103, loss: 0.2511677145957947, acc: 0.5, recall: 0.5, precision: 0.25, f_beta: 0.3333333333333333
train: step: 104, loss: 0.24965433776378632, acc: 0.5, recall: 0.5, precision: 0.25, f_beta: 0.3333333333333333
train: step: 105, loss: 0.25136882066726685, acc: 0.5, recall: 0.5, precision: 0.25, f_beta: 0.3333333333333333
train: step: 106, loss: 0.25530558824539185, acc: 0.65, recall: 0.65, precision: 0.65, f_beta: 0.65
train: step: 107, loss: 0.24924127757549286, acc: 0.625, recall: 0.625, precision: 0.7857142857142857, f_beta: 0.5636363636363637
train: step: 108, loss: 0.24997448921203613, acc: 0.55, recall: 0.55, precision: 0.5980392156862745, f_beta: 0.4871794871794872
train: step: 109, loss: 0.2455194741487503, acc: 0.875, recall: 0.875, precision: 0.9, f_beta: 0.873015873015873
train: step: 110, loss: 0.24965110421180725, acc: 0.6, recall: 0.6, precision: 0.696078431372549, f_beta: 0.5441595441595442
train: step: 111, loss: 0.24918265640735626, acc: 0.5, recall: 0.5, precision: 0.25, f_beta: 0.3333333333333333
train: step: 112, loss: 0.24600759148597717, acc: 0.725, recall: 0.7250000000000001, precision: 0.7301790281329923, f_beta: 0.7234443746071653
train: step: 113, loss: 0.2474980354309082, acc: 0.675, recall: 0.675, precision: 0.6754385964912281, f_beta: 0.6747967479674797
train: step: 114, loss: 0.2489626407623291, acc: 0.6, recall: 0.6000000000000001, precision: 0.601010101010101, f_beta: 0.5989974937343359
train: step: 115, loss: 0.24548307061195374, acc: 0.625, recall: 0.625, precision: 0.6567398119122256, f_beta: 0.6050032916392363
train: step: 116, loss: 0.2501014769077301, acc: 0.5, recall: 0.5, precision: 0.5, f_beta: 0.3730407523510972
train: step: 117, loss: 0.24459083378314972, acc: 0.775, recall: 0.775, precision: 0.7756892230576441, f_beta: 0.774859287054409
train: step: 118, loss: 0.24892544746398926, acc: 0.55, recall: 0.55, precision: 0.5980392156862745, f_beta: 0.4871794871794872
train: step: 119, loss: 0.24254660308361053, acc: 0.6, recall: 0.6, precision: 0.7777777777777778, f_beta: 0.5238095238095238
train: step: 120, loss: 0.2547633647918701, acc: 0.475, recall: 0.475, precision: 0.24358974358974358, f_beta: 0.3220338983050847
train: step: 121, loss: 0.24923710525035858, acc: 0.55, recall: 0.55, precision: 0.763157894736842, f_beta: 0.4357366771159875
train: step: 122, loss: 0.24826765060424805, acc: 0.525, recall: 0.525, precision: 0.7564102564102564, f_beta: 0.38660209846650523
train: step: 123, loss: 0.22660188376903534, acc: 0.85, recall: 0.85, precision: 0.85, f_beta: 0.85
train: step: 124, loss: 0.24394473433494568, acc: 0.55, recall: 0.55, precision: 0.5666666666666667, f_beta: 0.52
train: step: 125, loss: 0.24800710380077362, acc: 0.5, recall: 0.5, precision: 0.25, f_beta: 0.3333333333333333
train: step: 126, loss: 0.2444852590560913, acc: 0.6, recall: 0.6, precision: 0.6098901098901099, f_beta: 0.5907928388746804
train: step: 127, loss: 0.2449173927307129, acc: 0.65, recall: 0.65, precision: 0.6648351648351649, f_beta: 0.6419437340153452
train: step: 128, loss: 0.23581115901470184, acc: 0.675, recall: 0.675, precision: 0.7508960573476702, f_beta: 0.6484110885733605
train: step: 129, loss: 0.24738809466362, acc: 0.575, recall: 0.575, precision: 0.5940438871473355, f_beta: 0.5523370638578011
train: step: 130, loss: 0.2436814308166504, acc: 0.65, recall: 0.65, precision: 0.7941176470588236, f_beta: 0.6011396011396011
train: step: 131, loss: 0.21765363216400146, acc: 0.8, recall: 0.8, precision: 0.8571428571428572, f_beta: 0.7916666666666665
train: step: 132, loss: 0.19255012273788452, acc: 0.825, recall: 0.825, precision: 0.8258145363408521, f_beta: 0.8248905565978737
train: step: 133, loss: 0.20317713916301727, acc: 0.775, recall: 0.7749999999999999, precision: 0.7813299232736572, f_beta: 0.7737272155876807
train: step: 134, loss: 0.203825905919075, acc: 0.7, recall: 0.7, precision: 0.7083333333333333, f_beta: 0.6969696969696968
train: step: 135, loss: 0.21240997314453125, acc: 0.775, recall: 0.775, precision: 0.7933333333333333, f_beta: 0.7714285714285714
train: step: 136, loss: 0.24470877647399902, acc: 0.6, recall: 0.6, precision: 0.65625, f_beta: 0.5604395604395604
train: step: 137, loss: 0.21920356154441833, acc: 0.75, recall: 0.75, precision: 0.7525252525252526, f_beta: 0.7493734335839599
train: step: 138, loss: 0.21922504901885986, acc: 0.675, recall: 0.675, precision: 0.803030303030303, f_beta: 0.636617749825297
train: step: 139, loss: 0.16730618476867676, acc: 0.775, recall: 0.7749999999999999, precision: 0.8133903133903134, f_beta: 0.7678916827853
train: step: 140, loss: 0.2840147018432617, acc: 0.45, recall: 0.45, precision: 0.3611111111111111, f_beta: 0.34523809523809523
train: step: 141, loss: 0.234770730137825, acc: 0.625, recall: 0.625, precision: 0.6792114695340502, f_beta: 0.5943204868154157
train: step: 142, loss: 0.2346995323896408, acc: 0.75, recall: 0.75, precision: 0.7604166666666667, f_beta: 0.7474747474747475
train: step: 143, loss: 0.22020640969276428, acc: 0.75, recall: 0.75, precision: 0.7604166666666667, f_beta: 0.7474747474747475
train: step: 144, loss: 0.19817784428596497, acc: 0.85, recall: 0.8500000000000001, precision: 0.8535353535353536, f_beta: 0.849624060150376
train: step: 145, loss: 0.206414133310318, acc: 0.75, recall: 0.75, precision: 0.7747252747252746, f_beta: 0.7442455242966751
train: step: 146, loss: 0.2008918821811676, acc: 0.75, recall: 0.75, precision: 0.7747252747252746, f_beta: 0.7442455242966751
train: step: 147, loss: 0.21199846267700195, acc: 0.6, recall: 0.6, precision: 0.6098901098901099, f_beta: 0.5907928388746804
train: step: 148, loss: 0.1843908578157425, acc: 0.8, recall: 0.8, precision: 0.803030303030303, f_beta: 0.7994987468671679
train: step: 149, loss: 0.2228146493434906, acc: 0.75, recall: 0.75, precision: 0.7747252747252746, f_beta: 0.7442455242966751
train: step: 150, loss: 0.2821308970451355, acc: 0.65, recall: 0.65, precision: 0.7941176470588236, f_beta: 0.6011396011396011
train: step: 151, loss: 0.24692320823669434, acc: 0.525, recall: 0.525, precision: 0.525062656641604, f_beta: 0.5247029393370857
train: step: 152, loss: 0.22376516461372375, acc: 0.7, recall: 0.7, precision: 0.7197802197802198, f_beta: 0.6930946291560103
train: step: 153, loss: 0.24032799899578094, acc: 0.7, recall: 0.7, precision: 0.8125, f_beta: 0.6703296703296704
train: step: 154, loss: 0.24486365914344788, acc: 0.65, recall: 0.6499999999999999, precision: 0.734375, f_beta: 0.6153846153846154
train: step: 155, loss: 0.1868547797203064, acc: 0.75, recall: 0.75, precision: 0.7604166666666667, f_beta: 0.7474747474747475
train: step: 156, loss: 0.2661231458187103, acc: 0.8, recall: 0.8, precision: 0.8296703296703296, f_beta: 0.7953964194373402
train: step: 157, loss: 0.24966569244861603, acc: 0.6, recall: 0.6, precision: 0.6098901098901099, f_beta: 0.5907928388746804
train: step: 158, loss: 0.2701123356819153, acc: 0.4, recall: 0.4, precision: 0.3666666666666667, f_beta: 0.36
train: step: 159, loss: 0.3065202236175537, acc: 0.375, recall: 0.375, precision: 0.3207885304659498, f_beta: 0.32386747802569305
train: step: 160, loss: 0.14675188064575195, acc: 0.8, recall: 0.8, precision: 0.8125, f_beta: 0.797979797979798
train: step: 161, loss: 0.31990137696266174, acc: 0.6, recall: 0.6, precision: 0.7777777777777778, f_beta: 0.5238095238095238
train: step: 162, loss: 0.16224651038646698, acc: 0.75, recall: 0.75, precision: 0.7976190476190477, f_beta: 0.7395833333333333
train: step: 163, loss: 0.2903028130531311, acc: 0.675, recall: 0.675, precision: 0.6994301994301995, f_beta: 0.6647324306898775
train: step: 164, loss: 0.1399179995059967, acc: 0.875, recall: 0.875, precision: 0.8836317135549872, f_beta: 0.8742928975487115
train: step: 165, loss: 0.19918540120124817, acc: 0.675, recall: 0.675, precision: 0.6754385964912281, f_beta: 0.6747967479674797
train: step: 166, loss: 0.20116467773914337, acc: 0.725, recall: 0.7250000000000001, precision: 0.7301790281329923, f_beta: 0.7234443746071653
train: step: 167, loss: 0.17922396957874298, acc: 0.75, recall: 0.75, precision: 0.7747252747252746, f_beta: 0.7442455242966751
train: step: 168, loss: 0.1715954840183258, acc: 0.8, recall: 0.8, precision: 0.8296703296703296, f_beta: 0.7953964194373402
train: step: 169, loss: 0.20941820740699768, acc: 0.675, recall: 0.675, precision: 0.6790281329923273, f_beta: 0.6731615336266499
train: step: 170, loss: 0.16382046043872833, acc: 0.8, recall: 0.8, precision: 0.8125, f_beta: 0.797979797979798
train: step: 171, loss: 0.22097206115722656, acc: 0.7, recall: 0.7, precision: 0.7, f_beta: 0.7
train: step: 172, loss: 0.25192052125930786, acc: 0.65, recall: 0.65, precision: 0.7941176470588236, f_beta: 0.6011396011396011
train: step: 173, loss: 0.23987440764904022, acc: 0.625, recall: 0.625, precision: 0.6567398119122256, f_beta: 0.6050032916392363
train: step: 174, loss: 0.24987664818763733, acc: 0.575, recall: 0.575, precision: 0.5940438871473355, f_beta: 0.5523370638578011
train: step: 175, loss: 0.18587203323841095, acc: 0.8, recall: 0.8, precision: 0.803030303030303, f_beta: 0.7994987468671679
train: step: 176, loss: 0.18974030017852783, acc: 0.875, recall: 0.875, precision: 0.8759398496240601, f_beta: 0.8749218261413383
train: step: 177, loss: 0.14824029803276062, acc: 0.925, recall: 0.925, precision: 0.9260651629072681, f_beta: 0.924953095684803
train: step: 178, loss: 0.23314854502677917, acc: 0.6, recall: 0.6000000000000001, precision: 0.601010101010101, f_beta: 0.5989974937343359
train: step: 179, loss: 0.2475634068250656, acc: 0.75, recall: 0.75, precision: 0.7604166666666667, f_beta: 0.7474747474747475
train: step: 180, loss: 0.2663085460662842, acc: 0.5, recall: 0.5, precision: 0.25, f_beta: 0.3333333333333333
train: step: 181, loss: 0.19211633503437042, acc: 0.8, recall: 0.8, precision: 0.8125, f_beta: 0.797979797979798
train: step: 182, loss: 0.14357836544513702, acc: 0.8, recall: 0.8, precision: 0.8296703296703296, f_beta: 0.7953964194373402
train: step: 183, loss: 0.2162122279405594, acc: 0.75, recall: 0.75, precision: 0.7747252747252746, f_beta: 0.7442455242966751
train: step: 184, loss: 0.15126864612102509, acc: 0.75, recall: 0.75, precision: 0.7747252747252746, f_beta: 0.7442455242966751
train: step: 185, loss: 0.18679538369178772, acc: 0.7, recall: 0.7, precision: 0.7083333333333333, f_beta: 0.6969696969696968
train: step: 186, loss: 0.2108856439590454, acc: 0.8, recall: 0.8, precision: 0.8125, f_beta: 0.797979797979798
train: step: 187, loss: 0.2823716998100281, acc: 0.5, recall: 0.5, precision: 0.25, f_beta: 0.3333333333333333
train: step: 188, loss: 0.18854035437107086, acc: 0.775, recall: 0.7749999999999999, precision: 0.8133903133903134, f_beta: 0.7678916827853
train: step: 189, loss: 0.12581408023834229, acc: 0.875, recall: 0.875, precision: 0.8836317135549872, f_beta: 0.8742928975487115
train: step: 190, loss: 0.2736411988735199, acc: 0.725, recall: 0.725, precision: 0.74, f_beta: 0.7206349206349207
train: step: 191, loss: 0.23540444672107697, acc: 0.6, recall: 0.6, precision: 0.6, f_beta: 0.6
train: step: 192, loss: 0.21112361550331116, acc: 0.75, recall: 0.75, precision: 0.7525252525252526, f_beta: 0.7493734335839599
train: step: 193, loss: 0.1842307448387146, acc: 0.825, recall: 0.825, precision: 0.8258145363408521, f_beta: 0.8248905565978737
train: step: 194, loss: 0.23253993690013885, acc: 0.625, recall: 0.625, precision: 0.6424501424501424, f_beta: 0.6131528046421664
train: step: 195, loss: 0.17816026508808136, acc: 0.75, recall: 0.75, precision: 0.7604166666666667, f_beta: 0.7474747474747475
train: step: 196, loss: 0.19607625901699066, acc: 0.7, recall: 0.7, precision: 0.7083333333333333, f_beta: 0.6969696969696968
train: step: 197, loss: 0.1957819014787674, acc: 0.8, recall: 0.8, precision: 0.803030303030303, f_beta: 0.7994987468671679
train: step: 198, loss: 0.17339980602264404, acc: 0.8, recall: 0.8, precision: 0.8296703296703296, f_beta: 0.7953964194373402
train: step: 199, loss: 0.35247379541397095, acc: 0.5, recall: 0.5, precision: 0.25, f_beta: 0.3333333333333333
train: step: 200, loss: 0.2224811613559723, acc: 0.65, recall: 0.6499999999999999, precision: 0.6515151515151515, f_beta: 0.6491228070175439
train: step: 201, loss: 0.15459465980529785, acc: 0.875, recall: 0.875, precision: 0.8759398496240601, f_beta: 0.8749218261413383
train: step: 202, loss: 0.14735078811645508, acc: 0.8, recall: 0.8, precision: 0.803030303030303, f_beta: 0.7994987468671679
train: step: 203, loss: 0.17294104397296906, acc: 0.775, recall: 0.775, precision: 0.7933333333333333, f_beta: 0.7714285714285714
train: step: 204, loss: 0.11429488658905029, acc: 0.8, recall: 0.8, precision: 0.8125, f_beta: 0.797979797979798
train: step: 205, loss: 0.3774540424346924, acc: 0.45, recall: 0.45, precision: 0.44047619047619047, f_beta: 0.42708333333333337
train: step: 206, loss: 0.3121543526649475, acc: 0.575, recall: 0.575, precision: 0.6714285714285715, f_beta: 0.5054545454545454
train: step: 207, loss: 0.2422950714826584, acc: 0.8, recall: 0.8, precision: 0.8296703296703296, f_beta: 0.7953964194373402
train: step: 208, loss: 0.21375532448291779, acc: 0.825, recall: 0.825, precision: 0.8258145363408521, f_beta: 0.8248905565978737
train: step: 209, loss: 0.08643629401922226, acc: 0.925, recall: 0.925, precision: 0.9260651629072681, f_beta: 0.924953095684803
train: step: 210, loss: 0.1401519477367401, acc: 0.825, recall: 0.825, precision: 0.8324808184143222, f_beta: 0.824010056568196
train: step: 211, loss: 0.24655957520008087, acc: 0.575, recall: 0.575, precision: 0.5940438871473355, f_beta: 0.5523370638578011
train: step: 212, loss: 0.23415949940681458, acc: 0.825, recall: 0.825, precision: 0.8466666666666667, f_beta: 0.8222222222222222
train: step: 213, loss: 0.23800714313983917, acc: 0.625, recall: 0.625, precision: 0.6333333333333333, f_beta: 0.6190476190476191
train: step: 214, loss: 0.29388248920440674, acc: 0.625, recall: 0.625, precision: 0.6567398119122256, f_beta: 0.6050032916392363
train: step: 215, loss: 0.1762630194425583, acc: 0.75, recall: 0.75, precision: 0.7525252525252526, f_beta: 0.7493734335839599
train: step: 216, loss: 0.24820128083229065, acc: 0.7, recall: 0.7, precision: 0.7666666666666666, f_beta: 0.6799999999999999
train: step: 217, loss: 0.2654797434806824, acc: 0.7, recall: 0.7, precision: 0.8125, f_beta: 0.6703296703296704
train: step: 218, loss: 0.130779430270195, acc: 0.85, recall: 0.85, precision: 0.8846153846153846, f_beta: 0.8465473145780051
train: step: 219, loss: 0.2987290024757385, acc: 0.35, recall: 0.35, precision: 0.35, f_beta: 0.35
train: step: 220, loss: 0.1503281593322754, acc: 0.85, recall: 0.8500000000000001, precision: 0.8535353535353536, f_beta: 0.849624060150376
train: step: 221, loss: 0.1711157113313675, acc: 0.725, recall: 0.725, precision: 0.74, f_beta: 0.7206349206349207
train: step: 222, loss: 0.13157251477241516, acc: 0.85, recall: 0.8500000000000001, precision: 0.8535353535353536, f_beta: 0.849624060150376
train: step: 223, loss: 0.10546424239873886, acc: 0.85, recall: 0.85, precision: 0.85, f_beta: 0.85
train: step: 224, loss: 0.2794884145259857, acc: 0.65, recall: 0.6499999999999999, precision: 0.734375, f_beta: 0.6153846153846154
train: step: 225, loss: 0.29370856285095215, acc: 0.675, recall: 0.675, precision: 0.7508960573476702, f_beta: 0.6484110885733605
train: step: 226, loss: 0.16456039249897003, acc: 0.775, recall: 0.7749999999999999, precision: 0.7813299232736572, f_beta: 0.7737272155876807
train: step: 227, loss: 0.21045689284801483, acc: 0.8, recall: 0.8, precision: 0.8125, f_beta: 0.797979797979798
train: step: 228, loss: 0.2186272144317627, acc: 0.65, recall: 0.6499999999999999, precision: 0.734375, f_beta: 0.6153846153846154
train: step: 229, loss: 0.16469734907150269, acc: 0.975, recall: 0.975, precision: 0.9761904761904762, f_beta: 0.9749843652282677
train: step: 230, loss: 0.24079391360282898, acc: 0.7, recall: 0.7, precision: 0.7380952380952381, f_beta: 0.6875
train: step: 231, loss: 0.21029143035411835, acc: 0.8, recall: 0.8, precision: 0.8125, f_beta: 0.797979797979798
train: step: 232, loss: 0.19206602871418, acc: 0.775, recall: 0.7749999999999999, precision: 0.8133903133903134, f_beta: 0.7678916827853
train: step: 233, loss: 0.2612069249153137, acc: 0.675, recall: 0.675, precision: 0.6994301994301995, f_beta: 0.6647324306898775
train: step: 234, loss: 0.18767181038856506, acc: 0.675, recall: 0.675, precision: 0.6754385964912281, f_beta: 0.6747967479674797
train: step: 235, loss: 0.2448066771030426, acc: 0.6, recall: 0.6000000000000001, precision: 0.6190476190476191, f_beta: 0.5833333333333333
train: step: 236, loss: 0.09911305457353592, acc: 0.95, recall: 0.95, precision: 0.9545454545454546, f_beta: 0.949874686716792
train: step: 237, loss: 0.20073623955249786, acc: 0.7, recall: 0.7, precision: 0.7083333333333333, f_beta: 0.6969696969696968
train: step: 238, loss: 0.16298411786556244, acc: 0.85, recall: 0.85, precision: 0.8645833333333333, f_beta: 0.8484848484848484
train: step: 239, loss: 0.1766946017742157, acc: 0.85, recall: 0.8500000000000001, precision: 0.8535353535353536, f_beta: 0.849624060150376
train: step: 240, loss: 0.3929934501647949, acc: 0.25, recall: 0.25, precision: 0.20238095238095238, f_beta: 0.21875
train: step: 241, loss: 0.14701570570468903, acc: 0.875, recall: 0.875, precision: 0.8836317135549872, f_beta: 0.8742928975487115
train: step: 242, loss: 0.1883031576871872, acc: 0.725, recall: 0.725, precision: 0.7255639097744361, f_beta: 0.7248280175109443
train: step: 243, loss: 0.11816475540399551, acc: 0.8, recall: 0.8, precision: 0.8125, f_beta: 0.797979797979798
train: step: 244, loss: 0.15929469466209412, acc: 0.775, recall: 0.7749999999999999, precision: 0.8133903133903134, f_beta: 0.7678916827853
train: step: 245, loss: 0.3310588300228119, acc: 0.275, recall: 0.275, precision: 0.1774193548387097, f_beta: 0.21568627450980393
train: step: 246, loss: 0.10855346918106079, acc: 0.85, recall: 0.85, precision: 0.8645833333333333, f_beta: 0.8484848484848484
train: step: 247, loss: 0.11531160026788712, acc: 0.875, recall: 0.875, precision: 0.8759398496240601, f_beta: 0.8749218261413383
train: step: 248, loss: 0.24883690476417542, acc: 0.725, recall: 0.725, precision: 0.74, f_beta: 0.7206349206349207
train: step: 249, loss: 0.16452527046203613, acc: 0.775, recall: 0.7749999999999999, precision: 0.7813299232736572, f_beta: 0.7737272155876807
train: step: 250, loss: 0.15918216109275818, acc: 0.825, recall: 0.825, precision: 0.8466666666666667, f_beta: 0.8222222222222222
train: step: 251, loss: 0.21972699463367462, acc: 0.675, recall: 0.675, precision: 0.6994301994301995, f_beta: 0.6647324306898775
train: step: 252, loss: 0.2686464190483093, acc: 0.775, recall: 0.7749999999999999, precision: 0.7813299232736572, f_beta: 0.7737272155876807
train: step: 253, loss: 0.13145950436592102, acc: 0.875, recall: 0.875, precision: 0.8836317135549872, f_beta: 0.8742928975487115
train: step: 254, loss: 0.16389983892440796, acc: 0.925, recall: 0.925, precision: 0.9260651629072681, f_beta: 0.924953095684803
train: step: 255, loss: 0.3108311593532562, acc: 0.675, recall: 0.675, precision: 0.6866666666666666, f_beta: 0.6698412698412699
train: step: 256, loss: 0.23069310188293457, acc: 0.65, recall: 0.65, precision: 0.6785714285714286, f_beta: 0.6354166666666667
train: step: 257, loss: 0.1327974498271942, acc: 0.8, recall: 0.8, precision: 0.8296703296703296, f_beta: 0.7953964194373402
train: step: 258, loss: 0.3314085602760315, acc: 0.925, recall: 0.925, precision: 0.9347826086956521, f_beta: 0.9245757385292268
train: step: 259, loss: 0.3741151988506317, acc: 0.325, recall: 0.325, precision: 0.32097186700767266, f_beta: 0.32118164676304206
train: step: 260, loss: 0.13102510571479797, acc: 0.85, recall: 0.85, precision: 0.8846153846153846, f_beta: 0.8465473145780051
train: step: 261, loss: 0.12827332317829132, acc: 0.85, recall: 0.8500000000000001, precision: 0.8535353535353536, f_beta: 0.849624060150376
train: step: 262, loss: 0.1135508194565773, acc: 0.875, recall: 0.875, precision: 0.8836317135549872, f_beta: 0.8742928975487115
train: step: 263, loss: 0.17260627448558807, acc: 0.9, recall: 0.9, precision: 0.9, f_beta: 0.9
train: step: 264, loss: 0.1982899159193039, acc: 0.85, recall: 0.85, precision: 0.85, f_beta: 0.85
train: step: 265, loss: 0.2933472990989685, acc: 0.525, recall: 0.525, precision: 0.5571428571428572, f_beta: 0.44727272727272727
train: step: 266, loss: 0.22878292202949524, acc: 0.675, recall: 0.675, precision: 0.803030303030303, f_beta: 0.636617749825297
train: step: 267, loss: 0.24350491166114807, acc: 0.8, recall: 0.8, precision: 0.8296703296703296, f_beta: 0.7953964194373402
train: step: 268, loss: 0.2723998427391052, acc: 0.575, recall: 0.575, precision: 0.5854700854700855, f_beta: 0.5615731785944552
train: step: 269, loss: 0.19173823297023773, acc: 0.85, recall: 0.85, precision: 0.8846153846153846, f_beta: 0.8465473145780051
train: step: 270, loss: 0.3123528063297272, acc: 0.4, recall: 0.4, precision: 0.2222222222222222, f_beta: 0.2857142857142857
train: step: 271, loss: 0.23430772125720978, acc: 0.575, recall: 0.575, precision: 0.6298701298701299, f_beta: 0.5248078266946192
train: step: 272, loss: 0.3146640360355377, acc: 0.6, recall: 0.6, precision: 0.696078431372549, f_beta: 0.5441595441595442
train: step: 273, loss: 0.25726979970932007, acc: 0.425, recall: 0.425, precision: 0.3701298701298701, f_beta: 0.35709294199860236
train: step: 274, loss: 0.15132549405097961, acc: 0.8, recall: 0.8, precision: 0.8571428571428572, f_beta: 0.7916666666666665
train: step: 275, loss: 0.19436028599739075, acc: 0.7, recall: 0.7, precision: 0.7666666666666666, f_beta: 0.6799999999999999
train: step: 276, loss: 0.13542474806308746, acc: 0.925, recall: 0.925, precision: 0.9260651629072681, f_beta: 0.924953095684803
train: step: 277, loss: 0.25046306848526, acc: 0.675, recall: 0.675, precision: 0.6866666666666666, f_beta: 0.6698412698412699
train: step: 278, loss: 0.14103850722312927, acc: 0.825, recall: 0.825, precision: 0.8703703703703703, f_beta: 0.819471308833011
train: step: 279, loss: 0.22369447350502014, acc: 0.725, recall: 0.7250000000000001, precision: 0.7301790281329923, f_beta: 0.7234443746071653
train: step: 280, loss: 0.16838805377483368, acc: 0.75, recall: 0.75, precision: 0.7976190476190477, f_beta: 0.7395833333333333
train: step: 281, loss: 0.1475246548652649, acc: 0.8, recall: 0.8, precision: 0.8, f_beta: 0.8000000000000002
train: step: 282, loss: 0.17185620963573456, acc: 0.8, recall: 0.8, precision: 0.8296703296703296, f_beta: 0.7953964194373402
train: step: 283, loss: 0.16804227232933044, acc: 0.8, recall: 0.8, precision: 0.8571428571428572, f_beta: 0.7916666666666665
train: step: 284, loss: 0.12166335433721542, acc: 0.875, recall: 0.875, precision: 0.8759398496240601, f_beta: 0.8749218261413383
train: step: 285, loss: 0.22958028316497803, acc: 0.625, recall: 0.625, precision: 0.6424501424501424, f_beta: 0.6131528046421664
train: step: 286, loss: 0.12066671997308731, acc: 0.875, recall: 0.875, precision: 0.8836317135549872, f_beta: 0.8742928975487115
train: step: 287, loss: 0.19576875865459442, acc: 0.7, recall: 0.7, precision: 0.7197802197802198, f_beta: 0.6930946291560103
train: step: 288, loss: 0.2751735746860504, acc: 0.45, recall: 0.45, precision: 0.421875, f_beta: 0.39560439560439553
train: step: 289, loss: 0.2589191496372223, acc: 0.625, recall: 0.625, precision: 0.6424501424501424, f_beta: 0.6131528046421664
train: step: 290, loss: 0.20427103340625763, acc: 0.675, recall: 0.675, precision: 0.6866666666666666, f_beta: 0.6698412698412699
train: step: 291, loss: 0.18796232342720032, acc: 0.75, recall: 0.75, precision: 0.7747252747252746, f_beta: 0.7442455242966751
train: step: 292, loss: 0.1203281506896019, acc: 0.875, recall: 0.875, precision: 0.8759398496240601, f_beta: 0.8749218261413383
train: step: 293, loss: 0.2568942904472351, acc: 0.725, recall: 0.7250000000000001, precision: 0.7301790281329923, f_beta: 0.7234443746071653
train: step: 294, loss: 0.28425338864326477, acc: 0.875, recall: 0.875, precision: 0.8836317135549872, f_beta: 0.8742928975487115
train: step: 295, loss: 0.12269250303506851, acc: 0.875, recall: 0.875, precision: 0.8836317135549872, f_beta: 0.8742928975487115
train: step: 296, loss: 0.18265962600708008, acc: 0.7, recall: 0.7, precision: 0.7666666666666666, f_beta: 0.6799999999999999
train: step: 297, loss: 0.1952689290046692, acc: 0.75, recall: 0.75, precision: 0.7976190476190477, f_beta: 0.7395833333333333
train: step: 298, loss: 0.1300300806760788, acc: 0.8, recall: 0.8, precision: 0.8125, f_beta: 0.797979797979798
train: step: 299, loss: 0.14101043343544006, acc: 0.825, recall: 0.825, precision: 0.8324808184143222, f_beta: 0.824010056568196
train: step: 300, loss: 0.23846402764320374, acc: 0.65, recall: 0.65, precision: 0.7941176470588236, f_beta: 0.6011396011396011
train: step: 301, loss: 0.1247360110282898, acc: 0.825, recall: 0.825, precision: 0.8258145363408521, f_beta: 0.8248905565978737
train: step: 302, loss: 0.19991722702980042, acc: 0.825, recall: 0.825, precision: 0.8466666666666667, f_beta: 0.8222222222222222
train: step: 303, loss: 0.28857752680778503, acc: 0.6, recall: 0.6, precision: 0.6098901098901099, f_beta: 0.5907928388746804
train: step: 304, loss: 0.12036559730768204, acc: 0.9, recall: 0.9, precision: 0.9, f_beta: 0.9
train: step: 305, loss: 0.27242836356163025, acc: 0.575, recall: 0.575, precision: 0.60752688172043, f_beta: 0.5402298850574712
train: step: 306, loss: 0.18345756828784943, acc: 0.75, recall: 0.75, precision: 0.7604166666666667, f_beta: 0.7474747474747475
train: step: 307, loss: 0.11965930461883545, acc: 0.825, recall: 0.825, precision: 0.8258145363408521, f_beta: 0.8248905565978737
train: step: 308, loss: 0.3234991133213043, acc: 0.725, recall: 0.7250000000000001, precision: 0.7301790281329923, f_beta: 0.7234443746071653
train: step: 309, loss: 0.26080769300460815, acc: 0.525, recall: 0.525, precision: 0.5266666666666666, f_beta: 0.5174603174603175
train: step: 310, loss: 0.14792504906654358, acc: 0.775, recall: 0.775, precision: 0.7933333333333333, f_beta: 0.7714285714285714
train: step: 311, loss: 0.092924103140831, acc: 0.875, recall: 0.875, precision: 0.8759398496240601, f_beta: 0.8749218261413383
train: step: 312, loss: 0.09136028587818146, acc: 0.95, recall: 0.95, precision: 0.95, f_beta: 0.9500000000000001
train: step: 313, loss: 0.12870796024799347, acc: 0.85, recall: 0.85, precision: 0.85, f_beta: 0.85
train: step: 314, loss: 0.1764386147260666, acc: 0.75, recall: 0.75, precision: 0.7747252747252746, f_beta: 0.7442455242966751
train: step: 315, loss: 0.1718902289867401, acc: 0.775, recall: 0.7749999999999999, precision: 0.7813299232736572, f_beta: 0.7737272155876807
train: step: 316, loss: 0.187652125954628, acc: 0.75, recall: 0.75, precision: 0.7747252747252746, f_beta: 0.7442455242966751
train: step: 317, loss: 0.11418364942073822, acc: 0.825, recall: 0.825, precision: 0.8324808184143222, f_beta: 0.824010056568196
train: step: 318, loss: 0.16113349795341492, acc: 0.775, recall: 0.775, precision: 0.7756892230576441, f_beta: 0.774859287054409
train: step: 319, loss: 0.14494426548480988, acc: 0.775, recall: 0.775, precision: 0.7756892230576441, f_beta: 0.774859287054409
train: step: 320, loss: 0.1739024966955185, acc: 0.825, recall: 0.825, precision: 0.8466666666666667, f_beta: 0.8222222222222222
train: step: 321, loss: 0.09079022705554962, acc: 0.925, recall: 0.925, precision: 0.9260651629072681, f_beta: 0.924953095684803
train: step: 322, loss: 0.18310508131980896, acc: 0.775, recall: 0.7749999999999999, precision: 0.7813299232736572, f_beta: 0.7737272155876807
train: step: 323, loss: 0.11375293880701065, acc: 0.8, recall: 0.8, precision: 0.8125, f_beta: 0.797979797979798
train: step: 324, loss: 0.20166075229644775, acc: 0.775, recall: 0.7749999999999999, precision: 0.7813299232736572, f_beta: 0.7737272155876807
train: step: 325, loss: 0.2022291123867035, acc: 0.7, recall: 0.7, precision: 0.7083333333333333, f_beta: 0.6969696969696968
train: step: 326, loss: 0.20405873656272888, acc: 0.7, recall: 0.7, precision: 0.7380952380952381, f_beta: 0.6875
train: step: 327, loss: 0.26329857110977173, acc: 0.6, recall: 0.6, precision: 0.6333333333333333, f_beta: 0.5733333333333334
train: step: 328, loss: 0.11071974039077759, acc: 0.85, recall: 0.85, precision: 0.8645833333333333, f_beta: 0.8484848484848484
train: step: 329, loss: 0.16300736367702484, acc: 0.75, recall: 0.75, precision: 0.7525252525252526, f_beta: 0.7493734335839599
train: step: 330, loss: 0.2880480885505676, acc: 0.75, recall: 0.75, precision: 0.7525252525252526, f_beta: 0.7493734335839599
train: step: 331, loss: 0.1582152545452118, acc: 0.8, recall: 0.8, precision: 0.8, f_beta: 0.8000000000000002
train: step: 332, loss: 0.19307050108909607, acc: 0.675, recall: 0.675, precision: 0.6754385964912281, f_beta: 0.6747967479674797
train: step: 333, loss: 0.11959519237279892, acc: 0.9, recall: 0.8999999999999999, precision: 0.904040404040404, f_beta: 0.899749373433584
train: step: 334, loss: 0.13539402186870575, acc: 0.8, recall: 0.8, precision: 0.8296703296703296, f_beta: 0.7953964194373402
train: step: 335, loss: 0.25955164432525635, acc: 0.675, recall: 0.675, precision: 0.803030303030303, f_beta: 0.636617749825297
train: step: 336, loss: 0.21232828497886658, acc: 0.675, recall: 0.675, precision: 0.6790281329923273, f_beta: 0.6731615336266499
train: step: 337, loss: 0.175479918718338, acc: 0.775, recall: 0.7749999999999999, precision: 0.7813299232736572, f_beta: 0.7737272155876807
train: step: 338, loss: 0.11810474097728729, acc: 0.9, recall: 0.8999999999999999, precision: 0.904040404040404, f_beta: 0.899749373433584
train: step: 339, loss: 0.2854712903499603, acc: 0.375, recall: 0.375, precision: 0.3721227621483376, f_beta: 0.3714644877435575
train: step: 340, loss: 0.20177975296974182, acc: 0.7, recall: 0.7, precision: 0.7083333333333333, f_beta: 0.6969696969696968
train: step: 341, loss: 0.10624531656503677, acc: 0.875, recall: 0.875, precision: 0.8759398496240601, f_beta: 0.8749218261413383
train: step: 342, loss: 0.2858291268348694, acc: 0.875, recall: 0.875, precision: 0.8836317135549872, f_beta: 0.8742928975487115
train: step: 343, loss: 0.33908993005752563, acc: 0.275, recall: 0.275, precision: 0.2698209718670077, f_beta: 0.2708988057825267
train: step: 344, loss: 0.20490483939647675, acc: 0.775, recall: 0.775, precision: 0.7756892230576441, f_beta: 0.774859287054409
train: step: 345, loss: 0.15857145190238953, acc: 0.75, recall: 0.75, precision: 0.7525252525252526, f_beta: 0.7493734335839599
train: step: 346, loss: 0.10519541800022125, acc: 0.875, recall: 0.875, precision: 0.8836317135549872, f_beta: 0.8742928975487115
train: step: 347, loss: 0.28695982694625854, acc: 0.6, recall: 0.6, precision: 0.6041666666666667, f_beta: 0.595959595959596
train: step: 348, loss: 0.11008964478969574, acc: 0.825, recall: 0.825, precision: 0.8258145363408521, f_beta: 0.8248905565978737
train: step: 349, loss: 0.08021902292966843, acc: 0.9, recall: 0.8999999999999999, precision: 0.904040404040404, f_beta: 0.899749373433584
train: step: 350, loss: 0.32561349868774414, acc: 0.575, recall: 0.575, precision: 0.60752688172043, f_beta: 0.5402298850574712
train: step: 351, loss: 0.15047448873519897, acc: 0.775, recall: 0.775, precision: 0.7756892230576441, f_beta: 0.774859287054409
train: step: 352, loss: 0.26801180839538574, acc: 0.75, recall: 0.75, precision: 0.8333333333333333, f_beta: 0.7333333333333334
train: step: 353, loss: 0.12449581921100616, acc: 0.825, recall: 0.825, precision: 0.8466666666666667, f_beta: 0.8222222222222222
train: step: 354, loss: 0.13018035888671875, acc: 0.825, recall: 0.825, precision: 0.8324808184143222, f_beta: 0.824010056568196
train: step: 355, loss: 0.2417769879102707, acc: 0.675, recall: 0.675, precision: 0.7508960573476702, f_beta: 0.6484110885733605
train: step: 356, loss: 0.1765119582414627, acc: 0.775, recall: 0.7749999999999999, precision: 0.8133903133903134, f_beta: 0.7678916827853
train: step: 357, loss: 0.19435575604438782, acc: 0.725, recall: 0.725, precision: 0.7255639097744361, f_beta: 0.7248280175109443
train: step: 358, loss: 0.14146681129932404, acc: 0.85, recall: 0.85, precision: 0.8645833333333333, f_beta: 0.8484848484848484
train: step: 359, loss: 0.25971102714538574, acc: 0.625, recall: 0.625, precision: 0.62531328320802, f_beta: 0.6247654784240151
train: step: 360, loss: 0.25227123498916626, acc: 0.5, recall: 0.5, precision: 0.25, f_beta: 0.3333333333333333
train: step: 361, loss: 0.09918340295553207, acc: 0.875, recall: 0.875, precision: 0.8759398496240601, f_beta: 0.8749218261413383
train: step: 362, loss: 0.13427841663360596, acc: 0.825, recall: 0.825, precision: 0.8466666666666667, f_beta: 0.8222222222222222
train: step: 363, loss: 0.19185203313827515, acc: 0.675, recall: 0.675, precision: 0.6754385964912281, f_beta: 0.6747967479674797
train: step: 364, loss: 0.21629869937896729, acc: 0.775, recall: 0.7749999999999999, precision: 0.8133903133903134, f_beta: 0.7678916827853
train: step: 365, loss: 0.17355942726135254, acc: 0.75, recall: 0.75, precision: 0.8333333333333333, f_beta: 0.7333333333333334
train: step: 366, loss: 0.22231516242027283, acc: 0.65, recall: 0.6499999999999999, precision: 0.734375, f_beta: 0.6153846153846154
train: step: 367, loss: 0.2794215679168701, acc: 0.35, recall: 0.35000000000000003, precision: 0.265625, f_beta: 0.28571428571428575
train: step: 368, loss: 0.19477921724319458, acc: 0.9, recall: 0.9, precision: 0.9166666666666667, f_beta: 0.898989898989899
train: step: 369, loss: 0.24086126685142517, acc: 0.675, recall: 0.675, precision: 0.6866666666666666, f_beta: 0.6698412698412699
train: step: 370, loss: 0.12823033332824707, acc: 0.85, recall: 0.85, precision: 0.85, f_beta: 0.85
train: step: 371, loss: 0.13597777485847473, acc: 0.875, recall: 0.875, precision: 0.8836317135549872, f_beta: 0.8742928975487115
train: step: 372, loss: 0.23984718322753906, acc: 0.825, recall: 0.825, precision: 0.8324808184143222, f_beta: 0.824010056568196
train: step: 373, loss: 0.23644161224365234, acc: 0.65, recall: 0.65, precision: 0.6785714285714286, f_beta: 0.6354166666666667
train: step: 374, loss: 0.13348747789859772, acc: 0.85, recall: 0.8500000000000001, precision: 0.8535353535353536, f_beta: 0.849624060150376
train: step: 375, loss: 0.1630956381559372, acc: 0.775, recall: 0.775, precision: 0.7933333333333333, f_beta: 0.7714285714285714
train: step: 376, loss: 0.12185609340667725, acc: 0.85, recall: 0.8500000000000001, precision: 0.8535353535353536, f_beta: 0.849624060150376
train: step: 377, loss: 0.20574963092803955, acc: 0.7, recall: 0.7, precision: 0.7380952380952381, f_beta: 0.6875
train: step: 378, loss: 0.15934579074382782, acc: 0.725, recall: 0.7250000000000001, precision: 0.7301790281329923, f_beta: 0.7234443746071653
train: step: 379, loss: 0.2572553753852844, acc: 0.675, recall: 0.675, precision: 0.7508960573476702, f_beta: 0.6484110885733605
train: step: 380, loss: 0.14004480838775635, acc: 0.85, recall: 0.85, precision: 0.8846153846153846, f_beta: 0.8465473145780051
train: step: 381, loss: 0.1284099668264389, acc: 0.825, recall: 0.825, precision: 0.8324808184143222, f_beta: 0.824010056568196
train: step: 382, loss: 0.2884314954280853, acc: 0.825, recall: 0.825, precision: 0.8466666666666667, f_beta: 0.8222222222222222
train: step: 383, loss: 0.1891457587480545, acc: 0.8, recall: 0.8, precision: 0.8296703296703296, f_beta: 0.7953964194373402
train: step: 384, loss: 0.15581683814525604, acc: 0.75, recall: 0.75, precision: 0.7976190476190477, f_beta: 0.7395833333333333
train: step: 385, loss: 0.1841524839401245, acc: 0.825, recall: 0.825, precision: 0.8324808184143222, f_beta: 0.824010056568196
train: step: 386, loss: 0.14812692999839783, acc: 0.8, recall: 0.8, precision: 0.8, f_beta: 0.8000000000000002
train: step: 387, loss: 0.15490896999835968, acc: 0.825, recall: 0.825, precision: 0.8324808184143222, f_beta: 0.824010056568196
train: step: 388, loss: 0.10322214663028717, acc: 0.85, recall: 0.85, precision: 0.8846153846153846, f_beta: 0.8465473145780051
train: step: 389, loss: 0.19061991572380066, acc: 0.725, recall: 0.7250000000000001, precision: 0.7564102564102564, f_beta: 0.7163120567375887
train: step: 390, loss: 0.0748632624745369, acc: 0.95, recall: 0.95, precision: 0.9545454545454546, f_beta: 0.949874686716792
train: step: 391, loss: 0.22212544083595276, acc: 0.625, recall: 0.625, precision: 0.62531328320802, f_beta: 0.6247654784240151
train: step: 392, loss: 0.16304418444633484, acc: 0.75, recall: 0.75, precision: 0.7525252525252526, f_beta: 0.7493734335839599
train: step: 393, loss: 0.23663869500160217, acc: 0.725, recall: 0.725, precision: 0.74, f_beta: 0.7206349206349207
train: step: 394, loss: 0.12983672320842743, acc: 0.875, recall: 0.875, precision: 0.8759398496240601, f_beta: 0.8749218261413383
train: step: 395, loss: 0.22327843308448792, acc: 0.675, recall: 0.675, precision: 0.6754385964912281, f_beta: 0.6747967479674797
train: step: 396, loss: 0.1993720829486847, acc: 0.675, recall: 0.675, precision: 0.6866666666666666, f_beta: 0.6698412698412699
train: step: 397, loss: 0.1548890322446823, acc: 0.85, recall: 0.85, precision: 0.85, f_beta: 0.85
train: step: 398, loss: 0.12788668274879456, acc: 0.875, recall: 0.875, precision: 0.8759398496240601, f_beta: 0.8749218261413383
train: step: 399, loss: 0.16571751236915588, acc: 0.775, recall: 0.775, precision: 0.7933333333333333, f_beta: 0.7714285714285714
train: step: 400, loss: 0.22301054000854492, acc: 0.7, recall: 0.7, precision: 0.7666666666666666, f_beta: 0.6799999999999999
train: step: 401, loss: 0.3098236918449402, acc: 0.75, recall: 0.75, precision: 0.8333333333333333, f_beta: 0.7333333333333334
train: step: 402, loss: 0.25174054503440857, acc: 0.6, recall: 0.6, precision: 0.696078431372549, f_beta: 0.5441595441595442
train: step: 403, loss: 0.09407810866832733, acc: 0.925, recall: 0.925, precision: 0.9260651629072681, f_beta: 0.924953095684803
train: step: 404, loss: 0.142339825630188, acc: 0.825, recall: 0.825, precision: 0.8324808184143222, f_beta: 0.824010056568196
train: step: 405, loss: 0.23756535351276398, acc: 0.7, recall: 0.7, precision: 0.702020202020202, f_beta: 0.6992481203007519
train: step: 406, loss: 0.10427527129650116, acc: 0.875, recall: 0.875, precision: 0.8759398496240601, f_beta: 0.8749218261413383
train: step: 407, loss: 0.16872337460517883, acc: 0.825, recall: 0.825, precision: 0.8466666666666667, f_beta: 0.8222222222222222
train: step: 408, loss: 0.10855118930339813, acc: 0.825, recall: 0.825, precision: 0.8466666666666667, f_beta: 0.8222222222222222
train: step: 409, loss: 0.1396068036556244, acc: 0.8, recall: 0.8, precision: 0.8296703296703296, f_beta: 0.7953964194373402
train: step: 410, loss: 0.1500670611858368, acc: 0.875, recall: 0.875, precision: 0.8759398496240601, f_beta: 0.8749218261413383
train: step: 411, loss: 0.2618663012981415, acc: 0.75, recall: 0.75, precision: 0.7976190476190477, f_beta: 0.7395833333333333
train: step: 412, loss: 0.27868714928627014, acc: 0.7, recall: 0.7, precision: 0.7197802197802198, f_beta: 0.6930946291560103
train: step: 413, loss: 0.18096384406089783, acc: 0.7, recall: 0.7, precision: 0.702020202020202, f_beta: 0.6992481203007519
train: step: 414, loss: 0.32468730211257935, acc: 0.375, recall: 0.375, precision: 0.34326018808777425, f_beta: 0.34167215273206053
train: step: 415, loss: 0.14053478837013245, acc: 0.8, recall: 0.8, precision: 0.8125, f_beta: 0.797979797979798
train: step: 416, loss: 0.3332529366016388, acc: 0.325, recall: 0.325, precision: 0.24910394265232974, f_beta: 0.2697768762677485
train: step: 417, loss: 0.2696109712123871, acc: 0.9, recall: 0.8999999999999999, precision: 0.904040404040404, f_beta: 0.899749373433584
train: step: 418, loss: 0.18259622156620026, acc: 0.7, recall: 0.7, precision: 0.702020202020202, f_beta: 0.6992481203007519
train: step: 419, loss: 0.10876204073429108, acc: 0.925, recall: 0.925, precision: 0.9347826086956521, f_beta: 0.9245757385292268
train: step: 420, loss: 0.13389304280281067, acc: 0.85, recall: 0.85, precision: 0.85, f_beta: 0.85
train: step: 421, loss: 0.24754762649536133, acc: 0.675, recall: 0.675, precision: 0.6790281329923273, f_beta: 0.6731615336266499
train: step: 422, loss: 0.13898955285549164, acc: 0.775, recall: 0.775, precision: 0.7756892230576441, f_beta: 0.774859287054409
train: step: 423, loss: 0.11167991161346436, acc: 0.85, recall: 0.85, precision: 0.8645833333333333, f_beta: 0.8484848484848484
train: step: 424, loss: 0.3001086711883545, acc: 0.275, recall: 0.275, precision: 0.21786833855799376, f_beta: 0.23633969716919026
train: step: 425, loss: 0.15488216280937195, acc: 0.75, recall: 0.75, precision: 0.7525252525252526, f_beta: 0.7493734335839599
train: step: 426, loss: 0.14317569136619568, acc: 0.825, recall: 0.825, precision: 0.8466666666666667, f_beta: 0.8222222222222222
train: step: 427, loss: 0.1601502150297165, acc: 0.8, recall: 0.8, precision: 0.803030303030303, f_beta: 0.7994987468671679
train: step: 428, loss: 0.19536373019218445, acc: 0.825, recall: 0.825, precision: 0.8466666666666667, f_beta: 0.8222222222222222
train: step: 429, loss: 0.16960588097572327, acc: 0.75, recall: 0.75, precision: 0.7747252747252746, f_beta: 0.7442455242966751
train: step: 430, loss: 0.12169162929058075, acc: 0.85, recall: 0.85, precision: 0.8846153846153846, f_beta: 0.8465473145780051
train: step: 431, loss: 0.08783475309610367, acc: 0.9, recall: 0.9, precision: 0.9, f_beta: 0.9
train: step: 432, loss: 0.09321849048137665, acc: 0.875, recall: 0.875, precision: 0.8759398496240601, f_beta: 0.8749218261413383
train: step: 433, loss: 0.2400858849287033, acc: 0.8, recall: 0.8, precision: 0.8296703296703296, f_beta: 0.7953964194373402
train: step: 434, loss: 0.15422329306602478, acc: 0.825, recall: 0.825, precision: 0.8258145363408521, f_beta: 0.8248905565978737
train: step: 435, loss: 0.26901569962501526, acc: 0.65, recall: 0.6499999999999999, precision: 0.6515151515151515, f_beta: 0.6491228070175439
train: step: 436, loss: 0.12912940979003906, acc: 0.825, recall: 0.825, precision: 0.8466666666666667, f_beta: 0.8222222222222222
train: step: 437, loss: 0.09939960390329361, acc: 0.9, recall: 0.9, precision: 0.9, f_beta: 0.9
train: step: 438, loss: 0.120508573949337, acc: 0.85, recall: 0.85, precision: 0.8645833333333333, f_beta: 0.8484848484848484
train: step: 439, loss: 0.11974386125802994, acc: 0.825, recall: 0.825, precision: 0.8324808184143222, f_beta: 0.824010056568196
train: step: 440, loss: 0.13783510029315948, acc: 0.775, recall: 0.7749999999999999, precision: 0.7813299232736572, f_beta: 0.7737272155876807
train: step: 441, loss: 0.25022798776626587, acc: 0.675, recall: 0.675, precision: 0.7508960573476702, f_beta: 0.6484110885733605
train: step: 442, loss: 0.20554442703723907, acc: 0.725, recall: 0.7250000000000001, precision: 0.7564102564102564, f_beta: 0.7163120567375887
train: step: 443, loss: 0.1976906657218933, acc: 0.7, recall: 0.7, precision: 0.702020202020202, f_beta: 0.6992481203007519
train: step: 444, loss: 0.29472488164901733, acc: 0.7, recall: 0.7, precision: 0.7, f_beta: 0.7
train: step: 445, loss: 0.08610963076353073, acc: 0.875, recall: 0.875, precision: 0.8836317135549872, f_beta: 0.8742928975487115
train: step: 446, loss: 0.10120876878499985, acc: 0.9, recall: 0.9, precision: 0.9166666666666667, f_beta: 0.898989898989899
train: step: 447, loss: 0.15802215039730072, acc: 0.75, recall: 0.75, precision: 0.75, f_beta: 0.75
train: step: 448, loss: 0.14903803169727325, acc: 0.825, recall: 0.825, precision: 0.8258145363408521, f_beta: 0.8248905565978737
train: step: 449, loss: 0.16973581910133362, acc: 0.75, recall: 0.75, precision: 0.7604166666666667, f_beta: 0.7474747474747475
train: step: 450, loss: 0.20280727744102478, acc: 0.675, recall: 0.675, precision: 0.6994301994301995, f_beta: 0.6647324306898775
train: step: 451, loss: 0.17958186566829681, acc: 0.725, recall: 0.725, precision: 0.74, f_beta: 0.7206349206349207
train: step: 452, loss: 0.0915021225810051, acc: 0.9, recall: 0.8999999999999999, precision: 0.904040404040404, f_beta: 0.899749373433584
train: step: 453, loss: 0.3093356490135193, acc: 0.3, recall: 0.3, precision: 0.1875, f_beta: 0.23076923076923075
train: step: 454, loss: 0.09962489455938339, acc: 0.9, recall: 0.9, precision: 0.9166666666666667, f_beta: 0.898989898989899
train: step: 455, loss: 0.15043294429779053, acc: 0.875, recall: 0.875, precision: 0.8759398496240601, f_beta: 0.8749218261413383
train: step: 456, loss: 0.167040154337883, acc: 0.9, recall: 0.9, precision: 0.9166666666666667, f_beta: 0.898989898989899
train: step: 457, loss: 0.2022572010755539, acc: 0.925, recall: 0.925, precision: 0.9260651629072681, f_beta: 0.924953095684803
train: step: 458, loss: 0.1374162882566452, acc: 0.825, recall: 0.825, precision: 0.8258145363408521, f_beta: 0.8248905565978737
train: step: 459, loss: 0.10839001834392548, acc: 0.85, recall: 0.8500000000000001, precision: 0.8535353535353536, f_beta: 0.849624060150376
train: step: 460, loss: 0.12329323589801788, acc: 0.825, recall: 0.825, precision: 0.8466666666666667, f_beta: 0.8222222222222222
train: step: 461, loss: 0.3253580629825592, acc: 0.65, recall: 0.6499999999999999, precision: 0.734375, f_beta: 0.6153846153846154
train: step: 462, loss: 0.3749607503414154, acc: 0.675, recall: 0.675, precision: 0.7508960573476702, f_beta: 0.6484110885733605
train: step: 463, loss: 0.08573757112026215, acc: 0.925, recall: 0.925, precision: 0.9260651629072681, f_beta: 0.924953095684803
train: step: 464, loss: 0.3038382828235626, acc: 0.725, recall: 0.7250000000000001, precision: 0.7564102564102564, f_beta: 0.7163120567375887
train: step: 465, loss: 0.21468503773212433, acc: 0.8, recall: 0.8, precision: 0.8296703296703296, f_beta: 0.7953964194373402
train: step: 466, loss: 0.14425520598888397, acc: 0.8, recall: 0.8, precision: 0.8296703296703296, f_beta: 0.7953964194373402
train: step: 467, loss: 0.31507641077041626, acc: 0.6, recall: 0.6, precision: 0.65625, f_beta: 0.5604395604395604
train: step: 468, loss: 0.09889330714941025, acc: 0.875, recall: 0.875, precision: 0.8759398496240601, f_beta: 0.8749218261413383
train: step: 469, loss: 0.20843341946601868, acc: 0.775, recall: 0.775, precision: 0.7933333333333333, f_beta: 0.7714285714285714
train: step: 470, loss: 0.29889506101608276, acc: 0.7, recall: 0.7, precision: 0.7380952380952381, f_beta: 0.6875
train: step: 471, loss: 0.12624971568584442, acc: 0.85, recall: 0.85, precision: 0.85, f_beta: 0.85
train: step: 472, loss: 0.19046488404273987, acc: 0.7, recall: 0.7, precision: 0.7666666666666666, f_beta: 0.6799999999999999
train: step: 473, loss: 0.3092198669910431, acc: 0.675, recall: 0.675, precision: 0.6754385964912281, f_beta: 0.6747967479674797
train: step: 474, loss: 0.07932975143194199, acc: 0.95, recall: 0.95, precision: 0.9545454545454546, f_beta: 0.949874686716792
train: step: 475, loss: 0.33557623624801636, acc: 0.6, recall: 0.6, precision: 0.7777777777777778, f_beta: 0.5238095238095238
train: step: 476, loss: 0.15506967902183533, acc: 0.825, recall: 0.825, precision: 0.8703703703703703, f_beta: 0.819471308833011
train: step: 477, loss: 0.06518779695034027, acc: 0.95, recall: 0.95, precision: 0.95, f_beta: 0.9500000000000001
train: step: 478, loss: 0.30539554357528687, acc: 0.4, recall: 0.4, precision: 0.3901098901098901, f_beta: 0.38618925831202044
train: step: 479, loss: 0.16408637166023254, acc: 0.85, recall: 0.85, precision: 0.8645833333333333, f_beta: 0.8484848484848484
train: step: 480, loss: 0.3142857551574707, acc: 0.875, recall: 0.875, precision: 0.8836317135549872, f_beta: 0.8742928975487115
train: step: 481, loss: 0.19713826477527618, acc: 0.8, recall: 0.8, precision: 0.8, f_beta: 0.8000000000000002
train: step: 482, loss: 0.09680978953838348, acc: 0.925, recall: 0.925, precision: 0.9347826086956521, f_beta: 0.9245757385292268
train: step: 483, loss: 0.1984490156173706, acc: 0.7, recall: 0.7, precision: 0.7083333333333333, f_beta: 0.6969696969696968
train: step: 484, loss: 0.10011627525091171, acc: 0.9, recall: 0.9, precision: 0.9, f_beta: 0.9
train: step: 485, loss: 0.14932414889335632, acc: 0.825, recall: 0.825, precision: 0.8466666666666667, f_beta: 0.8222222222222222
train: step: 486, loss: 0.10116367042064667, acc: 0.9, recall: 0.9, precision: 0.9, f_beta: 0.9
train: step: 487, loss: 0.3068167269229889, acc: 0.8, recall: 0.8, precision: 0.8125, f_beta: 0.797979797979798
train: step: 488, loss: 0.11293532699346542, acc: 0.875, recall: 0.875, precision: 0.8759398496240601, f_beta: 0.8749218261413383
train: step: 489, loss: 0.17680040001869202, acc: 0.825, recall: 0.825, precision: 0.8324808184143222, f_beta: 0.824010056568196
train: step: 490, loss: 0.167006254196167, acc: 0.85, recall: 0.8500000000000001, precision: 0.8535353535353536, f_beta: 0.849624060150376
train: step: 491, loss: 0.12502847611904144, acc: 0.8, recall: 0.8, precision: 0.8, f_beta: 0.8000000000000002
train: step: 492, loss: 0.22840848565101624, acc: 0.675, recall: 0.675, precision: 0.803030303030303, f_beta: 0.636617749825297
train: step: 493, loss: 0.12608930468559265, acc: 0.85, recall: 0.85, precision: 0.85, f_beta: 0.85
train: step: 494, loss: 0.08447194844484329, acc: 0.875, recall: 0.875, precision: 0.8759398496240601, f_beta: 0.8749218261413383
train: step: 495, loss: 0.08207888901233673, acc: 0.9, recall: 0.8999999999999999, precision: 0.904040404040404, f_beta: 0.899749373433584
train: step: 496, loss: 0.15127351880073547, acc: 0.85, recall: 0.8500000000000001, precision: 0.8535353535353536, f_beta: 0.849624060150376
train: step: 497, loss: 0.1345243901014328, acc: 0.825, recall: 0.825, precision: 0.8258145363408521, f_beta: 0.8248905565978737
train: step: 498, loss: 0.1427612602710724, acc: 0.825, recall: 0.825, precision: 0.8324808184143222, f_beta: 0.824010056568196
train: step: 499, loss: 0.17661885917186737, acc: 0.75, recall: 0.75, precision: 0.7604166666666667, f_beta: 0.7474747474747475
train: step: 500, loss: 0.18262378871440887, acc: 0.725, recall: 0.725, precision: 0.7821316614420062, f_beta: 0.7103357472021066
train: step: 501, loss: 0.29240620136260986, acc: 0.825, recall: 0.825, precision: 0.8466666666666667, f_beta: 0.8222222222222222
train: step: 502, loss: 0.3139709532260895, acc: 0.65, recall: 0.65, precision: 0.6648351648351649, f_beta: 0.6419437340153452
train: step: 503, loss: 0.11968643963336945, acc: 0.825, recall: 0.825, precision: 0.8258145363408521, f_beta: 0.8248905565978737
train: step: 504, loss: 0.08711223304271698, acc: 0.9, recall: 0.8999999999999999, precision: 0.904040404040404, f_beta: 0.899749373433584
train: step: 505, loss: 0.17824392020702362, acc: 0.775, recall: 0.775, precision: 0.7933333333333333, f_beta: 0.7714285714285714
train: step: 506, loss: 0.1162034422159195, acc: 0.85, recall: 0.8500000000000001, precision: 0.8535353535353536, f_beta: 0.849624060150376
train: step: 507, loss: 0.3211623430252075, acc: 0.775, recall: 0.7749999999999999, precision: 0.8133903133903134, f_beta: 0.7678916827853
train: step: 508, loss: 0.15944458544254303, acc: 0.825, recall: 0.825, precision: 0.8324808184143222, f_beta: 0.824010056568196
train: step: 509, loss: 0.10714223235845566, acc: 0.875, recall: 0.875, precision: 0.8759398496240601, f_beta: 0.8749218261413383
train: step: 510, loss: 0.13560530543327332, acc: 0.85, recall: 0.85, precision: 0.8645833333333333, f_beta: 0.8484848484848484
train: step: 511, loss: 0.25194016098976135, acc: 0.6, recall: 0.6000000000000001, precision: 0.6190476190476191, f_beta: 0.5833333333333333
train: step: 512, loss: 0.18824689090251923, acc: 0.7, recall: 0.7, precision: 0.702020202020202, f_beta: 0.6992481203007519
train: step: 513, loss: 0.18081238865852356, acc: 0.7, recall: 0.7, precision: 0.7380952380952381, f_beta: 0.6875
train: step: 514, loss: 0.20695407688617706, acc: 0.675, recall: 0.675, precision: 0.6866666666666666, f_beta: 0.6698412698412699
train: step: 515, loss: 0.1181422621011734, acc: 0.825, recall: 0.825, precision: 0.8703703703703703, f_beta: 0.819471308833011
train: step: 516, loss: 0.12149207293987274, acc: 0.85, recall: 0.85, precision: 0.8645833333333333, f_beta: 0.8484848484848484
train: step: 517, loss: 0.12728193402290344, acc: 0.85, recall: 0.85, precision: 0.8846153846153846, f_beta: 0.8465473145780051
train: step: 518, loss: 0.21706919372081757, acc: 0.675, recall: 0.675, precision: 0.6994301994301995, f_beta: 0.6647324306898775
train: step: 519, loss: 0.2209082543849945, acc: 0.675, recall: 0.675, precision: 0.6754385964912281, f_beta: 0.6747967479674797
train: step: 520, loss: 0.3029767572879791, acc: 0.65, recall: 0.65, precision: 0.6785714285714286, f_beta: 0.6354166666666667
train: step: 521, loss: 0.1155940517783165, acc: 0.85, recall: 0.8500000000000001, precision: 0.8535353535353536, f_beta: 0.849624060150376
train: step: 522, loss: 0.1853109896183014, acc: 0.775, recall: 0.7749999999999999, precision: 0.7813299232736572, f_beta: 0.7737272155876807
train: step: 523, loss: 0.12895484268665314, acc: 0.875, recall: 0.875, precision: 0.9, f_beta: 0.873015873015873
train: step: 524, loss: 0.14876337349414825, acc: 0.775, recall: 0.775, precision: 0.7933333333333333, f_beta: 0.7714285714285714
train: step: 525, loss: 0.16467148065567017, acc: 0.925, recall: 0.925, precision: 0.9260651629072681, f_beta: 0.924953095684803
train: step: 526, loss: 0.2321702241897583, acc: 0.725, recall: 0.7250000000000001, precision: 0.7301790281329923, f_beta: 0.7234443746071653
train: step: 527, loss: 0.2289310246706009, acc: 0.6, recall: 0.6, precision: 0.6041666666666667, f_beta: 0.595959595959596
train: step: 528, loss: 0.13190901279449463, acc: 0.9, recall: 0.8999999999999999, precision: 0.904040404040404, f_beta: 0.899749373433584
train: step: 529, loss: 0.19610938429832458, acc: 0.775, recall: 0.7749999999999999, precision: 0.7813299232736572, f_beta: 0.7737272155876807
train: step: 530, loss: 0.0894264429807663, acc: 0.925, recall: 0.925, precision: 0.9260651629072681, f_beta: 0.924953095684803
train: step: 531, loss: 0.20123538374900818, acc: 0.725, recall: 0.7250000000000001, precision: 0.7301790281329923, f_beta: 0.7234443746071653
train: step: 532, loss: 0.08540612459182739, acc: 0.875, recall: 0.875, precision: 0.8836317135549872, f_beta: 0.8742928975487115
train: step: 533, loss: 0.23489904403686523, acc: 0.6, recall: 0.6, precision: 0.6, f_beta: 0.6
train: step: 534, loss: 0.1321951001882553, acc: 0.875, recall: 0.875, precision: 0.8836317135549872, f_beta: 0.8742928975487115
train: step: 535, loss: 0.14224977791309357, acc: 0.8, recall: 0.8, precision: 0.8125, f_beta: 0.797979797979798
train: step: 536, loss: 0.2496141940355301, acc: 0.75, recall: 0.75, precision: 0.7747252747252746, f_beta: 0.7442455242966751
train: step: 537, loss: 0.1206517219543457, acc: 0.825, recall: 0.825, precision: 0.8324808184143222, f_beta: 0.824010056568196
train: step: 538, loss: 0.1316005140542984, acc: 0.825, recall: 0.825, precision: 0.8258145363408521, f_beta: 0.8248905565978737
train: step: 539, loss: 0.19649679958820343, acc: 0.825, recall: 0.825, precision: 0.8258145363408521, f_beta: 0.8248905565978737
train: step: 540, loss: 0.1057879775762558, acc: 0.875, recall: 0.875, precision: 0.9, f_beta: 0.873015873015873
train: step: 541, loss: 0.211394265294075, acc: 0.825, recall: 0.825, precision: 0.8466666666666667, f_beta: 0.8222222222222222
train: step: 542, loss: 0.1273920238018036, acc: 0.8, recall: 0.8, precision: 0.8296703296703296, f_beta: 0.7953964194373402
train: step: 543, loss: 0.07147535681724548, acc: 0.9, recall: 0.9, precision: 0.9166666666666667, f_beta: 0.898989898989899
train: step: 544, loss: 0.1274179369211197, acc: 0.875, recall: 0.875, precision: 0.9, f_beta: 0.873015873015873
train: step: 545, loss: 0.20563773810863495, acc: 0.7, recall: 0.7, precision: 0.702020202020202, f_beta: 0.6992481203007519
train: step: 546, loss: 0.11418917030096054, acc: 0.925, recall: 0.925, precision: 0.9260651629072681, f_beta: 0.924953095684803
train: step: 547, loss: 0.2746677100658417, acc: 0.675, recall: 0.675, precision: 0.6866666666666666, f_beta: 0.6698412698412699
train: step: 548, loss: 0.12258510291576385, acc: 0.8, recall: 0.8, precision: 0.803030303030303, f_beta: 0.7994987468671679
train: step: 549, loss: 0.2043837606906891, acc: 0.85, recall: 0.85, precision: 0.8846153846153846, f_beta: 0.8465473145780051
train: step: 550, loss: 0.260670006275177, acc: 0.8, recall: 0.8, precision: 0.8125, f_beta: 0.797979797979798
train: step: 551, loss: 0.16045856475830078, acc: 0.875, recall: 0.875, precision: 0.9, f_beta: 0.873015873015873
train: step: 552, loss: 0.18061339855194092, acc: 0.725, recall: 0.725, precision: 0.7255639097744361, f_beta: 0.7248280175109443
train: step: 553, loss: 0.15951380133628845, acc: 0.8, recall: 0.8, precision: 0.8125, f_beta: 0.797979797979798
train: step: 554, loss: 0.10260043293237686, acc: 0.9, recall: 0.9, precision: 0.9166666666666667, f_beta: 0.898989898989899
train: step: 555, loss: 0.12412242591381073, acc: 0.825, recall: 0.825, precision: 0.8703703703703703, f_beta: 0.819471308833011
train: step: 556, loss: 0.07572996616363525, acc: 0.9, recall: 0.9, precision: 0.9, f_beta: 0.9
train: step: 557, loss: 0.19599387049674988, acc: 0.675, recall: 0.675, precision: 0.6754385964912281, f_beta: 0.6747967479674797
train: step: 558, loss: 0.17582152783870697, acc: 0.775, recall: 0.7749999999999999, precision: 0.8133903133903134, f_beta: 0.7678916827853
train: step: 559, loss: 0.19668349623680115, acc: 0.775, recall: 0.7749999999999999, precision: 0.7813299232736572, f_beta: 0.7737272155876807
train: step: 560, loss: 0.20248183608055115, acc: 0.8, recall: 0.8, precision: 0.8571428571428572, f_beta: 0.7916666666666665
train: step: 561, loss: 0.18901361525058746, acc: 0.75, recall: 0.75, precision: 0.7604166666666667, f_beta: 0.7474747474747475
train: step: 562, loss: 0.15014702081680298, acc: 0.9, recall: 0.8999999999999999, precision: 0.904040404040404, f_beta: 0.899749373433584
train: step: 563, loss: 0.09497882425785065, acc: 0.875, recall: 0.875, precision: 0.8836317135549872, f_beta: 0.8742928975487115
train: step: 564, loss: 0.1942710131406784, acc: 0.675, recall: 0.675, precision: 0.6994301994301995, f_beta: 0.6647324306898775
train: step: 565, loss: 0.1647120863199234, acc: 0.75, recall: 0.75, precision: 0.7525252525252526, f_beta: 0.7493734335839599
train: step: 566, loss: 0.14095452427864075, acc: 0.825, recall: 0.825, precision: 0.8466666666666667, f_beta: 0.8222222222222222
train: step: 567, loss: 0.3030416965484619, acc: 0.725, recall: 0.725, precision: 0.74, f_beta: 0.7206349206349207
train: step: 568, loss: 0.215990349650383, acc: 0.825, recall: 0.825, precision: 0.8258145363408521, f_beta: 0.8248905565978737
train: step: 569, loss: 0.22283658385276794, acc: 0.725, recall: 0.7250000000000001, precision: 0.7564102564102564, f_beta: 0.7163120567375887
train: step: 570, loss: 0.11372072994709015, acc: 0.8, recall: 0.8, precision: 0.8125, f_beta: 0.797979797979798
train: step: 571, loss: 0.10600559413433075, acc: 0.85, recall: 0.85, precision: 0.8645833333333333, f_beta: 0.8484848484848484
train: step: 572, loss: 0.10312924534082413, acc: 0.875, recall: 0.875, precision: 0.8836317135549872, f_beta: 0.8742928975487115
train: step: 573, loss: 0.1416521519422531, acc: 0.8, recall: 0.8, precision: 0.8571428571428572, f_beta: 0.7916666666666665
train: step: 574, loss: 0.07939416170120239, acc: 0.9, recall: 0.8999999999999999, precision: 0.904040404040404, f_beta: 0.899749373433584
train: step: 575, loss: 0.11593680083751678, acc: 0.875, recall: 0.875, precision: 0.8836317135549872, f_beta: 0.8742928975487115
train: step: 576, loss: 0.08594496548175812, acc: 0.9, recall: 0.8999999999999999, precision: 0.904040404040404, f_beta: 0.899749373433584
train: step: 577, loss: 0.04977475851774216, acc: 0.925, recall: 0.925, precision: 0.9260651629072681, f_beta: 0.924953095684803
train: step: 578, loss: 0.39888009428977966, acc: 0.475, recall: 0.475, precision: 0.47333333333333333, f_beta: 0.4666666666666666
train: step: 579, loss: 0.11288082599639893, acc: 0.85, recall: 0.85, precision: 0.8846153846153846, f_beta: 0.8465473145780051
train: step: 580, loss: 0.060000527650117874, acc: 0.95, recall: 0.95, precision: 0.95, f_beta: 0.9500000000000001
train: step: 581, loss: 0.08246739953756332, acc: 0.925, recall: 0.925, precision: 0.9260651629072681, f_beta: 0.924953095684803
train: step: 582, loss: 0.08508860319852829, acc: 0.9, recall: 0.9, precision: 0.9, f_beta: 0.9
train: step: 583, loss: 0.10835807025432587, acc: 0.85, recall: 0.85, precision: 0.85, f_beta: 0.85
train: step: 584, loss: 0.17531810700893402, acc: 0.75, recall: 0.75, precision: 0.7747252747252746, f_beta: 0.7442455242966751
train: step: 585, loss: 0.19931426644325256, acc: 0.75, recall: 0.75, precision: 0.75, f_beta: 0.75
train: step: 586, loss: 0.11730470508337021, acc: 0.875, recall: 0.875, precision: 0.9, f_beta: 0.873015873015873
train: step: 587, loss: 0.18037182092666626, acc: 0.875, recall: 0.875, precision: 0.8759398496240601, f_beta: 0.8749218261413383
train: step: 588, loss: 0.10406462848186493, acc: 0.9, recall: 0.9, precision: 0.9, f_beta: 0.9
train: step: 589, loss: 0.29643410444259644, acc: 0.6, recall: 0.6, precision: 0.6098901098901099, f_beta: 0.5907928388746804
train: step: 590, loss: 0.155551940202713, acc: 0.925, recall: 0.925, precision: 0.9347826086956521, f_beta: 0.9245757385292268
train: step: 591, loss: 0.09649515151977539, acc: 0.875, recall: 0.875, precision: 0.8836317135549872, f_beta: 0.8742928975487115
train: step: 592, loss: 0.18500098586082458, acc: 0.7, recall: 0.7, precision: 0.7197802197802198, f_beta: 0.6930946291560103
train: step: 593, loss: 0.36826521158218384, acc: 0.45, recall: 0.45, precision: 0.23684210526315788, f_beta: 0.3103448275862069
train: step: 594, loss: 0.20890040695667267, acc: 0.7, recall: 0.7, precision: 0.7380952380952381, f_beta: 0.6875
train: step: 595, loss: 0.0365338996052742, acc: 1.0, recall: 1.0, precision: 1.0, f_beta: 1.0
train: step: 596, loss: 0.15497906506061554, acc: 0.85, recall: 0.8500000000000001, precision: 0.8535353535353536, f_beta: 0.849624060150376
train: step: 597, loss: 0.11302857100963593, acc: 0.9, recall: 0.9, precision: 0.9166666666666667, f_beta: 0.898989898989899
train: step: 598, loss: 0.08057893067598343, acc: 0.95, recall: 0.95, precision: 0.9545454545454546, f_beta: 0.949874686716792
train: step: 599, loss: 0.14309129118919373, acc: 0.9, recall: 0.9, precision: 0.9, f_beta: 0.9
train: step: 600, loss: 0.13712820410728455, acc: 0.85, recall: 0.85, precision: 0.8645833333333333, f_beta: 0.8484848484848484
train: step: 601, loss: 0.23736698925495148, acc: 0.725, recall: 0.725, precision: 0.7821316614420062, f_beta: 0.7103357472021066
train: step: 602, loss: 0.12008485943078995, acc: 0.85, recall: 0.8500000000000001, precision: 0.8535353535353536, f_beta: 0.849624060150376
train: step: 603, loss: 0.32947736978530884, acc: 0.775, recall: 0.775, precision: 0.7756892230576441, f_beta: 0.774859287054409
train: step: 604, loss: 0.10783135890960693, acc: 0.85, recall: 0.85, precision: 0.85, f_beta: 0.85
train: step: 605, loss: 0.18394720554351807, acc: 0.7, recall: 0.7, precision: 0.7666666666666666, f_beta: 0.6799999999999999
train: step: 606, loss: 0.1514652818441391, acc: 0.8, recall: 0.8, precision: 0.8125, f_beta: 0.797979797979798
train: step: 607, loss: 0.09855800867080688, acc: 0.875, recall: 0.875, precision: 0.8836317135549872, f_beta: 0.8742928975487115
train: step: 608, loss: 0.18289580941200256, acc: 0.75, recall: 0.75, precision: 0.7525252525252526, f_beta: 0.7493734335839599
train: step: 609, loss: 0.13537411391735077, acc: 0.8, recall: 0.8, precision: 0.8296703296703296, f_beta: 0.7953964194373402
train: step: 610, loss: 0.1901436299085617, acc: 0.75, recall: 0.75, precision: 0.7525252525252526, f_beta: 0.7493734335839599
train: step: 611, loss: 0.28172075748443604, acc: 0.65, recall: 0.6499999999999999, precision: 0.6515151515151515, f_beta: 0.6491228070175439
train: step: 612, loss: 0.2505265176296234, acc: 0.625, recall: 0.625, precision: 0.6792114695340502, f_beta: 0.5943204868154157
train: step: 613, loss: 0.06486596167087555, acc: 0.9, recall: 0.8999999999999999, precision: 0.904040404040404, f_beta: 0.899749373433584
train: step: 614, loss: 0.12648291885852814, acc: 0.875, recall: 0.875, precision: 0.8759398496240601, f_beta: 0.8749218261413383
train: step: 615, loss: 0.10886310040950775, acc: 0.85, recall: 0.85, precision: 0.8645833333333333, f_beta: 0.8484848484848484
train: step: 616, loss: 0.08636955916881561, acc: 0.9, recall: 0.9, precision: 0.9166666666666667, f_beta: 0.898989898989899
train: step: 617, loss: 0.3241090178489685, acc: 0.875, recall: 0.875, precision: 0.8836317135549872, f_beta: 0.8742928975487115
train: step: 618, loss: 0.11134984344244003, acc: 0.925, recall: 0.925, precision: 0.9260651629072681, f_beta: 0.924953095684803
train: step: 619, loss: 0.09937427937984467, acc: 0.9, recall: 0.9, precision: 0.9166666666666667, f_beta: 0.898989898989899
train: step: 620, loss: 0.09594374150037766, acc: 0.925, recall: 0.925, precision: 0.9347826086956521, f_beta: 0.9245757385292268
train: step: 621, loss: 0.14135675132274628, acc: 0.825, recall: 0.825, precision: 0.8258145363408521, f_beta: 0.8248905565978737
train: step: 622, loss: 0.22178050875663757, acc: 0.725, recall: 0.725, precision: 0.7255639097744361, f_beta: 0.7248280175109443
train: step: 623, loss: 0.07408478111028671, acc: 0.95, recall: 0.95, precision: 0.9545454545454546, f_beta: 0.949874686716792
train: step: 624, loss: 0.17314250767230988, acc: 0.825, recall: 0.825, precision: 0.8258145363408521, f_beta: 0.8248905565978737
train: step: 625, loss: 0.20725607872009277, acc: 0.675, recall: 0.675, precision: 0.719435736677116, f_beta: 0.6576695194206715
train: step: 626, loss: 0.07465250790119171, acc: 0.875, recall: 0.875, precision: 0.8759398496240601, f_beta: 0.8749218261413383
train: step: 627, loss: 0.07209251075983047, acc: 0.9, recall: 0.9, precision: 0.9, f_beta: 0.9
train: step: 628, loss: 0.24824802577495575, acc: 0.65, recall: 0.65, precision: 0.6648351648351649, f_beta: 0.6419437340153452
train: step: 629, loss: 0.15263821184635162, acc: 0.775, recall: 0.7749999999999999, precision: 0.8133903133903134, f_beta: 0.7678916827853
train: step: 630, loss: 0.13929401338100433, acc: 0.8, recall: 0.8, precision: 0.803030303030303, f_beta: 0.7994987468671679
train: step: 631, loss: 0.14059515297412872, acc: 0.85, recall: 0.85, precision: 0.8645833333333333, f_beta: 0.8484848484848484
train: step: 632, loss: 0.11850030720233917, acc: 0.825, recall: 0.825, precision: 0.8258145363408521, f_beta: 0.8248905565978737
train: step: 633, loss: 0.07623694837093353, acc: 0.925, recall: 0.925, precision: 0.9260651629072681, f_beta: 0.924953095684803
train: step: 634, loss: 0.09817232191562653, acc: 0.95, recall: 0.95, precision: 0.9545454545454546, f_beta: 0.949874686716792
train: step: 635, loss: 0.27630022168159485, acc: 0.925, recall: 0.925, precision: 0.9260651629072681, f_beta: 0.924953095684803
train: step: 636, loss: 0.08047933876514435, acc: 0.875, recall: 0.875, precision: 0.8759398496240601, f_beta: 0.8749218261413383
train: step: 637, loss: 0.14864055812358856, acc: 0.775, recall: 0.775, precision: 0.7756892230576441, f_beta: 0.774859287054409
train: step: 638, loss: 0.08590809255838394, acc: 0.9, recall: 0.9, precision: 0.9, f_beta: 0.9
train: step: 639, loss: 0.16089682281017303, acc: 0.85, recall: 0.85, precision: 0.8645833333333333, f_beta: 0.8484848484848484
train: step: 640, loss: 0.04448772966861725, acc: 0.95, recall: 0.95, precision: 0.9545454545454546, f_beta: 0.949874686716792
train: step: 641, loss: 0.11486241966485977, acc: 0.875, recall: 0.875, precision: 0.8759398496240601, f_beta: 0.8749218261413383
train: step: 642, loss: 0.08374255150556564, acc: 0.875, recall: 0.875, precision: 0.8759398496240601, f_beta: 0.8749218261413383
train: step: 643, loss: 0.2101401388645172, acc: 0.65, recall: 0.65, precision: 0.65625, f_beta: 0.6464646464646464
train: step: 644, loss: 0.08356187492609024, acc: 0.9, recall: 0.8999999999999999, precision: 0.904040404040404, f_beta: 0.899749373433584
train: step: 645, loss: 0.16048358380794525, acc: 0.85, recall: 0.85, precision: 0.8645833333333333, f_beta: 0.8484848484848484
train: step: 646, loss: 0.1266004741191864, acc: 0.85, recall: 0.85, precision: 0.8645833333333333, f_beta: 0.8484848484848484
train: step: 647, loss: 0.09158659726381302, acc: 0.9, recall: 0.9, precision: 0.9166666666666667, f_beta: 0.898989898989899
train: step: 648, loss: 0.27728089690208435, acc: 0.675, recall: 0.675, precision: 0.7508960573476702, f_beta: 0.6484110885733605
train: step: 649, loss: 0.13970914483070374, acc: 0.875, recall: 0.875, precision: 0.8836317135549872, f_beta: 0.8742928975487115
train: step: 650, loss: 0.08346936851739883, acc: 0.925, recall: 0.925, precision: 0.9260651629072681, f_beta: 0.924953095684803
train: step: 651, loss: 0.193690225481987, acc: 0.775, recall: 0.7749999999999999, precision: 0.7813299232736572, f_beta: 0.7737272155876807
train: step: 652, loss: 0.18141400814056396, acc: 0.8, recall: 0.8, precision: 0.8125, f_beta: 0.797979797979798
train: step: 653, loss: 0.25910431146621704, acc: 0.75, recall: 0.75, precision: 0.7604166666666667, f_beta: 0.7474747474747475
train: step: 654, loss: 0.3006749451160431, acc: 0.775, recall: 0.775, precision: 0.7756892230576441, f_beta: 0.774859287054409
train: step: 655, loss: 0.07457499951124191, acc: 0.9, recall: 0.8999999999999999, precision: 0.904040404040404, f_beta: 0.899749373433584
train: step: 656, loss: 0.09457608312368393, acc: 0.875, recall: 0.875, precision: 0.8759398496240601, f_beta: 0.8749218261413383
train: step: 657, loss: 0.1636088341474533, acc: 0.725, recall: 0.7250000000000001, precision: 0.7301790281329923, f_beta: 0.7234443746071653
train: step: 658, loss: 0.1117967814207077, acc: 0.875, recall: 0.875, precision: 0.8836317135549872, f_beta: 0.8742928975487115
train: step: 659, loss: 0.08183713257312775, acc: 0.875, recall: 0.875, precision: 0.8759398496240601, f_beta: 0.8749218261413383
train: step: 660, loss: 0.19722643494606018, acc: 0.775, recall: 0.7749999999999999, precision: 0.7813299232736572, f_beta: 0.7737272155876807
train: step: 661, loss: 0.19055584073066711, acc: 0.825, recall: 0.825, precision: 0.8258145363408521, f_beta: 0.8248905565978737
train: step: 662, loss: 0.08473007380962372, acc: 0.875, recall: 0.875, precision: 0.8759398496240601, f_beta: 0.8749218261413383
train: step: 663, loss: 0.1553339660167694, acc: 0.8, recall: 0.8, precision: 0.8, f_beta: 0.8000000000000002
train: step: 664, loss: 0.12175238132476807, acc: 0.875, recall: 0.875, precision: 0.8759398496240601, f_beta: 0.8749218261413383
train: step: 665, loss: 0.1930748075246811, acc: 0.925, recall: 0.925, precision: 0.9260651629072681, f_beta: 0.924953095684803
train: step: 666, loss: 0.13916027545928955, acc: 0.85, recall: 0.8500000000000001, precision: 0.8535353535353536, f_beta: 0.849624060150376
train: step: 667, loss: 0.11634068191051483, acc: 0.875, recall: 0.875, precision: 0.8836317135549872, f_beta: 0.8742928975487115
train: step: 668, loss: 0.08145536482334137, acc: 0.925, recall: 0.925, precision: 0.9260651629072681, f_beta: 0.924953095684803
train: step: 669, loss: 0.3353675603866577, acc: 0.75, recall: 0.75, precision: 0.7747252747252746, f_beta: 0.7442455242966751
train: step: 670, loss: 0.20645232498645782, acc: 0.725, recall: 0.725, precision: 0.8225806451612903, f_beta: 0.7025016903313049
train: step: 671, loss: 0.08844693005084991, acc: 0.95, recall: 0.95, precision: 0.9545454545454546, f_beta: 0.949874686716792
train: step: 672, loss: 0.18309083580970764, acc: 0.875, recall: 0.875, precision: 0.8836317135549872, f_beta: 0.8742928975487115
train: step: 673, loss: 0.0856679156422615, acc: 0.95, recall: 0.95, precision: 0.9545454545454546, f_beta: 0.949874686716792
train: step: 674, loss: 0.09085551649332047, acc: 0.9, recall: 0.8999999999999999, precision: 0.904040404040404, f_beta: 0.899749373433584
train: step: 675, loss: 0.27911874651908875, acc: 0.725, recall: 0.7250000000000001, precision: 0.7301790281329923, f_beta: 0.7234443746071653
train: step: 676, loss: 0.13572359085083008, acc: 0.8, recall: 0.8, precision: 0.803030303030303, f_beta: 0.7994987468671679
train: step: 677, loss: 0.26469627022743225, acc: 0.775, recall: 0.775, precision: 0.7756892230576441, f_beta: 0.774859287054409
train: step: 678, loss: 0.19670724868774414, acc: 0.7, recall: 0.7, precision: 0.7197802197802198, f_beta: 0.6930946291560103
train: step: 679, loss: 0.17342416942119598, acc: 0.75, recall: 0.75, precision: 0.7976190476190477, f_beta: 0.7395833333333333
train: step: 680, loss: 0.1586293876171112, acc: 0.775, recall: 0.7749999999999999, precision: 0.7813299232736572, f_beta: 0.7737272155876807
train: step: 681, loss: 0.18428845703601837, acc: 0.8, recall: 0.8, precision: 0.8296703296703296, f_beta: 0.7953964194373402
train: step: 682, loss: 0.1002037301659584, acc: 0.9, recall: 0.9, precision: 0.9, f_beta: 0.9
train: step: 683, loss: 0.11807771027088165, acc: 0.9, recall: 0.8999999999999999, precision: 0.904040404040404, f_beta: 0.899749373433584
train: step: 684, loss: 0.16929425299167633, acc: 0.775, recall: 0.775, precision: 0.7756892230576441, f_beta: 0.774859287054409
train: step: 685, loss: 0.147430419921875, acc: 0.8, recall: 0.8, precision: 0.8296703296703296, f_beta: 0.7953964194373402
train: step: 686, loss: 0.15853750705718994, acc: 0.8, recall: 0.8, precision: 0.803030303030303, f_beta: 0.7994987468671679
train: step: 687, loss: 0.12690214812755585, acc: 0.85, recall: 0.8500000000000001, precision: 0.8535353535353536, f_beta: 0.849624060150376
train: step: 688, loss: 0.259318083524704, acc: 0.9, recall: 0.8999999999999999, precision: 0.904040404040404, f_beta: 0.899749373433584
train: step: 689, loss: 0.16970258951187134, acc: 0.75, recall: 0.75, precision: 0.8333333333333333, f_beta: 0.7333333333333334
train: step: 690, loss: 0.07977844774723053, acc: 0.9, recall: 0.9, precision: 0.9166666666666667, f_beta: 0.898989898989899
train: step: 691, loss: 0.2743605971336365, acc: 0.625, recall: 0.625, precision: 0.6567398119122256, f_beta: 0.6050032916392363
train: step: 692, loss: 0.1217954158782959, acc: 0.875, recall: 0.875, precision: 0.8836317135549872, f_beta: 0.8742928975487115
train: step: 693, loss: 0.05743680149316788, acc: 0.925, recall: 0.925, precision: 0.9260651629072681, f_beta: 0.924953095684803
train: step: 694, loss: 0.11851775646209717, acc: 0.825, recall: 0.825, precision: 0.8258145363408521, f_beta: 0.8248905565978737
train: step: 695, loss: 0.10539581626653671, acc: 0.85, recall: 0.8500000000000001, precision: 0.8535353535353536, f_beta: 0.849624060150376
train: step: 696, loss: 0.27994227409362793, acc: 0.85, recall: 0.85, precision: 0.8645833333333333, f_beta: 0.8484848484848484
train: step: 697, loss: 0.1853456199169159, acc: 0.825, recall: 0.825, precision: 0.8466666666666667, f_beta: 0.8222222222222222
train: step: 698, loss: 0.18442009389400482, acc: 0.8, recall: 0.8, precision: 0.8296703296703296, f_beta: 0.7953964194373402
train: step: 699, loss: 0.11348950862884521, acc: 0.85, recall: 0.8500000000000001, precision: 0.8535353535353536, f_beta: 0.849624060150376
train: step: 700, loss: 0.1071215495467186, acc: 0.925, recall: 0.925, precision: 0.9260651629072681, f_beta: 0.924953095684803
train: step: 701, loss: 0.0846000462770462, acc: 0.875, recall: 0.875, precision: 0.8759398496240601, f_beta: 0.8749218261413383
train: step: 702, loss: 0.19167418777942657, acc: 0.825, recall: 0.825, precision: 0.8324808184143222, f_beta: 0.824010056568196
train: step: 703, loss: 0.08468283712863922, acc: 0.95, recall: 0.95, precision: 0.95, f_beta: 0.9500000000000001
train: step: 704, loss: 0.30087390542030334, acc: 0.525, recall: 0.525, precision: 0.7564102564102564, f_beta: 0.38660209846650523
train: step: 705, loss: 0.10770142078399658, acc: 0.875, recall: 0.875, precision: 0.8836317135549872, f_beta: 0.8742928975487115
train: step: 706, loss: 0.11489695310592651, acc: 0.875, recall: 0.875, precision: 0.8836317135549872, f_beta: 0.8742928975487115
train: step: 707, loss: 0.2322501838207245, acc: 0.675, recall: 0.675, precision: 0.7508960573476702, f_beta: 0.6484110885733605
train: step: 708, loss: 0.14979568123817444, acc: 0.8, recall: 0.8, precision: 0.8296703296703296, f_beta: 0.7953964194373402
train: step: 709, loss: 0.1513301134109497, acc: 0.825, recall: 0.825, precision: 0.8258145363408521, f_beta: 0.8248905565978737
train: step: 710, loss: 0.10595685243606567, acc: 0.85, recall: 0.8500000000000001, precision: 0.8535353535353536, f_beta: 0.849624060150376
train: step: 711, loss: 0.05653632804751396, acc: 0.95, recall: 0.95, precision: 0.95, f_beta: 0.9500000000000001
train: step: 712, loss: 0.10243948549032211, acc: 0.9, recall: 0.9, precision: 0.9, f_beta: 0.9
train: step: 713, loss: 0.2402646541595459, acc: 0.825, recall: 0.825, precision: 0.8466666666666667, f_beta: 0.8222222222222222
train: step: 714, loss: 0.09107266366481781, acc: 0.9, recall: 0.8999999999999999, precision: 0.904040404040404, f_beta: 0.899749373433584
train: step: 715, loss: 0.14616969227790833, acc: 0.75, recall: 0.75, precision: 0.75, f_beta: 0.75
train: step: 716, loss: 0.10801907628774643, acc: 0.9, recall: 0.8999999999999999, precision: 0.904040404040404, f_beta: 0.899749373433584
train: step: 717, loss: 0.045943863689899445, acc: 0.95, recall: 0.95, precision: 0.95, f_beta: 0.9500000000000001
train: step: 718, loss: 0.2008407562971115, acc: 0.7, recall: 0.7, precision: 0.7380952380952381, f_beta: 0.6875
train: step: 719, loss: 0.06921558082103729, acc: 0.975, recall: 0.975, precision: 0.9761904761904762, f_beta: 0.9749843652282677
train: step: 720, loss: 0.08042751252651215, acc: 0.875, recall: 0.875, precision: 0.8759398496240601, f_beta: 0.8749218261413383
train: step: 721, loss: 0.3206564784049988, acc: 0.75, recall: 0.75, precision: 0.7747252747252746, f_beta: 0.7442455242966751
train: step: 722, loss: 0.1228531152009964, acc: 0.8, recall: 0.8, precision: 0.8, f_beta: 0.8000000000000002
train: step: 723, loss: 0.15539588034152985, acc: 0.725, recall: 0.725, precision: 0.7255639097744361, f_beta: 0.7248280175109443
train: step: 724, loss: 0.0690578743815422, acc: 0.95, recall: 0.95, precision: 0.95, f_beta: 0.9500000000000001
train: step: 725, loss: 0.11047239601612091, acc: 0.85, recall: 0.8500000000000001, precision: 0.8535353535353536, f_beta: 0.849624060150376
train: step: 726, loss: 0.10441412776708603, acc: 0.85, recall: 0.8500000000000001, precision: 0.8535353535353536, f_beta: 0.849624060150376
train: step: 727, loss: 0.12860266864299774, acc: 0.825, recall: 0.825, precision: 0.8258145363408521, f_beta: 0.8248905565978737
train: step: 728, loss: 0.13379643857479095, acc: 0.825, recall: 0.825, precision: 0.8703703703703703, f_beta: 0.819471308833011
train: step: 729, loss: 0.1592978686094284, acc: 0.775, recall: 0.7749999999999999, precision: 0.7813299232736572, f_beta: 0.7737272155876807
train: step: 730, loss: 0.10939431190490723, acc: 0.9, recall: 0.8999999999999999, precision: 0.904040404040404, f_beta: 0.899749373433584
train: step: 731, loss: 0.11615647375583649, acc: 0.8, recall: 0.8, precision: 0.8125, f_beta: 0.797979797979798
train: step: 732, loss: 0.0825994610786438, acc: 0.9, recall: 0.8999999999999999, precision: 0.904040404040404, f_beta: 0.899749373433584
train: step: 733, loss: 0.21258048713207245, acc: 0.675, recall: 0.675, precision: 0.6790281329923273, f_beta: 0.6731615336266499
train: step: 734, loss: 0.10605494678020477, acc: 0.9, recall: 0.9, precision: 0.9, f_beta: 0.9
train: step: 735, loss: 0.10146671533584595, acc: 0.85, recall: 0.85, precision: 0.8645833333333333, f_beta: 0.8484848484848484
train: step: 736, loss: 0.11254166066646576, acc: 0.85, recall: 0.8500000000000001, precision: 0.8535353535353536, f_beta: 0.849624060150376
train: step: 737, loss: 0.11565204709768295, acc: 0.825, recall: 0.825, precision: 0.8703703703703703, f_beta: 0.819471308833011
train: step: 738, loss: 0.26396507024765015, acc: 0.775, recall: 0.775, precision: 0.8448275862068966, f_beta: 0.7630019749835418
train: step: 739, loss: 0.10722193866968155, acc: 0.875, recall: 0.875, precision: 0.8836317135549872, f_beta: 0.8742928975487115
train: step: 740, loss: 0.1689639836549759, acc: 0.775, recall: 0.775, precision: 0.7756892230576441, f_beta: 0.774859287054409
train: step: 741, loss: 0.05448085814714432, acc: 0.9, recall: 0.9, precision: 0.9, f_beta: 0.9
train: step: 742, loss: 0.09684217721223831, acc: 0.875, recall: 0.875, precision: 0.8836317135549872, f_beta: 0.8742928975487115
train: step: 743, loss: 0.16750934720039368, acc: 0.8, recall: 0.8, precision: 0.8296703296703296, f_beta: 0.7953964194373402
train: step: 744, loss: 0.13024897873401642, acc: 0.775, recall: 0.775, precision: 0.7933333333333333, f_beta: 0.7714285714285714
train: step: 745, loss: 0.07592711597681046, acc: 0.925, recall: 0.925, precision: 0.9260651629072681, f_beta: 0.924953095684803
train: step: 746, loss: 0.10599561035633087, acc: 0.875, recall: 0.875, precision: 0.8759398496240601, f_beta: 0.8749218261413383
train: step: 747, loss: 0.06952647864818573, acc: 0.9, recall: 0.9, precision: 0.9, f_beta: 0.9
train: step: 748, loss: 0.09084149450063705, acc: 0.9, recall: 0.9, precision: 0.9166666666666667, f_beta: 0.898989898989899
train: step: 749, loss: 0.08543793857097626, acc: 0.875, recall: 0.875, precision: 0.9, f_beta: 0.873015873015873
train: step: 750, loss: 0.22260217368602753, acc: 0.8, recall: 0.8, precision: 0.8125, f_beta: 0.797979797979798
train: step: 751, loss: 0.1307486742734909, acc: 0.8, recall: 0.8, precision: 0.8, f_beta: 0.8000000000000002
train: step: 752, loss: 0.08451206982135773, acc: 0.925, recall: 0.925, precision: 0.9260651629072681, f_beta: 0.924953095684803
train: step: 753, loss: 0.04704554006457329, acc: 0.925, recall: 0.925, precision: 0.9260651629072681, f_beta: 0.924953095684803
train: step: 754, loss: 0.17798900604248047, acc: 0.725, recall: 0.725, precision: 0.74, f_beta: 0.7206349206349207
train: step: 755, loss: 0.09438709169626236, acc: 0.925, recall: 0.925, precision: 0.9260651629072681, f_beta: 0.924953095684803
train: step: 756, loss: 0.37704840302467346, acc: 0.75, recall: 0.75, precision: 0.7604166666666667, f_beta: 0.7474747474747475
train: step: 757, loss: 0.10211174190044403, acc: 0.875, recall: 0.875, precision: 0.8759398496240601, f_beta: 0.8749218261413383
train: step: 758, loss: 0.24545994400978088, acc: 0.675, recall: 0.675, precision: 0.6790281329923273, f_beta: 0.6731615336266499
train: step: 759, loss: 0.09632785618305206, acc: 0.875, recall: 0.875, precision: 0.8836317135549872, f_beta: 0.8742928975487115
train: step: 760, loss: 0.1896437704563141, acc: 0.75, recall: 0.75, precision: 0.7747252747252746, f_beta: 0.7442455242966751
train: step: 761, loss: 0.07054394483566284, acc: 0.925, recall: 0.925, precision: 0.9347826086956521, f_beta: 0.9245757385292268
train: step: 762, loss: 0.09015505015850067, acc: 0.85, recall: 0.85, precision: 0.8846153846153846, f_beta: 0.8465473145780051
train: step: 763, loss: 0.08902844786643982, acc: 0.9, recall: 0.9, precision: 0.9, f_beta: 0.9
train: step: 764, loss: 0.08889162540435791, acc: 0.9, recall: 0.9, precision: 0.9166666666666667, f_beta: 0.898989898989899
train: step: 765, loss: 0.20675912499427795, acc: 0.75, recall: 0.75, precision: 0.7604166666666667, f_beta: 0.7474747474747475
train: step: 766, loss: 0.09992349147796631, acc: 0.875, recall: 0.875, precision: 0.8759398496240601, f_beta: 0.8749218261413383
train: step: 767, loss: 0.15211300551891327, acc: 0.85, recall: 0.8500000000000001, precision: 0.8535353535353536, f_beta: 0.849624060150376
train: step: 768, loss: 0.08114717900753021, acc: 0.925, recall: 0.925, precision: 0.9260651629072681, f_beta: 0.924953095684803
train: step: 769, loss: 0.2244986593723297, acc: 0.7, recall: 0.7, precision: 0.7083333333333333, f_beta: 0.6969696969696968
train: step: 770, loss: 0.08299882709980011, acc: 0.925, recall: 0.925, precision: 0.9260651629072681, f_beta: 0.924953095684803
train: step: 771, loss: 0.15993207693099976, acc: 0.8, recall: 0.8, precision: 0.8125, f_beta: 0.797979797979798
train: step: 772, loss: 0.09690191596746445, acc: 0.875, recall: 0.875, precision: 0.8759398496240601, f_beta: 0.8749218261413383
train: step: 773, loss: 0.19258086383342743, acc: 0.875, recall: 0.875, precision: 0.8836317135549872, f_beta: 0.8742928975487115
train: step: 774, loss: 0.1621214896440506, acc: 0.85, recall: 0.85, precision: 0.8846153846153846, f_beta: 0.8465473145780051
train: step: 775, loss: 0.16610074043273926, acc: 0.95, recall: 0.95, precision: 0.95, f_beta: 0.9500000000000001
train: step: 776, loss: 0.1765035092830658, acc: 0.725, recall: 0.725, precision: 0.74, f_beta: 0.7206349206349207
train: step: 777, loss: 0.14565148949623108, acc: 0.775, recall: 0.7749999999999999, precision: 0.8133903133903134, f_beta: 0.7678916827853
train: step: 778, loss: 0.2503627836704254, acc: 0.65, recall: 0.65, precision: 0.6785714285714286, f_beta: 0.6354166666666667
train: step: 779, loss: 0.17282402515411377, acc: 0.725, recall: 0.725, precision: 0.74, f_beta: 0.7206349206349207
train: step: 780, loss: 0.05371851474046707, acc: 0.95, recall: 0.95, precision: 0.9545454545454546, f_beta: 0.949874686716792
train: step: 781, loss: 0.256845086812973, acc: 0.725, recall: 0.725, precision: 0.7255639097744361, f_beta: 0.7248280175109443
train: step: 782, loss: 0.16852323710918427, acc: 0.8, recall: 0.8, precision: 0.8296703296703296, f_beta: 0.7953964194373402
train: step: 783, loss: 0.08549608290195465, acc: 0.925, recall: 0.925, precision: 0.9260651629072681, f_beta: 0.924953095684803
train: step: 784, loss: 0.23837082087993622, acc: 0.625, recall: 0.625, precision: 0.6792114695340502, f_beta: 0.5943204868154157
train: step: 785, loss: 0.1718374490737915, acc: 0.775, recall: 0.7749999999999999, precision: 0.7813299232736572, f_beta: 0.7737272155876807
train: step: 786, loss: 0.2468757927417755, acc: 0.85, recall: 0.8500000000000001, precision: 0.8535353535353536, f_beta: 0.849624060150376
train: step: 787, loss: 0.15554101765155792, acc: 0.75, recall: 0.75, precision: 0.7976190476190477, f_beta: 0.7395833333333333
train: step: 788, loss: 0.21031665802001953, acc: 0.675, recall: 0.675, precision: 0.6866666666666666, f_beta: 0.6698412698412699
train: step: 789, loss: 0.10795547813177109, acc: 0.825, recall: 0.825, precision: 0.8466666666666667, f_beta: 0.8222222222222222
train: step: 790, loss: 0.1614069938659668, acc: 0.775, recall: 0.775, precision: 0.7756892230576441, f_beta: 0.774859287054409
train: step: 791, loss: 0.10455094277858734, acc: 0.85, recall: 0.85, precision: 0.8645833333333333, f_beta: 0.8484848484848484
train: step: 792, loss: 0.08266942203044891, acc: 0.9, recall: 0.9, precision: 0.9166666666666667, f_beta: 0.898989898989899
train: step: 793, loss: 0.09448210895061493, acc: 0.9, recall: 0.8999999999999999, precision: 0.904040404040404, f_beta: 0.899749373433584
train: step: 794, loss: 0.04658083990216255, acc: 0.95, recall: 0.95, precision: 0.95, f_beta: 0.9500000000000001
train: step: 795, loss: 0.06822536140680313, acc: 0.925, recall: 0.925, precision: 0.9347826086956521, f_beta: 0.9245757385292268
train: step: 796, loss: 0.09895174205303192, acc: 0.85, recall: 0.8500000000000001, precision: 0.8535353535353536, f_beta: 0.849624060150376
train: step: 797, loss: 0.14561311900615692, acc: 0.8, recall: 0.8, precision: 0.8125, f_beta: 0.797979797979798
train: step: 798, loss: 0.08211062848567963, acc: 0.9, recall: 0.9, precision: 0.9166666666666667, f_beta: 0.898989898989899
train: step: 799, loss: 0.06689109653234482, acc: 0.95, recall: 0.95, precision: 0.9545454545454546, f_beta: 0.949874686716792
train: step: 800, loss: 0.05278234928846359, acc: 0.95, recall: 0.95, precision: 0.9545454545454546, f_beta: 0.949874686716792
train: step: 801, loss: 0.08230684697628021, acc: 0.9, recall: 0.9, precision: 0.9, f_beta: 0.9
train: step: 802, loss: 0.08966515213251114, acc: 0.875, recall: 0.875, precision: 0.8759398496240601, f_beta: 0.8749218261413383
train: step: 803, loss: 0.08780848979949951, acc: 0.875, recall: 0.875, precision: 0.8759398496240601, f_beta: 0.8749218261413383
train: step: 804, loss: 0.1829667091369629, acc: 0.8, recall: 0.8, precision: 0.8, f_beta: 0.8000000000000002
train: step: 805, loss: 0.08963669836521149, acc: 0.9, recall: 0.9, precision: 0.9, f_beta: 0.9
train: step: 806, loss: 0.21112576127052307, acc: 0.725, recall: 0.7250000000000001, precision: 0.7564102564102564, f_beta: 0.7163120567375887
train: step: 807, loss: 0.1117636188864708, acc: 0.85, recall: 0.8500000000000001, precision: 0.8535353535353536, f_beta: 0.849624060150376
train: step: 808, loss: 0.21381723880767822, acc: 0.725, recall: 0.725, precision: 0.74, f_beta: 0.7206349206349207
train: step: 809, loss: 0.1463041603565216, acc: 0.875, recall: 0.875, precision: 0.8836317135549872, f_beta: 0.8742928975487115
train: step: 810, loss: 0.21212120354175568, acc: 0.725, recall: 0.7250000000000001, precision: 0.7301790281329923, f_beta: 0.7234443746071653
train: step: 811, loss: 0.09916023910045624, acc: 0.875, recall: 0.875, precision: 0.8759398496240601, f_beta: 0.8749218261413383
train: step: 812, loss: 0.10392408072948456, acc: 0.85, recall: 0.85, precision: 0.8645833333333333, f_beta: 0.8484848484848484
train: step: 813, loss: 0.3453625440597534, acc: 0.3, recall: 0.3, precision: 0.3, f_beta: 0.3
train: step: 814, loss: 0.14757977426052094, acc: 0.825, recall: 0.825, precision: 0.8258145363408521, f_beta: 0.8248905565978737
train: step: 815, loss: 0.09794250875711441, acc: 0.925, recall: 0.925, precision: 0.9347826086956521, f_beta: 0.9245757385292268
train: step: 816, loss: 0.16568751633167267, acc: 0.8, recall: 0.8, precision: 0.803030303030303, f_beta: 0.7994987468671679
train: step: 817, loss: 0.201128289103508, acc: 0.675, recall: 0.675, precision: 0.6866666666666666, f_beta: 0.6698412698412699
train: step: 818, loss: 0.05678146332502365, acc: 0.975, recall: 0.975, precision: 0.9761904761904762, f_beta: 0.9749843652282677
train: step: 819, loss: 0.2069474160671234, acc: 0.725, recall: 0.725, precision: 0.8225806451612903, f_beta: 0.7025016903313049
train: step: 820, loss: 0.0986635833978653, acc: 0.9, recall: 0.9, precision: 0.9166666666666667, f_beta: 0.898989898989899
train: step: 821, loss: 0.16197220981121063, acc: 0.75, recall: 0.75, precision: 0.75, f_beta: 0.75
train: step: 822, loss: 0.27463236451148987, acc: 0.85, recall: 0.85, precision: 0.8645833333333333, f_beta: 0.8484848484848484
train: step: 823, loss: 0.2661663889884949, acc: 0.6, recall: 0.6, precision: 0.6333333333333333, f_beta: 0.5733333333333334
train: step: 824, loss: 0.15551678836345673, acc: 0.825, recall: 0.825, precision: 0.8466666666666667, f_beta: 0.8222222222222222
train: step: 825, loss: 0.13907918334007263, acc: 0.825, recall: 0.825, precision: 0.8466666666666667, f_beta: 0.8222222222222222
train: step: 826, loss: 0.1382019817829132, acc: 0.8, recall: 0.8, precision: 0.803030303030303, f_beta: 0.7994987468671679
train: step: 827, loss: 0.17161349952220917, acc: 0.8, recall: 0.8, precision: 0.8296703296703296, f_beta: 0.7953964194373402
train: step: 828, loss: 0.153469055891037, acc: 0.825, recall: 0.825, precision: 0.8466666666666667, f_beta: 0.8222222222222222
train: step: 829, loss: 0.22177577018737793, acc: 0.725, recall: 0.725, precision: 0.74, f_beta: 0.7206349206349207
train: step: 830, loss: 0.17305955290794373, acc: 0.775, recall: 0.775, precision: 0.7933333333333333, f_beta: 0.7714285714285714
train: step: 831, loss: 0.0732535570859909, acc: 0.95, recall: 0.95, precision: 0.95, f_beta: 0.9500000000000001
train: step: 832, loss: 0.3177333474159241, acc: 0.425, recall: 0.42500000000000004, precision: 0.4059561128526646, f_beta: 0.39433838051349573
train: step: 833, loss: 0.16235730051994324, acc: 0.825, recall: 0.825, precision: 0.8258145363408521, f_beta: 0.8248905565978737
train: step: 834, loss: 0.17514951527118683, acc: 0.75, recall: 0.75, precision: 0.7747252747252746, f_beta: 0.7442455242966751
train: step: 835, loss: 0.08137079328298569, acc: 0.925, recall: 0.925, precision: 0.9347826086956521, f_beta: 0.9245757385292268
train: step: 836, loss: 0.12039001286029816, acc: 0.875, recall: 0.875, precision: 0.8759398496240601, f_beta: 0.8749218261413383
train: step: 837, loss: 0.09216366708278656, acc: 0.9, recall: 0.9, precision: 0.9, f_beta: 0.9
train: step: 838, loss: 0.19594553112983704, acc: 0.725, recall: 0.725, precision: 0.7821316614420062, f_beta: 0.7103357472021066
train: step: 839, loss: 0.1831781566143036, acc: 0.825, recall: 0.825, precision: 0.8466666666666667, f_beta: 0.8222222222222222
train: step: 840, loss: 0.12759777903556824, acc: 0.825, recall: 0.825, precision: 0.8258145363408521, f_beta: 0.8248905565978737
train: step: 841, loss: 0.20975367724895477, acc: 0.775, recall: 0.7749999999999999, precision: 0.7813299232736572, f_beta: 0.7737272155876807
train: step: 842, loss: 0.1855706423521042, acc: 0.75, recall: 0.75, precision: 0.7976190476190477, f_beta: 0.7395833333333333
train: step: 843, loss: 0.05838245898485184, acc: 0.95, recall: 0.95, precision: 0.9545454545454546, f_beta: 0.949874686716792
train: step: 844, loss: 0.11449261009693146, acc: 0.9, recall: 0.8999999999999999, precision: 0.904040404040404, f_beta: 0.899749373433584
train: step: 845, loss: 0.20819811522960663, acc: 0.675, recall: 0.675, precision: 0.719435736677116, f_beta: 0.6576695194206715
train: step: 846, loss: 0.0968589335680008, acc: 0.875, recall: 0.875, precision: 0.8836317135549872, f_beta: 0.8742928975487115
train: step: 847, loss: 0.07994972169399261, acc: 0.9, recall: 0.9, precision: 0.9166666666666667, f_beta: 0.898989898989899
train: step: 848, loss: 0.09045170247554779, acc: 0.925, recall: 0.925, precision: 0.9347826086956521, f_beta: 0.9245757385292268
train: step: 849, loss: 0.1595529168844223, acc: 0.775, recall: 0.775, precision: 0.8448275862068966, f_beta: 0.7630019749835418
train: step: 850, loss: 0.1002546101808548, acc: 0.875, recall: 0.875, precision: 0.9, f_beta: 0.873015873015873
train: step: 851, loss: 0.04821252077817917, acc: 0.95, recall: 0.95, precision: 0.95, f_beta: 0.9500000000000001
train: step: 852, loss: 0.1487962305545807, acc: 0.85, recall: 0.8500000000000001, precision: 0.8535353535353536, f_beta: 0.849624060150376
train: step: 853, loss: 0.16001203656196594, acc: 0.8, recall: 0.8, precision: 0.803030303030303, f_beta: 0.7994987468671679
train: step: 854, loss: 0.18203045427799225, acc: 0.7, recall: 0.7, precision: 0.7380952380952381, f_beta: 0.6875
train: step: 855, loss: 0.2510542869567871, acc: 0.85, recall: 0.85, precision: 0.8846153846153846, f_beta: 0.8465473145780051
train: step: 856, loss: 0.11636557430028915, acc: 0.85, recall: 0.8500000000000001, precision: 0.8535353535353536, f_beta: 0.849624060150376
train: step: 857, loss: 0.09930366277694702, acc: 0.9, recall: 0.9, precision: 0.9, f_beta: 0.9
train: step: 858, loss: 0.09677980840206146, acc: 0.925, recall: 0.925, precision: 0.9260651629072681, f_beta: 0.924953095684803
train: step: 859, loss: 0.11220495402812958, acc: 0.875, recall: 0.875, precision: 0.8836317135549872, f_beta: 0.8742928975487115
train: step: 860, loss: 0.10577491670846939, acc: 0.875, recall: 0.875, precision: 0.8836317135549872, f_beta: 0.8742928975487115
train: step: 861, loss: 0.08067851513624191, acc: 0.9, recall: 0.8999999999999999, precision: 0.904040404040404, f_beta: 0.899749373433584
train: step: 862, loss: 0.06984337419271469, acc: 0.9, recall: 0.9, precision: 0.9166666666666667, f_beta: 0.898989898989899
train: step: 863, loss: 0.11166024208068848, acc: 0.875, recall: 0.875, precision: 0.9, f_beta: 0.873015873015873
train: step: 864, loss: 0.09497850388288498, acc: 0.875, recall: 0.875, precision: 0.8759398496240601, f_beta: 0.8749218261413383
train: step: 865, loss: 0.16250599920749664, acc: 0.75, recall: 0.75, precision: 0.7525252525252526, f_beta: 0.7493734335839599
train: step: 866, loss: 0.291486918926239, acc: 0.575, recall: 0.575, precision: 0.5854700854700855, f_beta: 0.5615731785944552
train: step: 867, loss: 0.1723935455083847, acc: 0.75, recall: 0.75, precision: 0.7525252525252526, f_beta: 0.7493734335839599
train: step: 868, loss: 0.1315268725156784, acc: 0.85, recall: 0.85, precision: 0.8846153846153846, f_beta: 0.8465473145780051
train: step: 869, loss: 0.3036009967327118, acc: 0.475, recall: 0.475, precision: 0.474937343358396, f_beta: 0.474671669793621
train: step: 870, loss: 0.0592498704791069, acc: 0.95, recall: 0.95, precision: 0.95, f_beta: 0.9500000000000001
train: step: 871, loss: 0.07584469020366669, acc: 0.875, recall: 0.875, precision: 0.8836317135549872, f_beta: 0.8742928975487115
train: step: 872, loss: 0.09135950356721878, acc: 0.9, recall: 0.8999999999999999, precision: 0.904040404040404, f_beta: 0.899749373433584
train: step: 873, loss: 0.07025466859340668, acc: 0.925, recall: 0.925, precision: 0.9260651629072681, f_beta: 0.924953095684803
train: step: 874, loss: 0.08386411517858505, acc: 0.9, recall: 0.9, precision: 0.9, f_beta: 0.9
train: step: 875, loss: 0.16315968334674835, acc: 0.775, recall: 0.775, precision: 0.7756892230576441, f_beta: 0.774859287054409
train: step: 876, loss: 0.22096166014671326, acc: 0.675, recall: 0.675, precision: 0.6994301994301995, f_beta: 0.6647324306898775
train: step: 877, loss: 0.07949555665254593, acc: 0.9, recall: 0.8999999999999999, precision: 0.904040404040404, f_beta: 0.899749373433584
train: step: 878, loss: 0.24246104061603546, acc: 0.7, recall: 0.7, precision: 0.7666666666666666, f_beta: 0.6799999999999999
train: step: 879, loss: 0.07338839769363403, acc: 0.925, recall: 0.925, precision: 0.9260651629072681, f_beta: 0.924953095684803
train: step: 880, loss: 0.19163261353969574, acc: 0.825, recall: 0.825, precision: 0.8324808184143222, f_beta: 0.824010056568196
train: step: 881, loss: 0.11150000244379044, acc: 0.8, recall: 0.8, precision: 0.8, f_beta: 0.8000000000000002
train: step: 882, loss: 0.16347025334835052, acc: 0.75, recall: 0.75, precision: 0.7604166666666667, f_beta: 0.7474747474747475
train: step: 883, loss: 0.12036541849374771, acc: 0.85, recall: 0.85, precision: 0.8846153846153846, f_beta: 0.8465473145780051
train: step: 884, loss: 0.07078271359205246, acc: 0.875, recall: 0.875, precision: 0.8759398496240601, f_beta: 0.8749218261413383
train: step: 885, loss: 0.31128543615341187, acc: 0.675, recall: 0.675, precision: 0.6994301994301995, f_beta: 0.6647324306898775
train: step: 886, loss: 0.1035553365945816, acc: 0.875, recall: 0.875, precision: 0.8759398496240601, f_beta: 0.8749218261413383
train: step: 887, loss: 0.13470444083213806, acc: 0.85, recall: 0.85, precision: 0.85, f_beta: 0.85
train: step: 888, loss: 0.11275229603052139, acc: 0.85, recall: 0.85, precision: 0.8846153846153846, f_beta: 0.8465473145780051
train: step: 889, loss: 0.355518102645874, acc: 0.775, recall: 0.775, precision: 0.8448275862068966, f_beta: 0.7630019749835418
train: step: 890, loss: 0.10185965150594711, acc: 0.875, recall: 0.875, precision: 0.8836317135549872, f_beta: 0.8742928975487115
train: step: 891, loss: 0.2167293280363083, acc: 0.725, recall: 0.725, precision: 0.7821316614420062, f_beta: 0.7103357472021066
train: step: 892, loss: 0.08907729387283325, acc: 0.875, recall: 0.875, precision: 0.8836317135549872, f_beta: 0.8742928975487115
train: step: 893, loss: 0.17489807307720184, acc: 0.725, recall: 0.7250000000000001, precision: 0.7564102564102564, f_beta: 0.7163120567375887
train: step: 894, loss: 0.23401515185832977, acc: 0.65, recall: 0.65, precision: 0.6785714285714286, f_beta: 0.6354166666666667
train: step: 895, loss: 0.08136005699634552, acc: 0.875, recall: 0.875, precision: 0.8759398496240601, f_beta: 0.8749218261413383
train: step: 896, loss: 0.1560811996459961, acc: 0.775, recall: 0.775, precision: 0.7933333333333333, f_beta: 0.7714285714285714
train: step: 897, loss: 0.09719928354024887, acc: 0.9, recall: 0.9, precision: 0.9, f_beta: 0.9
train: step: 898, loss: 0.14179468154907227, acc: 0.825, recall: 0.825, precision: 0.8324808184143222, f_beta: 0.824010056568196
train: step: 899, loss: 0.06509864330291748, acc: 0.925, recall: 0.925, precision: 0.9260651629072681, f_beta: 0.924953095684803
train: step: 900, loss: 0.05238821357488632, acc: 0.95, recall: 0.95, precision: 0.9545454545454546, f_beta: 0.949874686716792
train: step: 901, loss: 0.15707151591777802, acc: 0.8, recall: 0.8, precision: 0.8571428571428572, f_beta: 0.7916666666666665
train: step: 902, loss: 0.1841466724872589, acc: 0.825, recall: 0.825, precision: 0.8324808184143222, f_beta: 0.824010056568196
train: step: 903, loss: 0.1461380273103714, acc: 0.825, recall: 0.825, precision: 0.8703703703703703, f_beta: 0.819471308833011
train: step: 904, loss: 0.24088478088378906, acc: 0.6, recall: 0.6, precision: 0.6098901098901099, f_beta: 0.5907928388746804
train: step: 905, loss: 0.059881795197725296, acc: 0.925, recall: 0.925, precision: 0.9260651629072681, f_beta: 0.924953095684803
train: step: 906, loss: 0.1647554188966751, acc: 0.85, recall: 0.8500000000000001, precision: 0.8535353535353536, f_beta: 0.849624060150376
train: step: 907, loss: 0.10860850661993027, acc: 0.875, recall: 0.875, precision: 0.9, f_beta: 0.873015873015873
train: step: 908, loss: 0.327313095331192, acc: 0.9, recall: 0.8999999999999999, precision: 0.904040404040404, f_beta: 0.899749373433584
train: step: 909, loss: 0.05470838025212288, acc: 0.925, recall: 0.925, precision: 0.9260651629072681, f_beta: 0.924953095684803
train: step: 910, loss: 0.18081597983837128, acc: 0.7, recall: 0.7, precision: 0.7083333333333333, f_beta: 0.6969696969696968
train: step: 911, loss: 0.14364245533943176, acc: 0.8, recall: 0.8, precision: 0.8, f_beta: 0.8000000000000002
train: step: 912, loss: 0.08690784871578217, acc: 0.9, recall: 0.8999999999999999, precision: 0.904040404040404, f_beta: 0.899749373433584
train: step: 913, loss: 0.08196622878313065, acc: 0.9, recall: 0.9, precision: 0.9, f_beta: 0.9
train: step: 914, loss: 0.24204912781715393, acc: 0.825, recall: 0.825, precision: 0.8258145363408521, f_beta: 0.8248905565978737
train: step: 915, loss: 0.05499906465411186, acc: 0.925, recall: 0.925, precision: 0.9347826086956521, f_beta: 0.9245757385292268
train: step: 916, loss: 0.09961400926113129, acc: 0.9, recall: 0.8999999999999999, precision: 0.904040404040404, f_beta: 0.899749373433584
train: step: 917, loss: 0.09237337112426758, acc: 0.825, recall: 0.825, precision: 0.8258145363408521, f_beta: 0.8248905565978737
train: step: 918, loss: 0.13321217894554138, acc: 0.85, recall: 0.85, precision: 0.85, f_beta: 0.85
train: step: 919, loss: 0.08719982951879501, acc: 0.9, recall: 0.9, precision: 0.9, f_beta: 0.9
train: step: 920, loss: 0.1885128915309906, acc: 0.725, recall: 0.725, precision: 0.7821316614420062, f_beta: 0.7103357472021066
train: step: 921, loss: 0.14204546809196472, acc: 0.825, recall: 0.825, precision: 0.8258145363408521, f_beta: 0.8248905565978737
train: step: 922, loss: 0.07066608965396881, acc: 0.975, recall: 0.975, precision: 0.9761904761904762, f_beta: 0.9749843652282677
train: step: 923, loss: 0.06856437027454376, acc: 0.95, recall: 0.95, precision: 0.95, f_beta: 0.9500000000000001
train: step: 924, loss: 0.182475745677948, acc: 0.75, recall: 0.75, precision: 0.7604166666666667, f_beta: 0.7474747474747475
train: step: 925, loss: 0.13603927195072174, acc: 0.975, recall: 0.975, precision: 0.9761904761904762, f_beta: 0.9749843652282677
train: step: 926, loss: 0.10298202186822891, acc: 0.875, recall: 0.875, precision: 0.8836317135549872, f_beta: 0.8742928975487115
train: step: 927, loss: 0.09683825075626373, acc: 0.875, recall: 0.875, precision: 0.8759398496240601, f_beta: 0.8749218261413383
train: step: 928, loss: 0.13690677285194397, acc: 0.825, recall: 0.825, precision: 0.8258145363408521, f_beta: 0.8248905565978737
train: step: 929, loss: 0.2937929928302765, acc: 0.9, recall: 0.9, precision: 0.9166666666666667, f_beta: 0.898989898989899
train: step: 930, loss: 0.05422490835189819, acc: 0.95, recall: 0.95, precision: 0.9545454545454546, f_beta: 0.949874686716792
train: step: 931, loss: 0.05820467323064804, acc: 0.925, recall: 0.925, precision: 0.9260651629072681, f_beta: 0.924953095684803
train: step: 932, loss: 0.05889115482568741, acc: 0.9, recall: 0.8999999999999999, precision: 0.904040404040404, f_beta: 0.899749373433584
train: step: 933, loss: 0.11883798986673355, acc: 0.875, recall: 0.875, precision: 0.8759398496240601, f_beta: 0.8749218261413383
train: step: 934, loss: 0.12494109570980072, acc: 0.825, recall: 0.825, precision: 0.8466666666666667, f_beta: 0.8222222222222222
train: step: 935, loss: 0.0476963110268116, acc: 0.95, recall: 0.95, precision: 0.95, f_beta: 0.9500000000000001
train: step: 936, loss: 0.26061582565307617, acc: 0.575, recall: 0.575, precision: 0.6714285714285715, f_beta: 0.5054545454545454
train: step: 937, loss: 0.03913882374763489, acc: 0.975, recall: 0.975, precision: 0.9761904761904762, f_beta: 0.9749843652282677
train: step: 938, loss: 0.09104552119970322, acc: 0.9, recall: 0.9, precision: 0.9, f_beta: 0.9
train: step: 939, loss: 0.34827572107315063, acc: 0.875, recall: 0.875, precision: 0.8759398496240601, f_beta: 0.8749218261413383
train: step: 940, loss: 0.10551898181438446, acc: 0.85, recall: 0.85, precision: 0.85, f_beta: 0.85
train: step: 941, loss: 0.18216070532798767, acc: 0.9, recall: 0.9, precision: 0.9, f_beta: 0.9
train: step: 942, loss: 0.11106972396373749, acc: 0.95, recall: 0.95, precision: 0.95, f_beta: 0.9500000000000001
train: step: 943, loss: 0.02105642296373844, acc: 1.0, recall: 1.0, precision: 1.0, f_beta: 1.0
train: step: 944, loss: 0.1575613021850586, acc: 0.875, recall: 0.875, precision: 0.8836317135549872, f_beta: 0.8742928975487115
train: step: 945, loss: 0.1169198602437973, acc: 0.85, recall: 0.85, precision: 0.8846153846153846, f_beta: 0.8465473145780051
train: step: 946, loss: 0.11280597746372223, acc: 0.9, recall: 0.9, precision: 0.9, f_beta: 0.9
train: step: 947, loss: 0.06046184152364731, acc: 0.925, recall: 0.925, precision: 0.9260651629072681, f_beta: 0.924953095684803
train: step: 948, loss: 0.18171145021915436, acc: 0.775, recall: 0.775, precision: 0.7756892230576441, f_beta: 0.774859287054409
train: step: 949, loss: 0.2504921853542328, acc: 0.825, recall: 0.825, precision: 0.8258145363408521, f_beta: 0.8248905565978737
train: step: 950, loss: 0.05499429628252983, acc: 0.95, recall: 0.95, precision: 0.9545454545454546, f_beta: 0.949874686716792
train: step: 951, loss: 0.04085289314389229, acc: 0.975, recall: 0.975, precision: 0.9761904761904762, f_beta: 0.9749843652282677
train: step: 952, loss: 0.1116732731461525, acc: 0.875, recall: 0.875, precision: 0.8836317135549872, f_beta: 0.8742928975487115
train: step: 953, loss: 0.12053477764129639, acc: 0.8, recall: 0.8, precision: 0.803030303030303, f_beta: 0.7994987468671679
train: step: 954, loss: 0.11711225658655167, acc: 0.85, recall: 0.85, precision: 0.8645833333333333, f_beta: 0.8484848484848484
train: step: 955, loss: 0.12613670527935028, acc: 0.85, recall: 0.85, precision: 0.8846153846153846, f_beta: 0.8465473145780051
train: step: 956, loss: 0.13114134967327118, acc: 0.85, recall: 0.85, precision: 0.8645833333333333, f_beta: 0.8484848484848484
train: step: 957, loss: 0.08849705010652542, acc: 0.9, recall: 0.8999999999999999, precision: 0.904040404040404, f_beta: 0.899749373433584
train: step: 958, loss: 0.14122812449932098, acc: 0.85, recall: 0.85, precision: 0.85, f_beta: 0.85
train: step: 959, loss: 0.08228334039449692, acc: 0.9, recall: 0.8999999999999999, precision: 0.904040404040404, f_beta: 0.899749373433584
train: step: 960, loss: 0.17547014355659485, acc: 0.875, recall: 0.875, precision: 0.8759398496240601, f_beta: 0.8749218261413383
train: step: 961, loss: 0.0727492943406105, acc: 0.9, recall: 0.9, precision: 0.9, f_beta: 0.9
train: step: 962, loss: 0.04364513233304024, acc: 0.975, recall: 0.975, precision: 0.9761904761904762, f_beta: 0.9749843652282677
train: step: 963, loss: 0.05691526085138321, acc: 0.9, recall: 0.9, precision: 0.9, f_beta: 0.9
train: step: 964, loss: 0.21216168999671936, acc: 0.75, recall: 0.75, precision: 0.7525252525252526, f_beta: 0.7493734335839599
train: step: 965, loss: 0.131039097905159, acc: 0.95, recall: 0.95, precision: 0.95, f_beta: 0.9500000000000001
train: step: 966, loss: 0.232252836227417, acc: 0.725, recall: 0.725, precision: 0.8225806451612903, f_beta: 0.7025016903313049
train: step: 967, loss: 0.1681438386440277, acc: 0.75, recall: 0.75, precision: 0.7525252525252526, f_beta: 0.7493734335839599
train: step: 968, loss: 0.052496492862701416, acc: 0.925, recall: 0.925, precision: 0.9260651629072681, f_beta: 0.924953095684803
train: step: 969, loss: 0.11810748279094696, acc: 0.85, recall: 0.85, precision: 0.8645833333333333, f_beta: 0.8484848484848484
train: step: 970, loss: 0.05441563203930855, acc: 0.925, recall: 0.925, precision: 0.9347826086956521, f_beta: 0.9245757385292268
train: step: 971, loss: 0.09997352957725525, acc: 0.875, recall: 0.875, precision: 0.8759398496240601, f_beta: 0.8749218261413383
train: step: 972, loss: 0.11521033942699432, acc: 0.95, recall: 0.95, precision: 0.9545454545454546, f_beta: 0.949874686716792
train: step: 973, loss: 0.18278396129608154, acc: 0.725, recall: 0.7250000000000001, precision: 0.7301790281329923, f_beta: 0.7234443746071653
train: step: 974, loss: 0.10427532345056534, acc: 0.85, recall: 0.85, precision: 0.85, f_beta: 0.85
train: step: 975, loss: 0.30384236574172974, acc: 0.7, recall: 0.7, precision: 0.7666666666666666, f_beta: 0.6799999999999999
train: step: 976, loss: 0.10370127856731415, acc: 0.85, recall: 0.8500000000000001, precision: 0.8535353535353536, f_beta: 0.849624060150376
train: step: 977, loss: 0.07439432293176651, acc: 0.925, recall: 0.925, precision: 0.9347826086956521, f_beta: 0.9245757385292268
train: step: 978, loss: 0.2859481871128082, acc: 0.575, recall: 0.575, precision: 0.60752688172043, f_beta: 0.5402298850574712
train: step: 979, loss: 0.10769546031951904, acc: 0.85, recall: 0.85, precision: 0.8645833333333333, f_beta: 0.8484848484848484
train: step: 980, loss: 0.12063400447368622, acc: 0.825, recall: 0.825, precision: 0.8258145363408521, f_beta: 0.8248905565978737
train: step: 981, loss: 0.040331948548555374, acc: 0.95, recall: 0.95, precision: 0.9545454545454546, f_beta: 0.949874686716792
train: step: 982, loss: 0.09425326436758041, acc: 0.875, recall: 0.875, precision: 0.8836317135549872, f_beta: 0.8742928975487115
train: step: 983, loss: 0.11458227783441544, acc: 0.85, recall: 0.85, precision: 0.85, f_beta: 0.85
train: step: 984, loss: 0.07158432900905609, acc: 0.925, recall: 0.925, precision: 0.9260651629072681, f_beta: 0.924953095684803
train: step: 985, loss: 0.07900580018758774, acc: 0.9, recall: 0.9, precision: 0.9, f_beta: 0.9
train: step: 986, loss: 0.1705958992242813, acc: 0.8, recall: 0.8, precision: 0.8571428571428572, f_beta: 0.7916666666666665
train: step: 987, loss: 0.12728504836559296, acc: 0.85, recall: 0.85, precision: 0.85, f_beta: 0.85
train: step: 988, loss: 0.06753627955913544, acc: 0.925, recall: 0.925, precision: 0.9347826086956521, f_beta: 0.9245757385292268
train: step: 989, loss: 0.19314002990722656, acc: 0.725, recall: 0.7250000000000001, precision: 0.7564102564102564, f_beta: 0.7163120567375887
train: step: 990, loss: 0.055238254368305206, acc: 0.975, recall: 0.975, precision: 0.9761904761904762, f_beta: 0.9749843652282677
train: step: 991, loss: 0.37385839223861694, acc: 0.275, recall: 0.275, precision: 0.24358974358974358, f_beta: 0.25209542230818827
train: step: 992, loss: 0.04705610126256943, acc: 0.975, recall: 0.975, precision: 0.9761904761904762, f_beta: 0.9749843652282677
train: step: 993, loss: 0.09663067758083344, acc: 0.9, recall: 0.8999999999999999, precision: 0.904040404040404, f_beta: 0.899749373433584
train: step: 994, loss: 0.052007198333740234, acc: 0.925, recall: 0.925, precision: 0.9347826086956521, f_beta: 0.9245757385292268
train: step: 995, loss: 0.12406150251626968, acc: 0.8, recall: 0.8, precision: 0.8125, f_beta: 0.797979797979798
train: step: 996, loss: 0.2993897795677185, acc: 0.675, recall: 0.675, precision: 0.6754385964912281, f_beta: 0.6747967479674797
train: step: 997, loss: 0.1786065399646759, acc: 0.75, recall: 0.75, precision: 0.7747252747252746, f_beta: 0.7442455242966751
train: step: 998, loss: 0.10901274532079697, acc: 0.9, recall: 0.8999999999999999, precision: 0.904040404040404, f_beta: 0.899749373433584
train: step: 999, loss: 0.10480525344610214, acc: 0.85, recall: 0.85, precision: 0.85, f_beta: 0.85
train: step: 1000, loss: 0.11820276826620102, acc: 0.875, recall: 0.875, precision: 0.8836317135549872, f_beta: 0.8742928975487115
train: step: 1001, loss: 0.08937496691942215, acc: 0.875, recall: 0.875, precision: 0.8836317135549872, f_beta: 0.8742928975487115
train: step: 1002, loss: 0.15502558648586273, acc: 0.8, recall: 0.8, precision: 0.803030303030303, f_beta: 0.7994987468671679
train: step: 1003, loss: 0.07328281551599503, acc: 0.9, recall: 0.9, precision: 0.9, f_beta: 0.9
train: step: 1004, loss: 0.14292588829994202, acc: 0.775, recall: 0.775, precision: 0.7756892230576441, f_beta: 0.774859287054409
train: step: 1005, loss: 0.06462692469358444, acc: 0.95, recall: 0.95, precision: 0.95, f_beta: 0.9500000000000001
train: step: 1006, loss: 0.15282157063484192, acc: 0.85, recall: 0.85, precision: 0.8846153846153846, f_beta: 0.8465473145780051
train: step: 1007, loss: 0.17885378003120422, acc: 0.9, recall: 0.8999999999999999, precision: 0.904040404040404, f_beta: 0.899749373433584
train: step: 1008, loss: 0.10426757484674454, acc: 0.95, recall: 0.95, precision: 0.9545454545454546, f_beta: 0.949874686716792
train: step: 1009, loss: 0.17630963027477264, acc: 0.725, recall: 0.725, precision: 0.74, f_beta: 0.7206349206349207
train: step: 1010, loss: 0.12504282593727112, acc: 0.85, recall: 0.8500000000000001, precision: 0.8535353535353536, f_beta: 0.849624060150376
train: step: 1011, loss: 0.1151997298002243, acc: 0.825, recall: 0.825, precision: 0.8258145363408521, f_beta: 0.8248905565978737
train: step: 1012, loss: 0.05748375505208969, acc: 0.95, recall: 0.95, precision: 0.9545454545454546, f_beta: 0.949874686716792
train: step: 1013, loss: 0.09224572777748108, acc: 0.85, recall: 0.8500000000000001, precision: 0.8535353535353536, f_beta: 0.849624060150376
train: step: 1014, loss: 0.0662258043885231, acc: 0.95, recall: 0.95, precision: 0.9545454545454546, f_beta: 0.949874686716792
train: step: 1015, loss: 0.08896783739328384, acc: 0.9, recall: 0.9, precision: 0.9166666666666667, f_beta: 0.898989898989899
train: step: 1016, loss: 0.05841146036982536, acc: 0.925, recall: 0.925, precision: 0.9260651629072681, f_beta: 0.924953095684803
train: step: 1017, loss: 0.0955914556980133, acc: 0.875, recall: 0.875, precision: 0.8759398496240601, f_beta: 0.8749218261413383
train: step: 1018, loss: 0.054287802428007126, acc: 0.95, recall: 0.95, precision: 0.95, f_beta: 0.9500000000000001
train: step: 1019, loss: 0.1514543741941452, acc: 0.75, recall: 0.75, precision: 0.7747252747252746, f_beta: 0.7442455242966751
train: step: 1020, loss: 0.3500641882419586, acc: 0.825, recall: 0.825, precision: 0.8466666666666667, f_beta: 0.8222222222222222
train: step: 1021, loss: 0.11698102951049805, acc: 0.825, recall: 0.825, precision: 0.8258145363408521, f_beta: 0.8248905565978737
train: step: 1022, loss: 0.26570072770118713, acc: 0.65, recall: 0.65, precision: 0.6785714285714286, f_beta: 0.6354166666666667
train: step: 1023, loss: 0.11277849972248077, acc: 0.875, recall: 0.875, precision: 0.8836317135549872, f_beta: 0.8742928975487115
train: step: 1024, loss: 0.1701941192150116, acc: 0.85, recall: 0.8500000000000001, precision: 0.8535353535353536, f_beta: 0.849624060150376
train: step: 1025, loss: 0.17840690910816193, acc: 0.725, recall: 0.7250000000000001, precision: 0.7564102564102564, f_beta: 0.7163120567375887
train: step: 1026, loss: 0.34405431151390076, acc: 0.725, recall: 0.725, precision: 0.7255639097744361, f_beta: 0.7248280175109443
train: step: 1027, loss: 0.20494695007801056, acc: 0.775, recall: 0.7749999999999999, precision: 0.8133903133903134, f_beta: 0.7678916827853
train: step: 1028, loss: 0.1403450071811676, acc: 0.825, recall: 0.825, precision: 0.8258145363408521, f_beta: 0.8248905565978737
train: step: 1029, loss: 0.05956636741757393, acc: 0.95, recall: 0.95, precision: 0.9545454545454546, f_beta: 0.949874686716792
train: step: 1030, loss: 0.12730762362480164, acc: 0.75, recall: 0.75, precision: 0.7747252747252746, f_beta: 0.7442455242966751
train: step: 1031, loss: 0.27378028631210327, acc: 0.775, recall: 0.7749999999999999, precision: 0.7813299232736572, f_beta: 0.7737272155876807
train: step: 1032, loss: 0.14121171832084656, acc: 0.85, recall: 0.8500000000000001, precision: 0.8535353535353536, f_beta: 0.849624060150376
train: step: 1033, loss: 0.29542237520217896, acc: 0.7, recall: 0.7, precision: 0.7197802197802198, f_beta: 0.6930946291560103
train: step: 1034, loss: 0.10678396373987198, acc: 0.875, recall: 0.875, precision: 0.8759398496240601, f_beta: 0.8749218261413383
train: step: 1035, loss: 0.06313611567020416, acc: 0.925, recall: 0.925, precision: 0.9260651629072681, f_beta: 0.924953095684803
train: step: 1036, loss: 0.07676489651203156, acc: 0.9, recall: 0.9, precision: 0.9166666666666667, f_beta: 0.898989898989899
train: step: 1037, loss: 0.06604310125112534, acc: 0.9, recall: 0.9, precision: 0.9, f_beta: 0.9
train: step: 1038, loss: 0.08134350180625916, acc: 0.9, recall: 0.9, precision: 0.9166666666666667, f_beta: 0.898989898989899
train: step: 1039, loss: 0.14850978553295135, acc: 0.775, recall: 0.775, precision: 0.8448275862068966, f_beta: 0.7630019749835418
train: step: 1040, loss: 0.11573992669582367, acc: 0.825, recall: 0.825, precision: 0.8703703703703703, f_beta: 0.819471308833011
train: step: 1041, loss: 0.08093857765197754, acc: 0.925, recall: 0.925, precision: 0.9260651629072681, f_beta: 0.924953095684803
train: step: 1042, loss: 0.2900521457195282, acc: 0.875, recall: 0.875, precision: 0.8759398496240601, f_beta: 0.8749218261413383
train: step: 1043, loss: 0.16146811842918396, acc: 0.8, recall: 0.8, precision: 0.803030303030303, f_beta: 0.7994987468671679
train: step: 1044, loss: 0.17676737904548645, acc: 0.775, recall: 0.7749999999999999, precision: 0.8133903133903134, f_beta: 0.7678916827853
train: step: 1045, loss: 0.1158205047249794, acc: 0.85, recall: 0.85, precision: 0.8645833333333333, f_beta: 0.8484848484848484
train: step: 1046, loss: 0.0528859868645668, acc: 0.95, recall: 0.95, precision: 0.95, f_beta: 0.9500000000000001
train: step: 1047, loss: 0.1120971068739891, acc: 0.85, recall: 0.8500000000000001, precision: 0.8535353535353536, f_beta: 0.849624060150376
train: step: 1048, loss: 0.06633994728326797, acc: 0.925, recall: 0.925, precision: 0.9260651629072681, f_beta: 0.924953095684803
train: step: 1049, loss: 0.3108880817890167, acc: 0.8, recall: 0.8, precision: 0.8296703296703296, f_beta: 0.7953964194373402
train: step: 1050, loss: 0.04565359279513359, acc: 0.975, recall: 0.975, precision: 0.9761904761904762, f_beta: 0.9749843652282677
train: step: 1051, loss: 0.08012144267559052, acc: 0.9, recall: 0.8999999999999999, precision: 0.904040404040404, f_beta: 0.899749373433584
train: step: 1052, loss: 0.1231345385313034, acc: 0.8, recall: 0.8, precision: 0.8, f_beta: 0.8000000000000002
train: step: 1053, loss: 0.09948264062404633, acc: 0.9, recall: 0.8999999999999999, precision: 0.904040404040404, f_beta: 0.899749373433584
train: step: 1054, loss: 0.11879906803369522, acc: 0.825, recall: 0.825, precision: 0.8258145363408521, f_beta: 0.8248905565978737
train: step: 1055, loss: 0.06069908291101456, acc: 1.0, recall: 1.0, precision: 1.0, f_beta: 1.0
train: step: 1056, loss: 0.08208142220973969, acc: 0.9, recall: 0.9, precision: 0.9166666666666667, f_beta: 0.898989898989899
train: step: 1057, loss: 0.33267927169799805, acc: 0.9, recall: 0.8999999999999999, precision: 0.904040404040404, f_beta: 0.899749373433584
train: step: 1058, loss: 0.08080766350030899, acc: 0.9, recall: 0.8999999999999999, precision: 0.904040404040404, f_beta: 0.899749373433584
train: step: 1059, loss: 0.08664030581712723, acc: 0.9, recall: 0.9, precision: 0.9, f_beta: 0.9
train: step: 1060, loss: 0.04780564457178116, acc: 0.925, recall: 0.925, precision: 0.9260651629072681, f_beta: 0.924953095684803
train: step: 1061, loss: 0.03897877410054207, acc: 0.975, recall: 0.975, precision: 0.9761904761904762, f_beta: 0.9749843652282677
train: step: 1062, loss: 0.2835918068885803, acc: 0.675, recall: 0.675, precision: 0.6994301994301995, f_beta: 0.6647324306898775
train: step: 1063, loss: 0.15630307793617249, acc: 0.85, recall: 0.85, precision: 0.85, f_beta: 0.85
train: step: 1064, loss: 0.09854025393724442, acc: 0.9, recall: 0.8999999999999999, precision: 0.904040404040404, f_beta: 0.899749373433584
train: step: 1065, loss: 0.07805640995502472, acc: 0.95, recall: 0.95, precision: 0.9545454545454546, f_beta: 0.949874686716792
train: step: 1066, loss: 0.1893387883901596, acc: 0.725, recall: 0.7250000000000001, precision: 0.7301790281329923, f_beta: 0.7234443746071653
train: step: 1067, loss: 0.03749207779765129, acc: 0.95, recall: 0.95, precision: 0.9545454545454546, f_beta: 0.949874686716792
train: step: 1068, loss: 0.314879834651947, acc: 0.7, recall: 0.7, precision: 0.7666666666666666, f_beta: 0.6799999999999999
train: step: 1069, loss: 0.30661505460739136, acc: 0.8, recall: 0.8, precision: 0.8, f_beta: 0.8000000000000002
train: step: 1070, loss: 0.08849257975816727, acc: 0.875, recall: 0.875, precision: 0.8836317135549872, f_beta: 0.8742928975487115
train: step: 1071, loss: 0.11087165772914886, acc: 0.875, recall: 0.875, precision: 0.9, f_beta: 0.873015873015873
train: step: 1072, loss: 0.1261870414018631, acc: 0.85, recall: 0.85, precision: 0.8645833333333333, f_beta: 0.8484848484848484
train: step: 1073, loss: 0.0831441730260849, acc: 0.9, recall: 0.8999999999999999, precision: 0.904040404040404, f_beta: 0.899749373433584
train: step: 1074, loss: 0.1416214108467102, acc: 0.85, recall: 0.8500000000000001, precision: 0.8535353535353536, f_beta: 0.849624060150376
train: step: 1075, loss: 0.0532710961997509, acc: 0.95, recall: 0.95, precision: 0.9545454545454546, f_beta: 0.949874686716792
train: step: 1076, loss: 0.11414416134357452, acc: 0.85, recall: 0.85, precision: 0.8846153846153846, f_beta: 0.8465473145780051
train: step: 1077, loss: 0.21302683651447296, acc: 0.9, recall: 0.9, precision: 0.9166666666666667, f_beta: 0.898989898989899
train: step: 1078, loss: 0.20317021012306213, acc: 0.7, recall: 0.7, precision: 0.7197802197802198, f_beta: 0.6930946291560103
train: step: 1079, loss: 0.07360196113586426, acc: 0.925, recall: 0.925, precision: 0.9260651629072681, f_beta: 0.924953095684803
train: step: 1080, loss: 0.16993720829486847, acc: 0.775, recall: 0.775, precision: 0.7756892230576441, f_beta: 0.774859287054409
train: step: 1081, loss: 0.2239701747894287, acc: 0.7, recall: 0.7, precision: 0.7380952380952381, f_beta: 0.6875
train: step: 1082, loss: 0.12083041667938232, acc: 0.95, recall: 0.95, precision: 0.95, f_beta: 0.9500000000000001
train: step: 1083, loss: 0.08546974509954453, acc: 0.95, recall: 0.95, precision: 0.95, f_beta: 0.9500000000000001
train: step: 1084, loss: 0.06327364593744278, acc: 0.925, recall: 0.925, precision: 0.9260651629072681, f_beta: 0.924953095684803
train: step: 1085, loss: 0.255059152841568, acc: 0.775, recall: 0.7749999999999999, precision: 0.8133903133903134, f_beta: 0.7678916827853
train: step: 1086, loss: 0.2599141001701355, acc: 0.65, recall: 0.65, precision: 0.65625, f_beta: 0.6464646464646464
train: step: 1087, loss: 0.2811490595340729, acc: 0.775, recall: 0.7749999999999999, precision: 0.7813299232736572, f_beta: 0.7737272155876807
train: step: 1088, loss: 0.13115772604942322, acc: 0.825, recall: 0.825, precision: 0.8258145363408521, f_beta: 0.8248905565978737
train: step: 1089, loss: 0.1042550653219223, acc: 0.875, recall: 0.875, precision: 0.8836317135549872, f_beta: 0.8742928975487115
train: step: 1090, loss: 0.11026258766651154, acc: 0.875, recall: 0.875, precision: 0.9, f_beta: 0.873015873015873
train: step: 1091, loss: 0.10827647149562836, acc: 0.8, recall: 0.8, precision: 0.8296703296703296, f_beta: 0.7953964194373402
train: step: 1092, loss: 0.10699820518493652, acc: 0.925, recall: 0.925, precision: 0.9260651629072681, f_beta: 0.924953095684803
train: step: 1093, loss: 0.10792489349842072, acc: 0.9, recall: 0.9, precision: 0.9, f_beta: 0.9
train: step: 1094, loss: 0.10089826583862305, acc: 0.9, recall: 0.9, precision: 0.9166666666666667, f_beta: 0.898989898989899
train: step: 1095, loss: 0.03873348981142044, acc: 0.975, recall: 0.975, precision: 0.9761904761904762, f_beta: 0.9749843652282677
train: step: 1096, loss: 0.10860462486743927, acc: 0.85, recall: 0.85, precision: 0.85, f_beta: 0.85
train: step: 1097, loss: 0.13198041915893555, acc: 0.825, recall: 0.825, precision: 0.8466666666666667, f_beta: 0.8222222222222222
train: step: 1098, loss: 0.10070280730724335, acc: 0.9, recall: 0.9, precision: 0.9166666666666667, f_beta: 0.898989898989899
train: step: 1099, loss: 0.09473113715648651, acc: 0.875, recall: 0.875, precision: 0.9, f_beta: 0.873015873015873
train: step: 1100, loss: 0.26948368549346924, acc: 0.85, recall: 0.85, precision: 0.85, f_beta: 0.85
train: step: 1101, loss: 0.18465498089790344, acc: 0.725, recall: 0.7250000000000001, precision: 0.7301790281329923, f_beta: 0.7234443746071653
train: step: 1102, loss: 0.16461241245269775, acc: 0.775, recall: 0.775, precision: 0.7933333333333333, f_beta: 0.7714285714285714
train: step: 1103, loss: 0.09692449867725372, acc: 0.9, recall: 0.8999999999999999, precision: 0.904040404040404, f_beta: 0.899749373433584
train: step: 1104, loss: 0.07975952327251434, acc: 0.9, recall: 0.9, precision: 0.9, f_beta: 0.9
train: step: 1105, loss: 0.10261337459087372, acc: 0.9, recall: 0.9, precision: 0.9166666666666667, f_beta: 0.898989898989899
train: step: 1106, loss: 0.09866292029619217, acc: 0.9, recall: 0.8999999999999999, precision: 0.904040404040404, f_beta: 0.899749373433584
train: step: 1107, loss: 0.064588263630867, acc: 0.975, recall: 0.975, precision: 0.9761904761904762, f_beta: 0.9749843652282677
train: step: 1108, loss: 0.04055362194776535, acc: 0.95, recall: 0.95, precision: 0.9545454545454546, f_beta: 0.949874686716792
train: step: 1109, loss: 0.03888822719454765, acc: 0.95, recall: 0.95, precision: 0.9545454545454546, f_beta: 0.949874686716792
train: step: 1110, loss: 0.09742242097854614, acc: 0.85, recall: 0.85, precision: 0.85, f_beta: 0.85
train: step: 1111, loss: 0.049037665128707886, acc: 0.975, recall: 0.975, precision: 0.9761904761904762, f_beta: 0.9749843652282677
train: step: 1112, loss: 0.08054598420858383, acc: 0.9, recall: 0.9, precision: 0.9, f_beta: 0.9
train: step: 1113, loss: 0.10852619260549545, acc: 0.85, recall: 0.85, precision: 0.8846153846153846, f_beta: 0.8465473145780051
train: step: 1114, loss: 0.07982619106769562, acc: 0.925, recall: 0.925, precision: 0.9260651629072681, f_beta: 0.924953095684803
train: step: 1115, loss: 0.0637282133102417, acc: 0.925, recall: 0.925, precision: 0.9260651629072681, f_beta: 0.924953095684803
train: step: 1116, loss: 0.05619348958134651, acc: 0.975, recall: 0.975, precision: 0.9761904761904762, f_beta: 0.9749843652282677
train: step: 1117, loss: 0.0861484706401825, acc: 0.925, recall: 0.925, precision: 0.9260651629072681, f_beta: 0.924953095684803
train: step: 1118, loss: 0.14945296943187714, acc: 0.75, recall: 0.75, precision: 0.7976190476190477, f_beta: 0.7395833333333333
train: step: 1119, loss: 0.04251604154706001, acc: 0.975, recall: 0.975, precision: 0.9761904761904762, f_beta: 0.9749843652282677
train: step: 1120, loss: 0.1165759414434433, acc: 0.825, recall: 0.825, precision: 0.8324808184143222, f_beta: 0.824010056568196
train: step: 1121, loss: 0.08376854658126831, acc: 0.975, recall: 0.975, precision: 0.9761904761904762, f_beta: 0.9749843652282677
train: step: 1122, loss: 0.01569351553916931, acc: 1.0, recall: 1.0, precision: 1.0, f_beta: 1.0
train: step: 1123, loss: 0.0677340030670166, acc: 0.85, recall: 0.85, precision: 0.8846153846153846, f_beta: 0.8465473145780051
train: step: 1124, loss: 0.084368497133255, acc: 0.875, recall: 0.875, precision: 0.9, f_beta: 0.873015873015873
train: step: 1125, loss: 0.1122225970029831, acc: 0.875, recall: 0.875, precision: 0.9, f_beta: 0.873015873015873
train: step: 1126, loss: 0.11030600965023041, acc: 0.825, recall: 0.825, precision: 0.8324808184143222, f_beta: 0.824010056568196
train: step: 1127, loss: 0.06807238608598709, acc: 0.9, recall: 0.9, precision: 0.9, f_beta: 0.9
train: step: 1128, loss: 0.07043246179819107, acc: 0.95, recall: 0.95, precision: 0.95, f_beta: 0.9500000000000001
train: step: 1129, loss: 0.09052841365337372, acc: 0.875, recall: 0.875, precision: 0.8759398496240601, f_beta: 0.8749218261413383
train: step: 1130, loss: 0.3682302236557007, acc: 0.825, recall: 0.825, precision: 0.8466666666666667, f_beta: 0.8222222222222222
train: step: 1131, loss: 0.3710539937019348, acc: 0.5, recall: 0.5, precision: 0.25, f_beta: 0.3333333333333333
train: step: 1132, loss: 0.07597239315509796, acc: 0.925, recall: 0.925, precision: 0.9347826086956521, f_beta: 0.9245757385292268
train: step: 1133, loss: 0.05706091970205307, acc: 0.925, recall: 0.925, precision: 0.9260651629072681, f_beta: 0.924953095684803
train: step: 1134, loss: 0.36998313665390015, acc: 0.825, recall: 0.825, precision: 0.8324808184143222, f_beta: 0.824010056568196
train: step: 1135, loss: 0.12771335244178772, acc: 0.85, recall: 0.8500000000000001, precision: 0.8535353535353536, f_beta: 0.849624060150376
train: step: 1136, loss: 0.17153067886829376, acc: 0.825, recall: 0.825, precision: 0.8466666666666667, f_beta: 0.8222222222222222
train: step: 1137, loss: 0.061260830610990524, acc: 0.925, recall: 0.925, precision: 0.9260651629072681, f_beta: 0.924953095684803
train: step: 1138, loss: 0.2268221378326416, acc: 0.625, recall: 0.625, precision: 0.6792114695340502, f_beta: 0.5943204868154157
train: step: 1139, loss: 0.16097332537174225, acc: 0.75, recall: 0.75, precision: 0.75, f_beta: 0.75
train: step: 1140, loss: 0.10618095099925995, acc: 0.975, recall: 0.975, precision: 0.9761904761904762, f_beta: 0.9749843652282677
train: step: 1141, loss: 0.32249322533607483, acc: 0.7, recall: 0.7, precision: 0.7197802197802198, f_beta: 0.6930946291560103
train: step: 1142, loss: 0.106411874294281, acc: 0.875, recall: 0.875, precision: 0.8759398496240601, f_beta: 0.8749218261413383
train: step: 1143, loss: 0.1328703910112381, acc: 0.825, recall: 0.825, precision: 0.8258145363408521, f_beta: 0.8248905565978737
train: step: 1144, loss: 0.08776859939098358, acc: 0.875, recall: 0.875, precision: 0.8759398496240601, f_beta: 0.8749218261413383
train: step: 1145, loss: 0.05927922576665878, acc: 0.95, recall: 0.95, precision: 0.9545454545454546, f_beta: 0.949874686716792
train: step: 1146, loss: 0.19730070233345032, acc: 0.7, recall: 0.7, precision: 0.7666666666666666, f_beta: 0.6799999999999999
train: step: 1147, loss: 0.09317314624786377, acc: 0.9, recall: 0.8999999999999999, precision: 0.904040404040404, f_beta: 0.899749373433584
train: step: 1148, loss: 0.1732822209596634, acc: 0.8, recall: 0.8, precision: 0.8296703296703296, f_beta: 0.7953964194373402
train: step: 1149, loss: 0.3256024718284607, acc: 0.225, recall: 0.225, precision: 0.22431077694235588, f_beta: 0.22451532207629768
train: step: 1150, loss: 0.10089049488306046, acc: 0.875, recall: 0.875, precision: 0.8759398496240601, f_beta: 0.8749218261413383
train: step: 1151, loss: 0.10713066905736923, acc: 0.9, recall: 0.8999999999999999, precision: 0.904040404040404, f_beta: 0.899749373433584
train: step: 1152, loss: 0.05904495716094971, acc: 0.9, recall: 0.8999999999999999, precision: 0.904040404040404, f_beta: 0.899749373433584
train: step: 1153, loss: 0.17053799331188202, acc: 0.7, recall: 0.7, precision: 0.7380952380952381, f_beta: 0.6875
train: step: 1154, loss: 0.12489371001720428, acc: 0.85, recall: 0.85, precision: 0.8645833333333333, f_beta: 0.8484848484848484
train: step: 1155, loss: 0.20625941455364227, acc: 0.725, recall: 0.7250000000000001, precision: 0.7301790281329923, f_beta: 0.7234443746071653
train: step: 1156, loss: 0.29079297184944153, acc: 0.875, recall: 0.875, precision: 0.8836317135549872, f_beta: 0.8742928975487115
train: step: 1157, loss: 0.08237089961767197, acc: 0.875, recall: 0.875, precision: 0.8836317135549872, f_beta: 0.8742928975487115
train: step: 1158, loss: 0.1658591479063034, acc: 0.8, recall: 0.8, precision: 0.803030303030303, f_beta: 0.7994987468671679
train: step: 1159, loss: 0.07125777006149292, acc: 0.875, recall: 0.875, precision: 0.8759398496240601, f_beta: 0.8749218261413383
train: step: 1160, loss: 0.1253892183303833, acc: 0.825, recall: 0.825, precision: 0.8466666666666667, f_beta: 0.8222222222222222
train: step: 1161, loss: 0.18757621943950653, acc: 0.8, recall: 0.8, precision: 0.8, f_beta: 0.8000000000000002
train: step: 1162, loss: 0.07305409014225006, acc: 0.875, recall: 0.875, precision: 0.8759398496240601, f_beta: 0.8749218261413383
train: step: 1163, loss: 0.05231904983520508, acc: 0.95, recall: 0.95, precision: 0.95, f_beta: 0.9500000000000001
train: step: 1164, loss: 0.08072993904352188, acc: 0.925, recall: 0.925, precision: 0.9260651629072681, f_beta: 0.924953095684803
train: step: 1165, loss: 0.09213433414697647, acc: 0.9, recall: 0.9, precision: 0.9, f_beta: 0.9
train: step: 1166, loss: 0.13383281230926514, acc: 0.85, recall: 0.8500000000000001, precision: 0.8535353535353536, f_beta: 0.849624060150376
train: step: 1167, loss: 0.08861324936151505, acc: 0.9, recall: 0.9, precision: 0.9166666666666667, f_beta: 0.898989898989899
train: step: 1168, loss: 0.09114930778741837, acc: 0.875, recall: 0.875, precision: 0.8759398496240601, f_beta: 0.8749218261413383
train: step: 1169, loss: 0.03328743204474449, acc: 0.975, recall: 0.975, precision: 0.9761904761904762, f_beta: 0.9749843652282677
train: step: 1170, loss: 0.0992705449461937, acc: 0.825, recall: 0.825, precision: 0.8324808184143222, f_beta: 0.824010056568196
train: step: 1171, loss: 0.15980778634548187, acc: 0.875, recall: 0.875, precision: 0.8759398496240601, f_beta: 0.8749218261413383
train: step: 1172, loss: 0.13373060524463654, acc: 0.85, recall: 0.85, precision: 0.8645833333333333, f_beta: 0.8484848484848484
train: step: 1173, loss: 0.33299192786216736, acc: 0.775, recall: 0.7749999999999999, precision: 0.7813299232736572, f_beta: 0.7737272155876807
train: step: 1174, loss: 0.11977145820856094, acc: 0.875, recall: 0.875, precision: 0.8759398496240601, f_beta: 0.8749218261413383
train: step: 1175, loss: 0.17700481414794922, acc: 0.875, recall: 0.875, precision: 0.8836317135549872, f_beta: 0.8742928975487115
train: step: 1176, loss: 0.0930887833237648, acc: 0.9, recall: 0.8999999999999999, precision: 0.904040404040404, f_beta: 0.899749373433584
train: step: 1177, loss: 0.0865049809217453, acc: 0.875, recall: 0.875, precision: 0.8759398496240601, f_beta: 0.8749218261413383
train: step: 1178, loss: 0.11200983822345734, acc: 0.85, recall: 0.85, precision: 0.8846153846153846, f_beta: 0.8465473145780051
train: step: 1179, loss: 0.27797621488571167, acc: 0.725, recall: 0.725, precision: 0.7821316614420062, f_beta: 0.7103357472021066
train: step: 1180, loss: 0.30212804675102234, acc: 0.55, recall: 0.55, precision: 0.554945054945055, f_beta: 0.5396419437340153
train: step: 1181, loss: 0.03749857097864151, acc: 1.0, recall: 1.0, precision: 1.0, f_beta: 1.0
train: step: 1182, loss: 0.04680229350924492, acc: 0.95, recall: 0.95, precision: 0.95, f_beta: 0.9500000000000001
train: step: 1183, loss: 0.10321201384067535, acc: 0.925, recall: 0.925, precision: 0.9260651629072681, f_beta: 0.924953095684803
train: step: 1184, loss: 0.1662290096282959, acc: 0.8, recall: 0.8, precision: 0.8296703296703296, f_beta: 0.7953964194373402
train: step: 1185, loss: 0.1524846851825714, acc: 0.825, recall: 0.825, precision: 0.8258145363408521, f_beta: 0.8248905565978737
train: step: 1186, loss: 0.049873605370521545, acc: 0.975, recall: 0.975, precision: 0.9761904761904762, f_beta: 0.9749843652282677
train: step: 1187, loss: 0.17545315623283386, acc: 0.825, recall: 0.825, precision: 0.8703703703703703, f_beta: 0.819471308833011
train: step: 1188, loss: 0.20467455685138702, acc: 0.725, recall: 0.7250000000000001, precision: 0.7301790281329923, f_beta: 0.7234443746071653
train: step: 1189, loss: 0.15261253714561462, acc: 0.775, recall: 0.775, precision: 0.8448275862068966, f_beta: 0.7630019749835418
train: step: 1190, loss: 0.09253116697072983, acc: 0.875, recall: 0.875, precision: 0.8836317135549872, f_beta: 0.8742928975487115
train: step: 1191, loss: 0.16316327452659607, acc: 0.8, recall: 0.8, precision: 0.8, f_beta: 0.8000000000000002
train: step: 1192, loss: 0.05985739827156067, acc: 0.95, recall: 0.95, precision: 0.9545454545454546, f_beta: 0.949874686716792
train: step: 1193, loss: 0.1384948194026947, acc: 0.825, recall: 0.825, precision: 0.8324808184143222, f_beta: 0.824010056568196
train: step: 1194, loss: 0.08148851990699768, acc: 0.9, recall: 0.9, precision: 0.9166666666666667, f_beta: 0.898989898989899
train: step: 1195, loss: 0.08169355243444443, acc: 0.9, recall: 0.8999999999999999, precision: 0.904040404040404, f_beta: 0.899749373433584
train: step: 1196, loss: 0.10036154091358185, acc: 0.875, recall: 0.875, precision: 0.8759398496240601, f_beta: 0.8749218261413383
train: step: 1197, loss: 0.19648976624011993, acc: 0.725, recall: 0.725, precision: 0.7255639097744361, f_beta: 0.7248280175109443
train: step: 1198, loss: 0.2116188257932663, acc: 0.85, recall: 0.85, precision: 0.85, f_beta: 0.85
train: step: 1199, loss: 0.18936018645763397, acc: 0.825, recall: 0.825, precision: 0.8324808184143222, f_beta: 0.824010056568196
train: step: 1200, loss: 0.029600387439131737, acc: 0.975, recall: 0.975, precision: 0.9761904761904762, f_beta: 0.9749843652282677
train: step: 1201, loss: 0.14068181812763214, acc: 0.75, recall: 0.75, precision: 0.7525252525252526, f_beta: 0.7493734335839599
train: step: 1202, loss: 0.24125346541404724, acc: 0.825, recall: 0.825, precision: 0.8324808184143222, f_beta: 0.824010056568196
train: step: 1203, loss: 0.036627136170864105, acc: 0.95, recall: 0.95, precision: 0.9545454545454546, f_beta: 0.949874686716792
train: step: 1204, loss: 0.2502717077732086, acc: 0.7, recall: 0.7, precision: 0.7380952380952381, f_beta: 0.6875
train: step: 1205, loss: 0.10356666147708893, acc: 0.85, recall: 0.85, precision: 0.85, f_beta: 0.85
train: step: 1206, loss: 0.12084116786718369, acc: 0.825, recall: 0.825, precision: 0.8258145363408521, f_beta: 0.8248905565978737
train: step: 1207, loss: 0.0778362974524498, acc: 0.925, recall: 0.925, precision: 0.9260651629072681, f_beta: 0.924953095684803
train: step: 1208, loss: 0.07381687313318253, acc: 0.875, recall: 0.875, precision: 0.8836317135549872, f_beta: 0.8742928975487115
train: step: 1209, loss: 0.15014943480491638, acc: 0.85, recall: 0.85, precision: 0.8645833333333333, f_beta: 0.8484848484848484
train: step: 1210, loss: 0.10523428022861481, acc: 0.875, recall: 0.875, precision: 0.8759398496240601, f_beta: 0.8749218261413383
train: step: 1211, loss: 0.0409817099571228, acc: 0.95, recall: 0.95, precision: 0.95, f_beta: 0.9500000000000001
train: step: 1212, loss: 0.060244373977184296, acc: 0.95, recall: 0.95, precision: 0.95, f_beta: 0.9500000000000001
train: step: 1213, loss: 0.1304038017988205, acc: 0.975, recall: 0.975, precision: 0.9761904761904762, f_beta: 0.9749843652282677
train: step: 1214, loss: 0.06457877159118652, acc: 0.95, recall: 0.95, precision: 0.9545454545454546, f_beta: 0.949874686716792
train: step: 1215, loss: 0.2070118635892868, acc: 0.825, recall: 0.825, precision: 0.8466666666666667, f_beta: 0.8222222222222222
train: step: 1216, loss: 0.2525448203086853, acc: 0.75, recall: 0.75, precision: 0.7525252525252526, f_beta: 0.7493734335839599
train: step: 1217, loss: 0.07363104820251465, acc: 0.925, recall: 0.925, precision: 0.9347826086956521, f_beta: 0.9245757385292268
train: step: 1218, loss: 0.14873428642749786, acc: 0.925, recall: 0.925, precision: 0.9260651629072681, f_beta: 0.924953095684803
train: step: 1219, loss: 0.06321883946657181, acc: 0.875, recall: 0.875, precision: 0.8759398496240601, f_beta: 0.8749218261413383
train: step: 1220, loss: 0.1293284147977829, acc: 0.875, recall: 0.875, precision: 0.8759398496240601, f_beta: 0.8749218261413383
train: step: 1221, loss: 0.27839601039886475, acc: 0.55, recall: 0.55, precision: 0.5980392156862745, f_beta: 0.4871794871794872
train: step: 1222, loss: 0.127458855509758, acc: 0.8, recall: 0.8, precision: 0.8125, f_beta: 0.797979797979798
train: step: 1223, loss: 0.07517315447330475, acc: 0.95, recall: 0.95, precision: 0.9545454545454546, f_beta: 0.949874686716792
train: step: 1224, loss: 0.04198586195707321, acc: 0.975, recall: 0.975, precision: 0.9761904761904762, f_beta: 0.9749843652282677
train: step: 1225, loss: 0.12424115836620331, acc: 0.85, recall: 0.85, precision: 0.85, f_beta: 0.85
train: step: 1226, loss: 0.3820968270301819, acc: 0.775, recall: 0.7749999999999999, precision: 0.7813299232736572, f_beta: 0.7737272155876807
train: step: 1227, loss: 0.09441912174224854, acc: 0.85, recall: 0.85, precision: 0.8645833333333333, f_beta: 0.8484848484848484
train: step: 1228, loss: 0.30401039123535156, acc: 0.65, recall: 0.65, precision: 0.65625, f_beta: 0.6464646464646464
train: step: 1229, loss: 0.34223753213882446, acc: 0.625, recall: 0.625, precision: 0.6333333333333333, f_beta: 0.6190476190476191
train: step: 1230, loss: 0.041921719908714294, acc: 0.95, recall: 0.95, precision: 0.95, f_beta: 0.9500000000000001
train: step: 1231, loss: 0.134057879447937, acc: 0.85, recall: 0.85, precision: 0.8846153846153846, f_beta: 0.8465473145780051
train: step: 1232, loss: 0.21441328525543213, acc: 0.75, recall: 0.75, precision: 0.7976190476190477, f_beta: 0.7395833333333333
train: step: 1233, loss: 0.2494124472141266, acc: 0.6, recall: 0.6, precision: 0.65625, f_beta: 0.5604395604395604
train: step: 1234, loss: 0.2713884711265564, acc: 0.475, recall: 0.475, precision: 0.24358974358974358, f_beta: 0.3220338983050847
train: step: 1235, loss: 0.0813787579536438, acc: 0.95, recall: 0.95, precision: 0.9545454545454546, f_beta: 0.949874686716792
train: step: 1236, loss: 0.3588964343070984, acc: 0.325, recall: 0.325, precision: 0.30056980056980054, f_beta: 0.3036750483558994
train: step: 1237, loss: 0.07632283866405487, acc: 0.95, recall: 0.95, precision: 0.9545454545454546, f_beta: 0.949874686716792
train: step: 1238, loss: 0.2265600711107254, acc: 0.675, recall: 0.675, precision: 0.6866666666666666, f_beta: 0.6698412698412699
train: step: 1239, loss: 0.10736358165740967, acc: 0.875, recall: 0.875, precision: 0.8759398496240601, f_beta: 0.8749218261413383
train: step: 1240, loss: 0.09905161708593369, acc: 0.875, recall: 0.875, precision: 0.8836317135549872, f_beta: 0.8742928975487115
train: step: 1241, loss: 0.1153484582901001, acc: 0.925, recall: 0.925, precision: 0.9260651629072681, f_beta: 0.924953095684803
train: step: 1242, loss: 0.2089952677488327, acc: 0.65, recall: 0.65, precision: 0.6648351648351649, f_beta: 0.6419437340153452
train: step: 1243, loss: 0.2575502395629883, acc: 0.625, recall: 0.625, precision: 0.6333333333333333, f_beta: 0.6190476190476191
train: step: 1244, loss: 0.12646274268627167, acc: 0.85, recall: 0.85, precision: 0.85, f_beta: 0.85
train: step: 1245, loss: 0.06427241861820221, acc: 1.0, recall: 1.0, precision: 1.0, f_beta: 1.0
train: step: 1246, loss: 0.0710110068321228, acc: 0.9, recall: 0.8999999999999999, precision: 0.904040404040404, f_beta: 0.899749373433584
train: step: 1247, loss: 0.17657433450222015, acc: 0.825, recall: 0.825, precision: 0.8324808184143222, f_beta: 0.824010056568196
train: step: 1248, loss: 0.07553356140851974, acc: 0.9, recall: 0.9, precision: 0.9, f_beta: 0.9
train: step: 1249, loss: 0.06215798854827881, acc: 0.925, recall: 0.925, precision: 0.9347826086956521, f_beta: 0.9245757385292268
train: step: 1250, loss: 0.20000025629997253, acc: 0.775, recall: 0.7749999999999999, precision: 0.7813299232736572, f_beta: 0.7737272155876807
train: step: 1251, loss: 0.04731258377432823, acc: 1.0, recall: 1.0, precision: 1.0, f_beta: 1.0
train: step: 1252, loss: 0.09918588399887085, acc: 0.9, recall: 0.8999999999999999, precision: 0.904040404040404, f_beta: 0.899749373433584
train: step: 1253, loss: 0.15393880009651184, acc: 0.8, recall: 0.8, precision: 0.8296703296703296, f_beta: 0.7953964194373402
train: step: 1254, loss: 0.10861214250326157, acc: 0.9, recall: 0.8999999999999999, precision: 0.904040404040404, f_beta: 0.899749373433584
train: step: 1255, loss: 0.18118277192115784, acc: 0.725, recall: 0.7250000000000001, precision: 0.7564102564102564, f_beta: 0.7163120567375887
train: step: 1256, loss: 0.043884553015232086, acc: 0.95, recall: 0.95, precision: 0.9545454545454546, f_beta: 0.949874686716792
train: step: 1257, loss: 0.27285197377204895, acc: 0.725, recall: 0.7250000000000001, precision: 0.7301790281329923, f_beta: 0.7234443746071653
train: step: 1258, loss: 0.15717950463294983, acc: 0.725, recall: 0.7250000000000001, precision: 0.7301790281329923, f_beta: 0.7234443746071653
train: step: 1259, loss: 0.34777316451072693, acc: 0.9, recall: 0.9, precision: 0.9166666666666667, f_beta: 0.898989898989899
train: step: 1260, loss: 0.07916687428951263, acc: 0.95, recall: 0.95, precision: 0.9545454545454546, f_beta: 0.949874686716792
train: step: 1261, loss: 0.07772386819124222, acc: 0.925, recall: 0.925, precision: 0.9347826086956521, f_beta: 0.9245757385292268
train: step: 1262, loss: 0.09198305755853653, acc: 0.9, recall: 0.8999999999999999, precision: 0.904040404040404, f_beta: 0.899749373433584
train: step: 1263, loss: 0.10140260308980942, acc: 0.875, recall: 0.875, precision: 0.8759398496240601, f_beta: 0.8749218261413383
train: step: 1264, loss: 0.2402380406856537, acc: 0.725, recall: 0.725, precision: 0.7821316614420062, f_beta: 0.7103357472021066
train: step: 1265, loss: 0.22784125804901123, acc: 0.9, recall: 0.9, precision: 0.9, f_beta: 0.9
train: step: 1266, loss: 0.16880333423614502, acc: 0.8, recall: 0.8, precision: 0.8571428571428572, f_beta: 0.7916666666666665
train: step: 1267, loss: 0.1623077243566513, acc: 0.775, recall: 0.7749999999999999, precision: 0.8133903133903134, f_beta: 0.7678916827853
train: step: 1268, loss: 0.07928915321826935, acc: 0.9, recall: 0.8999999999999999, precision: 0.904040404040404, f_beta: 0.899749373433584
train: step: 1269, loss: 0.06950151175260544, acc: 0.95, recall: 0.95, precision: 0.95, f_beta: 0.9500000000000001
train: step: 1270, loss: 0.13560231029987335, acc: 0.825, recall: 0.825, precision: 0.8258145363408521, f_beta: 0.8248905565978737
train: step: 1271, loss: 0.14386756718158722, acc: 0.85, recall: 0.85, precision: 0.85, f_beta: 0.85
train: step: 1272, loss: 0.0870598554611206, acc: 0.925, recall: 0.925, precision: 0.9260651629072681, f_beta: 0.924953095684803
train: step: 1273, loss: 0.12416724115610123, acc: 0.825, recall: 0.825, precision: 0.8703703703703703, f_beta: 0.819471308833011
train: step: 1274, loss: 0.09649314731359482, acc: 0.925, recall: 0.925, precision: 0.9260651629072681, f_beta: 0.924953095684803
train: step: 1275, loss: 0.2775571942329407, acc: 0.85, recall: 0.8500000000000001, precision: 0.8535353535353536, f_beta: 0.849624060150376
train: step: 1276, loss: 0.07036110758781433, acc: 0.9, recall: 0.8999999999999999, precision: 0.904040404040404, f_beta: 0.899749373433584
train: step: 1277, loss: 0.15453417599201202, acc: 0.825, recall: 0.825, precision: 0.8466666666666667, f_beta: 0.8222222222222222
train: step: 1278, loss: 0.09594838321208954, acc: 0.925, recall: 0.925, precision: 0.9260651629072681, f_beta: 0.924953095684803
train: step: 1279, loss: 0.06702914088964462, acc: 0.95, recall: 0.95, precision: 0.9545454545454546, f_beta: 0.949874686716792
train: step: 1280, loss: 0.08218507468700409, acc: 0.975, recall: 0.975, precision: 0.9761904761904762, f_beta: 0.9749843652282677
train: step: 1281, loss: 0.0714278593659401, acc: 0.925, recall: 0.925, precision: 0.9347826086956521, f_beta: 0.9245757385292268
train: step: 1282, loss: 0.0925702303647995, acc: 0.875, recall: 0.875, precision: 0.8836317135549872, f_beta: 0.8742928975487115
train: step: 1283, loss: 0.1306953728199005, acc: 0.825, recall: 0.825, precision: 0.8258145363408521, f_beta: 0.8248905565978737
train: step: 1284, loss: 0.09857919812202454, acc: 0.9, recall: 0.8999999999999999, precision: 0.904040404040404, f_beta: 0.899749373433584
train: step: 1285, loss: 0.19260230660438538, acc: 0.725, recall: 0.725, precision: 0.74, f_beta: 0.7206349206349207
train: step: 1286, loss: 0.1041930690407753, acc: 0.875, recall: 0.875, precision: 0.8759398496240601, f_beta: 0.8749218261413383
train: step: 1287, loss: 0.09444296360015869, acc: 0.85, recall: 0.85, precision: 0.8645833333333333, f_beta: 0.8484848484848484
train: step: 1288, loss: 0.10240241140127182, acc: 0.85, recall: 0.8500000000000001, precision: 0.8535353535353536, f_beta: 0.849624060150376
train: step: 1289, loss: 0.18398982286453247, acc: 0.775, recall: 0.775, precision: 0.8448275862068966, f_beta: 0.7630019749835418
train: step: 1290, loss: 0.020058725029230118, acc: 1.0, recall: 1.0, precision: 1.0, f_beta: 1.0
train: step: 1291, loss: 0.11823713779449463, acc: 0.875, recall: 0.875, precision: 0.8836317135549872, f_beta: 0.8742928975487115
train: step: 1292, loss: 0.04932644963264465, acc: 0.95, recall: 0.95, precision: 0.95, f_beta: 0.9500000000000001
train: step: 1293, loss: 0.07023750245571136, acc: 0.875, recall: 0.875, precision: 0.8759398496240601, f_beta: 0.8749218261413383
train: step: 1294, loss: 0.18359582126140594, acc: 0.775, recall: 0.775, precision: 0.7933333333333333, f_beta: 0.7714285714285714
train: step: 1295, loss: 0.21776776015758514, acc: 0.725, recall: 0.7250000000000001, precision: 0.7564102564102564, f_beta: 0.7163120567375887
train: step: 1296, loss: 0.13378188014030457, acc: 0.825, recall: 0.825, precision: 0.8466666666666667, f_beta: 0.8222222222222222
train: step: 1297, loss: 0.15580298006534576, acc: 0.9, recall: 0.8999999999999999, precision: 0.904040404040404, f_beta: 0.899749373433584
train: step: 1298, loss: 0.1635746955871582, acc: 0.8, recall: 0.8, precision: 0.803030303030303, f_beta: 0.7994987468671679
train: step: 1299, loss: 0.03264421597123146, acc: 0.975, recall: 0.975, precision: 0.9761904761904762, f_beta: 0.9749843652282677
train: step: 1300, loss: 0.1245187297463417, acc: 0.85, recall: 0.8500000000000001, precision: 0.8535353535353536, f_beta: 0.849624060150376
train: step: 1301, loss: 0.16582389175891876, acc: 0.75, recall: 0.75, precision: 0.7747252747252746, f_beta: 0.7442455242966751
train: step: 1302, loss: 0.07334606349468231, acc: 0.925, recall: 0.925, precision: 0.9347826086956521, f_beta: 0.9245757385292268
train: step: 1303, loss: 0.08906881511211395, acc: 0.925, recall: 0.925, precision: 0.9260651629072681, f_beta: 0.924953095684803
train: step: 1304, loss: 0.13239607214927673, acc: 0.825, recall: 0.825, precision: 0.8703703703703703, f_beta: 0.819471308833011
train: step: 1305, loss: 0.26730090379714966, acc: 0.85, recall: 0.85, precision: 0.8645833333333333, f_beta: 0.8484848484848484
train: step: 1306, loss: 0.06332866102457047, acc: 0.95, recall: 0.95, precision: 0.95, f_beta: 0.9500000000000001
train: step: 1307, loss: 0.03812708333134651, acc: 0.975, recall: 0.975, precision: 0.9761904761904762, f_beta: 0.9749843652282677
train: step: 1308, loss: 0.19177505373954773, acc: 0.7, recall: 0.7, precision: 0.702020202020202, f_beta: 0.6992481203007519
train: step: 1309, loss: 0.0251675546169281, acc: 0.975, recall: 0.975, precision: 0.9761904761904762, f_beta: 0.9749843652282677
train: step: 1310, loss: 0.05639020726084709, acc: 0.95, recall: 0.95, precision: 0.9545454545454546, f_beta: 0.949874686716792
train: step: 1311, loss: 0.3365335464477539, acc: 0.8, recall: 0.8, precision: 0.803030303030303, f_beta: 0.7994987468671679
train: step: 1312, loss: 0.21570917963981628, acc: 0.825, recall: 0.825, precision: 0.8258145363408521, f_beta: 0.8248905565978737
train: step: 1313, loss: 0.08304817229509354, acc: 0.925, recall: 0.925, precision: 0.9347826086956521, f_beta: 0.9245757385292268
train: step: 1314, loss: 0.09417398273944855, acc: 0.975, recall: 0.975, precision: 0.9761904761904762, f_beta: 0.9749843652282677
train: step: 1315, loss: 0.10214543342590332, acc: 0.85, recall: 0.8500000000000001, precision: 0.8535353535353536, f_beta: 0.849624060150376
train: step: 1316, loss: 0.11870088428258896, acc: 0.825, recall: 0.825, precision: 0.8466666666666667, f_beta: 0.8222222222222222
train: step: 1317, loss: 0.17687365412712097, acc: 0.725, recall: 0.7250000000000001, precision: 0.7301790281329923, f_beta: 0.7234443746071653
train: step: 1318, loss: 0.3384179174900055, acc: 0.3, recall: 0.3, precision: 0.2619047619047619, f_beta: 0.2708333333333333
train: step: 1319, loss: 0.13227428495883942, acc: 0.825, recall: 0.825, precision: 0.8258145363408521, f_beta: 0.8248905565978737
train: step: 1320, loss: 0.1348041146993637, acc: 0.875, recall: 0.875, precision: 0.8836317135549872, f_beta: 0.8742928975487115
train: step: 1321, loss: 0.38575029373168945, acc: 0.875, recall: 0.875, precision: 0.8759398496240601, f_beta: 0.8749218261413383
train: step: 1322, loss: 0.1680295765399933, acc: 0.8, recall: 0.8, precision: 0.8125, f_beta: 0.797979797979798
train: step: 1323, loss: 0.07100021839141846, acc: 0.9, recall: 0.9, precision: 0.9166666666666667, f_beta: 0.898989898989899
train: step: 1324, loss: 0.09550730884075165, acc: 0.825, recall: 0.825, precision: 0.8324808184143222, f_beta: 0.824010056568196
train: step: 1325, loss: 0.08424662053585052, acc: 0.9, recall: 0.9, precision: 0.9166666666666667, f_beta: 0.898989898989899
train: step: 1326, loss: 0.15324708819389343, acc: 0.85, recall: 0.85, precision: 0.8645833333333333, f_beta: 0.8484848484848484
train: step: 1327, loss: 0.13068343698978424, acc: 0.9, recall: 0.9, precision: 0.9166666666666667, f_beta: 0.898989898989899
train: step: 1328, loss: 0.22161047160625458, acc: 0.65, recall: 0.65, precision: 0.6785714285714286, f_beta: 0.6354166666666667
train: step: 1329, loss: 0.17179374396800995, acc: 0.775, recall: 0.7749999999999999, precision: 0.8133903133903134, f_beta: 0.7678916827853
train: step: 1330, loss: 0.06894408166408539, acc: 0.925, recall: 0.925, precision: 0.9347826086956521, f_beta: 0.9245757385292268
train: step: 1331, loss: 0.29861730337142944, acc: 0.6, recall: 0.6, precision: 0.696078431372549, f_beta: 0.5441595441595442
train: step: 1332, loss: 0.12131668627262115, acc: 0.9, recall: 0.9, precision: 0.9166666666666667, f_beta: 0.898989898989899
train: step: 1333, loss: 0.16995561122894287, acc: 0.725, recall: 0.725, precision: 0.7821316614420062, f_beta: 0.7103357472021066
train: step: 1334, loss: 0.18985585868358612, acc: 0.75, recall: 0.75, precision: 0.7525252525252526, f_beta: 0.7493734335839599
train: step: 1335, loss: 0.0650884285569191, acc: 0.95, recall: 0.95, precision: 0.95, f_beta: 0.9500000000000001
train: step: 1336, loss: 0.12713707983493805, acc: 0.85, recall: 0.85, precision: 0.85, f_beta: 0.85
train: step: 1337, loss: 0.14395079016685486, acc: 0.9, recall: 0.9, precision: 0.9166666666666667, f_beta: 0.898989898989899
train: step: 1338, loss: 0.1475374847650528, acc: 0.775, recall: 0.775, precision: 0.7756892230576441, f_beta: 0.774859287054409
train: step: 1339, loss: 0.08186052739620209, acc: 0.9, recall: 0.8999999999999999, precision: 0.904040404040404, f_beta: 0.899749373433584
train: step: 1340, loss: 0.04180207848548889, acc: 0.95, recall: 0.95, precision: 0.95, f_beta: 0.9500000000000001
train: step: 1341, loss: 0.09002643823623657, acc: 0.875, recall: 0.875, precision: 0.9, f_beta: 0.873015873015873
train: step: 1342, loss: 0.041846781969070435, acc: 0.95, recall: 0.95, precision: 0.9545454545454546, f_beta: 0.949874686716792
train: step: 1343, loss: 0.12351305782794952, acc: 0.825, recall: 0.825, precision: 0.8258145363408521, f_beta: 0.8248905565978737
train: step: 1344, loss: 0.03700781986117363, acc: 0.95, recall: 0.95, precision: 0.9545454545454546, f_beta: 0.949874686716792
train: step: 1345, loss: 0.13823238015174866, acc: 0.875, recall: 0.875, precision: 0.8759398496240601, f_beta: 0.8749218261413383
train: step: 1346, loss: 0.09323470294475555, acc: 0.925, recall: 0.925, precision: 0.9260651629072681, f_beta: 0.924953095684803
train: step: 1347, loss: 0.2113947868347168, acc: 0.775, recall: 0.775, precision: 0.7933333333333333, f_beta: 0.7714285714285714
train: step: 1348, loss: 0.07147466391324997, acc: 0.925, recall: 0.925, precision: 0.9347826086956521, f_beta: 0.9245757385292268
train: step: 1349, loss: 0.252739816904068, acc: 0.775, recall: 0.775, precision: 0.7756892230576441, f_beta: 0.774859287054409
train: step: 1350, loss: 0.04535715654492378, acc: 0.975, recall: 0.975, precision: 0.9761904761904762, f_beta: 0.9749843652282677
train: step: 1351, loss: 0.12934216856956482, acc: 0.85, recall: 0.85, precision: 0.85, f_beta: 0.85
train: step: 1352, loss: 0.16198812425136566, acc: 0.775, recall: 0.775, precision: 0.7933333333333333, f_beta: 0.7714285714285714
train: step: 1353, loss: 0.11855483055114746, acc: 0.85, recall: 0.8500000000000001, precision: 0.8535353535353536, f_beta: 0.849624060150376
train: step: 1354, loss: 0.11934787034988403, acc: 0.85, recall: 0.85, precision: 0.85, f_beta: 0.85
train: step: 1355, loss: 0.20581679046154022, acc: 0.8, recall: 0.8, precision: 0.803030303030303, f_beta: 0.7994987468671679
train: step: 1356, loss: 0.10854245722293854, acc: 0.85, recall: 0.85, precision: 0.8846153846153846, f_beta: 0.8465473145780051
train: step: 1357, loss: 0.05562534183263779, acc: 0.95, recall: 0.95, precision: 0.9545454545454546, f_beta: 0.949874686716792
train: step: 1358, loss: 0.08075318485498428, acc: 0.925, recall: 0.925, precision: 0.9347826086956521, f_beta: 0.9245757385292268
train: step: 1359, loss: 0.17563952505588531, acc: 0.7, recall: 0.7, precision: 0.7380952380952381, f_beta: 0.6875
train: step: 1360, loss: 0.09547485411167145, acc: 0.9, recall: 0.9, precision: 0.9166666666666667, f_beta: 0.898989898989899
train: step: 1361, loss: 0.057697515934705734, acc: 0.925, recall: 0.925, precision: 0.9347826086956521, f_beta: 0.9245757385292268
train: step: 1362, loss: 0.08074153959751129, acc: 0.9, recall: 0.8999999999999999, precision: 0.904040404040404, f_beta: 0.899749373433584
train: step: 1363, loss: 0.22535791993141174, acc: 0.7, recall: 0.7, precision: 0.7197802197802198, f_beta: 0.6930946291560103
train: step: 1364, loss: 0.058920226991176605, acc: 0.925, recall: 0.925, precision: 0.9260651629072681, f_beta: 0.924953095684803
train: step: 1365, loss: 0.1329323798418045, acc: 0.85, recall: 0.8500000000000001, precision: 0.8535353535353536, f_beta: 0.849624060150376
train: step: 1366, loss: 0.07213959842920303, acc: 0.925, recall: 0.925, precision: 0.9347826086956521, f_beta: 0.9245757385292268
train: step: 1367, loss: 0.0787619799375534, acc: 0.9, recall: 0.8999999999999999, precision: 0.904040404040404, f_beta: 0.899749373433584
train: step: 1368, loss: 0.01285643596202135, acc: 1.0, recall: 1.0, precision: 1.0, f_beta: 1.0
train: step: 1369, loss: 0.045655906200408936, acc: 0.975, recall: 0.975, precision: 0.9761904761904762, f_beta: 0.9749843652282677
train: step: 1370, loss: 0.02915031835436821, acc: 1.0, recall: 1.0, precision: 1.0, f_beta: 1.0
train: step: 1371, loss: 0.05234807729721069, acc: 0.925, recall: 0.925, precision: 0.9260651629072681, f_beta: 0.924953095684803
train: step: 1372, loss: 0.09685469418764114, acc: 0.85, recall: 0.85, precision: 0.85, f_beta: 0.85
train: step: 1373, loss: 0.2027701884508133, acc: 0.675, recall: 0.675, precision: 0.719435736677116, f_beta: 0.6576695194206715
train: step: 1374, loss: 0.09638579934835434, acc: 0.875, recall: 0.875, precision: 0.8836317135549872, f_beta: 0.8742928975487115
train: step: 1375, loss: 0.07595132291316986, acc: 0.925, recall: 0.925, precision: 0.9260651629072681, f_beta: 0.924953095684803
train: step: 1376, loss: 0.2928295135498047, acc: 0.725, recall: 0.725, precision: 0.7255639097744361, f_beta: 0.7248280175109443
train: step: 1377, loss: 0.04945485293865204, acc: 0.95, recall: 0.95, precision: 0.95, f_beta: 0.9500000000000001
train: step: 1378, loss: 0.048014260828495026, acc: 0.95, recall: 0.95, precision: 0.9545454545454546, f_beta: 0.949874686716792
train: step: 1379, loss: 0.08318661153316498, acc: 0.875, recall: 0.875, precision: 0.8836317135549872, f_beta: 0.8742928975487115
train: step: 1380, loss: 0.05436438322067261, acc: 0.95, recall: 0.95, precision: 0.9545454545454546, f_beta: 0.949874686716792
train: step: 1381, loss: 0.10595782101154327, acc: 0.85, recall: 0.8500000000000001, precision: 0.8535353535353536, f_beta: 0.849624060150376
train: step: 1382, loss: 0.3616947531700134, acc: 0.625, recall: 0.625, precision: 0.6278772378516624, f_beta: 0.6228786926461345
train: step: 1383, loss: 0.0470995232462883, acc: 0.95, recall: 0.95, precision: 0.95, f_beta: 0.9500000000000001
train: step: 1384, loss: 0.07952418178319931, acc: 0.9, recall: 0.8999999999999999, precision: 0.904040404040404, f_beta: 0.899749373433584
train: step: 1385, loss: 0.1669158935546875, acc: 0.775, recall: 0.7749999999999999, precision: 0.8133903133903134, f_beta: 0.7678916827853
train: step: 1386, loss: 0.0728960782289505, acc: 0.9, recall: 0.9, precision: 0.9, f_beta: 0.9
train: step: 1387, loss: 0.0566522590816021, acc: 0.925, recall: 0.925, precision: 0.9347826086956521, f_beta: 0.9245757385292268
train: step: 1388, loss: 0.02398408204317093, acc: 1.0, recall: 1.0, precision: 1.0, f_beta: 1.0
train: step: 1389, loss: 0.060083210468292236, acc: 0.925, recall: 0.925, precision: 0.9260651629072681, f_beta: 0.924953095684803
train: step: 1390, loss: 0.0929335504770279, acc: 0.9, recall: 0.8999999999999999, precision: 0.904040404040404, f_beta: 0.899749373433584
train: step: 1391, loss: 0.09992110729217529, acc: 0.875, recall: 0.875, precision: 0.9, f_beta: 0.873015873015873
train: step: 1392, loss: 0.08735992014408112, acc: 0.925, recall: 0.925, precision: 0.9260651629072681, f_beta: 0.924953095684803
train: step: 1393, loss: 0.1445016860961914, acc: 0.825, recall: 0.825, precision: 0.8466666666666667, f_beta: 0.8222222222222222
train: step: 1394, loss: 0.1572016179561615, acc: 0.825, recall: 0.825, precision: 0.8324808184143222, f_beta: 0.824010056568196
train: step: 1395, loss: 0.13438847661018372, acc: 0.825, recall: 0.825, precision: 0.8258145363408521, f_beta: 0.8248905565978737
train: step: 1396, loss: 0.09220556169748306, acc: 0.9, recall: 0.9, precision: 0.9166666666666667, f_beta: 0.898989898989899
train: step: 1397, loss: 0.12267059087753296, acc: 0.9, recall: 0.8999999999999999, precision: 0.904040404040404, f_beta: 0.899749373433584
train: step: 1398, loss: 0.0691719576716423, acc: 0.925, recall: 0.925, precision: 0.9347826086956521, f_beta: 0.9245757385292268
train: step: 1399, loss: 0.10398371517658234, acc: 0.875, recall: 0.875, precision: 0.8836317135549872, f_beta: 0.8742928975487115
train: step: 1400, loss: 0.16734060645103455, acc: 0.75, recall: 0.75, precision: 0.7604166666666667, f_beta: 0.7474747474747475
train: step: 1401, loss: 0.040382206439971924, acc: 0.95, recall: 0.95, precision: 0.9545454545454546, f_beta: 0.949874686716792
train: step: 1402, loss: 0.0296010784804821, acc: 0.95, recall: 0.95, precision: 0.9545454545454546, f_beta: 0.949874686716792
train: step: 1403, loss: 0.07631494104862213, acc: 0.925, recall: 0.925, precision: 0.9260651629072681, f_beta: 0.924953095684803
train: step: 1404, loss: 0.17064456641674042, acc: 0.825, recall: 0.825, precision: 0.8466666666666667, f_beta: 0.8222222222222222
train: step: 1405, loss: 0.0862477645277977, acc: 0.85, recall: 0.85, precision: 0.8645833333333333, f_beta: 0.8484848484848484
train: step: 1406, loss: 0.05664151906967163, acc: 0.975, recall: 0.975, precision: 0.9761904761904762, f_beta: 0.9749843652282677
train: step: 1407, loss: 0.0722484141588211, acc: 0.9, recall: 0.8999999999999999, precision: 0.904040404040404, f_beta: 0.899749373433584
train: step: 1408, loss: 0.07096250355243683, acc: 0.9, recall: 0.8999999999999999, precision: 0.904040404040404, f_beta: 0.899749373433584
train: step: 1409, loss: 0.140382781624794, acc: 0.875, recall: 0.875, precision: 0.8836317135549872, f_beta: 0.8742928975487115
train: step: 1410, loss: 0.04252994805574417, acc: 0.95, recall: 0.95, precision: 0.95, f_beta: 0.9500000000000001
train: step: 1411, loss: 0.05717863887548447, acc: 0.925, recall: 0.925, precision: 0.9347826086956521, f_beta: 0.9245757385292268
train: step: 1412, loss: 0.10068899393081665, acc: 0.9, recall: 0.9, precision: 0.9, f_beta: 0.9
train: step: 1413, loss: 0.20488640666007996, acc: 0.7, recall: 0.7, precision: 0.7380952380952381, f_beta: 0.6875
train: step: 1414, loss: 0.04595700651407242, acc: 0.95, recall: 0.95, precision: 0.9545454545454546, f_beta: 0.949874686716792
train: step: 1415, loss: 0.11378387361764908, acc: 0.925, recall: 0.925, precision: 0.9260651629072681, f_beta: 0.924953095684803
train: step: 1416, loss: 0.12747840583324432, acc: 0.825, recall: 0.825, precision: 0.8324808184143222, f_beta: 0.824010056568196
train: step: 1417, loss: 0.04759969562292099, acc: 0.925, recall: 0.925, precision: 0.9347826086956521, f_beta: 0.9245757385292268
train: step: 1418, loss: 0.0493115559220314, acc: 0.975, recall: 0.975, precision: 0.9761904761904762, f_beta: 0.9749843652282677
train: step: 1419, loss: 0.25840896368026733, acc: 0.8, recall: 0.8, precision: 0.8125, f_beta: 0.797979797979798
train: step: 1420, loss: 0.19817069172859192, acc: 0.675, recall: 0.675, precision: 0.719435736677116, f_beta: 0.6576695194206715
train: step: 1421, loss: 0.16540780663490295, acc: 0.775, recall: 0.7749999999999999, precision: 0.8133903133903134, f_beta: 0.7678916827853
train: step: 1422, loss: 0.12701012194156647, acc: 0.85, recall: 0.85, precision: 0.8645833333333333, f_beta: 0.8484848484848484
train: step: 1423, loss: 0.3669869303703308, acc: 0.25, recall: 0.25, precision: 0.16666666666666666, f_beta: 0.2
train: step: 1424, loss: 0.09556391090154648, acc: 0.9, recall: 0.9, precision: 0.9166666666666667, f_beta: 0.898989898989899
train: step: 1425, loss: 0.16190358996391296, acc: 0.825, recall: 0.825, precision: 0.8703703703703703, f_beta: 0.819471308833011
train: step: 1426, loss: 0.0704173818230629, acc: 0.9, recall: 0.9, precision: 0.9, f_beta: 0.9
train: step: 1427, loss: 0.32481497526168823, acc: 0.8, recall: 0.8, precision: 0.8296703296703296, f_beta: 0.7953964194373402
train: step: 1428, loss: 0.09664984047412872, acc: 0.875, recall: 0.875, precision: 0.9, f_beta: 0.873015873015873
train: step: 1429, loss: 0.16621188819408417, acc: 0.75, recall: 0.75, precision: 0.7525252525252526, f_beta: 0.7493734335839599
train: step: 1430, loss: 0.14568395912647247, acc: 0.825, recall: 0.825, precision: 0.8258145363408521, f_beta: 0.8248905565978737
train: step: 1431, loss: 0.1325126588344574, acc: 0.8, recall: 0.8, precision: 0.803030303030303, f_beta: 0.7994987468671679
train: step: 1432, loss: 0.1314588487148285, acc: 0.825, recall: 0.825, precision: 0.8324808184143222, f_beta: 0.824010056568196
train: step: 1433, loss: 0.0426502451300621, acc: 0.95, recall: 0.95, precision: 0.95, f_beta: 0.9500000000000001
train: step: 1434, loss: 0.06521061062812805, acc: 0.925, recall: 0.925, precision: 0.9260651629072681, f_beta: 0.924953095684803
train: step: 1435, loss: 0.09784386307001114, acc: 0.9, recall: 0.8999999999999999, precision: 0.904040404040404, f_beta: 0.899749373433584
train: step: 1436, loss: 0.1988886296749115, acc: 0.7, recall: 0.7, precision: 0.7083333333333333, f_beta: 0.6969696969696968
train: step: 1437, loss: 0.06915900856256485, acc: 0.95, recall: 0.95, precision: 0.9545454545454546, f_beta: 0.949874686716792
train: step: 1438, loss: 0.18102356791496277, acc: 0.75, recall: 0.75, precision: 0.7604166666666667, f_beta: 0.7474747474747475
train: step: 1439, loss: 0.1688292771577835, acc: 0.75, recall: 0.75, precision: 0.7976190476190477, f_beta: 0.7395833333333333
train: step: 1440, loss: 0.40253329277038574, acc: 0.9, recall: 0.9, precision: 0.9166666666666667, f_beta: 0.898989898989899
train: step: 1441, loss: 0.15571026504039764, acc: 0.825, recall: 0.825, precision: 0.8258145363408521, f_beta: 0.8248905565978737
train: step: 1442, loss: 0.06834299862384796, acc: 0.925, recall: 0.925, precision: 0.9260651629072681, f_beta: 0.924953095684803
train: step: 1443, loss: 0.09356068074703217, acc: 0.975, recall: 0.975, precision: 0.9761904761904762, f_beta: 0.9749843652282677
train: step: 1444, loss: 0.021725976839661598, acc: 1.0, recall: 1.0, precision: 1.0, f_beta: 1.0
train: step: 1445, loss: 0.11914937198162079, acc: 0.825, recall: 0.825, precision: 0.8258145363408521, f_beta: 0.8248905565978737
train: step: 1446, loss: 0.0984402522444725, acc: 0.875, recall: 0.875, precision: 0.8759398496240601, f_beta: 0.8749218261413383
train: step: 1447, loss: 0.06801040470600128, acc: 0.9, recall: 0.9, precision: 0.9166666666666667, f_beta: 0.898989898989899
train: step: 1448, loss: 0.0525272972881794, acc: 0.925, recall: 0.925, precision: 0.9260651629072681, f_beta: 0.924953095684803
train: step: 1449, loss: 0.16205087304115295, acc: 0.85, recall: 0.8500000000000001, precision: 0.8535353535353536, f_beta: 0.849624060150376
train: step: 1450, loss: 0.2422572821378708, acc: 0.675, recall: 0.675, precision: 0.803030303030303, f_beta: 0.636617749825297
train: step: 1451, loss: 0.08744678646326065, acc: 0.9, recall: 0.8999999999999999, precision: 0.904040404040404, f_beta: 0.899749373433584
train: step: 1452, loss: 0.07065080106258392, acc: 0.9, recall: 0.9, precision: 0.9166666666666667, f_beta: 0.898989898989899
train: step: 1453, loss: 0.058043621480464935, acc: 0.95, recall: 0.95, precision: 0.9545454545454546, f_beta: 0.949874686716792
train: step: 1454, loss: 0.12537269294261932, acc: 0.875, recall: 0.875, precision: 0.9, f_beta: 0.873015873015873
train: step: 1455, loss: 0.09329649806022644, acc: 0.875, recall: 0.875, precision: 0.8759398496240601, f_beta: 0.8749218261413383
train: step: 1456, loss: 0.17465156316757202, acc: 0.875, recall: 0.875, precision: 0.8759398496240601, f_beta: 0.8749218261413383
train: step: 1457, loss: 0.2676665186882019, acc: 0.725, recall: 0.725, precision: 0.7255639097744361, f_beta: 0.7248280175109443
train: step: 1458, loss: 0.04891340062022209, acc: 0.95, recall: 0.95, precision: 0.95, f_beta: 0.9500000000000001
train: step: 1459, loss: 0.0820411667227745, acc: 0.925, recall: 0.925, precision: 0.9260651629072681, f_beta: 0.924953095684803
train: step: 1460, loss: 0.03727233037352562, acc: 0.95, recall: 0.95, precision: 0.9545454545454546, f_beta: 0.949874686716792
train: step: 1461, loss: 0.13693980872631073, acc: 0.775, recall: 0.775, precision: 0.8448275862068966, f_beta: 0.7630019749835418
train: step: 1462, loss: 0.2055683583021164, acc: 0.7, recall: 0.7, precision: 0.7197802197802198, f_beta: 0.6930946291560103
train: step: 1463, loss: 0.09829509258270264, acc: 0.85, recall: 0.8500000000000001, precision: 0.8535353535353536, f_beta: 0.849624060150376
train: step: 1464, loss: 0.07171974331140518, acc: 0.9, recall: 0.8999999999999999, precision: 0.904040404040404, f_beta: 0.899749373433584
train: step: 1465, loss: 0.18936431407928467, acc: 0.7, recall: 0.7, precision: 0.702020202020202, f_beta: 0.6992481203007519
train: step: 1466, loss: 0.10112164914608002, acc: 0.85, recall: 0.85, precision: 0.85, f_beta: 0.85
train: step: 1467, loss: 0.026946639642119408, acc: 0.975, recall: 0.975, precision: 0.9761904761904762, f_beta: 0.9749843652282677
train: step: 1468, loss: 0.2983303368091583, acc: 0.675, recall: 0.675, precision: 0.6754385964912281, f_beta: 0.6747967479674797
train: step: 1469, loss: 0.0958014577627182, acc: 0.9, recall: 0.9, precision: 0.9166666666666667, f_beta: 0.898989898989899
train: step: 1470, loss: 0.0710659846663475, acc: 0.925, recall: 0.925, precision: 0.9260651629072681, f_beta: 0.924953095684803
train: step: 1471, loss: 0.05462029576301575, acc: 0.925, recall: 0.925, precision: 0.9260651629072681, f_beta: 0.924953095684803
train: step: 1472, loss: 0.08459372818470001, acc: 0.875, recall: 0.875, precision: 0.8759398496240601, f_beta: 0.8749218261413383
train: step: 1473, loss: 0.21163420379161835, acc: 0.975, recall: 0.975, precision: 0.9761904761904762, f_beta: 0.9749843652282677
train: step: 1474, loss: 0.08906514942646027, acc: 0.875, recall: 0.875, precision: 0.8759398496240601, f_beta: 0.8749218261413383
train: step: 1475, loss: 0.1972360610961914, acc: 0.9, recall: 0.9, precision: 0.9, f_beta: 0.9
train: step: 1476, loss: 0.2434471845626831, acc: 0.675, recall: 0.675, precision: 0.6994301994301995, f_beta: 0.6647324306898775
train: step: 1477, loss: 0.038134053349494934, acc: 0.925, recall: 0.925, precision: 0.9260651629072681, f_beta: 0.924953095684803
train: step: 1478, loss: 0.08610888570547104, acc: 0.925, recall: 0.925, precision: 0.9347826086956521, f_beta: 0.9245757385292268
train: step: 1479, loss: 0.056754596531391144, acc: 0.95, recall: 0.95, precision: 0.95, f_beta: 0.9500000000000001
train: step: 1480, loss: 0.1729852259159088, acc: 0.725, recall: 0.7250000000000001, precision: 0.7564102564102564, f_beta: 0.7163120567375887
train: step: 1481, loss: 0.22075970470905304, acc: 0.7, recall: 0.7, precision: 0.7380952380952381, f_beta: 0.6875
train: step: 1482, loss: 0.227132648229599, acc: 0.675, recall: 0.675, precision: 0.6994301994301995, f_beta: 0.6647324306898775
train: step: 1483, loss: 0.2108103334903717, acc: 0.7, recall: 0.7, precision: 0.7666666666666666, f_beta: 0.6799999999999999
train: step: 1484, loss: 0.05121409893035889, acc: 0.95, recall: 0.95, precision: 0.9545454545454546, f_beta: 0.949874686716792
train: step: 1485, loss: 0.15853798389434814, acc: 0.775, recall: 0.775, precision: 0.7933333333333333, f_beta: 0.7714285714285714
train: step: 1486, loss: 0.1926920861005783, acc: 0.8, recall: 0.8, precision: 0.8296703296703296, f_beta: 0.7953964194373402
train: step: 1487, loss: 0.26769667863845825, acc: 0.65, recall: 0.65, precision: 0.65, f_beta: 0.65
train: step: 1488, loss: 0.18200618028640747, acc: 0.8, recall: 0.8, precision: 0.8, f_beta: 0.8000000000000002
train: step: 1489, loss: 0.07442130893468857, acc: 0.9, recall: 0.8999999999999999, precision: 0.904040404040404, f_beta: 0.899749373433584
train: step: 1490, loss: 0.06937985122203827, acc: 0.875, recall: 0.875, precision: 0.8759398496240601, f_beta: 0.8749218261413383
train: step: 1491, loss: 0.13444536924362183, acc: 0.95, recall: 0.95, precision: 0.95, f_beta: 0.9500000000000001
train: step: 1492, loss: 0.12258787453174591, acc: 0.85, recall: 0.85, precision: 0.8645833333333333, f_beta: 0.8484848484848484
train: step: 1493, loss: 0.2580440044403076, acc: 0.975, recall: 0.975, precision: 0.9761904761904762, f_beta: 0.9749843652282677
train: step: 1494, loss: 0.3022342920303345, acc: 0.25, recall: 0.25, precision: 0.16666666666666666, f_beta: 0.2
train: step: 1495, loss: 0.11337605863809586, acc: 0.825, recall: 0.825, precision: 0.8258145363408521, f_beta: 0.8248905565978737
train: step: 1496, loss: 0.14292220771312714, acc: 0.775, recall: 0.775, precision: 0.7933333333333333, f_beta: 0.7714285714285714
train: step: 1497, loss: 0.08508794009685516, acc: 0.9, recall: 0.9, precision: 0.9, f_beta: 0.9
train: step: 1498, loss: 0.11748454719781876, acc: 0.875, recall: 0.875, precision: 0.8836317135549872, f_beta: 0.8742928975487115
train: step: 1499, loss: 0.12199459969997406, acc: 0.875, recall: 0.875, precision: 0.9, f_beta: 0.873015873015873
train: step: 1500, loss: 0.052936434745788574, acc: 0.975, recall: 0.975, precision: 0.9761904761904762, f_beta: 0.9749843652282677
train: step: 1501, loss: 0.17395761609077454, acc: 0.875, recall: 0.875, precision: 0.8759398496240601, f_beta: 0.8749218261413383
train: step: 1502, loss: 0.08854372799396515, acc: 0.9, recall: 0.8999999999999999, precision: 0.904040404040404, f_beta: 0.899749373433584
train: step: 1503, loss: 0.21872015297412872, acc: 0.675, recall: 0.675, precision: 0.803030303030303, f_beta: 0.636617749825297
train: step: 1504, loss: 0.056063033640384674, acc: 0.95, recall: 0.95, precision: 0.9545454545454546, f_beta: 0.949874686716792
train: step: 1505, loss: 0.07245311886072159, acc: 0.925, recall: 0.925, precision: 0.9260651629072681, f_beta: 0.924953095684803
train: step: 1506, loss: 0.1346566528081894, acc: 0.85, recall: 0.85, precision: 0.8846153846153846, f_beta: 0.8465473145780051
train: step: 1507, loss: 0.09148873388767242, acc: 0.9, recall: 0.9, precision: 0.9166666666666667, f_beta: 0.898989898989899
train: step: 1508, loss: 0.030307423323392868, acc: 0.975, recall: 0.975, precision: 0.9761904761904762, f_beta: 0.9749843652282677
train: step: 1509, loss: 0.07932507246732712, acc: 0.875, recall: 0.875, precision: 0.8759398496240601, f_beta: 0.8749218261413383
train: step: 1510, loss: 0.12051872909069061, acc: 0.825, recall: 0.825, precision: 0.8258145363408521, f_beta: 0.8248905565978737
train: step: 1511, loss: 0.09902319312095642, acc: 0.85, recall: 0.85, precision: 0.8645833333333333, f_beta: 0.8484848484848484
train: step: 1512, loss: 0.030438493937253952, acc: 0.95, recall: 0.95, precision: 0.9545454545454546, f_beta: 0.949874686716792
train: step: 1513, loss: 0.32914814352989197, acc: 0.75, recall: 0.75, precision: 0.7525252525252526, f_beta: 0.7493734335839599
train: step: 1514, loss: 0.07283283770084381, acc: 0.925, recall: 0.925, precision: 0.9260651629072681, f_beta: 0.924953095684803
train: step: 1515, loss: 0.05074276402592659, acc: 0.95, recall: 0.95, precision: 0.95, f_beta: 0.9500000000000001
train: step: 1516, loss: 0.1250608265399933, acc: 0.85, recall: 0.85, precision: 0.8846153846153846, f_beta: 0.8465473145780051
train: step: 1517, loss: 0.03542513772845268, acc: 0.975, recall: 0.975, precision: 0.9761904761904762, f_beta: 0.9749843652282677
train: step: 1518, loss: 0.10628093779087067, acc: 0.875, recall: 0.875, precision: 0.8759398496240601, f_beta: 0.8749218261413383
train: step: 1519, loss: 0.08979233354330063, acc: 0.9, recall: 0.9, precision: 0.9166666666666667, f_beta: 0.898989898989899
train: step: 1520, loss: 0.023674376308918, acc: 0.975, recall: 0.975, precision: 0.9761904761904762, f_beta: 0.9749843652282677
train: step: 1521, loss: 0.0964430496096611, acc: 0.9, recall: 0.8999999999999999, precision: 0.904040404040404, f_beta: 0.899749373433584
train: step: 1522, loss: 0.06398531049489975, acc: 0.9, recall: 0.9, precision: 0.9166666666666667, f_beta: 0.898989898989899
train: step: 1523, loss: 0.2872467041015625, acc: 0.675, recall: 0.675, precision: 0.6866666666666666, f_beta: 0.6698412698412699
train: step: 1524, loss: 0.062064267694950104, acc: 0.925, recall: 0.925, precision: 0.9260651629072681, f_beta: 0.924953095684803
train: step: 1525, loss: 0.05108971148729324, acc: 0.95, recall: 0.95, precision: 0.95, f_beta: 0.9500000000000001
train: step: 1526, loss: 0.047518789768218994, acc: 0.95, recall: 0.95, precision: 0.9545454545454546, f_beta: 0.949874686716792
train: step: 1527, loss: 0.039463937282562256, acc: 0.95, recall: 0.95, precision: 0.9545454545454546, f_beta: 0.949874686716792
train: step: 1528, loss: 0.37545761466026306, acc: 0.55, recall: 0.55, precision: 0.554945054945055, f_beta: 0.5396419437340153
train: step: 1529, loss: 0.06780482828617096, acc: 0.95, recall: 0.95, precision: 0.9545454545454546, f_beta: 0.949874686716792
train: step: 1530, loss: 0.09392421692609787, acc: 0.825, recall: 0.825, precision: 0.8324808184143222, f_beta: 0.824010056568196
train: step: 1531, loss: 0.1771087497472763, acc: 0.75, recall: 0.75, precision: 0.7604166666666667, f_beta: 0.7474747474747475
train: step: 1532, loss: 0.35734492540359497, acc: 0.8, recall: 0.8, precision: 0.8125, f_beta: 0.797979797979798
train: step: 1533, loss: 0.038905657827854156, acc: 0.975, recall: 0.975, precision: 0.9761904761904762, f_beta: 0.9749843652282677
train: step: 1534, loss: 0.3086027503013611, acc: 0.825, recall: 0.825, precision: 0.8466666666666667, f_beta: 0.8222222222222222
train: step: 1535, loss: 0.05967475101351738, acc: 0.925, recall: 0.925, precision: 0.9260651629072681, f_beta: 0.924953095684803
train: step: 1536, loss: 0.1301937997341156, acc: 0.85, recall: 0.85, precision: 0.8846153846153846, f_beta: 0.8465473145780051
train: step: 1537, loss: 0.16888025403022766, acc: 0.775, recall: 0.7749999999999999, precision: 0.8133903133903134, f_beta: 0.7678916827853
train: step: 1538, loss: 0.34430617094039917, acc: 0.65, recall: 0.65, precision: 0.65, f_beta: 0.65
train: step: 1539, loss: 0.13053379952907562, acc: 0.875, recall: 0.875, precision: 0.8759398496240601, f_beta: 0.8749218261413383
train: step: 1540, loss: 0.30098074674606323, acc: 0.775, recall: 0.7749999999999999, precision: 0.8133903133903134, f_beta: 0.7678916827853
train: step: 1541, loss: 0.16428646445274353, acc: 0.8, recall: 0.8, precision: 0.8296703296703296, f_beta: 0.7953964194373402
train: step: 1542, loss: 0.13613562285900116, acc: 0.85, recall: 0.8500000000000001, precision: 0.8535353535353536, f_beta: 0.849624060150376
train: step: 1543, loss: 0.12045296281576157, acc: 0.85, recall: 0.8500000000000001, precision: 0.8535353535353536, f_beta: 0.849624060150376
train: step: 1544, loss: 0.08387763798236847, acc: 0.875, recall: 0.875, precision: 0.9, f_beta: 0.873015873015873
train: step: 1545, loss: 0.13955101370811462, acc: 0.85, recall: 0.85, precision: 0.8846153846153846, f_beta: 0.8465473145780051
train: step: 1546, loss: 0.1793341189622879, acc: 0.875, recall: 0.875, precision: 0.9, f_beta: 0.873015873015873
train: step: 1547, loss: 0.09454619884490967, acc: 0.875, recall: 0.875, precision: 0.8759398496240601, f_beta: 0.8749218261413383
train: step: 1548, loss: 0.09340765327215195, acc: 0.9, recall: 0.8999999999999999, precision: 0.904040404040404, f_beta: 0.899749373433584
train: step: 1549, loss: 0.18427887558937073, acc: 0.725, recall: 0.7250000000000001, precision: 0.7564102564102564, f_beta: 0.7163120567375887
train: step: 1550, loss: 0.15880846977233887, acc: 0.8, recall: 0.8, precision: 0.8125, f_beta: 0.797979797979798
train: step: 1551, loss: 0.1175769567489624, acc: 0.875, recall: 0.875, precision: 0.8759398496240601, f_beta: 0.8749218261413383
train: step: 1552, loss: 0.09499617666006088, acc: 0.9, recall: 0.8999999999999999, precision: 0.904040404040404, f_beta: 0.899749373433584
train: step: 1553, loss: 0.14536097645759583, acc: 0.85, recall: 0.85, precision: 0.8645833333333333, f_beta: 0.8484848484848484
train: step: 1554, loss: 0.12218542397022247, acc: 0.85, recall: 0.8500000000000001, precision: 0.8535353535353536, f_beta: 0.849624060150376
train: step: 1555, loss: 0.08394815772771835, acc: 0.925, recall: 0.925, precision: 0.9260651629072681, f_beta: 0.924953095684803
train: step: 1556, loss: 0.25272947549819946, acc: 0.725, recall: 0.725, precision: 0.7255639097744361, f_beta: 0.7248280175109443
train: step: 1557, loss: 0.10159003734588623, acc: 0.875, recall: 0.875, precision: 0.8836317135549872, f_beta: 0.8742928975487115
train: step: 1558, loss: 0.25579050183296204, acc: 0.675, recall: 0.675, precision: 0.719435736677116, f_beta: 0.6576695194206715
train: step: 1559, loss: 0.18934185802936554, acc: 0.8, recall: 0.8, precision: 0.8296703296703296, f_beta: 0.7953964194373402
train: step: 1560, loss: 0.09248752146959305, acc: 0.9, recall: 0.9, precision: 0.9, f_beta: 0.9
train: step: 1561, loss: 0.19753527641296387, acc: 0.725, recall: 0.7250000000000001, precision: 0.7301790281329923, f_beta: 0.7234443746071653
train: step: 1562, loss: 0.20016512274742126, acc: 0.675, recall: 0.675, precision: 0.6790281329923273, f_beta: 0.6731615336266499
train: step: 1563, loss: 0.23132061958312988, acc: 0.7, recall: 0.7, precision: 0.7380952380952381, f_beta: 0.6875
train: step: 1564, loss: 0.10678122192621231, acc: 0.9, recall: 0.9, precision: 0.9166666666666667, f_beta: 0.898989898989899
train: step: 1565, loss: 0.06656961143016815, acc: 0.875, recall: 0.875, precision: 0.8759398496240601, f_beta: 0.8749218261413383
train: step: 1566, loss: 0.12712042033672333, acc: 0.825, recall: 0.825, precision: 0.8258145363408521, f_beta: 0.8248905565978737
train: step: 1567, loss: 0.12278227508068085, acc: 0.825, recall: 0.825, precision: 0.8703703703703703, f_beta: 0.819471308833011
train: step: 1568, loss: 0.12262117862701416, acc: 0.85, recall: 0.85, precision: 0.8846153846153846, f_beta: 0.8465473145780051
train: step: 1569, loss: 0.19227388501167297, acc: 0.875, recall: 0.875, precision: 0.8759398496240601, f_beta: 0.8749218261413383
train: step: 1570, loss: 0.2819725275039673, acc: 0.85, recall: 0.85, precision: 0.8645833333333333, f_beta: 0.8484848484848484
train: step: 1571, loss: 0.11200523376464844, acc: 0.85, recall: 0.85, precision: 0.8645833333333333, f_beta: 0.8484848484848484
train: step: 1572, loss: 0.08340264111757278, acc: 0.9, recall: 0.8999999999999999, precision: 0.904040404040404, f_beta: 0.899749373433584
train: step: 1573, loss: 0.028824040666222572, acc: 0.95, recall: 0.95, precision: 0.9545454545454546, f_beta: 0.949874686716792
train: step: 1574, loss: 0.04445422813296318, acc: 0.95, recall: 0.95, precision: 0.95, f_beta: 0.9500000000000001
train: step: 1575, loss: 0.14318743348121643, acc: 0.825, recall: 0.825, precision: 0.8324808184143222, f_beta: 0.824010056568196
train: step: 1576, loss: 0.10088610649108887, acc: 0.875, recall: 0.875, precision: 0.8759398496240601, f_beta: 0.8749218261413383
train: step: 1577, loss: 0.11971871554851532, acc: 0.9, recall: 0.8999999999999999, precision: 0.904040404040404, f_beta: 0.899749373433584
train: step: 1578, loss: 0.04228196293115616, acc: 1.0, recall: 1.0, precision: 1.0, f_beta: 1.0
train: step: 1579, loss: 0.0777924656867981, acc: 0.9, recall: 0.9, precision: 0.9166666666666667, f_beta: 0.898989898989899
train: step: 1580, loss: 0.1686466634273529, acc: 0.775, recall: 0.7749999999999999, precision: 0.7813299232736572, f_beta: 0.7737272155876807
train: step: 1581, loss: 0.13818220794200897, acc: 0.85, recall: 0.8500000000000001, precision: 0.8535353535353536, f_beta: 0.849624060150376
train: step: 1582, loss: 0.08979475498199463, acc: 0.9, recall: 0.8999999999999999, precision: 0.904040404040404, f_beta: 0.899749373433584
train: step: 1583, loss: 0.07867971062660217, acc: 0.925, recall: 0.925, precision: 0.9260651629072681, f_beta: 0.924953095684803
train: step: 1584, loss: 0.09611347317695618, acc: 0.925, recall: 0.925, precision: 0.9260651629072681, f_beta: 0.924953095684803
train: step: 1585, loss: 0.22626809775829315, acc: 0.675, recall: 0.675, precision: 0.7508960573476702, f_beta: 0.6484110885733605
train: step: 1586, loss: 0.10216452926397324, acc: 0.925, recall: 0.925, precision: 0.9260651629072681, f_beta: 0.924953095684803
train: step: 1587, loss: 0.13043615221977234, acc: 0.8, recall: 0.8, precision: 0.8125, f_beta: 0.797979797979798
train: step: 1588, loss: 0.15846893191337585, acc: 0.775, recall: 0.775, precision: 0.7933333333333333, f_beta: 0.7714285714285714
train: step: 1589, loss: 0.08215516805648804, acc: 0.9, recall: 0.8999999999999999, precision: 0.904040404040404, f_beta: 0.899749373433584
train: step: 1590, loss: 0.0673135444521904, acc: 0.925, recall: 0.925, precision: 0.9260651629072681, f_beta: 0.924953095684803
train: step: 1591, loss: 0.08668960630893707, acc: 0.875, recall: 0.875, precision: 0.8759398496240601, f_beta: 0.8749218261413383
train: step: 1592, loss: 0.08644529432058334, acc: 0.925, recall: 0.925, precision: 0.9260651629072681, f_beta: 0.924953095684803
train: step: 1593, loss: 0.046092234551906586, acc: 0.95, recall: 0.95, precision: 0.95, f_beta: 0.9500000000000001
train: step: 1594, loss: 0.22522079944610596, acc: 0.75, recall: 0.75, precision: 0.7604166666666667, f_beta: 0.7474747474747475
train: step: 1595, loss: 0.18126803636550903, acc: 0.775, recall: 0.7749999999999999, precision: 0.7813299232736572, f_beta: 0.7737272155876807
train: step: 1596, loss: 0.2894469201564789, acc: 0.625, recall: 0.625, precision: 0.6567398119122256, f_beta: 0.6050032916392363
train: step: 1597, loss: 0.12959814071655273, acc: 0.825, recall: 0.825, precision: 0.8258145363408521, f_beta: 0.8248905565978737
train: step: 1598, loss: 0.09169481694698334, acc: 0.875, recall: 0.875, precision: 0.9, f_beta: 0.873015873015873
train: step: 1599, loss: 0.3760830760002136, acc: 0.8, recall: 0.8, precision: 0.8296703296703296, f_beta: 0.7953964194373402
train: step: 1600, loss: 0.03607585281133652, acc: 0.975, recall: 0.975, precision: 0.9761904761904762, f_beta: 0.9749843652282677
train: step: 1601, loss: 0.08321353048086166, acc: 0.925, recall: 0.925, precision: 0.9260651629072681, f_beta: 0.924953095684803
train: step: 1602, loss: 0.15026921033859253, acc: 0.825, recall: 0.825, precision: 0.8324808184143222, f_beta: 0.824010056568196
train: step: 1603, loss: 0.282451331615448, acc: 0.625, recall: 0.625, precision: 0.6424501424501424, f_beta: 0.6131528046421664
train: step: 1604, loss: 0.02403033897280693, acc: 1.0, recall: 1.0, precision: 1.0, f_beta: 1.0
train: step: 1605, loss: 0.28546538949012756, acc: 0.825, recall: 0.825, precision: 0.8466666666666667, f_beta: 0.8222222222222222
train: step: 1606, loss: 0.09187901765108109, acc: 0.9, recall: 0.9, precision: 0.9, f_beta: 0.9
train: step: 1607, loss: 0.16027934849262238, acc: 0.775, recall: 0.775, precision: 0.7933333333333333, f_beta: 0.7714285714285714
train: step: 1608, loss: 0.07389792054891586, acc: 0.95, recall: 0.95, precision: 0.9545454545454546, f_beta: 0.949874686716792
train: step: 1609, loss: 0.08700641989707947, acc: 0.9, recall: 0.8999999999999999, precision: 0.904040404040404, f_beta: 0.899749373433584
train: step: 1610, loss: 0.08071403205394745, acc: 0.875, recall: 0.875, precision: 0.9, f_beta: 0.873015873015873
train: step: 1611, loss: 0.21320033073425293, acc: 0.875, recall: 0.875, precision: 0.8759398496240601, f_beta: 0.8749218261413383
train: step: 1612, loss: 0.11348302662372589, acc: 0.875, recall: 0.875, precision: 0.8759398496240601, f_beta: 0.8749218261413383
train: step: 1613, loss: 0.15427811443805695, acc: 0.825, recall: 0.825, precision: 0.8466666666666667, f_beta: 0.8222222222222222
train: step: 1614, loss: 0.04052529111504555, acc: 0.95, recall: 0.95, precision: 0.95, f_beta: 0.9500000000000001
train: step: 1615, loss: 0.06229112297296524, acc: 0.925, recall: 0.925, precision: 0.9260651629072681, f_beta: 0.924953095684803
train: step: 1616, loss: 0.0984404981136322, acc: 0.9, recall: 0.9, precision: 0.9166666666666667, f_beta: 0.898989898989899
train: step: 1617, loss: 0.10176029056310654, acc: 0.925, recall: 0.925, precision: 0.9260651629072681, f_beta: 0.924953095684803
train: step: 1618, loss: 0.13842466473579407, acc: 0.975, recall: 0.975, precision: 0.9761904761904762, f_beta: 0.9749843652282677
train: step: 1619, loss: 0.3486183285713196, acc: 0.65, recall: 0.6499999999999999, precision: 0.6515151515151515, f_beta: 0.6491228070175439
train: step: 1620, loss: 0.08790735900402069, acc: 0.875, recall: 0.875, precision: 0.9, f_beta: 0.873015873015873
train: step: 1621, loss: 0.08237628638744354, acc: 0.875, recall: 0.875, precision: 0.9, f_beta: 0.873015873015873
train: step: 1622, loss: 0.03456232696771622, acc: 0.95, recall: 0.95, precision: 0.95, f_beta: 0.9500000000000001
train: step: 1623, loss: 0.0570715069770813, acc: 0.925, recall: 0.925, precision: 0.9347826086956521, f_beta: 0.9245757385292268
train: step: 1624, loss: 0.09970380365848541, acc: 0.875, recall: 0.875, precision: 0.9, f_beta: 0.873015873015873
train: step: 1625, loss: 0.06716294586658478, acc: 0.9, recall: 0.9, precision: 0.9166666666666667, f_beta: 0.898989898989899
train: step: 1626, loss: 0.28743332624435425, acc: 0.95, recall: 0.95, precision: 0.9545454545454546, f_beta: 0.949874686716792
train: step: 1627, loss: 0.07186023145914078, acc: 0.95, recall: 0.95, precision: 0.95, f_beta: 0.9500000000000001
train: step: 1628, loss: 0.10149526596069336, acc: 0.9, recall: 0.8999999999999999, precision: 0.904040404040404, f_beta: 0.899749373433584
train: step: 1629, loss: 0.029078179970383644, acc: 0.975, recall: 0.975, precision: 0.9761904761904762, f_beta: 0.9749843652282677
train: step: 1630, loss: 0.3444898724555969, acc: 0.7, recall: 0.7, precision: 0.702020202020202, f_beta: 0.6992481203007519
train: step: 1631, loss: 0.06052219867706299, acc: 0.925, recall: 0.925, precision: 0.9347826086956521, f_beta: 0.9245757385292268
train: step: 1632, loss: 0.09885008633136749, acc: 0.875, recall: 0.875, precision: 0.8759398496240601, f_beta: 0.8749218261413383
train: step: 1633, loss: 0.08873756974935532, acc: 0.925, recall: 0.925, precision: 0.9260651629072681, f_beta: 0.924953095684803
train: step: 1634, loss: 0.09991632401943207, acc: 0.875, recall: 0.875, precision: 0.8836317135549872, f_beta: 0.8742928975487115
train: step: 1635, loss: 0.22321510314941406, acc: 0.75, recall: 0.75, precision: 0.7976190476190477, f_beta: 0.7395833333333333
train: step: 1636, loss: 0.08699077367782593, acc: 0.925, recall: 0.925, precision: 0.9347826086956521, f_beta: 0.9245757385292268
train: step: 1637, loss: 0.16756102442741394, acc: 0.75, recall: 0.75, precision: 0.7976190476190477, f_beta: 0.7395833333333333
train: step: 1638, loss: 0.07049237191677094, acc: 0.925, recall: 0.925, precision: 0.9260651629072681, f_beta: 0.924953095684803
train: step: 1639, loss: 0.17668035626411438, acc: 0.775, recall: 0.7749999999999999, precision: 0.8133903133903134, f_beta: 0.7678916827853
train: step: 1640, loss: 0.11188540607690811, acc: 0.825, recall: 0.825, precision: 0.8466666666666667, f_beta: 0.8222222222222222
train: step: 1641, loss: 0.1938568502664566, acc: 0.775, recall: 0.775, precision: 0.8448275862068966, f_beta: 0.7630019749835418
train: step: 1642, loss: 0.0854438915848732, acc: 0.9, recall: 0.9, precision: 0.9166666666666667, f_beta: 0.898989898989899
train: step: 1643, loss: 0.07355935871601105, acc: 0.95, recall: 0.95, precision: 0.9545454545454546, f_beta: 0.949874686716792
train: step: 1644, loss: 0.07221799343824387, acc: 0.925, recall: 0.925, precision: 0.9260651629072681, f_beta: 0.924953095684803
train: step: 1645, loss: 0.08892073482275009, acc: 0.925, recall: 0.925, precision: 0.9347826086956521, f_beta: 0.9245757385292268
train: step: 1646, loss: 0.09509120881557465, acc: 0.875, recall: 0.875, precision: 0.8836317135549872, f_beta: 0.8742928975487115
train: step: 1647, loss: 0.029761429876089096, acc: 0.95, recall: 0.95, precision: 0.9545454545454546, f_beta: 0.949874686716792
train: step: 1648, loss: 0.12975765764713287, acc: 0.85, recall: 0.85, precision: 0.8645833333333333, f_beta: 0.8484848484848484
train: step: 1649, loss: 0.17018850147724152, acc: 0.775, recall: 0.775, precision: 0.8448275862068966, f_beta: 0.7630019749835418
train: step: 1650, loss: 0.14426690340042114, acc: 0.825, recall: 0.825, precision: 0.8258145363408521, f_beta: 0.8248905565978737
train: step: 1651, loss: 0.15459662675857544, acc: 0.8, recall: 0.8, precision: 0.803030303030303, f_beta: 0.7994987468671679
train: step: 1652, loss: 0.06626884639263153, acc: 0.95, recall: 0.95, precision: 0.9545454545454546, f_beta: 0.949874686716792
train: step: 1653, loss: 0.1004214733839035, acc: 0.875, recall: 0.875, precision: 0.8836317135549872, f_beta: 0.8742928975487115
train: step: 1654, loss: 0.09078168869018555, acc: 0.9, recall: 0.9, precision: 0.9166666666666667, f_beta: 0.898989898989899
train: step: 1655, loss: 0.07151763141155243, acc: 0.95, recall: 0.95, precision: 0.9545454545454546, f_beta: 0.949874686716792
train: step: 1656, loss: 0.06206796318292618, acc: 0.95, recall: 0.95, precision: 0.9545454545454546, f_beta: 0.949874686716792
train: step: 1657, loss: 0.13579563796520233, acc: 0.85, recall: 0.85, precision: 0.8645833333333333, f_beta: 0.8484848484848484
train: step: 1658, loss: 0.0847785621881485, acc: 0.95, recall: 0.95, precision: 0.9545454545454546, f_beta: 0.949874686716792
train: step: 1659, loss: 0.16827091574668884, acc: 0.825, recall: 0.825, precision: 0.8324808184143222, f_beta: 0.824010056568196
train: step: 1660, loss: 0.15281888842582703, acc: 0.85, recall: 0.8500000000000001, precision: 0.8535353535353536, f_beta: 0.849624060150376
train: step: 1661, loss: 0.025518452748656273, acc: 0.975, recall: 0.975, precision: 0.9761904761904762, f_beta: 0.9749843652282677
train: step: 1662, loss: 0.25549864768981934, acc: 0.725, recall: 0.7250000000000001, precision: 0.7564102564102564, f_beta: 0.7163120567375887
train: step: 1663, loss: 0.15316377580165863, acc: 0.85, recall: 0.85, precision: 0.8645833333333333, f_beta: 0.8484848484848484
train: step: 1664, loss: 0.1534002274274826, acc: 0.775, recall: 0.7749999999999999, precision: 0.8133903133903134, f_beta: 0.7678916827853
train: step: 1665, loss: 0.036268871277570724, acc: 0.975, recall: 0.975, precision: 0.9761904761904762, f_beta: 0.9749843652282677
train: step: 1666, loss: 0.01788368634879589, acc: 1.0, recall: 1.0, precision: 1.0, f_beta: 1.0
train: step: 1667, loss: 0.09057275950908661, acc: 0.9, recall: 0.9, precision: 0.9166666666666667, f_beta: 0.898989898989899
train: step: 1668, loss: 0.172930046916008, acc: 0.825, recall: 0.825, precision: 0.8324808184143222, f_beta: 0.824010056568196
train: step: 1669, loss: 0.03136827424168587, acc: 0.975, recall: 0.975, precision: 0.9761904761904762, f_beta: 0.9749843652282677
train: step: 1670, loss: 0.28154420852661133, acc: 0.625, recall: 0.625, precision: 0.6792114695340502, f_beta: 0.5943204868154157
train: step: 1671, loss: 0.060931093990802765, acc: 0.925, recall: 0.925, precision: 0.9260651629072681, f_beta: 0.924953095684803
train: step: 1672, loss: 0.16308799386024475, acc: 0.85, recall: 0.85, precision: 0.8846153846153846, f_beta: 0.8465473145780051
train: step: 1673, loss: 0.11010563373565674, acc: 0.875, recall: 0.875, precision: 0.8759398496240601, f_beta: 0.8749218261413383
train: step: 1674, loss: 0.2029763013124466, acc: 0.7, recall: 0.7, precision: 0.7197802197802198, f_beta: 0.6930946291560103
train: step: 1675, loss: 0.09099491685628891, acc: 0.85, recall: 0.85, precision: 0.85, f_beta: 0.85
train: step: 1676, loss: 0.16071215271949768, acc: 0.8, recall: 0.8, precision: 0.8296703296703296, f_beta: 0.7953964194373402
train: step: 1677, loss: 0.058063071221113205, acc: 0.925, recall: 0.925, precision: 0.9260651629072681, f_beta: 0.924953095684803
train: step: 1678, loss: 0.1884436309337616, acc: 0.85, recall: 0.85, precision: 0.8645833333333333, f_beta: 0.8484848484848484
train: step: 1679, loss: 0.23981556296348572, acc: 0.7, recall: 0.7, precision: 0.7380952380952381, f_beta: 0.6875
train: step: 1680, loss: 0.13143233954906464, acc: 0.85, recall: 0.8500000000000001, precision: 0.8535353535353536, f_beta: 0.849624060150376
train: step: 1681, loss: 0.33545494079589844, acc: 0.625, recall: 0.625, precision: 0.6278772378516624, f_beta: 0.6228786926461345
train: step: 1682, loss: 0.09958302974700928, acc: 0.875, recall: 0.875, precision: 0.8836317135549872, f_beta: 0.8742928975487115
train: step: 1683, loss: 0.24400274455547333, acc: 0.775, recall: 0.775, precision: 0.7756892230576441, f_beta: 0.774859287054409
train: step: 1684, loss: 0.09593819081783295, acc: 0.9, recall: 0.9, precision: 0.9166666666666667, f_beta: 0.898989898989899
train: step: 1685, loss: 0.08076994121074677, acc: 0.9, recall: 0.9, precision: 0.9166666666666667, f_beta: 0.898989898989899
train: step: 1686, loss: 0.12072696536779404, acc: 0.95, recall: 0.95, precision: 0.9545454545454546, f_beta: 0.949874686716792
train: step: 1687, loss: 0.30045849084854126, acc: 0.775, recall: 0.775, precision: 0.7933333333333333, f_beta: 0.7714285714285714
train: step: 1688, loss: 0.08219417929649353, acc: 0.9, recall: 0.9, precision: 0.9166666666666667, f_beta: 0.898989898989899
train: step: 1689, loss: 0.07318490743637085, acc: 0.925, recall: 0.925, precision: 0.9347826086956521, f_beta: 0.9245757385292268
train: step: 1690, loss: 0.17998972535133362, acc: 0.775, recall: 0.7749999999999999, precision: 0.8133903133903134, f_beta: 0.7678916827853
train: step: 1691, loss: 0.16495414078235626, acc: 0.825, recall: 0.825, precision: 0.8324808184143222, f_beta: 0.824010056568196
train: step: 1692, loss: 0.062226783484220505, acc: 0.925, recall: 0.925, precision: 0.9347826086956521, f_beta: 0.9245757385292268
train: step: 1693, loss: 0.154212087392807, acc: 0.825, recall: 0.825, precision: 0.8324808184143222, f_beta: 0.824010056568196
train: step: 1694, loss: 0.049826618283987045, acc: 0.95, recall: 0.95, precision: 0.95, f_beta: 0.9500000000000001
train: step: 1695, loss: 0.05023545026779175, acc: 0.95, recall: 0.95, precision: 0.9545454545454546, f_beta: 0.949874686716792
train: step: 1696, loss: 0.029937053099274635, acc: 0.975, recall: 0.975, precision: 0.9761904761904762, f_beta: 0.9749843652282677
train: step: 1697, loss: 0.11115682125091553, acc: 0.9, recall: 0.9, precision: 0.9166666666666667, f_beta: 0.898989898989899
train: step: 1698, loss: 0.27548089623451233, acc: 0.825, recall: 0.825, precision: 0.8258145363408521, f_beta: 0.8248905565978737
train: step: 1699, loss: 0.0383489616215229, acc: 0.975, recall: 0.975, precision: 0.9761904761904762, f_beta: 0.9749843652282677
train: step: 1700, loss: 0.08658047020435333, acc: 0.875, recall: 0.875, precision: 0.8836317135549872, f_beta: 0.8742928975487115
train: step: 1701, loss: 0.08278732001781464, acc: 0.875, recall: 0.875, precision: 0.8759398496240601, f_beta: 0.8749218261413383
train: step: 1702, loss: 0.014758172444999218, acc: 1.0, recall: 1.0, precision: 1.0, f_beta: 1.0
train: step: 1703, loss: 0.067923903465271, acc: 0.95, recall: 0.95, precision: 0.95, f_beta: 0.9500000000000001
train: step: 1704, loss: 0.03221440315246582, acc: 1.0, recall: 1.0, precision: 1.0, f_beta: 1.0
train: step: 1705, loss: 0.08116869628429413, acc: 0.9, recall: 0.9, precision: 0.9166666666666667, f_beta: 0.898989898989899
train: step: 1706, loss: 0.04550199955701828, acc: 0.95, recall: 0.95, precision: 0.9545454545454546, f_beta: 0.949874686716792
train: step: 1707, loss: 0.07854004204273224, acc: 0.9, recall: 0.8999999999999999, precision: 0.904040404040404, f_beta: 0.899749373433584
train: step: 1708, loss: 0.0717615857720375, acc: 0.875, recall: 0.875, precision: 0.8759398496240601, f_beta: 0.8749218261413383
train: step: 1709, loss: 0.17000845074653625, acc: 0.825, recall: 0.825, precision: 0.8466666666666667, f_beta: 0.8222222222222222
train: step: 1710, loss: 0.06298760324716568, acc: 0.9, recall: 0.8999999999999999, precision: 0.904040404040404, f_beta: 0.899749373433584
train: step: 1711, loss: 0.06878798454999924, acc: 0.925, recall: 0.925, precision: 0.9260651629072681, f_beta: 0.924953095684803
train: step: 1712, loss: 0.10744442790746689, acc: 0.875, recall: 0.875, precision: 0.9, f_beta: 0.873015873015873
train: step: 1713, loss: 0.08797498792409897, acc: 0.9, recall: 0.9, precision: 0.9166666666666667, f_beta: 0.898989898989899
train: step: 1714, loss: 0.2912435531616211, acc: 0.375, recall: 0.375, precision: 0.28354978354978355, f_beta: 0.3011879804332634
train: step: 1715, loss: 0.15998783707618713, acc: 0.75, recall: 0.75, precision: 0.7747252747252746, f_beta: 0.7442455242966751
train: step: 1716, loss: 0.060254573822021484, acc: 0.95, recall: 0.95, precision: 0.95, f_beta: 0.9500000000000001
train: step: 1717, loss: 0.12563900649547577, acc: 0.85, recall: 0.8500000000000001, precision: 0.8535353535353536, f_beta: 0.849624060150376
train: step: 1718, loss: 0.17503675818443298, acc: 0.725, recall: 0.725, precision: 0.74, f_beta: 0.7206349206349207
train: step: 1719, loss: 0.06167719513177872, acc: 0.95, recall: 0.95, precision: 0.95, f_beta: 0.9500000000000001
train: step: 1720, loss: 0.2517489194869995, acc: 0.55, recall: 0.55, precision: 0.763157894736842, f_beta: 0.4357366771159875
train: step: 1721, loss: 0.36367228627204895, acc: 0.7, recall: 0.7, precision: 0.7, f_beta: 0.7
train: step: 1722, loss: 0.048193834722042084, acc: 0.925, recall: 0.925, precision: 0.9347826086956521, f_beta: 0.9245757385292268
train: step: 1723, loss: 0.1367609202861786, acc: 0.85, recall: 0.85, precision: 0.8846153846153846, f_beta: 0.8465473145780051
train: step: 1724, loss: 0.11279328912496567, acc: 0.825, recall: 0.825, precision: 0.8466666666666667, f_beta: 0.8222222222222222
train: step: 1725, loss: 0.09895266592502594, acc: 0.875, recall: 0.875, precision: 0.8759398496240601, f_beta: 0.8749218261413383
train: step: 1726, loss: 0.05902455002069473, acc: 0.95, recall: 0.95, precision: 0.9545454545454546, f_beta: 0.949874686716792
train: step: 1727, loss: 0.07941209524869919, acc: 0.925, recall: 0.925, precision: 0.9260651629072681, f_beta: 0.924953095684803
train: step: 1728, loss: 0.07274667173624039, acc: 0.9, recall: 0.9, precision: 0.9166666666666667, f_beta: 0.898989898989899
train: step: 1729, loss: 0.13361215591430664, acc: 0.825, recall: 0.825, precision: 0.8703703703703703, f_beta: 0.819471308833011
train: step: 1730, loss: 0.058440692722797394, acc: 0.925, recall: 0.925, precision: 0.9347826086956521, f_beta: 0.9245757385292268
train: step: 1731, loss: 0.21592159569263458, acc: 0.675, recall: 0.675, precision: 0.719435736677116, f_beta: 0.6576695194206715
train: step: 1732, loss: 0.10063445568084717, acc: 0.875, recall: 0.875, precision: 0.8836317135549872, f_beta: 0.8742928975487115
train: step: 1733, loss: 0.17446786165237427, acc: 0.775, recall: 0.775, precision: 0.7933333333333333, f_beta: 0.7714285714285714
train: step: 1734, loss: 0.06855998933315277, acc: 0.9, recall: 0.8999999999999999, precision: 0.904040404040404, f_beta: 0.899749373433584
train: step: 1735, loss: 0.0846141129732132, acc: 0.95, recall: 0.95, precision: 0.9545454545454546, f_beta: 0.949874686716792
train: step: 1736, loss: 0.041105207055807114, acc: 0.925, recall: 0.925, precision: 0.9347826086956521, f_beta: 0.9245757385292268
train: step: 1737, loss: 0.1247125118970871, acc: 0.8, recall: 0.8, precision: 0.803030303030303, f_beta: 0.7994987468671679
train: step: 1738, loss: 0.2386714667081833, acc: 0.65, recall: 0.6499999999999999, precision: 0.734375, f_beta: 0.6153846153846154
train: step: 1739, loss: 0.04411286860704422, acc: 0.95, recall: 0.95, precision: 0.95, f_beta: 0.9500000000000001
train: step: 1740, loss: 0.14066629111766815, acc: 0.775, recall: 0.775, precision: 0.7756892230576441, f_beta: 0.774859287054409
train: step: 1741, loss: 0.13804033398628235, acc: 0.8, recall: 0.8, precision: 0.8296703296703296, f_beta: 0.7953964194373402
train: step: 1742, loss: 0.2198052853345871, acc: 0.65, recall: 0.65, precision: 0.6785714285714286, f_beta: 0.6354166666666667
train: step: 1743, loss: 0.03089284896850586, acc: 0.975, recall: 0.975, precision: 0.9761904761904762, f_beta: 0.9749843652282677
train: step: 1744, loss: 0.15734954178333282, acc: 0.8, recall: 0.8, precision: 0.8571428571428572, f_beta: 0.7916666666666665
train: step: 1745, loss: 0.21100182831287384, acc: 0.725, recall: 0.725, precision: 0.74, f_beta: 0.7206349206349207
train: step: 1746, loss: 0.09528050571680069, acc: 0.85, recall: 0.85, precision: 0.8645833333333333, f_beta: 0.8484848484848484
train: step: 1747, loss: 0.13555200397968292, acc: 0.775, recall: 0.775, precision: 0.7933333333333333, f_beta: 0.7714285714285714
train: step: 1748, loss: 0.1385987251996994, acc: 0.825, recall: 0.825, precision: 0.8703703703703703, f_beta: 0.819471308833011
train: step: 1749, loss: 0.10415659844875336, acc: 0.875, recall: 0.875, precision: 0.9, f_beta: 0.873015873015873
train: step: 1750, loss: 0.07728958129882812, acc: 0.925, recall: 0.925, precision: 0.9347826086956521, f_beta: 0.9245757385292268
train: step: 1751, loss: 0.0606524720788002, acc: 0.925, recall: 0.925, precision: 0.9260651629072681, f_beta: 0.924953095684803
train: step: 1752, loss: 0.11955615133047104, acc: 0.8, recall: 0.8, precision: 0.8125, f_beta: 0.797979797979798
train: step: 1753, loss: 0.054342467337846756, acc: 0.95, recall: 0.95, precision: 0.9545454545454546, f_beta: 0.949874686716792
train: step: 1754, loss: 0.041584562510252, acc: 0.95, recall: 0.95, precision: 0.9545454545454546, f_beta: 0.949874686716792
train: step: 1755, loss: 0.22325579822063446, acc: 0.775, recall: 0.7749999999999999, precision: 0.8133903133903134, f_beta: 0.7678916827853
train: step: 1756, loss: 0.21213659644126892, acc: 0.725, recall: 0.7250000000000001, precision: 0.7301790281329923, f_beta: 0.7234443746071653
train: step: 1757, loss: 0.18048253655433655, acc: 0.775, recall: 0.775, precision: 0.7756892230576441, f_beta: 0.774859287054409
train: step: 1758, loss: 0.1226295679807663, acc: 0.8, recall: 0.8, precision: 0.803030303030303, f_beta: 0.7994987468671679
train: step: 1759, loss: 0.07717931270599365, acc: 0.925, recall: 0.925, precision: 0.9347826086956521, f_beta: 0.9245757385292268
train: step: 1760, loss: 0.0594315342605114, acc: 0.925, recall: 0.925, precision: 0.9347826086956521, f_beta: 0.9245757385292268
train: step: 1761, loss: 0.06460977345705032, acc: 0.95, recall: 0.95, precision: 0.9545454545454546, f_beta: 0.949874686716792
train: step: 1762, loss: 0.15412139892578125, acc: 0.8, recall: 0.8, precision: 0.803030303030303, f_beta: 0.7994987468671679
train: step: 1763, loss: 0.0673600286245346, acc: 0.925, recall: 0.925, precision: 0.9260651629072681, f_beta: 0.924953095684803
train: step: 1764, loss: 0.3065184950828552, acc: 0.5, recall: 0.5, precision: 0.5, f_beta: 0.4666666666666667
train: step: 1765, loss: 0.0706726685166359, acc: 0.95, recall: 0.95, precision: 0.9545454545454546, f_beta: 0.949874686716792
train: step: 1766, loss: 0.21480019390583038, acc: 0.9, recall: 0.9, precision: 0.9, f_beta: 0.9
train: step: 1767, loss: 0.20909655094146729, acc: 0.7, recall: 0.7, precision: 0.7083333333333333, f_beta: 0.6969696969696968
train: step: 1768, loss: 0.30322664976119995, acc: 0.7, recall: 0.7, precision: 0.7, f_beta: 0.7
train: step: 1769, loss: 0.16152851283550262, acc: 0.8, recall: 0.8, precision: 0.8, f_beta: 0.8000000000000002
train: step: 1770, loss: 0.0896562933921814, acc: 0.875, recall: 0.875, precision: 0.8759398496240601, f_beta: 0.8749218261413383
train: step: 1771, loss: 0.040123917162418365, acc: 0.975, recall: 0.975, precision: 0.9761904761904762, f_beta: 0.9749843652282677
train: step: 1772, loss: 0.07600533962249756, acc: 0.9, recall: 0.8999999999999999, precision: 0.904040404040404, f_beta: 0.899749373433584
train: step: 1773, loss: 0.02912002243101597, acc: 0.975, recall: 0.975, precision: 0.9761904761904762, f_beta: 0.9749843652282677
train: step: 1774, loss: 0.10173429548740387, acc: 0.85, recall: 0.85, precision: 0.8645833333333333, f_beta: 0.8484848484848484
train: step: 1775, loss: 0.04444974660873413, acc: 0.975, recall: 0.975, precision: 0.9761904761904762, f_beta: 0.9749843652282677
train: step: 1776, loss: 0.06382610648870468, acc: 0.925, recall: 0.925, precision: 0.9260651629072681, f_beta: 0.924953095684803
train: step: 1777, loss: 0.23573565483093262, acc: 0.725, recall: 0.725, precision: 0.8225806451612903, f_beta: 0.7025016903313049
train: step: 1778, loss: 0.01761111244559288, acc: 1.0, recall: 1.0, precision: 1.0, f_beta: 1.0
train: step: 1779, loss: 0.031143689528107643, acc: 0.975, recall: 0.975, precision: 0.9761904761904762, f_beta: 0.9749843652282677
train: step: 1780, loss: 0.33605286478996277, acc: 0.775, recall: 0.775, precision: 0.7933333333333333, f_beta: 0.7714285714285714
train: step: 1781, loss: 0.019933167845010757, acc: 1.0, recall: 1.0, precision: 1.0, f_beta: 1.0
train: step: 1782, loss: 0.1021537035703659, acc: 0.875, recall: 0.875, precision: 0.8836317135549872, f_beta: 0.8742928975487115
train: step: 1783, loss: 0.10450558364391327, acc: 0.9, recall: 0.8999999999999999, precision: 0.904040404040404, f_beta: 0.899749373433584
train: step: 1784, loss: 0.06153909116983414, acc: 0.925, recall: 0.925, precision: 0.9260651629072681, f_beta: 0.924953095684803
train: step: 1785, loss: 0.28375324606895447, acc: 0.775, recall: 0.7749999999999999, precision: 0.8133903133903134, f_beta: 0.7678916827853
train: step: 1786, loss: 0.3120954930782318, acc: 0.85, recall: 0.85, precision: 0.8645833333333333, f_beta: 0.8484848484848484
train: step: 1787, loss: 0.08521337807178497, acc: 0.925, recall: 0.925, precision: 0.9347826086956521, f_beta: 0.9245757385292268
train: step: 1788, loss: 0.10525530576705933, acc: 0.85, recall: 0.85, precision: 0.8645833333333333, f_beta: 0.8484848484848484
train: step: 1789, loss: 0.1038932055234909, acc: 0.875, recall: 0.875, precision: 0.9, f_beta: 0.873015873015873
train: step: 1790, loss: 0.2446746528148651, acc: 0.675, recall: 0.675, precision: 0.6866666666666666, f_beta: 0.6698412698412699
train: step: 1791, loss: 0.026067007333040237, acc: 1.0, recall: 1.0, precision: 1.0, f_beta: 1.0
train: step: 1792, loss: 0.3026414215564728, acc: 0.9, recall: 0.8999999999999999, precision: 0.904040404040404, f_beta: 0.899749373433584
train: step: 1793, loss: 0.08734296262264252, acc: 0.925, recall: 0.925, precision: 0.9260651629072681, f_beta: 0.924953095684803
train: step: 1794, loss: 0.06397758424282074, acc: 0.925, recall: 0.925, precision: 0.9347826086956521, f_beta: 0.9245757385292268
train: step: 1795, loss: 0.11701588332653046, acc: 0.85, recall: 0.8500000000000001, precision: 0.8535353535353536, f_beta: 0.849624060150376
train: step: 1796, loss: 0.0750529021024704, acc: 0.9, recall: 0.9, precision: 0.9166666666666667, f_beta: 0.898989898989899
train: step: 1797, loss: 0.06376563012599945, acc: 0.95, recall: 0.95, precision: 0.9545454545454546, f_beta: 0.949874686716792
train: step: 1798, loss: 0.07928923517465591, acc: 0.9, recall: 0.9, precision: 0.9, f_beta: 0.9
train: step: 1799, loss: 0.14081647992134094, acc: 0.9, recall: 0.8999999999999999, precision: 0.904040404040404, f_beta: 0.899749373433584
train: step: 1800, loss: 0.17483170330524445, acc: 0.775, recall: 0.775, precision: 0.8448275862068966, f_beta: 0.7630019749835418
train: step: 1801, loss: 0.13416528701782227, acc: 0.875, recall: 0.875, precision: 0.8759398496240601, f_beta: 0.8749218261413383
train: step: 1802, loss: 0.09308943897485733, acc: 0.875, recall: 0.875, precision: 0.8836317135549872, f_beta: 0.8742928975487115
train: step: 1803, loss: 0.1263599991798401, acc: 0.875, recall: 0.875, precision: 0.8759398496240601, f_beta: 0.8749218261413383
train: step: 1804, loss: 0.09870036691427231, acc: 0.875, recall: 0.875, precision: 0.8759398496240601, f_beta: 0.8749218261413383
train: step: 1805, loss: 0.1326673924922943, acc: 0.85, recall: 0.8500000000000001, precision: 0.8535353535353536, f_beta: 0.849624060150376
train: step: 1806, loss: 0.18946364521980286, acc: 0.775, recall: 0.775, precision: 0.8448275862068966, f_beta: 0.7630019749835418
train: step: 1807, loss: 0.3333195149898529, acc: 0.525, recall: 0.525, precision: 0.7564102564102564, f_beta: 0.38660209846650523
train: step: 1808, loss: 0.02960704267024994, acc: 1.0, recall: 1.0, precision: 1.0, f_beta: 1.0
train: step: 1809, loss: 0.07088299840688705, acc: 0.925, recall: 0.925, precision: 0.9347826086956521, f_beta: 0.9245757385292268
train: step: 1810, loss: 0.08808524906635284, acc: 0.9, recall: 0.8999999999999999, precision: 0.904040404040404, f_beta: 0.899749373433584
train: step: 1811, loss: 0.05144800618290901, acc: 0.975, recall: 0.975, precision: 0.9761904761904762, f_beta: 0.9749843652282677
train: step: 1812, loss: 0.25050249695777893, acc: 0.725, recall: 0.725, precision: 0.74, f_beta: 0.7206349206349207
train: step: 1813, loss: 0.08284696936607361, acc: 0.925, recall: 0.925, precision: 0.9260651629072681, f_beta: 0.924953095684803
train: step: 1814, loss: 0.07556487619876862, acc: 0.9, recall: 0.9, precision: 0.9166666666666667, f_beta: 0.898989898989899
train: step: 1815, loss: 0.21332962810993195, acc: 0.875, recall: 0.875, precision: 0.8836317135549872, f_beta: 0.8742928975487115
train: step: 1816, loss: 0.16664040088653564, acc: 0.775, recall: 0.7749999999999999, precision: 0.7813299232736572, f_beta: 0.7737272155876807
train: step: 1817, loss: 0.016201261430978775, acc: 1.0, recall: 1.0, precision: 1.0, f_beta: 1.0
train: step: 1818, loss: 0.0632869228720665, acc: 0.9, recall: 0.9, precision: 0.9, f_beta: 0.9
train: step: 1819, loss: 0.09042273461818695, acc: 0.9, recall: 0.9, precision: 0.9, f_beta: 0.9
train: step: 1820, loss: 0.2427384853363037, acc: 0.65, recall: 0.6499999999999999, precision: 0.734375, f_beta: 0.6153846153846154
train: step: 1821, loss: 0.1454497128725052, acc: 0.75, recall: 0.75, precision: 0.7604166666666667, f_beta: 0.7474747474747475
train: step: 1822, loss: 0.06979396194219589, acc: 0.9, recall: 0.9, precision: 0.9, f_beta: 0.9
train: step: 1823, loss: 0.22019362449645996, acc: 0.75, recall: 0.75, precision: 0.7604166666666667, f_beta: 0.7474747474747475
train: step: 1824, loss: 0.2012898176908493, acc: 0.75, recall: 0.75, precision: 0.7604166666666667, f_beta: 0.7474747474747475
train: step: 1825, loss: 0.17641103267669678, acc: 0.8, recall: 0.8, precision: 0.8, f_beta: 0.8000000000000002
train: step: 1826, loss: 0.07761366665363312, acc: 0.925, recall: 0.925, precision: 0.9260651629072681, f_beta: 0.924953095684803
train: step: 1827, loss: 0.17586007714271545, acc: 0.825, recall: 0.825, precision: 0.8703703703703703, f_beta: 0.819471308833011
train: step: 1828, loss: 0.17781881988048553, acc: 0.8, recall: 0.8, precision: 0.8296703296703296, f_beta: 0.7953964194373402
train: step: 1829, loss: 0.03398169204592705, acc: 0.975, recall: 0.975, precision: 0.9761904761904762, f_beta: 0.9749843652282677
train: step: 1830, loss: 0.14789755642414093, acc: 0.8, recall: 0.8, precision: 0.8125, f_beta: 0.797979797979798
train: step: 1831, loss: 0.05917694419622421, acc: 0.925, recall: 0.925, precision: 0.9260651629072681, f_beta: 0.924953095684803
train: step: 1832, loss: 0.17284110188484192, acc: 0.975, recall: 0.975, precision: 0.9761904761904762, f_beta: 0.9749843652282677
train: step: 1833, loss: 0.12484041601419449, acc: 0.975, recall: 0.975, precision: 0.9761904761904762, f_beta: 0.9749843652282677
train: step: 1834, loss: 0.04344503954052925, acc: 0.95, recall: 0.95, precision: 0.9545454545454546, f_beta: 0.949874686716792
train: step: 1835, loss: 0.022738486528396606, acc: 1.0, recall: 1.0, precision: 1.0, f_beta: 1.0
train: step: 1836, loss: 0.2219640016555786, acc: 0.7, recall: 0.7, precision: 0.7083333333333333, f_beta: 0.6969696969696968
train: step: 1837, loss: 0.08004120737314224, acc: 0.9, recall: 0.8999999999999999, precision: 0.904040404040404, f_beta: 0.899749373433584
train: step: 1838, loss: 0.20465834438800812, acc: 0.7, recall: 0.7, precision: 0.8125, f_beta: 0.6703296703296704
train: step: 1839, loss: 0.01539912074804306, acc: 1.0, recall: 1.0, precision: 1.0, f_beta: 1.0
train: step: 1840, loss: 0.25279468297958374, acc: 0.7, recall: 0.7, precision: 0.7666666666666666, f_beta: 0.6799999999999999
train: step: 1841, loss: 0.024762194603681564, acc: 1.0, recall: 1.0, precision: 1.0, f_beta: 1.0
train: step: 1842, loss: 0.09864281117916107, acc: 0.9, recall: 0.8999999999999999, precision: 0.904040404040404, f_beta: 0.899749373433584
train: step: 1843, loss: 0.0523853674530983, acc: 0.925, recall: 0.925, precision: 0.9260651629072681, f_beta: 0.924953095684803
train: step: 1844, loss: 0.1029655709862709, acc: 0.875, recall: 0.875, precision: 0.8759398496240601, f_beta: 0.8749218261413383
train: step: 1845, loss: 0.23203754425048828, acc: 0.7, recall: 0.7, precision: 0.7083333333333333, f_beta: 0.6969696969696968
train: step: 1846, loss: 0.09515408426523209, acc: 0.925, recall: 0.925, precision: 0.9347826086956521, f_beta: 0.9245757385292268
train: step: 1847, loss: 0.029294561594724655, acc: 0.975, recall: 0.975, precision: 0.9761904761904762, f_beta: 0.9749843652282677
train: step: 1848, loss: 0.1225103884935379, acc: 0.825, recall: 0.825, precision: 0.8324808184143222, f_beta: 0.824010056568196
train: step: 1849, loss: 0.05791749432682991, acc: 0.95, recall: 0.95, precision: 0.9545454545454546, f_beta: 0.949874686716792
train: step: 1850, loss: 0.27072155475616455, acc: 0.725, recall: 0.7250000000000001, precision: 0.7564102564102564, f_beta: 0.7163120567375887
train: step: 1851, loss: 0.1593732237815857, acc: 0.8, recall: 0.8, precision: 0.8125, f_beta: 0.797979797979798
train: step: 1852, loss: 0.15456989407539368, acc: 0.8, recall: 0.8, precision: 0.8125, f_beta: 0.797979797979798
train: step: 1853, loss: 0.11570563167333603, acc: 0.9, recall: 0.8999999999999999, precision: 0.904040404040404, f_beta: 0.899749373433584
train: step: 1854, loss: 0.2916216552257538, acc: 0.7, recall: 0.7, precision: 0.7, f_beta: 0.7
train: step: 1855, loss: 0.01863321289420128, acc: 1.0, recall: 1.0, precision: 1.0, f_beta: 1.0
train: step: 1856, loss: 0.07390475273132324, acc: 0.875, recall: 0.875, precision: 0.8759398496240601, f_beta: 0.8749218261413383
train: step: 1857, loss: 0.04331287741661072, acc: 0.95, recall: 0.95, precision: 0.95, f_beta: 0.9500000000000001
train: step: 1858, loss: 0.13566049933433533, acc: 0.825, recall: 0.825, precision: 0.8324808184143222, f_beta: 0.824010056568196
train: step: 1859, loss: 0.08166207373142242, acc: 0.9, recall: 0.9, precision: 0.9, f_beta: 0.9
train: step: 1860, loss: 0.047673892229795456, acc: 0.95, recall: 0.95, precision: 0.95, f_beta: 0.9500000000000001
train: step: 1861, loss: 0.04282150790095329, acc: 0.95, recall: 0.95, precision: 0.9545454545454546, f_beta: 0.949874686716792
train: step: 1862, loss: 0.06018983572721481, acc: 0.925, recall: 0.925, precision: 0.9260651629072681, f_beta: 0.924953095684803
train: step: 1863, loss: 0.06495795398950577, acc: 0.925, recall: 0.925, precision: 0.9260651629072681, f_beta: 0.924953095684803
train: step: 1864, loss: 0.06745117902755737, acc: 0.9, recall: 0.9, precision: 0.9166666666666667, f_beta: 0.898989898989899
train: step: 1865, loss: 0.04129564389586449, acc: 0.975, recall: 0.975, precision: 0.9761904761904762, f_beta: 0.9749843652282677
train: step: 1866, loss: 0.08017782866954803, acc: 0.875, recall: 0.875, precision: 0.8836317135549872, f_beta: 0.8742928975487115
train: step: 1867, loss: 0.02899806760251522, acc: 0.975, recall: 0.975, precision: 0.9761904761904762, f_beta: 0.9749843652282677
train: step: 1868, loss: 0.04599415883421898, acc: 0.95, recall: 0.95, precision: 0.9545454545454546, f_beta: 0.949874686716792
train: step: 1869, loss: 0.07056251168251038, acc: 0.9, recall: 0.8999999999999999, precision: 0.904040404040404, f_beta: 0.899749373433584
train: step: 1870, loss: 0.1005861759185791, acc: 0.875, recall: 0.875, precision: 0.8759398496240601, f_beta: 0.8749218261413383
train: step: 1871, loss: 0.06336642801761627, acc: 0.925, recall: 0.925, precision: 0.9260651629072681, f_beta: 0.924953095684803
train: step: 1872, loss: 0.04317440837621689, acc: 0.925, recall: 0.925, precision: 0.9347826086956521, f_beta: 0.9245757385292268
train: step: 1873, loss: 0.07349570840597153, acc: 0.95, recall: 0.95, precision: 0.9545454545454546, f_beta: 0.949874686716792
train: step: 1874, loss: 0.04386427253484726, acc: 0.925, recall: 0.925, precision: 0.9347826086956521, f_beta: 0.9245757385292268
train: step: 1875, loss: 0.2250254899263382, acc: 0.7, recall: 0.7, precision: 0.7380952380952381, f_beta: 0.6875
train: step: 1876, loss: 0.0692906379699707, acc: 0.9, recall: 0.8999999999999999, precision: 0.904040404040404, f_beta: 0.899749373433584
train: step: 1877, loss: 0.013392706401646137, acc: 1.0, recall: 1.0, precision: 1.0, f_beta: 1.0
train: step: 1878, loss: 0.22356590628623962, acc: 0.725, recall: 0.725, precision: 0.74, f_beta: 0.7206349206349207
train: step: 1879, loss: 0.08122783899307251, acc: 0.925, recall: 0.925, precision: 0.9260651629072681, f_beta: 0.924953095684803
train: step: 1880, loss: 0.22230824828147888, acc: 0.725, recall: 0.725, precision: 0.8225806451612903, f_beta: 0.7025016903313049
train: step: 1881, loss: 0.120552197098732, acc: 0.825, recall: 0.825, precision: 0.8466666666666667, f_beta: 0.8222222222222222
train: step: 1882, loss: 0.04579949378967285, acc: 0.95, recall: 0.95, precision: 0.9545454545454546, f_beta: 0.949874686716792
train: step: 1883, loss: 0.18482694029808044, acc: 0.825, recall: 0.825, precision: 0.8466666666666667, f_beta: 0.8222222222222222
train: step: 1884, loss: 0.18231184780597687, acc: 0.775, recall: 0.7749999999999999, precision: 0.8133903133903134, f_beta: 0.7678916827853
train: step: 1885, loss: 0.07556530088186264, acc: 0.9, recall: 0.9, precision: 0.9, f_beta: 0.9
train: step: 1886, loss: 0.10460001230239868, acc: 0.9, recall: 0.8999999999999999, precision: 0.904040404040404, f_beta: 0.899749373433584
train: step: 1887, loss: 0.1522478610277176, acc: 0.85, recall: 0.85, precision: 0.85, f_beta: 0.85
train: step: 1888, loss: 0.1287422627210617, acc: 0.85, recall: 0.85, precision: 0.8846153846153846, f_beta: 0.8465473145780051
train: step: 1889, loss: 0.07609869539737701, acc: 0.95, recall: 0.95, precision: 0.9545454545454546, f_beta: 0.949874686716792
train: step: 1890, loss: 0.0866989865899086, acc: 0.875, recall: 0.875, precision: 0.8836317135549872, f_beta: 0.8742928975487115
train: step: 1891, loss: 0.040899090468883514, acc: 0.975, recall: 0.975, precision: 0.9761904761904762, f_beta: 0.9749843652282677
train: step: 1892, loss: 0.045668888837099075, acc: 0.95, recall: 0.95, precision: 0.9545454545454546, f_beta: 0.949874686716792
train: step: 1893, loss: 0.14252762496471405, acc: 0.8, recall: 0.8, precision: 0.8125, f_beta: 0.797979797979798
train: step: 1894, loss: 0.3698049485683441, acc: 0.85, recall: 0.85, precision: 0.8645833333333333, f_beta: 0.8484848484848484
train: step: 1895, loss: 0.13136644661426544, acc: 0.875, recall: 0.875, precision: 0.9, f_beta: 0.873015873015873
train: step: 1896, loss: 0.19637782871723175, acc: 0.825, recall: 0.825, precision: 0.8258145363408521, f_beta: 0.8248905565978737
train: step: 1897, loss: 0.05683553218841553, acc: 0.95, recall: 0.95, precision: 0.95, f_beta: 0.9500000000000001
train: step: 1898, loss: 0.19773414731025696, acc: 0.75, recall: 0.75, precision: 0.7747252747252746, f_beta: 0.7442455242966751
train: step: 1899, loss: 0.1406513899564743, acc: 0.8, recall: 0.8, precision: 0.8125, f_beta: 0.797979797979798
train: step: 1900, loss: 0.08591966331005096, acc: 0.875, recall: 0.875, precision: 0.9, f_beta: 0.873015873015873
train: step: 1901, loss: 0.04807920381426811, acc: 0.95, recall: 0.95, precision: 0.9545454545454546, f_beta: 0.949874686716792
train: step: 1902, loss: 0.0784294381737709, acc: 0.9, recall: 0.8999999999999999, precision: 0.904040404040404, f_beta: 0.899749373433584
train: step: 1903, loss: 0.07293237745761871, acc: 0.9, recall: 0.9, precision: 0.9166666666666667, f_beta: 0.898989898989899
train: step: 1904, loss: 0.2923106253147125, acc: 0.75, recall: 0.75, precision: 0.7604166666666667, f_beta: 0.7474747474747475
train: step: 1905, loss: 0.09859979897737503, acc: 0.875, recall: 0.875, precision: 0.8759398496240601, f_beta: 0.8749218261413383
train: step: 1906, loss: 0.12793686985969543, acc: 0.85, recall: 0.8500000000000001, precision: 0.8535353535353536, f_beta: 0.849624060150376
train: step: 1907, loss: 0.08875364065170288, acc: 0.9, recall: 0.8999999999999999, precision: 0.904040404040404, f_beta: 0.899749373433584
train: step: 1908, loss: 0.0779896005988121, acc: 0.9, recall: 0.9, precision: 0.9166666666666667, f_beta: 0.898989898989899
train: step: 1909, loss: 0.07189079374074936, acc: 0.95, recall: 0.95, precision: 0.95, f_beta: 0.9500000000000001
train: step: 1910, loss: 0.055524297058582306, acc: 1.0, recall: 1.0, precision: 1.0, f_beta: 1.0
train: step: 1911, loss: 0.13991473615169525, acc: 0.775, recall: 0.775, precision: 0.8448275862068966, f_beta: 0.7630019749835418
train: step: 1912, loss: 0.11470372974872589, acc: 0.825, recall: 0.825, precision: 0.8258145363408521, f_beta: 0.8248905565978737
train: step: 1913, loss: 0.14558449387550354, acc: 0.875, recall: 0.875, precision: 0.8759398496240601, f_beta: 0.8749218261413383
train: step: 1914, loss: 0.1966988742351532, acc: 0.8, recall: 0.8, precision: 0.8571428571428572, f_beta: 0.7916666666666665
train: step: 1915, loss: 0.10433938354253769, acc: 0.875, recall: 0.875, precision: 0.8759398496240601, f_beta: 0.8749218261413383
train: step: 1916, loss: 0.27134159207344055, acc: 0.675, recall: 0.675, precision: 0.7508960573476702, f_beta: 0.6484110885733605
train: step: 1917, loss: 0.18401487171649933, acc: 0.75, recall: 0.75, precision: 0.8333333333333333, f_beta: 0.7333333333333334
train: step: 1918, loss: 0.0958789736032486, acc: 0.85, recall: 0.85, precision: 0.85, f_beta: 0.85
train: step: 1919, loss: 0.13505606353282928, acc: 0.85, recall: 0.85, precision: 0.8645833333333333, f_beta: 0.8484848484848484
train: step: 1920, loss: 0.1700616180896759, acc: 0.775, recall: 0.775, precision: 0.8448275862068966, f_beta: 0.7630019749835418
train: step: 1921, loss: 0.11047828197479248, acc: 0.85, recall: 0.8500000000000001, precision: 0.8535353535353536, f_beta: 0.849624060150376
train: step: 1922, loss: 0.046265967190265656, acc: 0.925, recall: 0.925, precision: 0.9347826086956521, f_beta: 0.9245757385292268
train: step: 1923, loss: 0.08278483152389526, acc: 0.875, recall: 0.875, precision: 0.8759398496240601, f_beta: 0.8749218261413383
train: step: 1924, loss: 0.06402717530727386, acc: 0.95, recall: 0.95, precision: 0.95, f_beta: 0.9500000000000001
train: step: 1925, loss: 0.057793449610471725, acc: 0.95, recall: 0.95, precision: 0.9545454545454546, f_beta: 0.949874686716792
train: step: 1926, loss: 0.07405789196491241, acc: 0.875, recall: 0.875, precision: 0.8836317135549872, f_beta: 0.8742928975487115
train: step: 1927, loss: 0.06398685276508331, acc: 0.925, recall: 0.925, precision: 0.9347826086956521, f_beta: 0.9245757385292268
train: step: 1928, loss: 0.07046450674533844, acc: 0.9, recall: 0.9, precision: 0.9166666666666667, f_beta: 0.898989898989899
train: step: 1929, loss: 0.0751945972442627, acc: 0.925, recall: 0.925, precision: 0.9347826086956521, f_beta: 0.9245757385292268
train: step: 1930, loss: 0.04576210305094719, acc: 0.95, recall: 0.95, precision: 0.95, f_beta: 0.9500000000000001
train: step: 1931, loss: 0.11800429970026016, acc: 0.875, recall: 0.875, precision: 0.8759398496240601, f_beta: 0.8749218261413383
train: step: 1932, loss: 0.05208528786897659, acc: 0.925, recall: 0.925, precision: 0.9260651629072681, f_beta: 0.924953095684803
train: step: 1933, loss: 0.08300450444221497, acc: 0.9, recall: 0.9, precision: 0.9166666666666667, f_beta: 0.898989898989899
train: step: 1934, loss: 0.08994970470666885, acc: 0.9, recall: 0.9, precision: 0.9, f_beta: 0.9
train: step: 1935, loss: 0.02676079235970974, acc: 0.975, recall: 0.975, precision: 0.9761904761904762, f_beta: 0.9749843652282677
train: step: 1936, loss: 0.17630942165851593, acc: 0.75, recall: 0.75, precision: 0.8333333333333333, f_beta: 0.7333333333333334
train: step: 1937, loss: 0.10114800930023193, acc: 0.875, recall: 0.875, precision: 0.8759398496240601, f_beta: 0.8749218261413383
train: step: 1938, loss: 0.0585109107196331, acc: 0.925, recall: 0.925, precision: 0.9260651629072681, f_beta: 0.924953095684803
train: step: 1939, loss: 0.12215688079595566, acc: 0.825, recall: 0.825, precision: 0.8258145363408521, f_beta: 0.8248905565978737
train: step: 1940, loss: 0.11900623142719269, acc: 0.8, recall: 0.8, precision: 0.8, f_beta: 0.8000000000000002
train: step: 1941, loss: 0.09301652014255524, acc: 0.875, recall: 0.875, precision: 0.8836317135549872, f_beta: 0.8742928975487115
train: step: 1942, loss: 0.06715208292007446, acc: 0.95, recall: 0.95, precision: 0.9545454545454546, f_beta: 0.949874686716792
train: step: 1943, loss: 0.23543782532215118, acc: 0.75, recall: 0.75, precision: 0.7525252525252526, f_beta: 0.7493734335839599
train: step: 1944, loss: 0.08022205531597137, acc: 0.925, recall: 0.925, precision: 0.9347826086956521, f_beta: 0.9245757385292268
train: step: 1945, loss: 0.08485095202922821, acc: 0.9, recall: 0.8999999999999999, precision: 0.904040404040404, f_beta: 0.899749373433584
train: step: 1946, loss: 0.0993102639913559, acc: 0.875, recall: 0.875, precision: 0.8836317135549872, f_beta: 0.8742928975487115
train: step: 1947, loss: 0.08484265953302383, acc: 0.9, recall: 0.9, precision: 0.9166666666666667, f_beta: 0.898989898989899
train: step: 1948, loss: 0.029729921370744705, acc: 0.975, recall: 0.975, precision: 0.9761904761904762, f_beta: 0.9749843652282677
train: step: 1949, loss: 0.061330199241638184, acc: 0.95, recall: 0.95, precision: 0.95, f_beta: 0.9500000000000001
train: step: 1950, loss: 0.3360104262828827, acc: 0.85, recall: 0.85, precision: 0.85, f_beta: 0.85
train: step: 1951, loss: 0.34483346343040466, acc: 0.75, recall: 0.75, precision: 0.75, f_beta: 0.75
train: step: 1952, loss: 0.02422584965825081, acc: 0.975, recall: 0.975, precision: 0.9761904761904762, f_beta: 0.9749843652282677
train: step: 1953, loss: 0.16850467026233673, acc: 0.8, recall: 0.8, precision: 0.803030303030303, f_beta: 0.7994987468671679
train: step: 1954, loss: 0.09939839690923691, acc: 0.875, recall: 0.875, precision: 0.8836317135549872, f_beta: 0.8742928975487115
train: step: 1955, loss: 0.2175716906785965, acc: 0.75, recall: 0.75, precision: 0.7747252747252746, f_beta: 0.7442455242966751
train: step: 1956, loss: 0.2067519724369049, acc: 0.75, recall: 0.75, precision: 0.8333333333333333, f_beta: 0.7333333333333334
train: step: 1957, loss: 0.20220179855823517, acc: 0.75, recall: 0.75, precision: 0.7747252747252746, f_beta: 0.7442455242966751
train: step: 1958, loss: 0.054407160729169846, acc: 0.975, recall: 0.975, precision: 0.9761904761904762, f_beta: 0.9749843652282677
train: step: 1959, loss: 0.0776236355304718, acc: 0.9, recall: 0.9, precision: 0.9166666666666667, f_beta: 0.898989898989899
train: step: 1960, loss: 0.02683248743414879, acc: 1.0, recall: 1.0, precision: 1.0, f_beta: 1.0
train: step: 1961, loss: 0.042330410331487656, acc: 0.95, recall: 0.95, precision: 0.9545454545454546, f_beta: 0.949874686716792
train: step: 1962, loss: 0.17075589299201965, acc: 0.8, recall: 0.8, precision: 0.803030303030303, f_beta: 0.7994987468671679
train: step: 1963, loss: 0.0280368123203516, acc: 0.975, recall: 0.975, precision: 0.9761904761904762, f_beta: 0.9749843652282677
train: step: 1964, loss: 0.12914347648620605, acc: 0.825, recall: 0.825, precision: 0.8466666666666667, f_beta: 0.8222222222222222
train: step: 1965, loss: 0.16309580206871033, acc: 0.8, recall: 0.8, precision: 0.8125, f_beta: 0.797979797979798
train: step: 1966, loss: 0.05866997689008713, acc: 0.925, recall: 0.925, precision: 0.9347826086956521, f_beta: 0.9245757385292268
train: step: 1967, loss: 0.12921856343746185, acc: 0.85, recall: 0.85, precision: 0.8645833333333333, f_beta: 0.8484848484848484
train: step: 1968, loss: 0.28825798630714417, acc: 0.45, recall: 0.45, precision: 0.23684210526315788, f_beta: 0.3103448275862069
train: step: 1969, loss: 0.0557347908616066, acc: 0.975, recall: 0.975, precision: 0.9761904761904762, f_beta: 0.9749843652282677
train: step: 1970, loss: 0.02468380704522133, acc: 1.0, recall: 1.0, precision: 1.0, f_beta: 1.0
train: step: 1971, loss: 0.06412013620138168, acc: 0.925, recall: 0.925, precision: 0.9347826086956521, f_beta: 0.9245757385292268
train: step: 1972, loss: 0.019120922312140465, acc: 1.0, recall: 1.0, precision: 1.0, f_beta: 1.0
train: step: 1973, loss: 0.07704801857471466, acc: 0.9, recall: 0.8999999999999999, precision: 0.904040404040404, f_beta: 0.899749373433584
train: step: 1974, loss: 0.16787993907928467, acc: 0.85, recall: 0.85, precision: 0.8846153846153846, f_beta: 0.8465473145780051
train: step: 1975, loss: 0.026413166895508766, acc: 0.975, recall: 0.975, precision: 0.9761904761904762, f_beta: 0.9749843652282677
train: step: 1976, loss: 0.006188131868839264, acc: 1.0, recall: 1.0, precision: 1.0, f_beta: 1.0
train: step: 1977, loss: 0.08453477919101715, acc: 1.0, recall: 1.0, precision: 1.0, f_beta: 1.0
train: step: 1978, loss: 0.05168784782290459, acc: 0.95, recall: 0.95, precision: 0.95, f_beta: 0.9500000000000001
train: step: 1979, loss: 0.03226052597165108, acc: 0.975, recall: 0.975, precision: 0.9761904761904762, f_beta: 0.9749843652282677
train: step: 1980, loss: 0.05047605186700821, acc: 0.95, recall: 0.95, precision: 0.9545454545454546, f_beta: 0.949874686716792
train: step: 1981, loss: 0.10015038400888443, acc: 0.825, recall: 0.825, precision: 0.8466666666666667, f_beta: 0.8222222222222222
train: step: 1982, loss: 0.22549884021282196, acc: 0.775, recall: 0.775, precision: 0.7933333333333333, f_beta: 0.7714285714285714
train: step: 1983, loss: 0.08456163108348846, acc: 0.95, recall: 0.95, precision: 0.9545454545454546, f_beta: 0.949874686716792
train: step: 1984, loss: 0.08078644424676895, acc: 0.9, recall: 0.8999999999999999, precision: 0.904040404040404, f_beta: 0.899749373433584
train: step: 1985, loss: 0.20140953361988068, acc: 0.65, recall: 0.65, precision: 0.6785714285714286, f_beta: 0.6354166666666667
train: step: 1986, loss: 0.27483829855918884, acc: 0.65, recall: 0.6499999999999999, precision: 0.734375, f_beta: 0.6153846153846154
train: step: 1987, loss: 0.19182664155960083, acc: 0.825, recall: 0.825, precision: 0.8466666666666667, f_beta: 0.8222222222222222
train: step: 1988, loss: 0.3012394905090332, acc: 0.7, recall: 0.7, precision: 0.7197802197802198, f_beta: 0.6930946291560103
train: step: 1989, loss: 0.10209908336400986, acc: 0.925, recall: 0.925, precision: 0.9260651629072681, f_beta: 0.924953095684803
train: step: 1990, loss: 0.3902858793735504, acc: 0.675, recall: 0.675, precision: 0.6754385964912281, f_beta: 0.6747967479674797
train: step: 1991, loss: 0.1578366905450821, acc: 0.8, recall: 0.8, precision: 0.803030303030303, f_beta: 0.7994987468671679
train: step: 1992, loss: 0.010820050723850727, acc: 1.0, recall: 1.0, precision: 1.0, f_beta: 1.0
train: step: 1993, loss: 0.06568646430969238, acc: 0.95, recall: 0.95, precision: 0.95, f_beta: 0.9500000000000001
train: step: 1994, loss: 0.11534267663955688, acc: 0.875, recall: 0.875, precision: 0.9, f_beta: 0.873015873015873
train: step: 1995, loss: 0.04411928355693817, acc: 0.95, recall: 0.95, precision: 0.95, f_beta: 0.9500000000000001
train: step: 1996, loss: 0.009204057976603508, acc: 1.0, recall: 1.0, precision: 1.0, f_beta: 1.0
train: step: 1997, loss: 0.07369913160800934, acc: 0.925, recall: 0.925, precision: 0.9260651629072681, f_beta: 0.924953095684803
train: step: 1998, loss: 0.0471036434173584, acc: 0.925, recall: 0.925, precision: 0.9347826086956521, f_beta: 0.9245757385292268
train: step: 1999, loss: 0.09564807265996933, acc: 0.875, recall: 0.875, precision: 0.8836317135549872, f_beta: 0.8742928975487115
train: step: 2000, loss: 0.1820296347141266, acc: 0.7, recall: 0.7, precision: 0.7, f_beta: 0.7
train: step: 2001, loss: 0.03086787462234497, acc: 0.95, recall: 0.95, precision: 0.9545454545454546, f_beta: 0.949874686716792
train: step: 2002, loss: 0.12074720859527588, acc: 0.825, recall: 0.825, precision: 0.8324808184143222, f_beta: 0.824010056568196
train: step: 2003, loss: 0.08134253323078156, acc: 0.9, recall: 0.9, precision: 0.9166666666666667, f_beta: 0.898989898989899
train: step: 2004, loss: 0.0626475140452385, acc: 0.95, recall: 0.95, precision: 0.95, f_beta: 0.9500000000000001
train: step: 2005, loss: 0.12290680408477783, acc: 0.825, recall: 0.825, precision: 0.8703703703703703, f_beta: 0.819471308833011
train: step: 2006, loss: 0.1381918042898178, acc: 0.9, recall: 0.8999999999999999, precision: 0.904040404040404, f_beta: 0.899749373433584
train: step: 2007, loss: 0.024701645597815514, acc: 0.975, recall: 0.975, precision: 0.9761904761904762, f_beta: 0.9749843652282677
train: step: 2008, loss: 0.0380488820374012, acc: 0.95, recall: 0.95, precision: 0.95, f_beta: 0.9500000000000001
train: step: 2009, loss: 0.011211764998733997, acc: 1.0, recall: 1.0, precision: 1.0, f_beta: 1.0
train: step: 2010, loss: 0.056215740740299225, acc: 0.925, recall: 0.925, precision: 0.9260651629072681, f_beta: 0.924953095684803
train: step: 2011, loss: 0.04597204178571701, acc: 0.925, recall: 0.925, precision: 0.9260651629072681, f_beta: 0.924953095684803
train: step: 2012, loss: 0.09321194887161255, acc: 0.875, recall: 0.875, precision: 0.8836317135549872, f_beta: 0.8742928975487115
train: step: 2013, loss: 0.021807339042425156, acc: 0.975, recall: 0.975, precision: 0.9761904761904762, f_beta: 0.9749843652282677
train: step: 2014, loss: 0.2817510664463043, acc: 0.6, recall: 0.6, precision: 0.696078431372549, f_beta: 0.5441595441595442
train: step: 2015, loss: 0.1345180720090866, acc: 0.8, recall: 0.8, precision: 0.8571428571428572, f_beta: 0.7916666666666665
train: step: 2016, loss: 0.03711627423763275, acc: 0.95, recall: 0.95, precision: 0.9545454545454546, f_beta: 0.949874686716792
train: step: 2017, loss: 0.06860856711864471, acc: 0.925, recall: 0.925, precision: 0.9260651629072681, f_beta: 0.924953095684803
train: step: 2018, loss: 0.0613236241042614, acc: 0.925, recall: 0.925, precision: 0.9347826086956521, f_beta: 0.9245757385292268
train: step: 2019, loss: 0.05753961205482483, acc: 0.925, recall: 0.925, precision: 0.9347826086956521, f_beta: 0.9245757385292268
train: step: 2020, loss: 0.11941440403461456, acc: 0.875, recall: 0.875, precision: 0.8836317135549872, f_beta: 0.8742928975487115
train: step: 2021, loss: 0.11081744730472565, acc: 0.875, recall: 0.875, precision: 0.8759398496240601, f_beta: 0.8749218261413383
train: step: 2022, loss: 0.05467678979039192, acc: 0.925, recall: 0.925, precision: 0.9260651629072681, f_beta: 0.924953095684803
train: step: 2023, loss: 0.21593523025512695, acc: 0.85, recall: 0.85, precision: 0.8645833333333333, f_beta: 0.8484848484848484
train: step: 2024, loss: 0.10578648746013641, acc: 0.875, recall: 0.875, precision: 0.8836317135549872, f_beta: 0.8742928975487115
train: step: 2025, loss: 0.032703012228012085, acc: 0.975, recall: 0.975, precision: 0.9761904761904762, f_beta: 0.9749843652282677
train: step: 2026, loss: 0.12996011972427368, acc: 0.825, recall: 0.825, precision: 0.8466666666666667, f_beta: 0.8222222222222222
train: step: 2027, loss: 0.11363992840051651, acc: 0.85, recall: 0.85, precision: 0.8645833333333333, f_beta: 0.8484848484848484
train: step: 2028, loss: 0.10971590131521225, acc: 0.9, recall: 0.8999999999999999, precision: 0.904040404040404, f_beta: 0.899749373433584
train: step: 2029, loss: 0.02466881088912487, acc: 0.975, recall: 0.975, precision: 0.9761904761904762, f_beta: 0.9749843652282677
train: step: 2030, loss: 0.06681913882493973, acc: 0.9, recall: 0.9, precision: 0.9166666666666667, f_beta: 0.898989898989899
train: step: 2031, loss: 0.10484924167394638, acc: 0.875, recall: 0.875, precision: 0.8836317135549872, f_beta: 0.8742928975487115
train: step: 2032, loss: 0.048587605357170105, acc: 0.925, recall: 0.925, precision: 0.9347826086956521, f_beta: 0.9245757385292268
train: step: 2033, loss: 0.10927631705999374, acc: 0.9, recall: 0.8999999999999999, precision: 0.904040404040404, f_beta: 0.899749373433584
train: step: 2034, loss: 0.040792156010866165, acc: 0.975, recall: 0.975, precision: 0.9761904761904762, f_beta: 0.9749843652282677
train: step: 2035, loss: 0.04246046394109726, acc: 0.975, recall: 0.975, precision: 0.9761904761904762, f_beta: 0.9749843652282677
train: step: 2036, loss: 0.08753689378499985, acc: 0.875, recall: 0.875, precision: 0.8836317135549872, f_beta: 0.8742928975487115
train: step: 2037, loss: 0.08084288984537125, acc: 0.9, recall: 0.9, precision: 0.9166666666666667, f_beta: 0.898989898989899
train: step: 2038, loss: 0.05558490753173828, acc: 0.9, recall: 0.9, precision: 0.9166666666666667, f_beta: 0.898989898989899
train: step: 2039, loss: 0.07817937433719635, acc: 0.9, recall: 0.9, precision: 0.9, f_beta: 0.9
train: step: 2040, loss: 0.32089680433273315, acc: 0.75, recall: 0.75, precision: 0.7525252525252526, f_beta: 0.7493734335839599
train: step: 2041, loss: 0.03214910626411438, acc: 0.975, recall: 0.975, precision: 0.9761904761904762, f_beta: 0.9749843652282677
train: step: 2042, loss: 0.15645289421081543, acc: 0.85, recall: 0.85, precision: 0.8645833333333333, f_beta: 0.8484848484848484
train: step: 2043, loss: 0.24204590916633606, acc: 0.775, recall: 0.7749999999999999, precision: 0.8133903133903134, f_beta: 0.7678916827853
train: step: 2044, loss: 0.05515965074300766, acc: 0.95, recall: 0.95, precision: 0.9545454545454546, f_beta: 0.949874686716792
train: step: 2045, loss: 0.07082244753837585, acc: 0.9, recall: 0.9, precision: 0.9, f_beta: 0.9
train: step: 2046, loss: 0.2198590338230133, acc: 0.95, recall: 0.95, precision: 0.9545454545454546, f_beta: 0.949874686716792
train: step: 2047, loss: 0.2152165174484253, acc: 0.775, recall: 0.7749999999999999, precision: 0.8133903133903134, f_beta: 0.7678916827853
train: step: 2048, loss: 0.12355692684650421, acc: 0.8, recall: 0.8, precision: 0.8125, f_beta: 0.797979797979798
train: step: 2049, loss: 0.10987682640552521, acc: 0.825, recall: 0.825, precision: 0.8703703703703703, f_beta: 0.819471308833011
train: step: 2050, loss: 0.06521250307559967, acc: 0.925, recall: 0.925, precision: 0.9347826086956521, f_beta: 0.9245757385292268
train: step: 2051, loss: 0.03613389655947685, acc: 0.975, recall: 0.975, precision: 0.9761904761904762, f_beta: 0.9749843652282677
train: step: 2052, loss: 0.11722943931818008, acc: 0.875, recall: 0.875, precision: 0.8836317135549872, f_beta: 0.8742928975487115
train: step: 2053, loss: 0.11734312772750854, acc: 0.85, recall: 0.85, precision: 0.85, f_beta: 0.85
train: step: 2054, loss: 0.13747183978557587, acc: 0.825, recall: 0.825, precision: 0.8703703703703703, f_beta: 0.819471308833011
train: step: 2055, loss: 0.07303328812122345, acc: 0.9, recall: 0.9, precision: 0.9166666666666667, f_beta: 0.898989898989899
train: step: 2056, loss: 0.2330014407634735, acc: 0.725, recall: 0.725, precision: 0.8225806451612903, f_beta: 0.7025016903313049
train: step: 2057, loss: 0.07312703132629395, acc: 0.9, recall: 0.9, precision: 0.9166666666666667, f_beta: 0.898989898989899
train: step: 2058, loss: 0.08887394517660141, acc: 0.875, recall: 0.875, precision: 0.8759398496240601, f_beta: 0.8749218261413383
train: step: 2059, loss: 0.026070168241858482, acc: 0.975, recall: 0.975, precision: 0.9761904761904762, f_beta: 0.9749843652282677
train: step: 2060, loss: 0.14307095110416412, acc: 0.8, recall: 0.8, precision: 0.8571428571428572, f_beta: 0.7916666666666665
train: step: 2061, loss: 0.0562080517411232, acc: 0.95, recall: 0.95, precision: 0.9545454545454546, f_beta: 0.949874686716792
train: step: 2062, loss: 0.09286488592624664, acc: 0.9, recall: 0.8999999999999999, precision: 0.904040404040404, f_beta: 0.899749373433584
train: step: 2063, loss: 0.10954798758029938, acc: 0.9, recall: 0.9, precision: 0.9166666666666667, f_beta: 0.898989898989899
train: step: 2064, loss: 0.25465890765190125, acc: 0.675, recall: 0.675, precision: 0.7508960573476702, f_beta: 0.6484110885733605
train: step: 2065, loss: 0.04679444432258606, acc: 0.95, recall: 0.95, precision: 0.9545454545454546, f_beta: 0.949874686716792
train: step: 2066, loss: 0.1984485238790512, acc: 0.7, recall: 0.7, precision: 0.7380952380952381, f_beta: 0.6875
train: step: 2067, loss: 0.03529183939099312, acc: 0.975, recall: 0.975, precision: 0.9761904761904762, f_beta: 0.9749843652282677
train: step: 2068, loss: 0.08407485485076904, acc: 0.875, recall: 0.875, precision: 0.8759398496240601, f_beta: 0.8749218261413383
train: step: 2069, loss: 0.1419508159160614, acc: 0.875, recall: 0.875, precision: 0.8836317135549872, f_beta: 0.8742928975487115
train: step: 2070, loss: 0.01909738779067993, acc: 0.975, recall: 0.975, precision: 0.9761904761904762, f_beta: 0.9749843652282677
train: step: 2071, loss: 0.15395799279212952, acc: 0.775, recall: 0.7749999999999999, precision: 0.8133903133903134, f_beta: 0.7678916827853
train: step: 2072, loss: 0.1757139265537262, acc: 0.8, recall: 0.8, precision: 0.8296703296703296, f_beta: 0.7953964194373402
train: step: 2073, loss: 0.06006264686584473, acc: 0.925, recall: 0.925, precision: 0.9260651629072681, f_beta: 0.924953095684803
train: step: 2074, loss: 0.06608869135379791, acc: 0.925, recall: 0.925, precision: 0.9260651629072681, f_beta: 0.924953095684803
train: step: 2075, loss: 0.15644530951976776, acc: 0.8, recall: 0.8, precision: 0.803030303030303, f_beta: 0.7994987468671679
train: step: 2076, loss: 0.0856325775384903, acc: 0.9, recall: 0.8999999999999999, precision: 0.904040404040404, f_beta: 0.899749373433584
train: step: 2077, loss: 0.3593204617500305, acc: 0.825, recall: 0.825, precision: 0.8258145363408521, f_beta: 0.8248905565978737
train: step: 2078, loss: 0.09984295070171356, acc: 0.9, recall: 0.8999999999999999, precision: 0.904040404040404, f_beta: 0.899749373433584
train: step: 2079, loss: 0.06407177448272705, acc: 0.925, recall: 0.925, precision: 0.9260651629072681, f_beta: 0.924953095684803
train: step: 2080, loss: 0.1712648570537567, acc: 0.775, recall: 0.775, precision: 0.7933333333333333, f_beta: 0.7714285714285714
train: step: 2081, loss: 0.08520960807800293, acc: 0.875, recall: 0.875, precision: 0.8836317135549872, f_beta: 0.8742928975487115
train: step: 2082, loss: 0.07785503566265106, acc: 0.9, recall: 0.8999999999999999, precision: 0.904040404040404, f_beta: 0.899749373433584
train: step: 2083, loss: 0.15836980938911438, acc: 0.875, recall: 0.875, precision: 0.8759398496240601, f_beta: 0.8749218261413383
train: step: 2084, loss: 0.07197032123804092, acc: 0.9, recall: 0.8999999999999999, precision: 0.904040404040404, f_beta: 0.899749373433584
train: step: 2085, loss: 0.09042267501354218, acc: 0.875, recall: 0.875, precision: 0.8836317135549872, f_beta: 0.8742928975487115
train: step: 2086, loss: 0.20886202156543732, acc: 0.8, recall: 0.8, precision: 0.8, f_beta: 0.8000000000000002
train: step: 2087, loss: 0.10078448057174683, acc: 0.875, recall: 0.875, precision: 0.8759398496240601, f_beta: 0.8749218261413383
train: step: 2088, loss: 0.07830016314983368, acc: 0.875, recall: 0.875, precision: 0.9, f_beta: 0.873015873015873
train: step: 2089, loss: 0.03608395904302597, acc: 0.975, recall: 0.975, precision: 0.9761904761904762, f_beta: 0.9749843652282677
train: step: 2090, loss: 0.05841684341430664, acc: 0.925, recall: 0.925, precision: 0.9260651629072681, f_beta: 0.924953095684803
train: step: 2091, loss: 0.026502355933189392, acc: 0.975, recall: 0.975, precision: 0.9761904761904762, f_beta: 0.9749843652282677
train: step: 2092, loss: 0.15116730332374573, acc: 0.775, recall: 0.7749999999999999, precision: 0.7813299232736572, f_beta: 0.7737272155876807
train: step: 2093, loss: 0.07856711000204086, acc: 0.9, recall: 0.9, precision: 0.9, f_beta: 0.9
train: step: 2094, loss: 0.343392550945282, acc: 0.675, recall: 0.675, precision: 0.6994301994301995, f_beta: 0.6647324306898775
train: step: 2095, loss: 0.03855803236365318, acc: 0.975, recall: 0.975, precision: 0.9761904761904762, f_beta: 0.9749843652282677
train: step: 2096, loss: 0.09543638676404953, acc: 0.9, recall: 0.9, precision: 0.9166666666666667, f_beta: 0.898989898989899
train: step: 2097, loss: 0.3499733507633209, acc: 0.3, recall: 0.3, precision: 0.297979797979798, f_beta: 0.2982456140350877
train: step: 2098, loss: 0.1255330741405487, acc: 0.825, recall: 0.825, precision: 0.8324808184143222, f_beta: 0.824010056568196
train: step: 2099, loss: 0.056352078914642334, acc: 0.95, recall: 0.95, precision: 0.95, f_beta: 0.9500000000000001
train: step: 2100, loss: 0.34736770391464233, acc: 0.825, recall: 0.825, precision: 0.8324808184143222, f_beta: 0.824010056568196
train: step: 2101, loss: 0.060188256204128265, acc: 0.9, recall: 0.8999999999999999, precision: 0.904040404040404, f_beta: 0.899749373433584
train: step: 2102, loss: 0.06067316606640816, acc: 0.95, recall: 0.95, precision: 0.9545454545454546, f_beta: 0.949874686716792
train: step: 2103, loss: 0.06680110096931458, acc: 0.925, recall: 0.925, precision: 0.9347826086956521, f_beta: 0.9245757385292268
train: step: 2104, loss: 0.11411473900079727, acc: 0.825, recall: 0.825, precision: 0.8258145363408521, f_beta: 0.8248905565978737
train: step: 2105, loss: 0.06069643422961235, acc: 0.925, recall: 0.925, precision: 0.9347826086956521, f_beta: 0.9245757385292268
train: step: 2106, loss: 0.12620586156845093, acc: 0.85, recall: 0.85, precision: 0.85, f_beta: 0.85
train: step: 2107, loss: 0.09023167192935944, acc: 0.9, recall: 0.8999999999999999, precision: 0.904040404040404, f_beta: 0.899749373433584
train: step: 2108, loss: 0.07747782766819, acc: 0.9, recall: 0.9, precision: 0.9, f_beta: 0.9
train: step: 2109, loss: 0.05688212066888809, acc: 0.925, recall: 0.925, precision: 0.9347826086956521, f_beta: 0.9245757385292268
train: step: 2110, loss: 0.07959594577550888, acc: 0.9, recall: 0.8999999999999999, precision: 0.904040404040404, f_beta: 0.899749373433584
train: step: 2111, loss: 0.1947614997625351, acc: 0.725, recall: 0.725, precision: 0.7255639097744361, f_beta: 0.7248280175109443
train: step: 2112, loss: 0.015786757692694664, acc: 0.975, recall: 0.975, precision: 0.9761904761904762, f_beta: 0.9749843652282677
train: step: 2113, loss: 0.04694070667028427, acc: 0.925, recall: 0.925, precision: 0.9347826086956521, f_beta: 0.9245757385292268
train: step: 2114, loss: 0.04350700229406357, acc: 0.95, recall: 0.95, precision: 0.9545454545454546, f_beta: 0.949874686716792
train: step: 2115, loss: 0.1779077649116516, acc: 0.775, recall: 0.775, precision: 0.7933333333333333, f_beta: 0.7714285714285714
train: step: 2116, loss: 0.08338389545679092, acc: 0.875, recall: 0.875, precision: 0.8759398496240601, f_beta: 0.8749218261413383
train: step: 2117, loss: 0.038396187126636505, acc: 0.95, recall: 0.95, precision: 0.9545454545454546, f_beta: 0.949874686716792
train: step: 2118, loss: 0.06812959909439087, acc: 0.975, recall: 0.975, precision: 0.9761904761904762, f_beta: 0.9749843652282677
train: step: 2119, loss: 0.17005237936973572, acc: 0.8, recall: 0.8, precision: 0.8125, f_beta: 0.797979797979798
train: step: 2120, loss: 0.04031207039952278, acc: 0.925, recall: 0.925, precision: 0.9347826086956521, f_beta: 0.9245757385292268
train: step: 2121, loss: 0.032071202993392944, acc: 0.95, recall: 0.95, precision: 0.9545454545454546, f_beta: 0.949874686716792
train: step: 2122, loss: 0.07331182062625885, acc: 0.95, recall: 0.95, precision: 0.95, f_beta: 0.9500000000000001
train: step: 2123, loss: 0.10506466776132584, acc: 0.85, recall: 0.8500000000000001, precision: 0.8535353535353536, f_beta: 0.849624060150376
train: step: 2124, loss: 0.03728128969669342, acc: 0.975, recall: 0.975, precision: 0.9761904761904762, f_beta: 0.9749843652282677
train: step: 2125, loss: 0.0421585813164711, acc: 0.95, recall: 0.95, precision: 0.9545454545454546, f_beta: 0.949874686716792
train: step: 2126, loss: 0.17718049883842468, acc: 0.75, recall: 0.75, precision: 0.8333333333333333, f_beta: 0.7333333333333334
train: step: 2127, loss: 0.07359027117490768, acc: 0.925, recall: 0.925, precision: 0.9260651629072681, f_beta: 0.924953095684803
train: step: 2128, loss: 0.026268750429153442, acc: 0.975, recall: 0.975, precision: 0.9761904761904762, f_beta: 0.9749843652282677
train: step: 2129, loss: 0.22584351897239685, acc: 0.675, recall: 0.675, precision: 0.803030303030303, f_beta: 0.636617749825297
train: step: 2130, loss: 0.2030194103717804, acc: 0.725, recall: 0.725, precision: 0.74, f_beta: 0.7206349206349207
train: step: 2131, loss: 0.12714695930480957, acc: 0.825, recall: 0.825, precision: 0.8703703703703703, f_beta: 0.819471308833011
train: step: 2132, loss: 0.046930134296417236, acc: 0.95, recall: 0.95, precision: 0.95, f_beta: 0.9500000000000001
train: step: 2133, loss: 0.04012996703386307, acc: 0.975, recall: 0.975, precision: 0.9761904761904762, f_beta: 0.9749843652282677
train: step: 2134, loss: 0.09738431870937347, acc: 0.875, recall: 0.875, precision: 0.8836317135549872, f_beta: 0.8742928975487115
train: step: 2135, loss: 0.14441116154193878, acc: 0.825, recall: 0.825, precision: 0.8324808184143222, f_beta: 0.824010056568196
train: step: 2136, loss: 0.0040405020117759705, acc: 1.0, recall: 1.0, precision: 1.0, f_beta: 1.0
train: step: 2137, loss: 0.04600949212908745, acc: 0.975, recall: 0.975, precision: 0.9761904761904762, f_beta: 0.9749843652282677
train: step: 2138, loss: 0.038426317274570465, acc: 0.95, recall: 0.95, precision: 0.95, f_beta: 0.9500000000000001
train: step: 2139, loss: 0.08826296776533127, acc: 0.9, recall: 0.9, precision: 0.9166666666666667, f_beta: 0.898989898989899
train: step: 2140, loss: 0.07347681373357773, acc: 0.925, recall: 0.925, precision: 0.9347826086956521, f_beta: 0.9245757385292268
train: step: 2141, loss: 0.1402720957994461, acc: 0.825, recall: 0.825, precision: 0.8466666666666667, f_beta: 0.8222222222222222
train: step: 2142, loss: 0.07762375473976135, acc: 0.9, recall: 0.9, precision: 0.9166666666666667, f_beta: 0.898989898989899
train: step: 2143, loss: 0.07997474819421768, acc: 0.875, recall: 0.875, precision: 0.8836317135549872, f_beta: 0.8742928975487115
train: step: 2144, loss: 0.3838810324668884, acc: 0.7, recall: 0.7, precision: 0.7380952380952381, f_beta: 0.6875
train: step: 2145, loss: 0.045715127140283585, acc: 0.95, recall: 0.95, precision: 0.9545454545454546, f_beta: 0.949874686716792
train: step: 2146, loss: 0.01924653723835945, acc: 0.975, recall: 0.975, precision: 0.9761904761904762, f_beta: 0.9749843652282677
train: step: 2147, loss: 0.05619167163968086, acc: 0.925, recall: 0.925, precision: 0.9260651629072681, f_beta: 0.924953095684803
train: step: 2148, loss: 0.07218904793262482, acc: 0.925, recall: 0.925, precision: 0.9260651629072681, f_beta: 0.924953095684803
train: step: 2149, loss: 0.0402514822781086, acc: 0.975, recall: 0.975, precision: 0.9761904761904762, f_beta: 0.9749843652282677
train: step: 2150, loss: 0.20759062469005585, acc: 0.825, recall: 0.825, precision: 0.8258145363408521, f_beta: 0.8248905565978737
train: step: 2151, loss: 0.16626957058906555, acc: 0.75, recall: 0.75, precision: 0.7525252525252526, f_beta: 0.7493734335839599
train: step: 2152, loss: 0.03913767635822296, acc: 0.95, recall: 0.95, precision: 0.95, f_beta: 0.9500000000000001
train: step: 2153, loss: 0.026090851053595543, acc: 0.975, recall: 0.975, precision: 0.9761904761904762, f_beta: 0.9749843652282677
train: step: 2154, loss: 0.1516747921705246, acc: 0.8, recall: 0.8, precision: 0.8571428571428572, f_beta: 0.7916666666666665
train: step: 2155, loss: 0.08337819576263428, acc: 0.975, recall: 0.975, precision: 0.9761904761904762, f_beta: 0.9749843652282677
train: step: 2156, loss: 0.17460452020168304, acc: 0.775, recall: 0.7749999999999999, precision: 0.8133903133903134, f_beta: 0.7678916827853
train: step: 2157, loss: 0.12310850620269775, acc: 0.925, recall: 0.925, precision: 0.9260651629072681, f_beta: 0.924953095684803
train: step: 2158, loss: 0.2495337426662445, acc: 1.0, recall: 1.0, precision: 1.0, f_beta: 1.0
train: step: 2159, loss: 0.09296838939189911, acc: 0.875, recall: 0.875, precision: 0.8759398496240601, f_beta: 0.8749218261413383
train: step: 2160, loss: 0.019140854477882385, acc: 1.0, recall: 1.0, precision: 1.0, f_beta: 1.0
train: step: 2161, loss: 0.08906211704015732, acc: 0.85, recall: 0.85, precision: 0.8645833333333333, f_beta: 0.8484848484848484
train: step: 2162, loss: 0.04784132540225983, acc: 0.95, recall: 0.95, precision: 0.9545454545454546, f_beta: 0.949874686716792
train: step: 2163, loss: 0.0887950211763382, acc: 0.875, recall: 0.875, precision: 0.8759398496240601, f_beta: 0.8749218261413383
train: step: 2164, loss: 0.20960068702697754, acc: 0.7, recall: 0.7, precision: 0.7197802197802198, f_beta: 0.6930946291560103
train: step: 2165, loss: 0.07445470243692398, acc: 0.9, recall: 0.9, precision: 0.9166666666666667, f_beta: 0.898989898989899
train: step: 2166, loss: 0.09165023267269135, acc: 0.9, recall: 0.8999999999999999, precision: 0.904040404040404, f_beta: 0.899749373433584
train: step: 2167, loss: 0.0863209217786789, acc: 0.9, recall: 0.9, precision: 0.9166666666666667, f_beta: 0.898989898989899
train: step: 2168, loss: 0.02671126462519169, acc: 1.0, recall: 1.0, precision: 1.0, f_beta: 1.0
train: step: 2169, loss: 0.02731495164334774, acc: 0.975, recall: 0.975, precision: 0.9761904761904762, f_beta: 0.9749843652282677
train: step: 2170, loss: 0.048073675483465195, acc: 0.95, recall: 0.95, precision: 0.95, f_beta: 0.9500000000000001
train: step: 2171, loss: 0.08140713721513748, acc: 0.875, recall: 0.875, precision: 0.8836317135549872, f_beta: 0.8742928975487115
train: step: 2172, loss: 0.35141658782958984, acc: 0.625, recall: 0.625, precision: 0.6333333333333333, f_beta: 0.6190476190476191
train: step: 2173, loss: 0.03806716203689575, acc: 0.975, recall: 0.975, precision: 0.9761904761904762, f_beta: 0.9749843652282677
train: step: 2174, loss: 0.3705390989780426, acc: 0.85, recall: 0.85, precision: 0.8645833333333333, f_beta: 0.8484848484848484
train: step: 2175, loss: 0.01176535990089178, acc: 1.0, recall: 1.0, precision: 1.0, f_beta: 1.0
train: step: 2176, loss: 0.09338464587926865, acc: 0.95, recall: 0.95, precision: 0.9545454545454546, f_beta: 0.949874686716792
train: step: 2177, loss: 0.04526437446475029, acc: 0.95, recall: 0.95, precision: 0.95, f_beta: 0.9500000000000001
train: step: 2178, loss: 0.23901386559009552, acc: 0.75, recall: 0.75, precision: 0.7747252747252746, f_beta: 0.7442455242966751
train: step: 2179, loss: 0.06527242809534073, acc: 0.925, recall: 0.925, precision: 0.9260651629072681, f_beta: 0.924953095684803
train: step: 2180, loss: 0.05315899848937988, acc: 0.95, recall: 0.95, precision: 0.9545454545454546, f_beta: 0.949874686716792
train: step: 2181, loss: 0.0577007420361042, acc: 0.95, recall: 0.95, precision: 0.9545454545454546, f_beta: 0.949874686716792
train: step: 2182, loss: 0.2202480584383011, acc: 0.675, recall: 0.675, precision: 0.803030303030303, f_beta: 0.636617749825297
train: step: 2183, loss: 0.10626338422298431, acc: 0.875, recall: 0.875, precision: 0.9, f_beta: 0.873015873015873
train: step: 2184, loss: 0.13904759287834167, acc: 0.775, recall: 0.7749999999999999, precision: 0.8133903133903134, f_beta: 0.7678916827853
train: step: 2185, loss: 0.1570415049791336, acc: 0.8, recall: 0.8, precision: 0.8296703296703296, f_beta: 0.7953964194373402
train: step: 2186, loss: 0.06715817749500275, acc: 0.925, recall: 0.925, precision: 0.9260651629072681, f_beta: 0.924953095684803
train: step: 2187, loss: 0.0647202879190445, acc: 0.925, recall: 0.925, precision: 0.9260651629072681, f_beta: 0.924953095684803
train: step: 2188, loss: 0.05744367837905884, acc: 0.95, recall: 0.95, precision: 0.9545454545454546, f_beta: 0.949874686716792
train: step: 2189, loss: 0.054510943591594696, acc: 0.925, recall: 0.925, precision: 0.9260651629072681, f_beta: 0.924953095684803
train: step: 2190, loss: 0.14179019629955292, acc: 0.775, recall: 0.775, precision: 0.7933333333333333, f_beta: 0.7714285714285714
train: step: 2191, loss: 0.2075670212507248, acc: 0.75, recall: 0.75, precision: 0.7525252525252526, f_beta: 0.7493734335839599
train: step: 2192, loss: 0.06813766062259674, acc: 0.925, recall: 0.925, precision: 0.9347826086956521, f_beta: 0.9245757385292268
train: step: 2193, loss: 0.28655973076820374, acc: 0.525, recall: 0.5249999999999999, precision: 0.5284900284900285, f_beta: 0.5099935525467441
train: step: 2194, loss: 0.0450744703412056, acc: 0.95, recall: 0.95, precision: 0.95, f_beta: 0.9500000000000001
train: step: 2195, loss: 0.15624308586120605, acc: 0.925, recall: 0.925, precision: 0.9260651629072681, f_beta: 0.924953095684803
train: step: 2196, loss: 0.08222724497318268, acc: 0.925, recall: 0.925, precision: 0.9260651629072681, f_beta: 0.924953095684803
train: step: 2197, loss: 0.05825505405664444, acc: 0.95, recall: 0.95, precision: 0.9545454545454546, f_beta: 0.949874686716792
train: step: 2198, loss: 0.04526378959417343, acc: 0.95, recall: 0.95, precision: 0.9545454545454546, f_beta: 0.949874686716792
train: step: 2199, loss: 0.10362429916858673, acc: 0.875, recall: 0.875, precision: 0.8836317135549872, f_beta: 0.8742928975487115
train: step: 2200, loss: 0.1631140410900116, acc: 0.675, recall: 0.675, precision: 0.6754385964912281, f_beta: 0.6747967479674797
train: step: 2201, loss: 0.11600863933563232, acc: 0.85, recall: 0.85, precision: 0.8846153846153846, f_beta: 0.8465473145780051
train: step: 2202, loss: 0.05175744742155075, acc: 0.95, recall: 0.95, precision: 0.95, f_beta: 0.9500000000000001
train: step: 2203, loss: 0.05642901733517647, acc: 0.925, recall: 0.925, precision: 0.9260651629072681, f_beta: 0.924953095684803
train: step: 2204, loss: 0.12031499296426773, acc: 0.875, recall: 0.875, precision: 0.8836317135549872, f_beta: 0.8742928975487115
train: step: 2205, loss: 0.25690847635269165, acc: 0.675, recall: 0.675, precision: 0.6994301994301995, f_beta: 0.6647324306898775
train: step: 2206, loss: 0.03943566605448723, acc: 0.975, recall: 0.975, precision: 0.9761904761904762, f_beta: 0.9749843652282677
train: step: 2207, loss: 0.20628423988819122, acc: 0.725, recall: 0.725, precision: 0.74, f_beta: 0.7206349206349207
train: step: 2208, loss: 0.055296726524829865, acc: 0.925, recall: 0.925, precision: 0.9347826086956521, f_beta: 0.9245757385292268
train: step: 2209, loss: 0.031105995178222656, acc: 0.975, recall: 0.975, precision: 0.9761904761904762, f_beta: 0.9749843652282677
train: step: 2210, loss: 0.07379201799631119, acc: 0.9, recall: 0.8999999999999999, precision: 0.904040404040404, f_beta: 0.899749373433584
train: step: 2211, loss: 0.18869781494140625, acc: 0.7, recall: 0.7, precision: 0.7083333333333333, f_beta: 0.6969696969696968
train: step: 2212, loss: 0.169248566031456, acc: 0.925, recall: 0.925, precision: 0.9260651629072681, f_beta: 0.924953095684803
train: step: 2213, loss: 0.017371278256177902, acc: 1.0, recall: 1.0, precision: 1.0, f_beta: 1.0
train: step: 2214, loss: 0.1484338492155075, acc: 0.825, recall: 0.825, precision: 0.8324808184143222, f_beta: 0.824010056568196
train: step: 2215, loss: 0.019150983542203903, acc: 1.0, recall: 1.0, precision: 1.0, f_beta: 1.0
train: step: 2216, loss: 0.10272900760173798, acc: 0.9, recall: 0.8999999999999999, precision: 0.904040404040404, f_beta: 0.899749373433584
train: step: 2217, loss: 0.06080816313624382, acc: 0.975, recall: 0.975, precision: 0.9761904761904762, f_beta: 0.9749843652282677
train: step: 2218, loss: 0.0440884530544281, acc: 0.975, recall: 0.975, precision: 0.9761904761904762, f_beta: 0.9749843652282677
train: step: 2219, loss: 0.12897911667823792, acc: 0.825, recall: 0.825, precision: 0.8258145363408521, f_beta: 0.8248905565978737
train: step: 2220, loss: 0.11934087425470352, acc: 0.875, recall: 0.875, precision: 0.8836317135549872, f_beta: 0.8742928975487115
train: step: 2221, loss: 0.06319290399551392, acc: 0.925, recall: 0.925, precision: 0.9260651629072681, f_beta: 0.924953095684803
train: step: 2222, loss: 0.03514878451824188, acc: 0.95, recall: 0.95, precision: 0.9545454545454546, f_beta: 0.949874686716792
train: step: 2223, loss: 0.016100235283374786, acc: 0.975, recall: 0.975, precision: 0.9761904761904762, f_beta: 0.9749843652282677
train: step: 2224, loss: 0.1411028504371643, acc: 0.85, recall: 0.8500000000000001, precision: 0.8535353535353536, f_beta: 0.849624060150376
train: step: 2225, loss: 0.10460805892944336, acc: 0.875, recall: 0.875, precision: 0.9, f_beta: 0.873015873015873
train: step: 2226, loss: 0.09314854443073273, acc: 0.825, recall: 0.825, precision: 0.8703703703703703, f_beta: 0.819471308833011
train: step: 2227, loss: 0.06881062686443329, acc: 0.925, recall: 0.925, precision: 0.9260651629072681, f_beta: 0.924953095684803
train: step: 2228, loss: 0.08356093615293503, acc: 0.875, recall: 0.875, precision: 0.8759398496240601, f_beta: 0.8749218261413383
train: step: 2229, loss: 0.06883843243122101, acc: 0.95, recall: 0.95, precision: 0.9545454545454546, f_beta: 0.949874686716792
train: step: 2230, loss: 0.053697604686021805, acc: 0.975, recall: 0.975, precision: 0.9761904761904762, f_beta: 0.9749843652282677
train: step: 2231, loss: 0.25938764214515686, acc: 0.9, recall: 0.9, precision: 0.9166666666666667, f_beta: 0.898989898989899
train: step: 2232, loss: 0.06807596981525421, acc: 0.925, recall: 0.925, precision: 0.9260651629072681, f_beta: 0.924953095684803
train: step: 2233, loss: 0.18210476636886597, acc: 0.825, recall: 0.825, precision: 0.8466666666666667, f_beta: 0.8222222222222222
train: step: 2234, loss: 0.09193427860736847, acc: 0.875, recall: 0.875, precision: 0.9, f_beta: 0.873015873015873
train: step: 2235, loss: 0.07354621589183807, acc: 0.925, recall: 0.925, precision: 0.9347826086956521, f_beta: 0.9245757385292268
train: step: 2236, loss: 0.12057046592235565, acc: 0.825, recall: 0.825, precision: 0.8324808184143222, f_beta: 0.824010056568196
train: step: 2237, loss: 0.03129153326153755, acc: 0.975, recall: 0.975, precision: 0.9761904761904762, f_beta: 0.9749843652282677
train: step: 2238, loss: 0.1534264087677002, acc: 0.8, recall: 0.8, precision: 0.8296703296703296, f_beta: 0.7953964194373402
train: step: 2239, loss: 0.12284352630376816, acc: 0.9, recall: 0.8999999999999999, precision: 0.904040404040404, f_beta: 0.899749373433584
train: step: 2240, loss: 0.06878020614385605, acc: 0.925, recall: 0.925, precision: 0.9347826086956521, f_beta: 0.9245757385292268
train: step: 2241, loss: 0.01733047142624855, acc: 0.975, recall: 0.975, precision: 0.9761904761904762, f_beta: 0.9749843652282677
train: step: 2242, loss: 0.048521291464567184, acc: 0.925, recall: 0.925, precision: 0.9347826086956521, f_beta: 0.9245757385292268
train: step: 2243, loss: 0.07414846122264862, acc: 0.9, recall: 0.9, precision: 0.9166666666666667, f_beta: 0.898989898989899
train: step: 2244, loss: 0.2344748079776764, acc: 0.925, recall: 0.925, precision: 0.9260651629072681, f_beta: 0.924953095684803
train: step: 2245, loss: 0.021206539124250412, acc: 0.975, recall: 0.975, precision: 0.9761904761904762, f_beta: 0.9749843652282677
train: step: 2246, loss: 0.07583276927471161, acc: 0.9, recall: 0.9, precision: 0.9166666666666667, f_beta: 0.898989898989899
train: step: 2247, loss: 0.06263206154108047, acc: 0.9, recall: 0.9, precision: 0.9166666666666667, f_beta: 0.898989898989899
train: step: 2248, loss: 0.16368472576141357, acc: 0.8, recall: 0.8, precision: 0.8125, f_beta: 0.797979797979798
train: step: 2249, loss: 0.18872687220573425, acc: 0.925, recall: 0.925, precision: 0.9347826086956521, f_beta: 0.9245757385292268
train: step: 2250, loss: 0.40187567472457886, acc: 0.75, recall: 0.75, precision: 0.7604166666666667, f_beta: 0.7474747474747475
train: step: 2251, loss: 0.06968925893306732, acc: 0.9, recall: 0.8999999999999999, precision: 0.904040404040404, f_beta: 0.899749373433584
train: step: 2252, loss: 0.04581338167190552, acc: 0.95, recall: 0.95, precision: 0.95, f_beta: 0.9500000000000001
train: step: 2253, loss: 0.18231752514839172, acc: 0.7, recall: 0.7, precision: 0.7197802197802198, f_beta: 0.6930946291560103
train: step: 2254, loss: 0.03687272220849991, acc: 0.95, recall: 0.95, precision: 0.9545454545454546, f_beta: 0.949874686716792
train: step: 2255, loss: 0.06393836438655853, acc: 0.95, recall: 0.95, precision: 0.9545454545454546, f_beta: 0.949874686716792
train: step: 2256, loss: 0.013913859613239765, acc: 0.975, recall: 0.975, precision: 0.9761904761904762, f_beta: 0.9749843652282677
train: step: 2257, loss: 0.02944103442132473, acc: 0.975, recall: 0.975, precision: 0.9761904761904762, f_beta: 0.9749843652282677
train: step: 2258, loss: 0.08162463456392288, acc: 0.925, recall: 0.925, precision: 0.9347826086956521, f_beta: 0.9245757385292268
train: step: 2259, loss: 0.0669379010796547, acc: 0.875, recall: 0.875, precision: 0.8759398496240601, f_beta: 0.8749218261413383
train: step: 2260, loss: 0.060460008680820465, acc: 0.925, recall: 0.925, precision: 0.9347826086956521, f_beta: 0.9245757385292268
train: step: 2261, loss: 0.18349841237068176, acc: 0.725, recall: 0.725, precision: 0.7821316614420062, f_beta: 0.7103357472021066
train: step: 2262, loss: 0.01601104810833931, acc: 0.975, recall: 0.975, precision: 0.9761904761904762, f_beta: 0.9749843652282677
train: step: 2263, loss: 0.05180966109037399, acc: 0.925, recall: 0.925, precision: 0.9260651629072681, f_beta: 0.924953095684803
train: step: 2264, loss: 0.050133027136325836, acc: 0.95, recall: 0.95, precision: 0.9545454545454546, f_beta: 0.949874686716792
train: step: 2265, loss: 0.017486458644270897, acc: 0.975, recall: 0.975, precision: 0.9761904761904762, f_beta: 0.9749843652282677
train: step: 2266, loss: 0.3198823928833008, acc: 0.675, recall: 0.675, precision: 0.6790281329923273, f_beta: 0.6731615336266499
train: step: 2267, loss: 0.15657244622707367, acc: 0.8, recall: 0.8, precision: 0.8296703296703296, f_beta: 0.7953964194373402
train: step: 2268, loss: 0.2344834804534912, acc: 0.725, recall: 0.725, precision: 0.8225806451612903, f_beta: 0.7025016903313049
train: step: 2269, loss: 0.15887512266635895, acc: 0.825, recall: 0.825, precision: 0.8466666666666667, f_beta: 0.8222222222222222
train: step: 2270, loss: 0.0949152484536171, acc: 0.9, recall: 0.8999999999999999, precision: 0.904040404040404, f_beta: 0.899749373433584
train: step: 2271, loss: 0.13837003707885742, acc: 0.875, recall: 0.875, precision: 0.9, f_beta: 0.873015873015873
train: step: 2272, loss: 0.34278541803359985, acc: 0.725, recall: 0.7250000000000001, precision: 0.7301790281329923, f_beta: 0.7234443746071653
train: step: 2273, loss: 0.19798079133033752, acc: 0.775, recall: 0.7749999999999999, precision: 0.8133903133903134, f_beta: 0.7678916827853
train: step: 2274, loss: 0.045585036277770996, acc: 0.95, recall: 0.95, precision: 0.9545454545454546, f_beta: 0.949874686716792
train: step: 2275, loss: 0.07009229809045792, acc: 0.9, recall: 0.9, precision: 0.9166666666666667, f_beta: 0.898989898989899
train: step: 2276, loss: 0.3261416554450989, acc: 0.5, recall: 0.5, precision: 0.5, f_beta: 0.47916666666666663
train: step: 2277, loss: 0.1647450178861618, acc: 0.8, recall: 0.8, precision: 0.8125, f_beta: 0.797979797979798
train: step: 2278, loss: 0.04419747740030289, acc: 0.975, recall: 0.975, precision: 0.9761904761904762, f_beta: 0.9749843652282677
train: step: 2279, loss: 0.011396149173378944, acc: 1.0, recall: 1.0, precision: 1.0, f_beta: 1.0
train: step: 2280, loss: 0.19567164778709412, acc: 0.775, recall: 0.775, precision: 0.7756892230576441, f_beta: 0.774859287054409
train: step: 2281, loss: 0.08965383470058441, acc: 0.85, recall: 0.8500000000000001, precision: 0.8535353535353536, f_beta: 0.849624060150376
train: step: 2282, loss: 0.1483738124370575, acc: 0.825, recall: 0.825, precision: 0.8258145363408521, f_beta: 0.8248905565978737
train: step: 2283, loss: 0.02996094897389412, acc: 0.975, recall: 0.975, precision: 0.9761904761904762, f_beta: 0.9749843652282677
train: step: 2284, loss: 0.10373838990926743, acc: 0.9, recall: 0.9, precision: 0.9, f_beta: 0.9
train: step: 2285, loss: 0.12978915870189667, acc: 0.8, recall: 0.8, precision: 0.8, f_beta: 0.8000000000000002
train: step: 2286, loss: 0.11146457493305206, acc: 0.875, recall: 0.875, precision: 0.9, f_beta: 0.873015873015873
train: step: 2287, loss: 0.3749919831752777, acc: 0.825, recall: 0.825, precision: 0.8324808184143222, f_beta: 0.824010056568196
train: step: 2288, loss: 0.08796767890453339, acc: 0.85, recall: 0.85, precision: 0.85, f_beta: 0.85
train: step: 2289, loss: 0.05210316181182861, acc: 0.925, recall: 0.925, precision: 0.9347826086956521, f_beta: 0.9245757385292268
train: step: 2290, loss: 0.13549843430519104, acc: 0.85, recall: 0.85, precision: 0.85, f_beta: 0.85
train: step: 2291, loss: 0.08170942962169647, acc: 0.9, recall: 0.8999999999999999, precision: 0.904040404040404, f_beta: 0.899749373433584
train: step: 2292, loss: 0.35189154744148254, acc: 0.275, recall: 0.275, precision: 0.2744360902255639, f_beta: 0.27454659161976236
train: step: 2293, loss: 0.06467752158641815, acc: 0.925, recall: 0.925, precision: 0.9347826086956521, f_beta: 0.9245757385292268
train: step: 2294, loss: 0.036849476397037506, acc: 0.975, recall: 0.975, precision: 0.9761904761904762, f_beta: 0.9749843652282677
train: step: 2295, loss: 0.3009258210659027, acc: 0.6, recall: 0.6, precision: 0.6333333333333333, f_beta: 0.5733333333333334
train: step: 2296, loss: 0.09325031191110611, acc: 0.9, recall: 0.9, precision: 0.9166666666666667, f_beta: 0.898989898989899
train: step: 2297, loss: 0.18166129291057587, acc: 0.825, recall: 0.825, precision: 0.8466666666666667, f_beta: 0.8222222222222222
train: step: 2298, loss: 0.11636114120483398, acc: 0.85, recall: 0.8500000000000001, precision: 0.8535353535353536, f_beta: 0.849624060150376
train: step: 2299, loss: 0.016366813331842422, acc: 1.0, recall: 1.0, precision: 1.0, f_beta: 1.0
train: step: 2300, loss: 0.0365578718483448, acc: 0.975, recall: 0.975, precision: 0.9761904761904762, f_beta: 0.9749843652282677
train: step: 2301, loss: 0.08750811219215393, acc: 0.925, recall: 0.925, precision: 0.9347826086956521, f_beta: 0.9245757385292268
train: step: 2302, loss: 0.06621123850345612, acc: 0.95, recall: 0.95, precision: 0.95, f_beta: 0.9500000000000001
train: step: 2303, loss: 0.017565583810210228, acc: 1.0, recall: 1.0, precision: 1.0, f_beta: 1.0
train: step: 2304, loss: 0.056343771517276764, acc: 0.95, recall: 0.95, precision: 0.95, f_beta: 0.9500000000000001
train: step: 2305, loss: 0.0921972319483757, acc: 0.9, recall: 0.8999999999999999, precision: 0.904040404040404, f_beta: 0.899749373433584
train: step: 2306, loss: 0.04295136407017708, acc: 1.0, recall: 1.0, precision: 1.0, f_beta: 1.0
train: step: 2307, loss: 0.13870637118816376, acc: 0.825, recall: 0.825, precision: 0.8324808184143222, f_beta: 0.824010056568196
train: step: 2308, loss: 0.14703403413295746, acc: 0.75, recall: 0.75, precision: 0.7976190476190477, f_beta: 0.7395833333333333
train: step: 2309, loss: 0.03900488093495369, acc: 0.95, recall: 0.95, precision: 0.9545454545454546, f_beta: 0.949874686716792
train: step: 2310, loss: 0.05685345083475113, acc: 0.95, recall: 0.95, precision: 0.95, f_beta: 0.9500000000000001
train: step: 2311, loss: 0.11359109729528427, acc: 0.85, recall: 0.85, precision: 0.8645833333333333, f_beta: 0.8484848484848484
train: step: 2312, loss: 0.11555568873882294, acc: 0.825, recall: 0.825, precision: 0.8324808184143222, f_beta: 0.824010056568196
train: step: 2313, loss: 0.20293426513671875, acc: 0.7, recall: 0.7, precision: 0.7380952380952381, f_beta: 0.6875
train: step: 2314, loss: 0.13234159350395203, acc: 0.825, recall: 0.825, precision: 0.8324808184143222, f_beta: 0.824010056568196
train: step: 2315, loss: 0.16818293929100037, acc: 0.825, recall: 0.825, precision: 0.8466666666666667, f_beta: 0.8222222222222222
train: step: 2316, loss: 0.13265660405158997, acc: 0.825, recall: 0.825, precision: 0.8466666666666667, f_beta: 0.8222222222222222
train: step: 2317, loss: 0.05866498872637749, acc: 0.95, recall: 0.95, precision: 0.9545454545454546, f_beta: 0.949874686716792
train: step: 2318, loss: 0.1973545253276825, acc: 0.65, recall: 0.65, precision: 0.6648351648351649, f_beta: 0.6419437340153452
train: step: 2319, loss: 0.0433884933590889, acc: 0.95, recall: 0.95, precision: 0.9545454545454546, f_beta: 0.949874686716792
train: step: 2320, loss: 0.045191727578639984, acc: 0.975, recall: 0.975, precision: 0.9761904761904762, f_beta: 0.9749843652282677
train: step: 2321, loss: 0.03639810159802437, acc: 0.975, recall: 0.975, precision: 0.9761904761904762, f_beta: 0.9749843652282677
train: step: 2322, loss: 0.08807585388422012, acc: 0.925, recall: 0.925, precision: 0.9260651629072681, f_beta: 0.924953095684803
train: step: 2323, loss: 0.07953754812479019, acc: 0.9, recall: 0.8999999999999999, precision: 0.904040404040404, f_beta: 0.899749373433584
train: step: 2324, loss: 0.19989140331745148, acc: 0.725, recall: 0.725, precision: 0.7255639097744361, f_beta: 0.7248280175109443
train: step: 2325, loss: 0.35604068636894226, acc: 0.725, recall: 0.725, precision: 0.7821316614420062, f_beta: 0.7103357472021066
train: step: 2326, loss: 0.044334206730127335, acc: 0.925, recall: 0.925, precision: 0.9347826086956521, f_beta: 0.9245757385292268
train: step: 2327, loss: 0.13268312811851501, acc: 0.875, recall: 0.875, precision: 0.8836317135549872, f_beta: 0.8742928975487115
train: step: 2328, loss: 0.17334473133087158, acc: 0.75, recall: 0.75, precision: 0.7976190476190477, f_beta: 0.7395833333333333
train: step: 2329, loss: 0.25008416175842285, acc: 0.7, recall: 0.7, precision: 0.7083333333333333, f_beta: 0.6969696969696968
train: step: 2330, loss: 0.0748038962483406, acc: 0.9, recall: 0.9, precision: 0.9166666666666667, f_beta: 0.898989898989899
train: step: 2331, loss: 0.10469599813222885, acc: 0.875, recall: 0.875, precision: 0.8836317135549872, f_beta: 0.8742928975487115
train: step: 2332, loss: 0.025146037340164185, acc: 1.0, recall: 1.0, precision: 1.0, f_beta: 1.0
train: step: 2333, loss: 0.24775204062461853, acc: 0.725, recall: 0.7250000000000001, precision: 0.7301790281329923, f_beta: 0.7234443746071653
train: step: 2334, loss: 0.08449666202068329, acc: 0.875, recall: 0.875, precision: 0.8836317135549872, f_beta: 0.8742928975487115
train: step: 2335, loss: 0.08555914461612701, acc: 0.875, recall: 0.875, precision: 0.8836317135549872, f_beta: 0.8742928975487115
train: step: 2336, loss: 0.08539494127035141, acc: 0.9, recall: 0.9, precision: 0.9, f_beta: 0.9
train: step: 2337, loss: 0.06748779118061066, acc: 0.925, recall: 0.925, precision: 0.9260651629072681, f_beta: 0.924953095684803
train: step: 2338, loss: 0.3318703770637512, acc: 0.7, recall: 0.7, precision: 0.7380952380952381, f_beta: 0.6875
train: step: 2339, loss: 0.19643980264663696, acc: 0.7, recall: 0.7, precision: 0.7197802197802198, f_beta: 0.6930946291560103
train: step: 2340, loss: 0.21858415007591248, acc: 0.75, recall: 0.75, precision: 0.7976190476190477, f_beta: 0.7395833333333333
train: step: 2341, loss: 0.05555982515215874, acc: 0.95, recall: 0.95, precision: 0.9545454545454546, f_beta: 0.949874686716792
train: step: 2342, loss: 0.06931400299072266, acc: 0.925, recall: 0.925, precision: 0.9347826086956521, f_beta: 0.9245757385292268
train: step: 2343, loss: 0.19306959211826324, acc: 0.775, recall: 0.775, precision: 0.7756892230576441, f_beta: 0.774859287054409
train: step: 2344, loss: 0.08832000195980072, acc: 0.925, recall: 0.925, precision: 0.9260651629072681, f_beta: 0.924953095684803
train: step: 2345, loss: 0.21502165496349335, acc: 0.85, recall: 0.8500000000000001, precision: 0.8535353535353536, f_beta: 0.849624060150376
train: step: 2346, loss: 0.10139065980911255, acc: 0.875, recall: 0.875, precision: 0.8759398496240601, f_beta: 0.8749218261413383
train: step: 2347, loss: 0.07701124250888824, acc: 0.95, recall: 0.95, precision: 0.9545454545454546, f_beta: 0.949874686716792
train: step: 2348, loss: 0.11914107948541641, acc: 0.9, recall: 0.8999999999999999, precision: 0.904040404040404, f_beta: 0.899749373433584
train: step: 2349, loss: 0.1290266513824463, acc: 0.85, recall: 0.85, precision: 0.8846153846153846, f_beta: 0.8465473145780051
train: step: 2350, loss: 0.06592303514480591, acc: 0.95, recall: 0.95, precision: 0.9545454545454546, f_beta: 0.949874686716792
train: step: 2351, loss: 0.0891779288649559, acc: 0.85, recall: 0.85, precision: 0.85, f_beta: 0.85
train: step: 2352, loss: 0.19752129912376404, acc: 0.75, recall: 0.75, precision: 0.7747252747252746, f_beta: 0.7442455242966751
train: step: 2353, loss: 0.06748814880847931, acc: 0.9, recall: 0.9, precision: 0.9166666666666667, f_beta: 0.898989898989899
train: step: 2354, loss: 0.23659202456474304, acc: 0.65, recall: 0.65, precision: 0.6785714285714286, f_beta: 0.6354166666666667
train: step: 2355, loss: 0.010226959362626076, acc: 1.0, recall: 1.0, precision: 1.0, f_beta: 1.0
train: step: 2356, loss: 0.14839161932468414, acc: 0.8, recall: 0.8, precision: 0.8571428571428572, f_beta: 0.7916666666666665
train: step: 2357, loss: 0.07806490361690521, acc: 0.875, recall: 0.875, precision: 0.9, f_beta: 0.873015873015873
train: step: 2358, loss: 0.14341333508491516, acc: 0.775, recall: 0.7749999999999999, precision: 0.7813299232736572, f_beta: 0.7737272155876807
train: step: 2359, loss: 0.1492779701948166, acc: 0.8, recall: 0.8, precision: 0.8571428571428572, f_beta: 0.7916666666666665
train: step: 2360, loss: 0.10136418044567108, acc: 0.875, recall: 0.875, precision: 0.8759398496240601, f_beta: 0.8749218261413383
train: step: 2361, loss: 0.11154329776763916, acc: 0.9, recall: 0.8999999999999999, precision: 0.904040404040404, f_beta: 0.899749373433584
train: step: 2362, loss: 0.05726684257388115, acc: 0.95, recall: 0.95, precision: 0.95, f_beta: 0.9500000000000001
train: step: 2363, loss: 0.3007511496543884, acc: 0.925, recall: 0.925, precision: 0.9260651629072681, f_beta: 0.924953095684803
train: step: 2364, loss: 0.07141240686178207, acc: 0.9, recall: 0.9, precision: 0.9166666666666667, f_beta: 0.898989898989899
train: step: 2365, loss: 0.02797754667699337, acc: 0.975, recall: 0.975, precision: 0.9761904761904762, f_beta: 0.9749843652282677
train: step: 2366, loss: 0.15172910690307617, acc: 0.825, recall: 0.825, precision: 0.8703703703703703, f_beta: 0.819471308833011
train: step: 2367, loss: 0.0857028141617775, acc: 0.9, recall: 0.9, precision: 0.9166666666666667, f_beta: 0.898989898989899
train: step: 2368, loss: 0.0471535325050354, acc: 0.975, recall: 0.975, precision: 0.9761904761904762, f_beta: 0.9749843652282677
train: step: 2369, loss: 0.06017129495739937, acc: 0.95, recall: 0.95, precision: 0.9545454545454546, f_beta: 0.949874686716792
train: step: 2370, loss: 0.08989615738391876, acc: 0.925, recall: 0.925, precision: 0.9347826086956521, f_beta: 0.9245757385292268
train: step: 2371, loss: 0.091428242623806, acc: 0.9, recall: 0.9, precision: 0.9166666666666667, f_beta: 0.898989898989899
train: step: 2372, loss: 0.02913324534893036, acc: 0.975, recall: 0.975, precision: 0.9761904761904762, f_beta: 0.9749843652282677
train: step: 2373, loss: 0.09281621128320694, acc: 0.925, recall: 0.925, precision: 0.9260651629072681, f_beta: 0.924953095684803
train: step: 2374, loss: 0.11610162258148193, acc: 0.9, recall: 0.8999999999999999, precision: 0.904040404040404, f_beta: 0.899749373433584
train: step: 2375, loss: 0.03058820590376854, acc: 1.0, recall: 1.0, precision: 1.0, f_beta: 1.0
train: step: 2376, loss: 0.09729376435279846, acc: 0.9, recall: 0.9, precision: 0.9, f_beta: 0.9
train: step: 2377, loss: 0.11027906090021133, acc: 0.875, recall: 0.875, precision: 0.9, f_beta: 0.873015873015873
train: step: 2378, loss: 0.07717347145080566, acc: 0.95, recall: 0.95, precision: 0.9545454545454546, f_beta: 0.949874686716792
train: step: 2379, loss: 0.05583231523633003, acc: 0.95, recall: 0.95, precision: 0.9545454545454546, f_beta: 0.949874686716792
train: step: 2380, loss: 0.28052034974098206, acc: 0.775, recall: 0.775, precision: 0.7756892230576441, f_beta: 0.774859287054409
train: step: 2381, loss: 0.36706605553627014, acc: 0.825, recall: 0.825, precision: 0.8324808184143222, f_beta: 0.824010056568196
train: step: 2382, loss: 0.222324937582016, acc: 0.75, recall: 0.75, precision: 0.7604166666666667, f_beta: 0.7474747474747475
train: step: 2383, loss: 0.0789809376001358, acc: 0.875, recall: 0.875, precision: 0.8836317135549872, f_beta: 0.8742928975487115
train: step: 2384, loss: 0.19175641238689423, acc: 0.75, recall: 0.75, precision: 0.7525252525252526, f_beta: 0.7493734335839599
train: step: 2385, loss: 0.0953904315829277, acc: 0.9, recall: 0.9, precision: 0.9166666666666667, f_beta: 0.898989898989899
train: step: 2386, loss: 0.09465572983026505, acc: 0.925, recall: 0.925, precision: 0.9260651629072681, f_beta: 0.924953095684803
train: step: 2387, loss: 0.2622133791446686, acc: 0.575, recall: 0.575, precision: 0.5800000000000001, f_beta: 0.5682539682539682
train: step: 2388, loss: 0.0623096339404583, acc: 0.95, recall: 0.95, precision: 0.9545454545454546, f_beta: 0.949874686716792
train: step: 2389, loss: 0.07053206861019135, acc: 0.925, recall: 0.925, precision: 0.9347826086956521, f_beta: 0.9245757385292268
train: step: 2390, loss: 0.140406996011734, acc: 0.825, recall: 0.825, precision: 0.8466666666666667, f_beta: 0.8222222222222222
train: step: 2391, loss: 0.08701284229755402, acc: 0.925, recall: 0.925, precision: 0.9260651629072681, f_beta: 0.924953095684803
train: step: 2392, loss: 0.08510351926088333, acc: 0.9, recall: 0.8999999999999999, precision: 0.904040404040404, f_beta: 0.899749373433584
train: step: 2393, loss: 0.011494145728647709, acc: 1.0, recall: 1.0, precision: 1.0, f_beta: 1.0
train: step: 2394, loss: 0.04093996807932854, acc: 0.975, recall: 0.975, precision: 0.9761904761904762, f_beta: 0.9749843652282677
train: step: 2395, loss: 0.1905188262462616, acc: 0.825, recall: 0.825, precision: 0.8258145363408521, f_beta: 0.8248905565978737
train: step: 2396, loss: 0.244971364736557, acc: 0.95, recall: 0.95, precision: 0.95, f_beta: 0.9500000000000001
train: step: 2397, loss: 0.09660856425762177, acc: 0.9, recall: 0.9, precision: 0.9, f_beta: 0.9
train: step: 2398, loss: 0.07137882709503174, acc: 0.9, recall: 0.9, precision: 0.9, f_beta: 0.9
train: step: 2399, loss: 0.013789799995720387, acc: 0.975, recall: 0.975, precision: 0.9761904761904762, f_beta: 0.9749843652282677
train: step: 2400, loss: 0.023405523970723152, acc: 0.975, recall: 0.975, precision: 0.9761904761904762, f_beta: 0.9749843652282677
train: step: 2401, loss: 0.05718766897916794, acc: 0.95, recall: 0.95, precision: 0.95, f_beta: 0.9500000000000001
train: step: 2402, loss: 0.21110136806964874, acc: 0.65, recall: 0.65, precision: 0.7941176470588236, f_beta: 0.6011396011396011
train: step: 2403, loss: 0.07846231758594513, acc: 0.95, recall: 0.95, precision: 0.9545454545454546, f_beta: 0.949874686716792
train: step: 2404, loss: 0.09879262745380402, acc: 0.95, recall: 0.95, precision: 0.9545454545454546, f_beta: 0.949874686716792
train: step: 2405, loss: 0.17220687866210938, acc: 0.8, recall: 0.8, precision: 0.8296703296703296, f_beta: 0.7953964194373402
train: step: 2406, loss: 0.070676788687706, acc: 0.9, recall: 0.8999999999999999, precision: 0.904040404040404, f_beta: 0.899749373433584
train: step: 2407, loss: 0.03276865556836128, acc: 0.975, recall: 0.975, precision: 0.9761904761904762, f_beta: 0.9749843652282677
train: step: 2408, loss: 0.1962544173002243, acc: 0.725, recall: 0.725, precision: 0.8225806451612903, f_beta: 0.7025016903313049
train: step: 2409, loss: 0.09164973348379135, acc: 0.9, recall: 0.8999999999999999, precision: 0.904040404040404, f_beta: 0.899749373433584
train: step: 2410, loss: 0.018573984503746033, acc: 0.975, recall: 0.975, precision: 0.9761904761904762, f_beta: 0.9749843652282677
train: step: 2411, loss: 0.1749477982521057, acc: 0.725, recall: 0.725, precision: 0.7821316614420062, f_beta: 0.7103357472021066
train: step: 2412, loss: 0.010164692997932434, acc: 1.0, recall: 1.0, precision: 1.0, f_beta: 1.0
train: step: 2413, loss: 0.021706171333789825, acc: 0.975, recall: 0.975, precision: 0.9761904761904762, f_beta: 0.9749843652282677
train: step: 2414, loss: 0.07529886066913605, acc: 0.875, recall: 0.875, precision: 0.8759398496240601, f_beta: 0.8749218261413383
train: step: 2415, loss: 0.07487644255161285, acc: 0.9, recall: 0.9, precision: 0.9, f_beta: 0.9
train: step: 2416, loss: 0.05781165510416031, acc: 0.95, recall: 0.95, precision: 0.9545454545454546, f_beta: 0.949874686716792
train: step: 2417, loss: 0.05485454946756363, acc: 0.9, recall: 0.8999999999999999, precision: 0.904040404040404, f_beta: 0.899749373433584
train: step: 2418, loss: 0.03594016283750534, acc: 0.925, recall: 0.925, precision: 0.9260651629072681, f_beta: 0.924953095684803
train: step: 2419, loss: 0.02438393607735634, acc: 0.975, recall: 0.975, precision: 0.9761904761904762, f_beta: 0.9749843652282677
train: step: 2420, loss: 0.03230326250195503, acc: 0.975, recall: 0.975, precision: 0.9761904761904762, f_beta: 0.9749843652282677
train: step: 2421, loss: 0.11813025176525116, acc: 0.85, recall: 0.8500000000000001, precision: 0.8535353535353536, f_beta: 0.849624060150376
train: step: 2422, loss: 0.15364012122154236, acc: 0.8, recall: 0.8, precision: 0.8, f_beta: 0.8000000000000002
train: step: 2423, loss: 0.15752194821834564, acc: 0.775, recall: 0.7749999999999999, precision: 0.8133903133903134, f_beta: 0.7678916827853
train: step: 2424, loss: 0.04899870231747627, acc: 0.975, recall: 0.975, precision: 0.9761904761904762, f_beta: 0.9749843652282677
train: step: 2425, loss: 0.06774044781923294, acc: 0.925, recall: 0.925, precision: 0.9260651629072681, f_beta: 0.924953095684803
train: step: 2426, loss: 0.06386543065309525, acc: 0.95, recall: 0.95, precision: 0.9545454545454546, f_beta: 0.949874686716792
train: step: 2427, loss: 0.04548388347029686, acc: 0.975, recall: 0.975, precision: 0.9761904761904762, f_beta: 0.9749843652282677
train: step: 2428, loss: 0.08554057031869888, acc: 0.9, recall: 0.8999999999999999, precision: 0.904040404040404, f_beta: 0.899749373433584
train: step: 2429, loss: 0.03486228734254837, acc: 0.95, recall: 0.95, precision: 0.9545454545454546, f_beta: 0.949874686716792
train: step: 2430, loss: 0.06447490304708481, acc: 0.925, recall: 0.925, precision: 0.9347826086956521, f_beta: 0.9245757385292268
train: step: 2431, loss: 0.11384718120098114, acc: 0.85, recall: 0.85, precision: 0.85, f_beta: 0.85
train: step: 2432, loss: 0.08113604784011841, acc: 0.875, recall: 0.875, precision: 0.8759398496240601, f_beta: 0.8749218261413383
train: step: 2433, loss: 0.21108794212341309, acc: 0.725, recall: 0.725, precision: 0.7821316614420062, f_beta: 0.7103357472021066
train: step: 2434, loss: 0.10849277675151825, acc: 0.825, recall: 0.825, precision: 0.8258145363408521, f_beta: 0.8248905565978737
train: step: 2435, loss: 0.017247885465621948, acc: 0.975, recall: 0.975, precision: 0.9761904761904762, f_beta: 0.9749843652282677
train: step: 2436, loss: 0.15970952808856964, acc: 0.9, recall: 0.9, precision: 0.9166666666666667, f_beta: 0.898989898989899
train: step: 2437, loss: 0.3949595093727112, acc: 0.675, recall: 0.675, precision: 0.6790281329923273, f_beta: 0.6731615336266499
train: step: 2438, loss: 0.15673881769180298, acc: 0.8, recall: 0.8, precision: 0.8296703296703296, f_beta: 0.7953964194373402
train: step: 2439, loss: 0.024684369564056396, acc: 0.975, recall: 0.975, precision: 0.9761904761904762, f_beta: 0.9749843652282677
train: step: 2440, loss: 0.09875836968421936, acc: 0.875, recall: 0.875, precision: 0.8836317135549872, f_beta: 0.8742928975487115
train: step: 2441, loss: 0.130332350730896, acc: 0.85, recall: 0.8500000000000001, precision: 0.8535353535353536, f_beta: 0.849624060150376
train: step: 2442, loss: 0.06424113363027573, acc: 0.875, recall: 0.875, precision: 0.8759398496240601, f_beta: 0.8749218261413383
train: step: 2443, loss: 0.10392316430807114, acc: 0.875, recall: 0.875, precision: 0.9, f_beta: 0.873015873015873
train: step: 2444, loss: 0.0501396581530571, acc: 0.925, recall: 0.925, precision: 0.9260651629072681, f_beta: 0.924953095684803
train: step: 2445, loss: 0.052912019193172455, acc: 0.925, recall: 0.925, precision: 0.9347826086956521, f_beta: 0.9245757385292268
train: step: 2446, loss: 0.2272917479276657, acc: 0.675, recall: 0.675, precision: 0.6866666666666666, f_beta: 0.6698412698412699
train: step: 2447, loss: 0.10142932087182999, acc: 0.875, recall: 0.875, precision: 0.9, f_beta: 0.873015873015873
train: step: 2448, loss: 0.043089888989925385, acc: 0.95, recall: 0.95, precision: 0.9545454545454546, f_beta: 0.949874686716792
train: step: 2449, loss: 0.029002821072936058, acc: 0.975, recall: 0.975, precision: 0.9761904761904762, f_beta: 0.9749843652282677
train: step: 2450, loss: 0.03274450823664665, acc: 0.975, recall: 0.975, precision: 0.9761904761904762, f_beta: 0.9749843652282677
train: step: 2451, loss: 0.38660526275634766, acc: 0.2, recall: 0.19999999999999998, precision: 0.17032967032967034, f_beta: 0.1815856777493606
train: step: 2452, loss: 0.1100262850522995, acc: 0.85, recall: 0.8500000000000001, precision: 0.8535353535353536, f_beta: 0.849624060150376
train: step: 2453, loss: 0.3272877335548401, acc: 0.65, recall: 0.65, precision: 0.65625, f_beta: 0.6464646464646464
train: step: 2454, loss: 0.19666197896003723, acc: 0.775, recall: 0.775, precision: 0.8448275862068966, f_beta: 0.7630019749835418
train: step: 2455, loss: 0.11161341518163681, acc: 0.875, recall: 0.875, precision: 0.8759398496240601, f_beta: 0.8749218261413383
train: step: 2456, loss: 0.03296094387769699, acc: 0.95, recall: 0.95, precision: 0.95, f_beta: 0.9500000000000001
train: step: 2457, loss: 0.07509297132492065, acc: 0.925, recall: 0.925, precision: 0.9347826086956521, f_beta: 0.9245757385292268
train: step: 2458, loss: 0.12422497570514679, acc: 0.9, recall: 0.8999999999999999, precision: 0.904040404040404, f_beta: 0.899749373433584
train: step: 2459, loss: 0.19794631004333496, acc: 0.725, recall: 0.7250000000000001, precision: 0.7301790281329923, f_beta: 0.7234443746071653
train: step: 2460, loss: 0.06174604967236519, acc: 0.95, recall: 0.95, precision: 0.95, f_beta: 0.9500000000000001
train: step: 2461, loss: 0.04229436442255974, acc: 0.975, recall: 0.975, precision: 0.9761904761904762, f_beta: 0.9749843652282677
train: step: 2462, loss: 0.07466902583837509, acc: 0.975, recall: 0.975, precision: 0.9761904761904762, f_beta: 0.9749843652282677
train: step: 2463, loss: 0.037741489708423615, acc: 0.975, recall: 0.975, precision: 0.9761904761904762, f_beta: 0.9749843652282677
train: step: 2464, loss: 0.10056768357753754, acc: 0.85, recall: 0.85, precision: 0.85, f_beta: 0.85
train: step: 2465, loss: 0.12838497757911682, acc: 0.9, recall: 0.8999999999999999, precision: 0.904040404040404, f_beta: 0.899749373433584
train: step: 2466, loss: 0.06289461255073547, acc: 0.975, recall: 0.975, precision: 0.9761904761904762, f_beta: 0.9749843652282677
train: step: 2467, loss: 0.012031798250973225, acc: 1.0, recall: 1.0, precision: 1.0, f_beta: 1.0
train: step: 2468, loss: 0.049091637134552, acc: 0.95, recall: 0.95, precision: 0.95, f_beta: 0.9500000000000001
train: step: 2469, loss: 0.07983128726482391, acc: 0.9, recall: 0.9, precision: 0.9166666666666667, f_beta: 0.898989898989899
train: step: 2470, loss: 0.035474956035614014, acc: 0.975, recall: 0.975, precision: 0.9761904761904762, f_beta: 0.9749843652282677
train: step: 2471, loss: 0.047359567135572433, acc: 0.925, recall: 0.925, precision: 0.9260651629072681, f_beta: 0.924953095684803
train: step: 2472, loss: 0.15504157543182373, acc: 0.775, recall: 0.7749999999999999, precision: 0.8133903133903134, f_beta: 0.7678916827853
train: step: 2473, loss: 0.2128666192293167, acc: 0.7, recall: 0.7, precision: 0.7380952380952381, f_beta: 0.6875
train: step: 2474, loss: 0.2587962746620178, acc: 0.625, recall: 0.625, precision: 0.6567398119122256, f_beta: 0.6050032916392363
train: step: 2475, loss: 0.022111084312200546, acc: 0.975, recall: 0.975, precision: 0.9761904761904762, f_beta: 0.9749843652282677
train: step: 2476, loss: 0.04370971396565437, acc: 0.925, recall: 0.925, precision: 0.9260651629072681, f_beta: 0.924953095684803
train: step: 2477, loss: 0.04175521805882454, acc: 0.95, recall: 0.95, precision: 0.95, f_beta: 0.9500000000000001
train: step: 2478, loss: 0.19966986775398254, acc: 0.7, recall: 0.7, precision: 0.702020202020202, f_beta: 0.6992481203007519
train: step: 2479, loss: 0.14744646847248077, acc: 0.8, recall: 0.8, precision: 0.8125, f_beta: 0.797979797979798
train: step: 2480, loss: 0.023718860000371933, acc: 0.975, recall: 0.975, precision: 0.9761904761904762, f_beta: 0.9749843652282677
train: step: 2481, loss: 0.0900617390871048, acc: 0.9, recall: 0.8999999999999999, precision: 0.904040404040404, f_beta: 0.899749373433584
train: step: 2482, loss: 0.12140870094299316, acc: 0.875, recall: 0.875, precision: 0.8836317135549872, f_beta: 0.8742928975487115
train: step: 2483, loss: 0.12664173543453217, acc: 0.825, recall: 0.825, precision: 0.8324808184143222, f_beta: 0.824010056568196
train: step: 2484, loss: 0.08985016494989395, acc: 0.925, recall: 0.925, precision: 0.9260651629072681, f_beta: 0.924953095684803
train: step: 2485, loss: 0.05779297277331352, acc: 0.925, recall: 0.925, precision: 0.9260651629072681, f_beta: 0.924953095684803
train: step: 2486, loss: 0.24470913410186768, acc: 0.85, recall: 0.85, precision: 0.85, f_beta: 0.85
train: step: 2487, loss: 0.036287423223257065, acc: 0.975, recall: 0.975, precision: 0.9761904761904762, f_beta: 0.9749843652282677
train: step: 2488, loss: 0.08352364599704742, acc: 0.9, recall: 0.9, precision: 0.9, f_beta: 0.9
train: step: 2489, loss: 0.020546650514006615, acc: 0.975, recall: 0.975, precision: 0.9761904761904762, f_beta: 0.9749843652282677
train: step: 2490, loss: 0.0168722216039896, acc: 0.975, recall: 0.975, precision: 0.9761904761904762, f_beta: 0.9749843652282677
train: step: 2491, loss: 0.18555359542369843, acc: 0.75, recall: 0.75, precision: 0.7976190476190477, f_beta: 0.7395833333333333
train: step: 2492, loss: 0.31087756156921387, acc: 0.4, recall: 0.4, precision: 0.30392156862745096, f_beta: 0.3162393162393162
train: step: 2493, loss: 0.11104290187358856, acc: 0.85, recall: 0.85, precision: 0.8846153846153846, f_beta: 0.8465473145780051
train: step: 2494, loss: 0.15001921355724335, acc: 0.8, recall: 0.8, precision: 0.8571428571428572, f_beta: 0.7916666666666665
train: step: 2495, loss: 0.11108074337244034, acc: 0.85, recall: 0.85, precision: 0.8846153846153846, f_beta: 0.8465473145780051
train: step: 2496, loss: 0.2468441277742386, acc: 0.8, recall: 0.8, precision: 0.8125, f_beta: 0.797979797979798
train: step: 2497, loss: 0.054978739470243454, acc: 0.95, recall: 0.95, precision: 0.9545454545454546, f_beta: 0.949874686716792
train: step: 2498, loss: 0.020030369982123375, acc: 1.0, recall: 1.0, precision: 1.0, f_beta: 1.0
train: step: 2499, loss: 0.04335734248161316, acc: 1.0, recall: 1.0, precision: 1.0, f_beta: 1.0
train: step: 2500, loss: 0.04260632395744324, acc: 0.95, recall: 0.95, precision: 0.9545454545454546, f_beta: 0.949874686716792
train: step: 2501, loss: 0.07691647857427597, acc: 0.975, recall: 0.975, precision: 0.9761904761904762, f_beta: 0.9749843652282677
train: step: 2502, loss: 0.022344574332237244, acc: 0.975, recall: 0.975, precision: 0.9761904761904762, f_beta: 0.9749843652282677
train: step: 2503, loss: 0.043925799429416656, acc: 0.95, recall: 0.95, precision: 0.9545454545454546, f_beta: 0.949874686716792
train: step: 2504, loss: 0.04136250168085098, acc: 0.95, recall: 0.95, precision: 0.9545454545454546, f_beta: 0.949874686716792
train: step: 2505, loss: 0.12180020660161972, acc: 0.825, recall: 0.825, precision: 0.8466666666666667, f_beta: 0.8222222222222222
train: step: 2506, loss: 0.020521771162748337, acc: 1.0, recall: 1.0, precision: 1.0, f_beta: 1.0
train: step: 2507, loss: 0.05151473730802536, acc: 0.95, recall: 0.95, precision: 0.9545454545454546, f_beta: 0.949874686716792
train: step: 2508, loss: 0.251962810754776, acc: 0.675, recall: 0.675, precision: 0.6790281329923273, f_beta: 0.6731615336266499
train: step: 2509, loss: 0.03803807124495506, acc: 0.975, recall: 0.975, precision: 0.9761904761904762, f_beta: 0.9749843652282677
train: step: 2510, loss: 0.05447572469711304, acc: 0.95, recall: 0.95, precision: 0.9545454545454546, f_beta: 0.949874686716792
train: step: 2511, loss: 0.09851594269275665, acc: 0.9, recall: 0.9, precision: 0.9166666666666667, f_beta: 0.898989898989899
train: step: 2512, loss: 0.08131261169910431, acc: 0.9, recall: 0.9, precision: 0.9166666666666667, f_beta: 0.898989898989899
train: step: 2513, loss: 0.2233353555202484, acc: 0.85, recall: 0.85, precision: 0.8645833333333333, f_beta: 0.8484848484848484
train: step: 2514, loss: 0.061454810202121735, acc: 0.95, recall: 0.95, precision: 0.9545454545454546, f_beta: 0.949874686716792
train: step: 2515, loss: 0.08047900348901749, acc: 0.9, recall: 0.9, precision: 0.9, f_beta: 0.9
train: step: 2516, loss: 0.10403616726398468, acc: 0.875, recall: 0.875, precision: 0.8836317135549872, f_beta: 0.8742928975487115
train: step: 2517, loss: 0.03391336649656296, acc: 0.95, recall: 0.95, precision: 0.95, f_beta: 0.9500000000000001
train: step: 2518, loss: 0.03207665681838989, acc: 0.975, recall: 0.975, precision: 0.9761904761904762, f_beta: 0.9749843652282677
train: step: 2519, loss: 0.09423045814037323, acc: 0.85, recall: 0.85, precision: 0.8846153846153846, f_beta: 0.8465473145780051
train: step: 2520, loss: 0.11958511918783188, acc: 0.825, recall: 0.825, precision: 0.8466666666666667, f_beta: 0.8222222222222222
train: step: 2521, loss: 0.02317780815064907, acc: 0.975, recall: 0.975, precision: 0.9761904761904762, f_beta: 0.9749843652282677
train: step: 2522, loss: 0.24122095108032227, acc: 0.7, recall: 0.7, precision: 0.7197802197802198, f_beta: 0.6930946291560103
train: step: 2523, loss: 0.24152645468711853, acc: 0.625, recall: 0.625, precision: 0.7857142857142857, f_beta: 0.5636363636363637
train: step: 2524, loss: 0.03534740209579468, acc: 0.975, recall: 0.975, precision: 0.9761904761904762, f_beta: 0.9749843652282677
train: step: 2525, loss: 0.037058085203170776, acc: 0.925, recall: 0.925, precision: 0.9347826086956521, f_beta: 0.9245757385292268
train: step: 2526, loss: 0.05904395133256912, acc: 0.975, recall: 0.975, precision: 0.9761904761904762, f_beta: 0.9749843652282677
train: step: 2527, loss: 0.09756016731262207, acc: 0.95, recall: 0.95, precision: 0.9545454545454546, f_beta: 0.949874686716792
train: step: 2528, loss: 0.266100138425827, acc: 0.675, recall: 0.675, precision: 0.7508960573476702, f_beta: 0.6484110885733605
train: step: 2529, loss: 0.0762593150138855, acc: 0.9, recall: 0.8999999999999999, precision: 0.904040404040404, f_beta: 0.899749373433584
train: step: 2530, loss: 0.21986499428749084, acc: 0.9, recall: 0.9, precision: 0.9166666666666667, f_beta: 0.898989898989899
train: step: 2531, loss: 0.34671956300735474, acc: 0.4, recall: 0.4, precision: 0.3666666666666667, f_beta: 0.36
train: step: 2532, loss: 0.043292272835969925, acc: 0.95, recall: 0.95, precision: 0.9545454545454546, f_beta: 0.949874686716792
train: step: 2533, loss: 0.07627865672111511, acc: 0.875, recall: 0.875, precision: 0.8759398496240601, f_beta: 0.8749218261413383
train: step: 2534, loss: 0.17563125491142273, acc: 0.775, recall: 0.7749999999999999, precision: 0.8133903133903134, f_beta: 0.7678916827853
train: step: 2535, loss: 0.07744916528463364, acc: 0.925, recall: 0.925, precision: 0.9260651629072681, f_beta: 0.924953095684803
train: step: 2536, loss: 0.03599641099572182, acc: 0.95, recall: 0.95, precision: 0.95, f_beta: 0.9500000000000001
train: step: 2537, loss: 0.2658609449863434, acc: 0.8, recall: 0.8, precision: 0.803030303030303, f_beta: 0.7994987468671679
train: step: 2538, loss: 0.07918823510408401, acc: 0.925, recall: 0.925, precision: 0.9347826086956521, f_beta: 0.9245757385292268
train: step: 2539, loss: 0.13500268757343292, acc: 0.85, recall: 0.85, precision: 0.8846153846153846, f_beta: 0.8465473145780051
train: step: 2540, loss: 0.046181727200746536, acc: 0.95, recall: 0.95, precision: 0.95, f_beta: 0.9500000000000001
train: step: 2541, loss: 0.3601953089237213, acc: 0.9, recall: 0.8999999999999999, precision: 0.904040404040404, f_beta: 0.899749373433584
train: step: 2542, loss: 0.07409453392028809, acc: 0.925, recall: 0.925, precision: 0.9347826086956521, f_beta: 0.9245757385292268
train: step: 2543, loss: 0.15221384167671204, acc: 0.8, recall: 0.8, precision: 0.8, f_beta: 0.8000000000000002
train: step: 2544, loss: 0.04788043722510338, acc: 0.95, recall: 0.95, precision: 0.9545454545454546, f_beta: 0.949874686716792
train: step: 2545, loss: 0.08768375217914581, acc: 0.875, recall: 0.875, precision: 0.8836317135549872, f_beta: 0.8742928975487115
train: step: 2546, loss: 0.017754659056663513, acc: 0.975, recall: 0.975, precision: 0.9761904761904762, f_beta: 0.9749843652282677
train: step: 2547, loss: 0.2878221869468689, acc: 0.6, recall: 0.6, precision: 0.696078431372549, f_beta: 0.5441595441595442
train: step: 2548, loss: 0.00973433442413807, acc: 1.0, recall: 1.0, precision: 1.0, f_beta: 1.0
train: step: 2549, loss: 0.15892831981182098, acc: 0.85, recall: 0.85, precision: 0.8645833333333333, f_beta: 0.8484848484848484
train: step: 2550, loss: 0.025707706809043884, acc: 0.95, recall: 0.95, precision: 0.9545454545454546, f_beta: 0.949874686716792
train: step: 2551, loss: 0.01813087798655033, acc: 1.0, recall: 1.0, precision: 1.0, f_beta: 1.0
train: step: 2552, loss: 0.07808830589056015, acc: 0.925, recall: 0.925, precision: 0.9260651629072681, f_beta: 0.924953095684803
train: step: 2553, loss: 0.09388953447341919, acc: 0.875, recall: 0.875, precision: 0.8759398496240601, f_beta: 0.8749218261413383
train: step: 2554, loss: 0.09890098869800568, acc: 0.875, recall: 0.875, precision: 0.8759398496240601, f_beta: 0.8749218261413383
train: step: 2555, loss: 0.19319534301757812, acc: 0.75, recall: 0.75, precision: 0.7525252525252526, f_beta: 0.7493734335839599
train: step: 2556, loss: 0.2383522242307663, acc: 0.825, recall: 0.825, precision: 0.8466666666666667, f_beta: 0.8222222222222222
train: step: 2557, loss: 0.08828602731227875, acc: 0.875, recall: 0.875, precision: 0.8759398496240601, f_beta: 0.8749218261413383
train: step: 2558, loss: 0.1885581910610199, acc: 0.775, recall: 0.7749999999999999, precision: 0.8133903133903134, f_beta: 0.7678916827853
train: step: 2559, loss: 0.02683962509036064, acc: 0.975, recall: 0.975, precision: 0.9761904761904762, f_beta: 0.9749843652282677
train: step: 2560, loss: 0.19201377034187317, acc: 0.775, recall: 0.7749999999999999, precision: 0.8133903133903134, f_beta: 0.7678916827853
train: step: 2561, loss: 0.16163232922554016, acc: 0.825, recall: 0.825, precision: 0.8466666666666667, f_beta: 0.8222222222222222
train: step: 2562, loss: 0.11305592209100723, acc: 0.85, recall: 0.8500000000000001, precision: 0.8535353535353536, f_beta: 0.849624060150376
train: step: 2563, loss: 0.11055966466665268, acc: 0.875, recall: 0.875, precision: 0.9, f_beta: 0.873015873015873
train: step: 2564, loss: 0.3028371036052704, acc: 0.175, recall: 0.175, precision: 0.16751918158567775, f_beta: 0.1703331238214959
train: step: 2565, loss: 0.026871269568800926, acc: 0.975, recall: 0.975, precision: 0.9761904761904762, f_beta: 0.9749843652282677
train: step: 2566, loss: 0.12647555768489838, acc: 0.8, recall: 0.8, precision: 0.8125, f_beta: 0.797979797979798
train: step: 2567, loss: 0.14168818295001984, acc: 0.825, recall: 0.825, precision: 0.8466666666666667, f_beta: 0.8222222222222222
train: step: 2568, loss: 0.033587515354156494, acc: 1.0, recall: 1.0, precision: 1.0, f_beta: 1.0
train: step: 2569, loss: 0.1820359081029892, acc: 0.8, recall: 0.8, precision: 0.803030303030303, f_beta: 0.7994987468671679
train: step: 2570, loss: 0.08935628086328506, acc: 0.875, recall: 0.875, precision: 0.9, f_beta: 0.873015873015873
train: step: 2571, loss: 0.20009902119636536, acc: 0.775, recall: 0.775, precision: 0.8448275862068966, f_beta: 0.7630019749835418
train: step: 2572, loss: 0.05871734768152237, acc: 0.925, recall: 0.925, precision: 0.9260651629072681, f_beta: 0.924953095684803
train: step: 2573, loss: 0.06289313733577728, acc: 0.95, recall: 0.95, precision: 0.95, f_beta: 0.9500000000000001
train: step: 2574, loss: 0.06292614340782166, acc: 0.925, recall: 0.925, precision: 0.9347826086956521, f_beta: 0.9245757385292268
train: step: 2575, loss: 0.08033362776041031, acc: 0.95, recall: 0.95, precision: 0.95, f_beta: 0.9500000000000001
train: step: 2576, loss: 0.22067265212535858, acc: 0.7, recall: 0.7, precision: 0.7380952380952381, f_beta: 0.6875
train: step: 2577, loss: 0.07182713598012924, acc: 0.9, recall: 0.9, precision: 0.9166666666666667, f_beta: 0.898989898989899
train: step: 2578, loss: 0.08807913213968277, acc: 0.9, recall: 0.8999999999999999, precision: 0.904040404040404, f_beta: 0.899749373433584
train: step: 2579, loss: 0.14452210068702698, acc: 0.8, recall: 0.8, precision: 0.803030303030303, f_beta: 0.7994987468671679
train: step: 2580, loss: 0.07891466468572617, acc: 0.9, recall: 0.8999999999999999, precision: 0.904040404040404, f_beta: 0.899749373433584
train: step: 2581, loss: 0.04969487339258194, acc: 0.95, recall: 0.95, precision: 0.95, f_beta: 0.9500000000000001
train: step: 2582, loss: 0.0951131135225296, acc: 0.85, recall: 0.8500000000000001, precision: 0.8535353535353536, f_beta: 0.849624060150376
train: step: 2583, loss: 0.0637328177690506, acc: 0.95, recall: 0.95, precision: 0.9545454545454546, f_beta: 0.949874686716792
train: step: 2584, loss: 0.038053303956985474, acc: 0.975, recall: 0.975, precision: 0.9761904761904762, f_beta: 0.9749843652282677
train: step: 2585, loss: 0.0520440936088562, acc: 0.95, recall: 0.95, precision: 0.95, f_beta: 0.9500000000000001
train: step: 2586, loss: 0.08590444177389145, acc: 0.9, recall: 0.9, precision: 0.9166666666666667, f_beta: 0.898989898989899
train: step: 2587, loss: 0.08419550210237503, acc: 0.9, recall: 0.9, precision: 0.9166666666666667, f_beta: 0.898989898989899
train: step: 2588, loss: 0.1540934145450592, acc: 0.9, recall: 0.8999999999999999, precision: 0.904040404040404, f_beta: 0.899749373433584
train: step: 2589, loss: 0.044348765164613724, acc: 0.95, recall: 0.95, precision: 0.95, f_beta: 0.9500000000000001
train: step: 2590, loss: 0.13734494149684906, acc: 0.825, recall: 0.825, precision: 0.8466666666666667, f_beta: 0.8222222222222222
train: step: 2591, loss: 0.039672642946243286, acc: 0.975, recall: 0.975, precision: 0.9761904761904762, f_beta: 0.9749843652282677
train: step: 2592, loss: 0.055068910121917725, acc: 0.95, recall: 0.95, precision: 0.9545454545454546, f_beta: 0.949874686716792
train: step: 2593, loss: 0.3517080247402191, acc: 0.925, recall: 0.925, precision: 0.9347826086956521, f_beta: 0.9245757385292268
train: step: 2594, loss: 0.12667986750602722, acc: 0.875, recall: 0.875, precision: 0.8759398496240601, f_beta: 0.8749218261413383
train: step: 2595, loss: 0.17639286816120148, acc: 0.75, recall: 0.75, precision: 0.7747252747252746, f_beta: 0.7442455242966751
train: step: 2596, loss: 0.046772830188274384, acc: 0.975, recall: 0.975, precision: 0.9761904761904762, f_beta: 0.9749843652282677
train: step: 2597, loss: 0.008357251062989235, acc: 1.0, recall: 1.0, precision: 1.0, f_beta: 1.0
train: step: 2598, loss: 0.09676418453454971, acc: 0.875, recall: 0.875, precision: 0.9, f_beta: 0.873015873015873
train: step: 2599, loss: 0.13011881709098816, acc: 0.825, recall: 0.825, precision: 0.8703703703703703, f_beta: 0.819471308833011
train: step: 2600, loss: 0.016435498371720314, acc: 0.975, recall: 0.975, precision: 0.9761904761904762, f_beta: 0.9749843652282677
train: step: 2601, loss: 0.0798911452293396, acc: 0.925, recall: 0.925, precision: 0.9260651629072681, f_beta: 0.924953095684803
train: step: 2602, loss: 0.051848966628313065, acc: 0.95, recall: 0.95, precision: 0.95, f_beta: 0.9500000000000001
train: step: 2603, loss: 0.3448403477668762, acc: 0.3, recall: 0.30000000000000004, precision: 0.29166666666666663, f_beta: 0.29292929292929293
train: step: 2604, loss: 0.06180097907781601, acc: 0.925, recall: 0.925, precision: 0.9260651629072681, f_beta: 0.924953095684803
train: step: 2605, loss: 0.14413289725780487, acc: 0.825, recall: 0.825, precision: 0.8466666666666667, f_beta: 0.8222222222222222
train: step: 2606, loss: 0.04347847029566765, acc: 0.95, recall: 0.95, precision: 0.9545454545454546, f_beta: 0.949874686716792
train: step: 2607, loss: 0.04679644852876663, acc: 0.95, recall: 0.95, precision: 0.9545454545454546, f_beta: 0.949874686716792
train: step: 2608, loss: 0.2716706395149231, acc: 0.625, recall: 0.625, precision: 0.62531328320802, f_beta: 0.6247654784240151
train: step: 2609, loss: 0.07668392360210419, acc: 0.95, recall: 0.95, precision: 0.9545454545454546, f_beta: 0.949874686716792
train: step: 2610, loss: 0.15817321836948395, acc: 0.8, recall: 0.8, precision: 0.8571428571428572, f_beta: 0.7916666666666665
train: step: 2611, loss: 0.20872393250465393, acc: 0.7, recall: 0.7, precision: 0.7666666666666666, f_beta: 0.6799999999999999
train: step: 2612, loss: 0.023151393979787827, acc: 0.975, recall: 0.975, precision: 0.9761904761904762, f_beta: 0.9749843652282677
train: step: 2613, loss: 0.04281807690858841, acc: 0.925, recall: 0.925, precision: 0.9260651629072681, f_beta: 0.924953095684803
train: step: 2614, loss: 0.01779485121369362, acc: 1.0, recall: 1.0, precision: 1.0, f_beta: 1.0
train: step: 2615, loss: 0.03489382937550545, acc: 0.975, recall: 0.975, precision: 0.9761904761904762, f_beta: 0.9749843652282677
train: step: 2616, loss: 0.11963009834289551, acc: 0.825, recall: 0.825, precision: 0.8324808184143222, f_beta: 0.824010056568196
train: step: 2617, loss: 0.0628010630607605, acc: 0.9, recall: 0.8999999999999999, precision: 0.904040404040404, f_beta: 0.899749373433584
train: step: 2618, loss: 0.023331720381975174, acc: 0.975, recall: 0.975, precision: 0.9761904761904762, f_beta: 0.9749843652282677
train: step: 2619, loss: 0.04916005954146385, acc: 0.95, recall: 0.95, precision: 0.95, f_beta: 0.9500000000000001
train: step: 2620, loss: 0.035738226026296616, acc: 0.95, recall: 0.95, precision: 0.9545454545454546, f_beta: 0.949874686716792
train: step: 2621, loss: 0.07962870597839355, acc: 0.9, recall: 0.8999999999999999, precision: 0.904040404040404, f_beta: 0.899749373433584
train: step: 2622, loss: 0.017815271392464638, acc: 0.975, recall: 0.975, precision: 0.9761904761904762, f_beta: 0.9749843652282677
train: step: 2623, loss: 0.05727037042379379, acc: 0.925, recall: 0.925, precision: 0.9260651629072681, f_beta: 0.924953095684803
train: step: 2624, loss: 0.006184544414281845, acc: 1.0, recall: 1.0, precision: 1.0, f_beta: 1.0
train: step: 2625, loss: 0.037435635924339294, acc: 0.95, recall: 0.95, precision: 0.95, f_beta: 0.9500000000000001
train: step: 2626, loss: 0.0639912337064743, acc: 0.925, recall: 0.925, precision: 0.9260651629072681, f_beta: 0.924953095684803
train: step: 2627, loss: 0.0034984233789145947, acc: 1.0, recall: 1.0, precision: 1.0, f_beta: 1.0
train: step: 2628, loss: 0.11524713039398193, acc: 0.85, recall: 0.8500000000000001, precision: 0.8535353535353536, f_beta: 0.849624060150376
train: step: 2629, loss: 0.0877264142036438, acc: 0.9, recall: 0.9, precision: 0.9, f_beta: 0.9
train: step: 2630, loss: 0.06216984987258911, acc: 0.95, recall: 0.95, precision: 0.9545454545454546, f_beta: 0.949874686716792
train: step: 2631, loss: 0.267901748418808, acc: 0.625, recall: 0.625, precision: 0.6333333333333333, f_beta: 0.6190476190476191
train: step: 2632, loss: 0.05960824340581894, acc: 0.925, recall: 0.925, precision: 0.9347826086956521, f_beta: 0.9245757385292268
train: step: 2633, loss: 0.2542065978050232, acc: 0.95, recall: 0.95, precision: 0.95, f_beta: 0.9500000000000001
train: step: 2634, loss: 0.22835025191307068, acc: 0.7, recall: 0.7, precision: 0.7666666666666666, f_beta: 0.6799999999999999
train: step: 2635, loss: 0.07030107825994492, acc: 0.925, recall: 0.925, precision: 0.9260651629072681, f_beta: 0.924953095684803
train: step: 2636, loss: 0.11452150344848633, acc: 0.925, recall: 0.925, precision: 0.9347826086956521, f_beta: 0.9245757385292268
train: step: 2637, loss: 0.04480838030576706, acc: 0.95, recall: 0.95, precision: 0.95, f_beta: 0.9500000000000001
train: step: 2638, loss: 0.09750572592020035, acc: 0.9, recall: 0.8999999999999999, precision: 0.904040404040404, f_beta: 0.899749373433584
train: step: 2639, loss: 0.09082715958356857, acc: 0.9, recall: 0.9, precision: 0.9, f_beta: 0.9
train: step: 2640, loss: 0.17759330570697784, acc: 0.85, recall: 0.85, precision: 0.8846153846153846, f_beta: 0.8465473145780051
train: step: 2641, loss: 0.10188965499401093, acc: 0.9, recall: 0.9, precision: 0.9, f_beta: 0.9
train: step: 2642, loss: 0.049871355295181274, acc: 0.925, recall: 0.925, precision: 0.9260651629072681, f_beta: 0.924953095684803
train: step: 2643, loss: 0.22164051234722137, acc: 0.95, recall: 0.95, precision: 0.95, f_beta: 0.9500000000000001
train: step: 2644, loss: 0.04172201454639435, acc: 0.95, recall: 0.95, precision: 0.9545454545454546, f_beta: 0.949874686716792
train: step: 2645, loss: 0.1609613299369812, acc: 0.8, recall: 0.8, precision: 0.8125, f_beta: 0.797979797979798
train: step: 2646, loss: 0.058052100241184235, acc: 0.95, recall: 0.95, precision: 0.9545454545454546, f_beta: 0.949874686716792
train: step: 2647, loss: 0.24249207973480225, acc: 0.7, recall: 0.7, precision: 0.7197802197802198, f_beta: 0.6930946291560103
train: step: 2648, loss: 0.20373645424842834, acc: 0.725, recall: 0.725, precision: 0.8225806451612903, f_beta: 0.7025016903313049
train: step: 2649, loss: 0.08283181488513947, acc: 0.9, recall: 0.8999999999999999, precision: 0.904040404040404, f_beta: 0.899749373433584
train: step: 2650, loss: 0.009495044127106667, acc: 1.0, recall: 1.0, precision: 1.0, f_beta: 1.0
train: step: 2651, loss: 0.2035653293132782, acc: 0.7, recall: 0.7, precision: 0.7197802197802198, f_beta: 0.6930946291560103
train: step: 2652, loss: 0.033848732709884644, acc: 0.95, recall: 0.95, precision: 0.9545454545454546, f_beta: 0.949874686716792
train: step: 2653, loss: 0.15818360447883606, acc: 0.775, recall: 0.775, precision: 0.7756892230576441, f_beta: 0.774859287054409
train: step: 2654, loss: 0.3019390106201172, acc: 0.55, recall: 0.55, precision: 0.763157894736842, f_beta: 0.4357366771159875
train: step: 2655, loss: 0.012498108670115471, acc: 1.0, recall: 1.0, precision: 1.0, f_beta: 1.0
train: step: 2656, loss: 0.13921770453453064, acc: 0.825, recall: 0.825, precision: 0.8466666666666667, f_beta: 0.8222222222222222
train: step: 2657, loss: 0.07356741279363632, acc: 0.925, recall: 0.925, precision: 0.9260651629072681, f_beta: 0.924953095684803
train: step: 2658, loss: 0.2087109535932541, acc: 0.7, recall: 0.7, precision: 0.7083333333333333, f_beta: 0.6969696969696968
train: step: 2659, loss: 0.05699696019291878, acc: 0.95, recall: 0.95, precision: 0.9545454545454546, f_beta: 0.949874686716792
train: step: 2660, loss: 0.1113022193312645, acc: 0.925, recall: 0.925, precision: 0.9347826086956521, f_beta: 0.9245757385292268
train: step: 2661, loss: 0.19454266130924225, acc: 0.775, recall: 0.775, precision: 0.7756892230576441, f_beta: 0.774859287054409
train: step: 2662, loss: 0.11606153100728989, acc: 0.825, recall: 0.825, precision: 0.8324808184143222, f_beta: 0.824010056568196
train: step: 2663, loss: 0.19226711988449097, acc: 0.775, recall: 0.775, precision: 0.8448275862068966, f_beta: 0.7630019749835418
train: step: 2664, loss: 0.026964670047163963, acc: 0.975, recall: 0.975, precision: 0.9761904761904762, f_beta: 0.9749843652282677
train: step: 2665, loss: 0.059199102222919464, acc: 0.95, recall: 0.95, precision: 0.9545454545454546, f_beta: 0.949874686716792
train: step: 2666, loss: 0.03566901013255119, acc: 0.975, recall: 0.975, precision: 0.9761904761904762, f_beta: 0.9749843652282677
train: step: 2667, loss: 0.07767058908939362, acc: 1.0, recall: 1.0, precision: 1.0, f_beta: 1.0
train: step: 2668, loss: 0.0327298529446125, acc: 0.975, recall: 0.975, precision: 0.9761904761904762, f_beta: 0.9749843652282677
train: step: 2669, loss: 0.10956238210201263, acc: 0.95, recall: 0.95, precision: 0.9545454545454546, f_beta: 0.949874686716792
train: step: 2670, loss: 0.09270356595516205, acc: 0.95, recall: 0.95, precision: 0.9545454545454546, f_beta: 0.949874686716792
train: step: 2671, loss: 0.015048692934215069, acc: 0.975, recall: 0.975, precision: 0.9761904761904762, f_beta: 0.9749843652282677
train: step: 2672, loss: 0.016327649354934692, acc: 1.0, recall: 1.0, precision: 1.0, f_beta: 1.0
train: step: 2673, loss: 0.10874436050653458, acc: 0.85, recall: 0.8500000000000001, precision: 0.8535353535353536, f_beta: 0.849624060150376
train: step: 2674, loss: 0.10392563045024872, acc: 0.875, recall: 0.875, precision: 0.8759398496240601, f_beta: 0.8749218261413383
train: step: 2675, loss: 0.18116514384746552, acc: 0.775, recall: 0.7749999999999999, precision: 0.7813299232736572, f_beta: 0.7737272155876807
train: step: 2676, loss: 0.029331285506486893, acc: 0.95, recall: 0.95, precision: 0.9545454545454546, f_beta: 0.949874686716792
train: step: 2677, loss: 0.10453873872756958, acc: 0.9, recall: 0.9, precision: 0.9, f_beta: 0.9
train: step: 2678, loss: 0.012328223325312138, acc: 1.0, recall: 1.0, precision: 1.0, f_beta: 1.0
train: step: 2679, loss: 0.16349676251411438, acc: 0.75, recall: 0.75, precision: 0.7747252747252746, f_beta: 0.7442455242966751
train: step: 2680, loss: 0.03191235661506653, acc: 0.975, recall: 0.975, precision: 0.9761904761904762, f_beta: 0.9749843652282677
train: step: 2681, loss: 0.03481141850352287, acc: 0.95, recall: 0.95, precision: 0.95, f_beta: 0.9500000000000001
train: step: 2682, loss: 0.03663802146911621, acc: 0.975, recall: 0.975, precision: 0.9761904761904762, f_beta: 0.9749843652282677
train: step: 2683, loss: 0.023381432518363, acc: 0.95, recall: 0.95, precision: 0.9545454545454546, f_beta: 0.949874686716792
train: step: 2684, loss: 0.2109743058681488, acc: 0.775, recall: 0.775, precision: 0.8448275862068966, f_beta: 0.7630019749835418
train: step: 2685, loss: 0.07533802837133408, acc: 0.9, recall: 0.8999999999999999, precision: 0.904040404040404, f_beta: 0.899749373433584
train: step: 2686, loss: 0.044683679938316345, acc: 0.925, recall: 0.925, precision: 0.9260651629072681, f_beta: 0.924953095684803
train: step: 2687, loss: 0.08807370811700821, acc: 0.9, recall: 0.8999999999999999, precision: 0.904040404040404, f_beta: 0.899749373433584
train: step: 2688, loss: 0.13237640261650085, acc: 0.8, recall: 0.8, precision: 0.8, f_beta: 0.8000000000000002
train: step: 2689, loss: 0.13693960011005402, acc: 0.825, recall: 0.825, precision: 0.8466666666666667, f_beta: 0.8222222222222222
train: step: 2690, loss: 0.23743899166584015, acc: 0.675, recall: 0.675, precision: 0.6994301994301995, f_beta: 0.6647324306898775
train: step: 2691, loss: 0.06336161494255066, acc: 0.925, recall: 0.925, precision: 0.9347826086956521, f_beta: 0.9245757385292268
train: step: 2692, loss: 0.08402754366397858, acc: 0.9, recall: 0.9, precision: 0.9166666666666667, f_beta: 0.898989898989899
train: step: 2693, loss: 0.12952367961406708, acc: 0.85, recall: 0.85, precision: 0.8846153846153846, f_beta: 0.8465473145780051
train: step: 2694, loss: 0.10862743854522705, acc: 0.875, recall: 0.875, precision: 0.8836317135549872, f_beta: 0.8742928975487115
train: step: 2695, loss: 0.03114301525056362, acc: 0.95, recall: 0.95, precision: 0.95, f_beta: 0.9500000000000001
train: step: 2696, loss: 0.12407863140106201, acc: 0.85, recall: 0.85, precision: 0.8846153846153846, f_beta: 0.8465473145780051
train: step: 2697, loss: 0.057152021676301956, acc: 0.95, recall: 0.95, precision: 0.9545454545454546, f_beta: 0.949874686716792
train: step: 2698, loss: 0.26315099000930786, acc: 0.825, recall: 0.825, precision: 0.8324808184143222, f_beta: 0.824010056568196
train: step: 2699, loss: 0.2489757537841797, acc: 0.625, recall: 0.625, precision: 0.7164502164502164, f_beta: 0.5807127882599581
train: step: 2700, loss: 0.02167774923145771, acc: 1.0, recall: 1.0, precision: 1.0, f_beta: 1.0
train: step: 2701, loss: 0.049257054924964905, acc: 0.925, recall: 0.925, precision: 0.9347826086956521, f_beta: 0.9245757385292268
train: step: 2702, loss: 0.03817524388432503, acc: 0.95, recall: 0.95, precision: 0.9545454545454546, f_beta: 0.949874686716792
train: step: 2703, loss: 0.03466508537530899, acc: 0.975, recall: 0.975, precision: 0.9761904761904762, f_beta: 0.9749843652282677
train: step: 2704, loss: 0.10464576631784439, acc: 0.95, recall: 0.95, precision: 0.95, f_beta: 0.9500000000000001
train: step: 2705, loss: 0.10720167309045792, acc: 0.8, recall: 0.8, precision: 0.8, f_beta: 0.8000000000000002
train: step: 2706, loss: 0.18008987605571747, acc: 0.85, recall: 0.8500000000000001, precision: 0.8535353535353536, f_beta: 0.849624060150376
train: step: 2707, loss: 0.061515022069215775, acc: 0.925, recall: 0.925, precision: 0.9260651629072681, f_beta: 0.924953095684803
train: step: 2708, loss: 0.06957115232944489, acc: 0.9, recall: 0.9, precision: 0.9, f_beta: 0.9
train: step: 2709, loss: 0.04512637108564377, acc: 0.95, recall: 0.95, precision: 0.9545454545454546, f_beta: 0.949874686716792
train: step: 2710, loss: 0.03503776714205742, acc: 0.95, recall: 0.95, precision: 0.9545454545454546, f_beta: 0.949874686716792
train: step: 2711, loss: 0.04563261196017265, acc: 0.9, recall: 0.9, precision: 0.9166666666666667, f_beta: 0.898989898989899
train: step: 2712, loss: 0.2385777235031128, acc: 0.7, recall: 0.7, precision: 0.7197802197802198, f_beta: 0.6930946291560103
train: step: 2713, loss: 0.23426881432533264, acc: 0.625, recall: 0.625, precision: 0.6333333333333333, f_beta: 0.6190476190476191
train: step: 2714, loss: 0.050748296082019806, acc: 0.95, recall: 0.95, precision: 0.95, f_beta: 0.9500000000000001
train: step: 2715, loss: 0.3021391034126282, acc: 0.825, recall: 0.825, precision: 0.8258145363408521, f_beta: 0.8248905565978737
train: step: 2716, loss: 0.3852567672729492, acc: 0.8, recall: 0.8, precision: 0.803030303030303, f_beta: 0.7994987468671679
train: step: 2717, loss: 0.17225821316242218, acc: 0.725, recall: 0.725, precision: 0.7821316614420062, f_beta: 0.7103357472021066
train: step: 2718, loss: 0.053287655115127563, acc: 0.95, recall: 0.95, precision: 0.9545454545454546, f_beta: 0.949874686716792
train: step: 2719, loss: 0.06708651781082153, acc: 0.925, recall: 0.925, precision: 0.9347826086956521, f_beta: 0.9245757385292268
train: step: 2720, loss: 0.12851344048976898, acc: 0.85, recall: 0.8500000000000001, precision: 0.8535353535353536, f_beta: 0.849624060150376
train: step: 2721, loss: 0.1536804586648941, acc: 0.8, recall: 0.8, precision: 0.8296703296703296, f_beta: 0.7953964194373402
train: step: 2722, loss: 0.19851234555244446, acc: 0.75, recall: 0.75, precision: 0.7976190476190477, f_beta: 0.7395833333333333
train: step: 2723, loss: 0.14107252657413483, acc: 0.825, recall: 0.825, precision: 0.8324808184143222, f_beta: 0.824010056568196
train: step: 2724, loss: 0.058844517916440964, acc: 0.95, recall: 0.95, precision: 0.9545454545454546, f_beta: 0.949874686716792
train: step: 2725, loss: 0.09701366722583771, acc: 0.9, recall: 0.9, precision: 0.9, f_beta: 0.9
train: step: 2726, loss: 0.03406751900911331, acc: 0.95, recall: 0.95, precision: 0.95, f_beta: 0.9500000000000001
train: step: 2727, loss: 0.288435161113739, acc: 0.575, recall: 0.575, precision: 0.575187969924812, f_beta: 0.5747342088805503
train: step: 2728, loss: 0.25941377878189087, acc: 0.65, recall: 0.65, precision: 0.7, f_beta: 0.6266666666666667
train: step: 2729, loss: 0.2515643537044525, acc: 0.65, recall: 0.65, precision: 0.6785714285714286, f_beta: 0.6354166666666667
train: step: 2730, loss: 0.08438238501548767, acc: 0.925, recall: 0.925, precision: 0.9260651629072681, f_beta: 0.924953095684803
train: step: 2731, loss: 0.13818970322608948, acc: 0.825, recall: 0.825, precision: 0.8324808184143222, f_beta: 0.824010056568196
train: step: 2732, loss: 0.05870179459452629, acc: 0.95, recall: 0.95, precision: 0.95, f_beta: 0.9500000000000001
train: step: 2733, loss: 0.0666150376200676, acc: 0.9, recall: 0.8999999999999999, precision: 0.904040404040404, f_beta: 0.899749373433584
train: step: 2734, loss: 0.0975063294172287, acc: 0.85, recall: 0.8500000000000001, precision: 0.8535353535353536, f_beta: 0.849624060150376
train: step: 2735, loss: 0.22956767678260803, acc: 0.75, recall: 0.75, precision: 0.7525252525252526, f_beta: 0.7493734335839599
train: step: 2736, loss: 0.142301544547081, acc: 0.8, recall: 0.8, precision: 0.8125, f_beta: 0.797979797979798
train: step: 2737, loss: 0.020725827664136887, acc: 0.975, recall: 0.975, precision: 0.9761904761904762, f_beta: 0.9749843652282677
train: step: 2738, loss: 0.20294062793254852, acc: 0.75, recall: 0.75, precision: 0.8333333333333333, f_beta: 0.7333333333333334
train: step: 2739, loss: 0.21593637764453888, acc: 0.8, recall: 0.8, precision: 0.8296703296703296, f_beta: 0.7953964194373402
train: step: 2740, loss: 0.102936290204525, acc: 0.875, recall: 0.875, precision: 0.8836317135549872, f_beta: 0.8742928975487115
train: step: 2741, loss: 0.06191028282046318, acc: 0.95, recall: 0.95, precision: 0.9545454545454546, f_beta: 0.949874686716792
train: step: 2742, loss: 0.0539478063583374, acc: 0.975, recall: 0.975, precision: 0.9761904761904762, f_beta: 0.9749843652282677
train: step: 2743, loss: 0.005818439647555351, acc: 1.0, recall: 1.0, precision: 1.0, f_beta: 1.0
train: step: 2744, loss: 0.2022307813167572, acc: 0.725, recall: 0.725, precision: 0.74, f_beta: 0.7206349206349207
train: step: 2745, loss: 0.10487931966781616, acc: 0.9, recall: 0.9, precision: 0.9166666666666667, f_beta: 0.898989898989899
train: step: 2746, loss: 0.15802034735679626, acc: 0.85, recall: 0.85, precision: 0.8645833333333333, f_beta: 0.8484848484848484
train: step: 2747, loss: 0.093658447265625, acc: 0.9, recall: 0.9, precision: 0.9166666666666667, f_beta: 0.898989898989899
train: step: 2748, loss: 0.12833288311958313, acc: 0.875, recall: 0.875, precision: 0.8759398496240601, f_beta: 0.8749218261413383
train: step: 2749, loss: 0.3074561357498169, acc: 0.325, recall: 0.325, precision: 0.19696969696969696, f_beta: 0.2452830188679245
train: step: 2750, loss: 0.06701071560382843, acc: 0.95, recall: 0.95, precision: 0.9545454545454546, f_beta: 0.949874686716792
train: step: 2751, loss: 0.19172121584415436, acc: 0.75, recall: 0.75, precision: 0.7525252525252526, f_beta: 0.7493734335839599
train: step: 2752, loss: 0.14056548476219177, acc: 0.775, recall: 0.775, precision: 0.7933333333333333, f_beta: 0.7714285714285714
train: step: 2753, loss: 0.06586530059576035, acc: 0.9, recall: 0.8999999999999999, precision: 0.904040404040404, f_beta: 0.899749373433584
train: step: 2754, loss: 0.017146369442343712, acc: 1.0, recall: 1.0, precision: 1.0, f_beta: 1.0
train: step: 2755, loss: 0.1343052089214325, acc: 0.75, recall: 0.75, precision: 0.7976190476190477, f_beta: 0.7395833333333333
train: step: 2756, loss: 0.14923706650733948, acc: 0.825, recall: 0.825, precision: 0.8324808184143222, f_beta: 0.824010056568196
train: step: 2757, loss: 0.023678284138441086, acc: 0.975, recall: 0.975, precision: 0.9761904761904762, f_beta: 0.9749843652282677
train: step: 2758, loss: 0.07169179618358612, acc: 0.95, recall: 0.95, precision: 0.9545454545454546, f_beta: 0.949874686716792
train: step: 2759, loss: 0.06219428777694702, acc: 0.925, recall: 0.925, precision: 0.9347826086956521, f_beta: 0.9245757385292268
train: step: 2760, loss: 0.08199726045131683, acc: 0.875, recall: 0.875, precision: 0.8759398496240601, f_beta: 0.8749218261413383
train: step: 2761, loss: 0.11980600655078888, acc: 0.85, recall: 0.8500000000000001, precision: 0.8535353535353536, f_beta: 0.849624060150376
train: step: 2762, loss: 0.06166039779782295, acc: 0.9, recall: 0.8999999999999999, precision: 0.904040404040404, f_beta: 0.899749373433584
train: step: 2763, loss: 0.05587947368621826, acc: 0.95, recall: 0.95, precision: 0.9545454545454546, f_beta: 0.949874686716792
train: step: 2764, loss: 0.11334314197301865, acc: 0.95, recall: 0.95, precision: 0.95, f_beta: 0.9500000000000001
train: step: 2765, loss: 0.05074957758188248, acc: 0.925, recall: 0.925, precision: 0.9260651629072681, f_beta: 0.924953095684803
train: step: 2766, loss: 0.00957325380295515, acc: 1.0, recall: 1.0, precision: 1.0, f_beta: 1.0
train: step: 2767, loss: 0.032050225883722305, acc: 0.975, recall: 0.975, precision: 0.9761904761904762, f_beta: 0.9749843652282677
train: step: 2768, loss: 0.19348742067813873, acc: 0.925, recall: 0.925, precision: 0.9260651629072681, f_beta: 0.924953095684803
train: step: 2769, loss: 0.14196820557117462, acc: 0.875, recall: 0.875, precision: 0.9, f_beta: 0.873015873015873
train: step: 2770, loss: 0.14589884877204895, acc: 0.85, recall: 0.8500000000000001, precision: 0.8535353535353536, f_beta: 0.849624060150376
train: step: 2771, loss: 0.14598879218101501, acc: 0.8, recall: 0.8, precision: 0.803030303030303, f_beta: 0.7994987468671679
train: step: 2772, loss: 0.043582577258348465, acc: 0.925, recall: 0.925, precision: 0.9260651629072681, f_beta: 0.924953095684803
train: step: 2773, loss: 0.15954908728599548, acc: 0.775, recall: 0.775, precision: 0.8448275862068966, f_beta: 0.7630019749835418
train: step: 2774, loss: 0.009381057694554329, acc: 1.0, recall: 1.0, precision: 1.0, f_beta: 1.0
train: step: 2775, loss: 0.25405728816986084, acc: 0.75, recall: 0.75, precision: 0.7604166666666667, f_beta: 0.7474747474747475
train: step: 2776, loss: 0.05633752420544624, acc: 0.925, recall: 0.925, precision: 0.9347826086956521, f_beta: 0.9245757385292268
train: step: 2777, loss: 0.03131646662950516, acc: 0.95, recall: 0.95, precision: 0.9545454545454546, f_beta: 0.949874686716792
train: step: 2778, loss: 0.04199846833944321, acc: 0.925, recall: 0.925, precision: 0.9347826086956521, f_beta: 0.9245757385292268
train: step: 2779, loss: 0.06014370918273926, acc: 0.95, recall: 0.95, precision: 0.9545454545454546, f_beta: 0.949874686716792
train: step: 2780, loss: 0.07299987971782684, acc: 0.9, recall: 0.9, precision: 0.9, f_beta: 0.9
train: step: 2781, loss: 0.07693216949701309, acc: 0.925, recall: 0.925, precision: 0.9260651629072681, f_beta: 0.924953095684803
train: step: 2782, loss: 0.15663732588291168, acc: 0.8, recall: 0.8, precision: 0.8125, f_beta: 0.797979797979798
train: step: 2783, loss: 0.14098182320594788, acc: 0.825, recall: 0.825, precision: 0.8324808184143222, f_beta: 0.824010056568196
train: step: 2784, loss: 0.057399820536375046, acc: 0.925, recall: 0.925, precision: 0.9347826086956521, f_beta: 0.9245757385292268
train: step: 2785, loss: 0.15556490421295166, acc: 0.8, recall: 0.8, precision: 0.803030303030303, f_beta: 0.7994987468671679
train: step: 2786, loss: 0.068449005484581, acc: 0.925, recall: 0.925, precision: 0.9347826086956521, f_beta: 0.9245757385292268
train: step: 2787, loss: 0.025427300482988358, acc: 1.0, recall: 1.0, precision: 1.0, f_beta: 1.0
train: step: 2788, loss: 0.04423508793115616, acc: 0.95, recall: 0.95, precision: 0.9545454545454546, f_beta: 0.949874686716792
train: step: 2789, loss: 0.17512790858745575, acc: 0.75, recall: 0.75, precision: 0.7976190476190477, f_beta: 0.7395833333333333
train: step: 2790, loss: 0.03435323387384415, acc: 0.95, recall: 0.95, precision: 0.9545454545454546, f_beta: 0.949874686716792
train: step: 2791, loss: 0.20709308981895447, acc: 0.75, recall: 0.75, precision: 0.8333333333333333, f_beta: 0.7333333333333334
train: step: 2792, loss: 0.12034871429204941, acc: 0.9, recall: 0.9, precision: 0.9166666666666667, f_beta: 0.898989898989899
train: step: 2793, loss: 0.03666604310274124, acc: 0.975, recall: 0.975, precision: 0.9761904761904762, f_beta: 0.9749843652282677
train: step: 2794, loss: 0.07403337955474854, acc: 0.9, recall: 0.8999999999999999, precision: 0.904040404040404, f_beta: 0.899749373433584
train: step: 2795, loss: 0.07133530080318451, acc: 0.9, recall: 0.9, precision: 0.9, f_beta: 0.9
train: step: 2796, loss: 0.01807979866862297, acc: 1.0, recall: 1.0, precision: 1.0, f_beta: 1.0
train: step: 2797, loss: 0.13419052958488464, acc: 0.875, recall: 0.875, precision: 0.9, f_beta: 0.873015873015873
train: step: 2798, loss: 0.05566009134054184, acc: 0.975, recall: 0.975, precision: 0.9761904761904762, f_beta: 0.9749843652282677
train: step: 2799, loss: 0.012898328714072704, acc: 0.975, recall: 0.975, precision: 0.9761904761904762, f_beta: 0.9749843652282677
train: step: 2800, loss: 0.14292854070663452, acc: 0.8, recall: 0.8, precision: 0.8125, f_beta: 0.797979797979798
train: step: 2801, loss: 0.04184194654226303, acc: 0.925, recall: 0.925, precision: 0.9347826086956521, f_beta: 0.9245757385292268
train: step: 2802, loss: 0.19681188464164734, acc: 0.725, recall: 0.725, precision: 0.7821316614420062, f_beta: 0.7103357472021066
train: step: 2803, loss: 0.04407510906457901, acc: 0.925, recall: 0.925, precision: 0.9347826086956521, f_beta: 0.9245757385292268
train: step: 2804, loss: 0.1495581567287445, acc: 0.8, recall: 0.8, precision: 0.8571428571428572, f_beta: 0.7916666666666665
train: step: 2805, loss: 0.06132104992866516, acc: 0.925, recall: 0.925, precision: 0.9347826086956521, f_beta: 0.9245757385292268
train: step: 2806, loss: 0.20669898390769958, acc: 0.75, recall: 0.75, precision: 0.7747252747252746, f_beta: 0.7442455242966751
train: step: 2807, loss: 0.0607663169503212, acc: 0.9, recall: 0.9, precision: 0.9, f_beta: 0.9
train: step: 2808, loss: 0.28661051392555237, acc: 0.825, recall: 0.825, precision: 0.8324808184143222, f_beta: 0.824010056568196
train: step: 2809, loss: 0.07480859756469727, acc: 0.9, recall: 0.9, precision: 0.9166666666666667, f_beta: 0.898989898989899
train: step: 2810, loss: 0.2550295293331146, acc: 0.95, recall: 0.95, precision: 0.95, f_beta: 0.9500000000000001
train: step: 2811, loss: 0.24832740426063538, acc: 0.675, recall: 0.675, precision: 0.803030303030303, f_beta: 0.636617749825297
train: step: 2812, loss: 0.04573221877217293, acc: 0.9, recall: 0.9, precision: 0.9, f_beta: 0.9
train: step: 2813, loss: 0.2298736274242401, acc: 0.7, recall: 0.7, precision: 0.7666666666666666, f_beta: 0.6799999999999999
train: step: 2814, loss: 0.026442861184477806, acc: 0.975, recall: 0.975, precision: 0.9761904761904762, f_beta: 0.9749843652282677
train: step: 2815, loss: 0.03456523269414902, acc: 0.95, recall: 0.95, precision: 0.9545454545454546, f_beta: 0.949874686716792
train: step: 2816, loss: 0.046872176229953766, acc: 0.95, recall: 0.95, precision: 0.9545454545454546, f_beta: 0.949874686716792
train: step: 2817, loss: 0.061834610998630524, acc: 0.95, recall: 0.95, precision: 0.9545454545454546, f_beta: 0.949874686716792
train: step: 2818, loss: 0.037485331296920776, acc: 0.975, recall: 0.975, precision: 0.9761904761904762, f_beta: 0.9749843652282677
train: step: 2819, loss: 0.1740323007106781, acc: 0.75, recall: 0.75, precision: 0.7747252747252746, f_beta: 0.7442455242966751
train: step: 2820, loss: 0.031047051772475243, acc: 0.975, recall: 0.975, precision: 0.9761904761904762, f_beta: 0.9749843652282677
train: step: 2821, loss: 0.05085856467485428, acc: 1.0, recall: 1.0, precision: 1.0, f_beta: 1.0
train: step: 2822, loss: 0.022891536355018616, acc: 1.0, recall: 1.0, precision: 1.0, f_beta: 1.0
train: step: 2823, loss: 0.10356118530035019, acc: 0.85, recall: 0.8500000000000001, precision: 0.8535353535353536, f_beta: 0.849624060150376
train: step: 2824, loss: 0.08675207197666168, acc: 0.9, recall: 0.9, precision: 0.9, f_beta: 0.9
train: step: 2825, loss: 0.04674693942070007, acc: 0.9, recall: 0.8999999999999999, precision: 0.904040404040404, f_beta: 0.899749373433584
train: step: 2826, loss: 0.011557395569980145, acc: 1.0, recall: 1.0, precision: 1.0, f_beta: 1.0
train: step: 2827, loss: 0.15163424611091614, acc: 0.775, recall: 0.775, precision: 0.7756892230576441, f_beta: 0.774859287054409
train: step: 2828, loss: 0.03173883631825447, acc: 0.95, recall: 0.95, precision: 0.9545454545454546, f_beta: 0.949874686716792
train: step: 2829, loss: 0.11100580543279648, acc: 0.85, recall: 0.85, precision: 0.8645833333333333, f_beta: 0.8484848484848484
train: step: 2830, loss: 0.10624438524246216, acc: 0.9, recall: 0.9, precision: 0.9166666666666667, f_beta: 0.898989898989899
train: step: 2831, loss: 0.029772479087114334, acc: 0.975, recall: 0.975, precision: 0.9761904761904762, f_beta: 0.9749843652282677
train: step: 2832, loss: 0.23376306891441345, acc: 0.875, recall: 0.875, precision: 0.8759398496240601, f_beta: 0.8749218261413383
train: step: 2833, loss: 0.028126772493124008, acc: 0.975, recall: 0.975, precision: 0.9761904761904762, f_beta: 0.9749843652282677
train: step: 2834, loss: 0.07895524799823761, acc: 0.9, recall: 0.8999999999999999, precision: 0.904040404040404, f_beta: 0.899749373433584
train: step: 2835, loss: 0.2916431128978729, acc: 0.75, recall: 0.75, precision: 0.7604166666666667, f_beta: 0.7474747474747475
train: step: 2836, loss: 0.07486139237880707, acc: 0.9, recall: 0.9, precision: 0.9166666666666667, f_beta: 0.898989898989899
train: step: 2837, loss: 0.09080782532691956, acc: 0.9, recall: 0.8999999999999999, precision: 0.904040404040404, f_beta: 0.899749373433584
train: step: 2838, loss: 0.11673416942358017, acc: 0.825, recall: 0.825, precision: 0.8324808184143222, f_beta: 0.824010056568196
train: step: 2839, loss: 0.13223452866077423, acc: 0.8, recall: 0.8, precision: 0.803030303030303, f_beta: 0.7994987468671679
train: step: 2840, loss: 0.06399927288293839, acc: 0.95, recall: 0.95, precision: 0.9545454545454546, f_beta: 0.949874686716792
train: step: 2841, loss: 0.22270043194293976, acc: 0.775, recall: 0.7749999999999999, precision: 0.7813299232736572, f_beta: 0.7737272155876807
train: step: 2842, loss: 0.08225005865097046, acc: 0.925, recall: 0.925, precision: 0.9260651629072681, f_beta: 0.924953095684803
train: step: 2843, loss: 0.10615170001983643, acc: 0.9, recall: 0.8999999999999999, precision: 0.904040404040404, f_beta: 0.899749373433584
train: step: 2844, loss: 0.26211294531822205, acc: 0.975, recall: 0.975, precision: 0.9761904761904762, f_beta: 0.9749843652282677
train: step: 2845, loss: 0.10139737278223038, acc: 0.85, recall: 0.85, precision: 0.8846153846153846, f_beta: 0.8465473145780051
train: step: 2846, loss: 0.1746930629014969, acc: 0.75, recall: 0.75, precision: 0.7976190476190477, f_beta: 0.7395833333333333
train: step: 2847, loss: 0.10942770540714264, acc: 0.875, recall: 0.875, precision: 0.8759398496240601, f_beta: 0.8749218261413383
train: step: 2848, loss: 0.01722702383995056, acc: 0.975, recall: 0.975, precision: 0.9761904761904762, f_beta: 0.9749843652282677
train: step: 2849, loss: 0.009927263483405113, acc: 1.0, recall: 1.0, precision: 1.0, f_beta: 1.0
train: step: 2850, loss: 0.007195995654910803, acc: 1.0, recall: 1.0, precision: 1.0, f_beta: 1.0
train: step: 2851, loss: 0.0527905710041523, acc: 0.95, recall: 0.95, precision: 0.9545454545454546, f_beta: 0.949874686716792
train: step: 2852, loss: 0.02786380983889103, acc: 0.975, recall: 0.975, precision: 0.9761904761904762, f_beta: 0.9749843652282677
train: step: 2853, loss: 0.07162497192621231, acc: 0.925, recall: 0.925, precision: 0.9347826086956521, f_beta: 0.9245757385292268
train: step: 2854, loss: 0.0345812626183033, acc: 0.975, recall: 0.975, precision: 0.9761904761904762, f_beta: 0.9749843652282677
train: step: 2855, loss: 0.03584107756614685, acc: 0.925, recall: 0.925, precision: 0.9260651629072681, f_beta: 0.924953095684803
train: step: 2856, loss: 0.17428764700889587, acc: 0.825, recall: 0.825, precision: 0.8466666666666667, f_beta: 0.8222222222222222
train: step: 2857, loss: 0.09403733164072037, acc: 0.875, recall: 0.875, precision: 0.8759398496240601, f_beta: 0.8749218261413383
train: step: 2858, loss: 0.06462601572275162, acc: 0.95, recall: 0.95, precision: 0.9545454545454546, f_beta: 0.949874686716792
train: step: 2859, loss: 0.07075913995504379, acc: 0.925, recall: 0.925, precision: 0.9347826086956521, f_beta: 0.9245757385292268
train: step: 2860, loss: 0.1767282485961914, acc: 0.775, recall: 0.775, precision: 0.7933333333333333, f_beta: 0.7714285714285714
train: step: 2861, loss: 0.08502393215894699, acc: 0.9, recall: 0.8999999999999999, precision: 0.904040404040404, f_beta: 0.899749373433584
train: step: 2862, loss: 0.09947054833173752, acc: 0.9, recall: 0.9, precision: 0.9, f_beta: 0.9
train: step: 2863, loss: 0.08385620266199112, acc: 0.95, recall: 0.95, precision: 0.95, f_beta: 0.9500000000000001
train: step: 2864, loss: 0.1978362500667572, acc: 0.85, recall: 0.85, precision: 0.8846153846153846, f_beta: 0.8465473145780051
train: step: 2865, loss: 0.1986154317855835, acc: 0.75, recall: 0.75, precision: 0.8333333333333333, f_beta: 0.7333333333333334
train: step: 2866, loss: 0.04914485663175583, acc: 0.925, recall: 0.925, precision: 0.9347826086956521, f_beta: 0.9245757385292268
train: step: 2867, loss: 0.16818979382514954, acc: 0.8, recall: 0.8, precision: 0.8571428571428572, f_beta: 0.7916666666666665
train: step: 2868, loss: 0.008114410564303398, acc: 1.0, recall: 1.0, precision: 1.0, f_beta: 1.0
train: step: 2869, loss: 0.17078271508216858, acc: 0.8, recall: 0.8, precision: 0.8125, f_beta: 0.797979797979798
train: step: 2870, loss: 0.0560712106525898, acc: 0.95, recall: 0.95, precision: 0.9545454545454546, f_beta: 0.949874686716792
train: step: 2871, loss: 0.07499434053897858, acc: 0.925, recall: 0.925, precision: 0.9260651629072681, f_beta: 0.924953095684803
train: step: 2872, loss: 0.04291114583611488, acc: 0.975, recall: 0.975, precision: 0.9761904761904762, f_beta: 0.9749843652282677
train: step: 2873, loss: 0.22758658230304718, acc: 0.7, recall: 0.7, precision: 0.7197802197802198, f_beta: 0.6930946291560103
train: step: 2874, loss: 0.2204699069261551, acc: 0.725, recall: 0.725, precision: 0.7821316614420062, f_beta: 0.7103357472021066
train: step: 2875, loss: 0.06419764459133148, acc: 0.925, recall: 0.925, precision: 0.9260651629072681, f_beta: 0.924953095684803
train: step: 2876, loss: 0.02134685218334198, acc: 1.0, recall: 1.0, precision: 1.0, f_beta: 1.0
train: step: 2877, loss: 0.0315006822347641, acc: 0.95, recall: 0.95, precision: 0.9545454545454546, f_beta: 0.949874686716792
train: step: 2878, loss: 0.2459951639175415, acc: 0.675, recall: 0.675, precision: 0.719435736677116, f_beta: 0.6576695194206715
train: step: 2879, loss: 0.014698331244289875, acc: 1.0, recall: 1.0, precision: 1.0, f_beta: 1.0
train: step: 2880, loss: 0.009112441912293434, acc: 1.0, recall: 1.0, precision: 1.0, f_beta: 1.0
train: step: 2881, loss: 0.03149660676717758, acc: 0.975, recall: 0.975, precision: 0.9761904761904762, f_beta: 0.9749843652282677
train: step: 2882, loss: 0.01293375063687563, acc: 1.0, recall: 1.0, precision: 1.0, f_beta: 1.0
train: step: 2883, loss: 0.017894649878144264, acc: 1.0, recall: 1.0, precision: 1.0, f_beta: 1.0
train: step: 2884, loss: 0.03870583325624466, acc: 0.975, recall: 0.975, precision: 0.9761904761904762, f_beta: 0.9749843652282677
train: step: 2885, loss: 0.054054372012615204, acc: 0.95, recall: 0.95, precision: 0.9545454545454546, f_beta: 0.949874686716792
train: step: 2886, loss: 0.09699424356222153, acc: 0.875, recall: 0.875, precision: 0.9, f_beta: 0.873015873015873
train: step: 2887, loss: 0.36546602845191956, acc: 0.7, recall: 0.7, precision: 0.8125, f_beta: 0.6703296703296704
train: step: 2888, loss: 0.048191994428634644, acc: 0.975, recall: 0.975, precision: 0.9761904761904762, f_beta: 0.9749843652282677
train: step: 2889, loss: 0.0824204832315445, acc: 0.925, recall: 0.925, precision: 0.9347826086956521, f_beta: 0.9245757385292268
train: step: 2890, loss: 0.09304298460483551, acc: 0.85, recall: 0.85, precision: 0.8846153846153846, f_beta: 0.8465473145780051
train: step: 2891, loss: 0.11393836885690689, acc: 0.975, recall: 0.975, precision: 0.9761904761904762, f_beta: 0.9749843652282677
train: step: 2892, loss: 0.015926605090498924, acc: 1.0, recall: 1.0, precision: 1.0, f_beta: 1.0
train: step: 2893, loss: 0.16681310534477234, acc: 0.8, recall: 0.8, precision: 0.8571428571428572, f_beta: 0.7916666666666665
train: step: 2894, loss: 0.1348317414522171, acc: 0.8, recall: 0.8, precision: 0.8, f_beta: 0.8000000000000002
train: step: 2895, loss: 0.008253676816821098, acc: 1.0, recall: 1.0, precision: 1.0, f_beta: 1.0
train: step: 2896, loss: 0.01160682737827301, acc: 1.0, recall: 1.0, precision: 1.0, f_beta: 1.0
train: step: 2897, loss: 0.10819144546985626, acc: 0.825, recall: 0.825, precision: 0.8466666666666667, f_beta: 0.8222222222222222
train: step: 2898, loss: 0.08116095513105392, acc: 0.925, recall: 0.925, precision: 0.9260651629072681, f_beta: 0.924953095684803
train: step: 2899, loss: 0.06896685808897018, acc: 0.875, recall: 0.875, precision: 0.8759398496240601, f_beta: 0.8749218261413383
train: step: 2900, loss: 0.010548804886639118, acc: 1.0, recall: 1.0, precision: 1.0, f_beta: 1.0
train: step: 2901, loss: 0.11615697294473648, acc: 0.85, recall: 0.8500000000000001, precision: 0.8535353535353536, f_beta: 0.849624060150376
train: step: 2902, loss: 0.18075524270534515, acc: 0.8, recall: 0.8, precision: 0.8296703296703296, f_beta: 0.7953964194373402
train: step: 2903, loss: 0.10039234161376953, acc: 0.85, recall: 0.8500000000000001, precision: 0.8535353535353536, f_beta: 0.849624060150376
train: step: 2904, loss: 0.051654696464538574, acc: 0.925, recall: 0.925, precision: 0.9347826086956521, f_beta: 0.9245757385292268
train: step: 2905, loss: 0.0113894147798419, acc: 1.0, recall: 1.0, precision: 1.0, f_beta: 1.0
train: step: 2906, loss: 0.040029846131801605, acc: 0.975, recall: 0.975, precision: 0.9761904761904762, f_beta: 0.9749843652282677
train: step: 2907, loss: 0.15099012851715088, acc: 0.75, recall: 0.75, precision: 0.7976190476190477, f_beta: 0.7395833333333333
train: step: 2908, loss: 0.03910725191235542, acc: 0.975, recall: 0.975, precision: 0.9761904761904762, f_beta: 0.9749843652282677
train: step: 2909, loss: 0.04736531153321266, acc: 0.95, recall: 0.95, precision: 0.95, f_beta: 0.9500000000000001
train: step: 2910, loss: 0.010462356731295586, acc: 1.0, recall: 1.0, precision: 1.0, f_beta: 1.0
train: step: 2911, loss: 0.07528064399957657, acc: 0.925, recall: 0.925, precision: 0.9260651629072681, f_beta: 0.924953095684803
train: step: 2912, loss: 0.014750510454177856, acc: 0.975, recall: 0.975, precision: 0.9761904761904762, f_beta: 0.9749843652282677
train: step: 2913, loss: 0.18205879628658295, acc: 0.85, recall: 0.85, precision: 0.8645833333333333, f_beta: 0.8484848484848484
train: step: 2914, loss: 0.14813999831676483, acc: 0.8, recall: 0.8, precision: 0.8571428571428572, f_beta: 0.7916666666666665
train: step: 2915, loss: 0.03102695383131504, acc: 1.0, recall: 1.0, precision: 1.0, f_beta: 1.0
train: step: 2916, loss: 0.20959730446338654, acc: 0.75, recall: 0.75, precision: 0.75, f_beta: 0.75
train: step: 2917, loss: 0.31307077407836914, acc: 0.6, recall: 0.6, precision: 0.6041666666666667, f_beta: 0.595959595959596
train: step: 2918, loss: 0.2247827798128128, acc: 0.725, recall: 0.725, precision: 0.8225806451612903, f_beta: 0.7025016903313049
train: step: 2919, loss: 0.3465566337108612, acc: 0.325, recall: 0.325, precision: 0.30056980056980054, f_beta: 0.3036750483558994
train: step: 2920, loss: 0.10650768131017685, acc: 0.85, recall: 0.8500000000000001, precision: 0.8535353535353536, f_beta: 0.849624060150376
train: step: 2921, loss: 0.05525268241763115, acc: 0.925, recall: 0.925, precision: 0.9347826086956521, f_beta: 0.9245757385292268
train: step: 2922, loss: 0.18128742277622223, acc: 0.925, recall: 0.925, precision: 0.9347826086956521, f_beta: 0.9245757385292268
train: step: 2923, loss: 0.06523499637842178, acc: 0.925, recall: 0.925, precision: 0.9347826086956521, f_beta: 0.9245757385292268
train: step: 2924, loss: 0.12579044699668884, acc: 0.925, recall: 0.925, precision: 0.9347826086956521, f_beta: 0.9245757385292268
train: step: 2925, loss: 0.19120554625988007, acc: 0.725, recall: 0.7250000000000001, precision: 0.7301790281329923, f_beta: 0.7234443746071653
train: step: 2926, loss: 0.03936018422245979, acc: 0.95, recall: 0.95, precision: 0.9545454545454546, f_beta: 0.949874686716792
train: step: 2927, loss: 0.27719566226005554, acc: 0.575, recall: 0.575, precision: 0.5940438871473355, f_beta: 0.5523370638578011
train: step: 2928, loss: 0.08639927953481674, acc: 0.9, recall: 0.8999999999999999, precision: 0.904040404040404, f_beta: 0.899749373433584
train: step: 2929, loss: 0.16900500655174255, acc: 0.825, recall: 0.825, precision: 0.8324808184143222, f_beta: 0.824010056568196
train: step: 2930, loss: 0.005291367881000042, acc: 1.0, recall: 1.0, precision: 1.0, f_beta: 1.0
train: step: 2931, loss: 0.06173047423362732, acc: 0.925, recall: 0.925, precision: 0.9260651629072681, f_beta: 0.924953095684803
train: step: 2932, loss: 0.008453610353171825, acc: 1.0, recall: 1.0, precision: 1.0, f_beta: 1.0
train: step: 2933, loss: 0.2912302017211914, acc: 0.825, recall: 0.825, precision: 0.8466666666666667, f_beta: 0.8222222222222222
train: step: 2934, loss: 0.4007975459098816, acc: 0.875, recall: 0.875, precision: 0.8836317135549872, f_beta: 0.8742928975487115
train: step: 2935, loss: 0.034007180482149124, acc: 0.975, recall: 0.975, precision: 0.9761904761904762, f_beta: 0.9749843652282677
train: step: 2936, loss: 0.29047366976737976, acc: 0.725, recall: 0.725, precision: 0.7821316614420062, f_beta: 0.7103357472021066
train: step: 2937, loss: 0.028833121061325073, acc: 0.975, recall: 0.975, precision: 0.9761904761904762, f_beta: 0.9749843652282677
train: step: 2938, loss: 0.028486019000411034, acc: 0.95, recall: 0.95, precision: 0.9545454545454546, f_beta: 0.949874686716792
train: step: 2939, loss: 0.050798654556274414, acc: 0.925, recall: 0.925, precision: 0.9347826086956521, f_beta: 0.9245757385292268
train: step: 2940, loss: 0.015093850903213024, acc: 0.975, recall: 0.975, precision: 0.9761904761904762, f_beta: 0.9749843652282677
train: step: 2941, loss: 0.09950102865695953, acc: 0.875, recall: 0.875, precision: 0.8836317135549872, f_beta: 0.8742928975487115
train: step: 2942, loss: 0.10704724490642548, acc: 0.85, recall: 0.85, precision: 0.8645833333333333, f_beta: 0.8484848484848484
train: step: 2943, loss: 0.17325298488140106, acc: 0.8, recall: 0.8, precision: 0.803030303030303, f_beta: 0.7994987468671679
train: step: 2944, loss: 0.03586987406015396, acc: 0.975, recall: 0.975, precision: 0.9761904761904762, f_beta: 0.9749843652282677
train: step: 2945, loss: 0.04927955940365791, acc: 0.95, recall: 0.95, precision: 0.95, f_beta: 0.9500000000000001
train: step: 2946, loss: 0.07178013026714325, acc: 0.9, recall: 0.9, precision: 0.9, f_beta: 0.9
train: step: 2947, loss: 0.0670943409204483, acc: 0.95, recall: 0.95, precision: 0.95, f_beta: 0.9500000000000001
train: step: 2948, loss: 0.2599615454673767, acc: 0.625, recall: 0.625, precision: 0.6792114695340502, f_beta: 0.5943204868154157
train: step: 2949, loss: 0.06511029601097107, acc: 0.925, recall: 0.925, precision: 0.9347826086956521, f_beta: 0.9245757385292268
