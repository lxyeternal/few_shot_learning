train: step: 0, loss: 0.285592645406723, acc: 0.49, recall: 0.49, precision: 0.48966515088879703, f_beta: 0.48583526565177937
train: step: 1, loss: 0.28479528427124023, acc: 0.59, recall: 0.5900000000000001, precision: 0.5909090909090908, f_beta: 0.5889724310776943
train: step: 2, loss: 0.283591628074646, acc: 0.64, recall: 0.64, precision: 0.681912681912682, f_beta: 0.6179966044142614
train: step: 3, loss: 0.27978143095970154, acc: 0.5, recall: 0.5, precision: 0.25, f_beta: 0.3333333333333333
train: step: 4, loss: 0.2797132730484009, acc: 0.5, recall: 0.5, precision: 0.25, f_beta: 0.3333333333333333
train: step: 5, loss: 0.27771422266960144, acc: 0.5, recall: 0.5, precision: 0.25, f_beta: 0.3333333333333333
train: step: 6, loss: 0.26572009921073914, acc: 0.5, recall: 0.5, precision: 0.25, f_beta: 0.3333333333333333
train: step: 7, loss: 0.2593422532081604, acc: 0.5, recall: 0.5, precision: 0.25, f_beta: 0.3333333333333333
train: step: 8, loss: 0.26617851853370667, acc: 0.5, recall: 0.5, precision: 0.25, f_beta: 0.3333333333333333
train: step: 9, loss: 0.25372037291526794, acc: 0.5, recall: 0.5, precision: 0.25, f_beta: 0.3333333333333333
train: step: 10, loss: 0.25444892048835754, acc: 0.5, recall: 0.5, precision: 0.25, f_beta: 0.3333333333333333
train: step: 11, loss: 0.25796979665756226, acc: 0.64, recall: 0.64, precision: 0.6666666666666666, f_beta: 0.625
train: step: 12, loss: 0.27725109457969666, acc: 0.49, recall: 0.49, precision: 0.4140893470790378, f_beta: 0.3453985367731998
train: step: 13, loss: 0.25337183475494385, acc: 0.5, recall: 0.5, precision: 0.25, f_beta: 0.3333333333333333
train: step: 14, loss: 0.24999211728572845, acc: 0.52, recall: 0.52, precision: 0.6302083333333333, f_beta: 0.39117199391172
train: step: 15, loss: 0.2533755600452423, acc: 0.5, recall: 0.5, precision: 0.25, f_beta: 0.3333333333333333
train: step: 16, loss: 0.25139838457107544, acc: 0.5, recall: 0.5, precision: 0.25, f_beta: 0.3333333333333333
train: step: 17, loss: 0.25052544474601746, acc: 0.5, recall: 0.5, precision: 0.25, f_beta: 0.3333333333333333
train: step: 18, loss: 0.2492275983095169, acc: 0.54, recall: 0.54, precision: 0.7604166666666667, f_beta: 0.4165398274987316
train: step: 19, loss: 0.24914191663265228, acc: 0.5, recall: 0.5, precision: 0.25, f_beta: 0.3333333333333333
train: step: 20, loss: 0.25506651401519775, acc: 0.53, recall: 0.53, precision: 0.7577319587628866, f_beta: 0.39673982800667434
train: step: 21, loss: 0.24962280690670013, acc: 0.67, recall: 0.6699999999999999, precision: 0.6922207146087743, f_beta: 0.6601791782514674
train: step: 22, loss: 0.2511255741119385, acc: 0.62, recall: 0.62, precision: 0.7840909090909092, f_beta: 0.5558672276764843
train: step: 23, loss: 0.2543250024318695, acc: 0.5, recall: 0.5, precision: 0.25, f_beta: 0.3333333333333333
train: step: 24, loss: 0.25106242299079895, acc: 0.57, recall: 0.5700000000000001, precision: 0.7688172043010753, f_beta: 0.4724573671942093
train: step: 25, loss: 0.24902130663394928, acc: 0.76, recall: 0.76, precision: 0.7821180555555556, f_beta: 0.755201958384333
train: step: 26, loss: 0.25146254897117615, acc: 0.68, recall: 0.6799999999999999, precision: 0.6847290640394088, f_beta: 0.677938808373591
train: step: 27, loss: 0.25005924701690674, acc: 0.52, recall: 0.52, precision: 0.6302083333333333, f_beta: 0.39117199391172
train: step: 28, loss: 0.24573105573654175, acc: 0.73, recall: 0.73, precision: 0.7323232323232323, f_beta: 0.7293233082706766
train: step: 29, loss: 0.24377590417861938, acc: 0.63, recall: 0.63, precision: 0.6733333333333333, f_beta: 0.6053333333333333
train: step: 30, loss: 0.24691219627857208, acc: 0.54, recall: 0.54, precision: 0.5946969696969697, f_beta: 0.46236559139784944
train: step: 31, loss: 0.24702735245227814, acc: 0.7, recall: 0.7, precision: 0.7297794117647058, f_beta: 0.6899545266639107
train: step: 32, loss: 0.24718168377876282, acc: 0.58, recall: 0.5800000000000001, precision: 0.5992063492063493, f_beta: 0.5586380832282472
train: step: 33, loss: 0.24681900441646576, acc: 0.6, recall: 0.6000000000000001, precision: 0.6693766937669376, f_beta: 0.554367201426025
train: step: 34, loss: 0.2471761256456375, acc: 0.5, recall: 0.5, precision: 0.5, f_beta: 0.47916666666666663
train: step: 35, loss: 0.24006198346614838, acc: 0.63, recall: 0.63, precision: 0.6733333333333333, f_beta: 0.6053333333333333
train: step: 36, loss: 0.24878352880477905, acc: 0.73, recall: 0.73, precision: 0.7416981925178646, f_beta: 0.7266929851199515
train: step: 37, loss: 0.2643994688987732, acc: 0.38, recall: 0.38, precision: 0.32517482517482516, f_beta: 0.3272569444444444
train: step: 38, loss: 0.2362474501132965, acc: 0.74, recall: 0.74, precision: 0.767379679144385, f_beta: 0.7331691297208538
train: step: 39, loss: 0.22682760655879974, acc: 0.63, recall: 0.63, precision: 0.6648909183155758, f_beta: 0.6093337556752191
train: step: 40, loss: 0.23767265677452087, acc: 0.69, recall: 0.69, precision: 0.693798449612403, f_beta: 0.6884735202492211
train: step: 41, loss: 0.20618757605552673, acc: 0.75, recall: 0.75, precision: 0.750903251706142, f_beta: 0.7497747973175859
train: step: 42, loss: 0.22371652722358704, acc: 0.76, recall: 0.76, precision: 0.7637987012987013, f_beta: 0.7591328783621035
train: step: 43, loss: 0.18798302114009857, acc: 0.84, recall: 0.8400000000000001, precision: 0.8449675324675325, f_beta: 0.839421918908069
train: step: 44, loss: 0.25084564089775085, acc: 0.63, recall: 0.63, precision: 0.7111760883690708, f_beta: 0.590662683925213
train: step: 45, loss: 0.25290167331695557, acc: 0.56, recall: 0.56, precision: 0.5714285714285714, f_beta: 0.5416666666666666
train: step: 46, loss: 0.2338656634092331, acc: 0.66, recall: 0.6599999999999999, precision: 0.67825311942959, f_beta: 0.6510673234811166
train: step: 47, loss: 0.17829151451587677, acc: 0.75, recall: 0.75, precision: 0.7681252681252682, f_beta: 0.7457023700539112
train: step: 48, loss: 0.19196583330631256, acc: 0.74, recall: 0.74, precision: 0.75, f_beta: 0.7373737373737373
train: step: 49, loss: 0.17833247780799866, acc: 0.75, recall: 0.75, precision: 0.7681252681252682, f_beta: 0.7457023700539112
train: step: 50, loss: 0.28316494822502136, acc: 0.62, recall: 0.62, precision: 0.6428571428571428, f_beta: 0.6041666666666667
train: step: 51, loss: 0.22056350111961365, acc: 0.66, recall: 0.66, precision: 0.669779286926995, f_beta: 0.6550324675324675
train: step: 52, loss: 0.29206156730651855, acc: 0.6, recall: 0.6, precision: 0.6148897058823529, f_beta: 0.586606035551881
train: step: 53, loss: 0.20301418006420135, acc: 0.66, recall: 0.66, precision: 0.669779286926995, f_beta: 0.6550324675324675
train: step: 54, loss: 0.28239569067955017, acc: 0.6, recall: 0.6, precision: 0.6370614035087719, f_beta: 0.570999570999571
train: step: 55, loss: 0.18465492129325867, acc: 0.77, recall: 0.77, precision: 0.7701080432172869, f_beta: 0.7699769976997699
train: step: 56, loss: 0.17241400480270386, acc: 0.8, recall: 0.8, precision: 0.8183361629881154, f_beta: 0.797077922077922
train: step: 57, loss: 0.24551059305667877, acc: 0.81, recall: 0.81, precision: 0.8324753324753325, f_beta: 0.8067338012409724
train: step: 58, loss: 0.22735248506069183, acc: 0.72, recall: 0.72, precision: 0.7619047619047619, f_beta: 0.7083333333333334
train: step: 59, loss: 0.19979266822338104, acc: 0.72, recall: 0.72, precision: 0.72, f_beta: 0.72
train: step: 60, loss: 0.17784538865089417, acc: 0.85, recall: 0.85, precision: 0.8617197188921042, f_beta: 0.8487750781328762
train: step: 61, loss: 0.2150196135044098, acc: 0.82, recall: 0.8200000000000001, precision: 0.8205128205128205, f_beta: 0.8199279711884755
train: step: 62, loss: 0.25432640314102173, acc: 0.72, recall: 0.72, precision: 0.7387152777777778, f_beta: 0.7144022847817217
train: step: 63, loss: 0.21104708313941956, acc: 0.77, recall: 0.77, precision: 0.7790409260024803, f_beta: 0.7681217864704104
train: step: 64, loss: 0.21276016533374786, acc: 0.71, recall: 0.71, precision: 0.7100840336134454, f_beta: 0.7099709970997101
train: step: 65, loss: 0.19005081057548523, acc: 0.74, recall: 0.74, precision: 0.7415458937198067, f_beta: 0.7395833333333335
train: step: 66, loss: 0.173030287027359, acc: 0.81, recall: 0.81, precision: 0.8131313131313131, f_beta: 0.8095238095238095
train: step: 67, loss: 0.16866004467010498, acc: 0.73, recall: 0.73, precision: 0.7323232323232323, f_beta: 0.7293233082706766
train: step: 68, loss: 0.16674107313156128, acc: 0.73, recall: 0.73, precision: 0.7377015295576685, f_beta: 0.7277951406391774
train: step: 69, loss: 0.17108659446239471, acc: 0.76, recall: 0.76, precision: 0.7616747181964574, f_beta: 0.7596153846153846
train: step: 70, loss: 0.1833430826663971, acc: 0.74, recall: 0.74, precision: 0.74, f_beta: 0.74
train: step: 71, loss: 0.1727774292230606, acc: 0.72, recall: 0.72, precision: 0.7214170692431562, f_beta: 0.7195512820512822
train: step: 72, loss: 0.17019543051719666, acc: 0.79, recall: 0.79, precision: 0.7957976336189311, f_beta: 0.7889659330720531
train: step: 73, loss: 0.11766587942838669, acc: 0.84, recall: 0.8400000000000001, precision: 0.8449675324675325, f_beta: 0.839421918908069
train: step: 74, loss: 0.15864045917987823, acc: 0.76, recall: 0.76, precision: 0.7616747181964574, f_beta: 0.7596153846153846
train: step: 75, loss: 0.1681344360113144, acc: 0.74, recall: 0.74, precision: 0.7463054187192117, f_beta: 0.7383252818035426
train: step: 76, loss: 0.12031115591526031, acc: 0.85, recall: 0.85, precision: 0.8617197188921042, f_beta: 0.8487750781328762
train: step: 77, loss: 0.24041195213794708, acc: 0.84, recall: 0.8400000000000001, precision: 0.8449675324675325, f_beta: 0.839421918908069
train: step: 78, loss: 0.13775932788848877, acc: 0.83, recall: 0.83, precision: 0.8301320528211285, f_beta: 0.8299829982998299
train: step: 79, loss: 0.15866045653820038, acc: 0.76, recall: 0.76, precision: 0.7637987012987013, f_beta: 0.7591328783621035
train: step: 80, loss: 0.1325642615556717, acc: 0.83, recall: 0.8300000000000001, precision: 0.8333333333333334, f_beta: 0.8295739348370927
train: step: 81, loss: 0.25057452917099, acc: 0.81, recall: 0.81, precision: 0.810124049619848, f_beta: 0.8099809980998101
train: step: 82, loss: 0.249109148979187, acc: 0.74, recall: 0.74, precision: 0.7435064935064934, f_beta: 0.7390606182256123
train: step: 83, loss: 0.19569753110408783, acc: 0.77, recall: 0.77, precision: 0.7790409260024803, f_beta: 0.7681217864704104
train: step: 84, loss: 0.10714487731456757, acc: 0.9, recall: 0.9, precision: 0.9, f_beta: 0.9
train: step: 85, loss: 0.14962613582611084, acc: 0.83, recall: 0.8300000000000001, precision: 0.8333333333333334, f_beta: 0.8295739348370927
train: step: 86, loss: 0.12512418627738953, acc: 0.85, recall: 0.85, precision: 0.8501400560224089, f_beta: 0.84998499849985
train: step: 87, loss: 0.13763496279716492, acc: 0.84, recall: 0.8400000000000001, precision: 0.8421900161030595, f_beta: 0.8397435897435896
train: step: 88, loss: 0.13955219089984894, acc: 0.8, recall: 0.8, precision: 0.8183361629881154, f_beta: 0.797077922077922
train: step: 89, loss: 0.14160528779029846, acc: 0.81, recall: 0.81, precision: 0.8203803224472923, f_beta: 0.8084484323016432
train: step: 90, loss: 0.1304396241903305, acc: 0.81, recall: 0.81, precision: 0.8161974704202366, f_beta: 0.8090644156366193
train: step: 91, loss: 0.10943379998207092, acc: 0.85, recall: 0.85, precision: 0.867801597309794, f_beta: 0.848162769511084
train: step: 92, loss: 0.25063708424568176, acc: 0.72, recall: 0.72, precision: 0.7527573529411764, f_beta: 0.7106242248863166
train: step: 93, loss: 0.15535707771778107, acc: 0.78, recall: 0.78, precision: 0.8038194444444444, f_beta: 0.7756017951856385
train: step: 94, loss: 0.1758313775062561, acc: 0.84, recall: 0.84, precision: 0.8405448717948718, f_beta: 0.8399359743897559
train: step: 95, loss: 0.15337587893009186, acc: 0.8, recall: 0.8, precision: 0.8043831168831169, f_beta: 0.7992773986350863
train: step: 96, loss: 0.22453750669956207, acc: 0.69, recall: 0.69, precision: 0.7148349163274537, f_beta: 0.6807743795695602
train: step: 97, loss: 0.1513129025697708, acc: 0.82, recall: 0.82, precision: 0.82, f_beta: 0.82
train: step: 98, loss: 0.29173335433006287, acc: 0.75, recall: 0.75, precision: 0.792192613370734, f_beta: 0.7406369955389562
train: step: 99, loss: 0.12039057910442352, acc: 0.82, recall: 0.8200000000000001, precision: 0.8246753246753247, f_beta: 0.8193496587715776
train: step: 100, loss: 0.14096632599830627, acc: 0.82, recall: 0.8200000000000001, precision: 0.8472222222222222, f_beta: 0.8164014687882497
train: step: 101, loss: 0.13115155696868896, acc: 0.82, recall: 0.8200000000000001, precision: 0.8284072249589491, f_beta: 0.818840579710145
train: step: 102, loss: 0.3500841557979584, acc: 0.84, recall: 0.84, precision: 0.84, f_beta: 0.8399999999999999
train: step: 103, loss: 0.16637061536312103, acc: 0.79, recall: 0.79, precision: 0.7929292929292929, f_beta: 0.7894736842105263
train: step: 104, loss: 0.12585686147212982, acc: 0.84, recall: 0.84, precision: 0.8607809847198642, f_beta: 0.8376623376623376
train: step: 105, loss: 0.09063266962766647, acc: 0.88, recall: 0.88, precision: 0.8958333333333333, f_beta: 0.8787878787878789
train: step: 106, loss: 0.17082294821739197, acc: 0.77, recall: 0.77, precision: 0.7709755118426336, f_beta: 0.769792813532179
train: step: 107, loss: 0.15414495766162872, acc: 0.79, recall: 0.79, precision: 0.8047498949138294, f_beta: 0.7874278773155178
train: step: 108, loss: 0.18145886063575745, acc: 0.78, recall: 0.78, precision: 0.8119429590017826, f_beta: 0.7742200328407225
train: step: 109, loss: 0.14111734926700592, acc: 0.82, recall: 0.8200000000000001, precision: 0.8472222222222222, f_beta: 0.8164014687882497
train: step: 110, loss: 0.1506689190864563, acc: 0.8, recall: 0.8, precision: 0.8004807692307692, f_beta: 0.7999199679871949
train: step: 111, loss: 0.09234612435102463, acc: 0.94, recall: 0.94, precision: 0.94, f_beta: 0.94
train: step: 112, loss: 0.15563476085662842, acc: 0.79, recall: 0.79, precision: 0.7929292929292929, f_beta: 0.7894736842105263
train: step: 113, loss: 0.12298598885536194, acc: 0.85, recall: 0.85, precision: 0.8501400560224089, f_beta: 0.84998499849985
train: step: 114, loss: 0.1098269447684288, acc: 0.85, recall: 0.8500000000000001, precision: 0.8535353535353536, f_beta: 0.849624060150376
train: step: 115, loss: 0.11755602806806564, acc: 0.82, recall: 0.8200000000000001, precision: 0.8333333333333334, f_beta: 0.8181818181818181
train: step: 116, loss: 0.16194039583206177, acc: 0.77, recall: 0.77, precision: 0.7701080432172869, f_beta: 0.7699769976997699
train: step: 117, loss: 0.14227162301540375, acc: 0.83, recall: 0.83, precision: 0.8301320528211285, f_beta: 0.8299829982998299
train: step: 118, loss: 0.19708605110645294, acc: 0.85, recall: 0.85, precision: 0.867801597309794, f_beta: 0.848162769511084
train: step: 119, loss: 0.24603624641895294, acc: 0.78, recall: 0.78, precision: 0.7840909090909092, f_beta: 0.779205138498595
train: step: 120, loss: 0.14522023499011993, acc: 0.79, recall: 0.79, precision: 0.8279059249208502, f_beta: 0.7837503861600247
train: step: 121, loss: 0.13594438135623932, acc: 0.85, recall: 0.85, precision: 0.851264552388599, f_beta: 0.8498648783905516
train: step: 122, loss: 0.17113186419010162, acc: 0.77, recall: 0.77, precision: 0.7895752895752897, f_beta: 0.7660461804495982
train: step: 123, loss: 0.1519153118133545, acc: 0.8, recall: 0.8, precision: 0.8004807692307692, f_beta: 0.7999199679871949
train: step: 124, loss: 0.08526069670915604, acc: 0.9, recall: 0.9, precision: 0.9006410256410255, f_beta: 0.8999599839935974
train: step: 125, loss: 0.13735461235046387, acc: 0.79, recall: 0.79, precision: 0.7910477719791249, f_beta: 0.7898108297467722
train: step: 126, loss: 0.13891509175300598, acc: 0.85, recall: 0.85, precision: 0.8501400560224089, f_beta: 0.84998499849985
train: step: 127, loss: 0.13329418003559113, acc: 0.78, recall: 0.78, precision: 0.7818035426731079, f_beta: 0.7796474358974359
train: step: 128, loss: 0.12741905450820923, acc: 0.82, recall: 0.8200000000000001, precision: 0.8284072249589491, f_beta: 0.818840579710145
train: step: 129, loss: 0.13897070288658142, acc: 0.8, recall: 0.8, precision: 0.8004807692307692, f_beta: 0.7999199679871949
train: step: 130, loss: 0.24747580289840698, acc: 0.73, recall: 0.73, precision: 0.7688172043010753, f_beta: 0.7198879551820727
train: step: 131, loss: 0.1115880012512207, acc: 0.86, recall: 0.86, precision: 0.8605769230769231, f_beta: 0.8599439775910365
train: step: 132, loss: 0.14540183544158936, acc: 0.78, recall: 0.78, precision: 0.78, f_beta: 0.78
train: step: 133, loss: 0.10682016611099243, acc: 0.88, recall: 0.88, precision: 0.8899835796387521, f_beta: 0.8792270531400966
train: step: 134, loss: 0.10965164005756378, acc: 0.86, recall: 0.8600000000000001, precision: 0.8652597402597402, f_beta: 0.8594941790445605
train: step: 135, loss: 0.20278853178024292, acc: 0.76, recall: 0.76, precision: 0.7758913412563667, f_beta: 0.7564935064935063
train: step: 136, loss: 0.15898311138153076, acc: 0.79, recall: 0.79, precision: 0.8047498949138294, f_beta: 0.7874278773155178
train: step: 137, loss: 0.27018484473228455, acc: 0.82, recall: 0.82, precision: 0.8395585738539898, f_beta: 0.8173701298701298
train: step: 138, loss: 0.16035033762454987, acc: 0.78, recall: 0.78, precision: 0.780448717948718, f_beta: 0.7799119647859143
train: step: 139, loss: 0.15639933943748474, acc: 0.76, recall: 0.76, precision: 0.7668308702791462, f_beta: 0.7584541062801933
train: step: 140, loss: 0.1992814838886261, acc: 0.72, recall: 0.72, precision: 0.72, f_beta: 0.72
train: step: 141, loss: 0.13463367521762848, acc: 0.84, recall: 0.84, precision: 0.8405448717948718, f_beta: 0.8399359743897559
train: step: 142, loss: 0.11305022984743118, acc: 0.88, recall: 0.88, precision: 0.8824476650563606, f_beta: 0.8798076923076923
train: step: 143, loss: 0.1380377560853958, acc: 0.8, recall: 0.8, precision: 0.8004807692307692, f_beta: 0.7999199679871949
train: step: 144, loss: 0.1601201444864273, acc: 0.76, recall: 0.76, precision: 0.76, f_beta: 0.76
train: step: 145, loss: 0.11366115510463715, acc: 0.83, recall: 0.8300000000000001, precision: 0.8311922922521076, f_beta: 0.8298468621759585
train: step: 146, loss: 0.12431392818689346, acc: 0.81, recall: 0.81, precision: 0.8161974704202366, f_beta: 0.8090644156366193
train: step: 147, loss: 0.14541637897491455, acc: 0.82, recall: 0.8200000000000001, precision: 0.8284072249589491, f_beta: 0.818840579710145
train: step: 148, loss: 0.11536342650651932, acc: 0.84, recall: 0.84, precision: 0.84, f_beta: 0.8399999999999999
train: step: 149, loss: 0.10973308235406876, acc: 0.85, recall: 0.85, precision: 0.851264552388599, f_beta: 0.8498648783905516
train: step: 150, loss: 0.125847727060318, acc: 0.83, recall: 0.83, precision: 0.8301320528211285, f_beta: 0.8299829982998299
train: step: 151, loss: 0.10864250361919403, acc: 0.86, recall: 0.86, precision: 0.86, f_beta: 0.8599999999999999
train: step: 152, loss: 0.17326946556568146, acc: 0.75, recall: 0.75, precision: 0.750903251706142, f_beta: 0.7497747973175859
train: step: 153, loss: 0.11514856666326523, acc: 0.84, recall: 0.8400000000000001, precision: 0.8489326765188834, f_beta: 0.8389694041867956
train: step: 154, loss: 0.15856380760669708, acc: 0.83, recall: 0.8300000000000001, precision: 0.8365973072215422, f_beta: 0.8291628982011858
train: step: 155, loss: 0.1307864785194397, acc: 0.85, recall: 0.85, precision: 0.851264552388599, f_beta: 0.8498648783905516
train: step: 156, loss: 0.12961417436599731, acc: 0.8, recall: 0.8, precision: 0.8125, f_beta: 0.797979797979798
train: step: 157, loss: 0.11364815384149551, acc: 0.84, recall: 0.84, precision: 0.8405448717948718, f_beta: 0.8399359743897559
train: step: 158, loss: 0.16129066050052643, acc: 0.82, recall: 0.8200000000000001, precision: 0.822061191626409, f_beta: 0.8197115384615385
train: step: 159, loss: 0.075784832239151, acc: 0.9, recall: 0.8999999999999999, precision: 0.9025764895330113, f_beta: 0.8998397435897436
train: step: 160, loss: 0.081367127597332, acc: 0.91, recall: 0.91, precision: 0.9101640656262505, f_beta: 0.90999099909991
train: step: 161, loss: 0.21316295862197876, acc: 0.81, recall: 0.81, precision: 0.8131313131313131, f_beta: 0.8095238095238095
train: step: 162, loss: 0.13266697525978088, acc: 0.85, recall: 0.8500000000000001, precision: 0.8569971440228478, f_beta: 0.8492613807657521
train: step: 163, loss: 0.14934946596622467, acc: 0.75, recall: 0.75, precision: 0.7501000400160064, f_beta: 0.7499749974997499
train: step: 164, loss: 0.11573116481304169, acc: 0.83, recall: 0.83, precision: 0.8301320528211285, f_beta: 0.8299829982998299
train: step: 165, loss: 0.10730018466711044, acc: 0.84, recall: 0.8400000000000001, precision: 0.8449675324675325, f_beta: 0.839421918908069
train: step: 166, loss: 0.12683071196079254, acc: 0.84, recall: 0.84, precision: 0.8405448717948718, f_beta: 0.8399359743897559
train: step: 167, loss: 0.1412200629711151, acc: 0.8, recall: 0.8, precision: 0.8078817733990147, f_beta: 0.7987117552334944
train: step: 168, loss: 0.13562755286693573, acc: 0.81, recall: 0.81, precision: 0.8257671290458175, f_beta: 0.8076728413807065
train: step: 169, loss: 0.10475172102451324, acc: 0.86, recall: 0.8600000000000001, precision: 0.8652597402597402, f_beta: 0.8594941790445605
train: step: 170, loss: 0.13829971849918365, acc: 0.86, recall: 0.86, precision: 0.8605769230769231, f_beta: 0.8599439775910365
train: step: 171, loss: 0.12736502289772034, acc: 0.81, recall: 0.81, precision: 0.8203803224472923, f_beta: 0.8084484323016432
train: step: 172, loss: 0.08955426514148712, acc: 0.88, recall: 0.88, precision: 0.88, f_beta: 0.88
train: step: 173, loss: 0.10426585376262665, acc: 0.88, recall: 0.8799999999999999, precision: 0.885551948051948, f_beta: 0.8795664391810518
train: step: 174, loss: 0.10770159959793091, acc: 0.87, recall: 0.87, precision: 0.8701480592236894, f_beta: 0.8699869986998701
train: step: 175, loss: 0.12819279730319977, acc: 0.82, recall: 0.8200000000000001, precision: 0.822061191626409, f_beta: 0.8197115384615385
train: step: 176, loss: 0.1430729329586029, acc: 0.8, recall: 0.8, precision: 0.8043831168831169, f_beta: 0.7992773986350863
train: step: 177, loss: 0.06485950946807861, acc: 0.9, recall: 0.9, precision: 0.9006410256410255, f_beta: 0.8999599839935974
train: step: 178, loss: 0.11573875695466995, acc: 0.86, recall: 0.86, precision: 0.8694581280788177, f_beta: 0.8590982286634461
train: step: 179, loss: 0.10795841366052628, acc: 0.86, recall: 0.86, precision: 0.8605769230769231, f_beta: 0.8599439775910365
train: step: 180, loss: 0.12778601050376892, acc: 0.82, recall: 0.8200000000000001, precision: 0.822061191626409, f_beta: 0.8197115384615385
train: step: 181, loss: 0.11966872215270996, acc: 0.85, recall: 0.85, precision: 0.851264552388599, f_beta: 0.8498648783905516
train: step: 182, loss: 0.11946286261081696, acc: 0.86, recall: 0.86, precision: 0.8605769230769231, f_beta: 0.8599439775910365
train: step: 183, loss: 0.1734439879655838, acc: 0.81, recall: 0.81, precision: 0.8131313131313131, f_beta: 0.8095238095238095
train: step: 184, loss: 0.13827987015247345, acc: 0.84, recall: 0.84, precision: 0.8405448717948718, f_beta: 0.8399359743897559
train: step: 185, loss: 0.10212142765522003, acc: 0.87, recall: 0.87, precision: 0.8713368125250903, f_beta: 0.8698828946051447
train: step: 186, loss: 0.1158970296382904, acc: 0.86, recall: 0.86, precision: 0.8605769230769231, f_beta: 0.8599439775910365
train: step: 187, loss: 0.19023534655570984, acc: 0.76, recall: 0.76, precision: 0.7604166666666667, f_beta: 0.7599039615846339
train: step: 188, loss: 0.21574276685714722, acc: 0.75, recall: 0.75, precision: 0.7627154266498528, f_beta: 0.7469379491851402
train: step: 189, loss: 0.11093763262033463, acc: 0.86, recall: 0.86, precision: 0.8623188405797102, f_beta: 0.859775641025641
train: step: 190, loss: 0.1140114888548851, acc: 0.86, recall: 0.8600000000000001, precision: 0.8652597402597402, f_beta: 0.8594941790445605
train: step: 191, loss: 0.15888133645057678, acc: 0.78, recall: 0.78, precision: 0.7840909090909092, f_beta: 0.779205138498595
train: step: 192, loss: 0.09972917288541794, acc: 0.87, recall: 0.87, precision: 0.8823894171145101, f_beta: 0.8689384010484928
train: step: 193, loss: 0.13641780614852905, acc: 0.79, recall: 0.79, precision: 0.7957976336189311, f_beta: 0.7889659330720531
train: step: 194, loss: 0.3014833927154541, acc: 0.79, recall: 0.79, precision: 0.7901160464185675, f_beta: 0.78997899789979
train: step: 195, loss: 0.12080612778663635, acc: 0.83, recall: 0.83, precision: 0.8467843631778058, f_beta: 0.8279178054458953
train: step: 196, loss: 0.13885597884655, acc: 0.81, recall: 0.81, precision: 0.8131313131313131, f_beta: 0.8095238095238095
train: step: 197, loss: 0.12398117035627365, acc: 0.84, recall: 0.8400000000000001, precision: 0.8421900161030595, f_beta: 0.8397435897435896
train: step: 198, loss: 0.13477467000484467, acc: 0.81, recall: 0.81, precision: 0.8161974704202366, f_beta: 0.8090644156366193
train: step: 199, loss: 0.1496073603630066, acc: 0.8, recall: 0.8, precision: 0.8004807692307692, f_beta: 0.7999199679871949
train: step: 200, loss: 0.08978309482336044, acc: 0.9, recall: 0.8999999999999999, precision: 0.9025764895330113, f_beta: 0.8998397435897436
train: step: 201, loss: 0.07514303177595139, acc: 0.92, recall: 0.9199999999999999, precision: 0.9261363636363636, f_beta: 0.9197109594540345
train: step: 202, loss: 0.15578874945640564, acc: 0.82, recall: 0.8200000000000001, precision: 0.8333333333333334, f_beta: 0.8181818181818181
train: step: 203, loss: 0.0932423323392868, acc: 0.87, recall: 0.87, precision: 0.8701480592236894, f_beta: 0.8699869986998701
train: step: 204, loss: 0.17553988099098206, acc: 0.8, recall: 0.8, precision: 0.8043831168831169, f_beta: 0.7992773986350863
train: step: 205, loss: 0.11786672472953796, acc: 0.84, recall: 0.84, precision: 0.8541666666666667, f_beta: 0.8383838383838385
train: step: 206, loss: 0.10675141960382462, acc: 0.86, recall: 0.86, precision: 0.8605769230769231, f_beta: 0.8599439775910365
train: step: 207, loss: 0.12961453199386597, acc: 0.83, recall: 0.83, precision: 0.8301320528211285, f_beta: 0.8299829982998299
train: step: 208, loss: 0.10050052404403687, acc: 0.87, recall: 0.87, precision: 0.8737373737373737, f_beta: 0.8696741854636592
train: step: 209, loss: 0.10496721416711807, acc: 0.85, recall: 0.85, precision: 0.8501400560224089, f_beta: 0.84998499849985
train: step: 210, loss: 0.12378515303134918, acc: 0.86, recall: 0.86, precision: 0.86, f_beta: 0.8599999999999999
train: step: 211, loss: 0.15523761510849, acc: 0.75, recall: 0.75, precision: 0.750903251706142, f_beta: 0.7497747973175859
train: step: 212, loss: 0.13595817983150482, acc: 0.84, recall: 0.8400000000000001, precision: 0.8421900161030595, f_beta: 0.8397435897435896
train: step: 213, loss: 0.16790375113487244, acc: 0.79, recall: 0.79, precision: 0.7929292929292929, f_beta: 0.7894736842105263
train: step: 214, loss: 0.11780866980552673, acc: 0.86, recall: 0.86, precision: 0.8605769230769231, f_beta: 0.8599439775910365
train: step: 215, loss: 0.14400304853916168, acc: 0.82, recall: 0.8200000000000001, precision: 0.822061191626409, f_beta: 0.8197115384615385
train: step: 216, loss: 0.1691337376832962, acc: 0.76, recall: 0.76, precision: 0.7821180555555556, f_beta: 0.755201958384333
train: step: 217, loss: 0.10143739730119705, acc: 0.86, recall: 0.86, precision: 0.875, f_beta: 0.8585858585858586
train: step: 218, loss: 0.18130819499492645, acc: 0.72, recall: 0.72, precision: 0.72, f_beta: 0.72
train: step: 219, loss: 0.09943540394306183, acc: 0.88, recall: 0.88, precision: 0.88, f_beta: 0.88
train: step: 220, loss: 0.1159672960639, acc: 0.82, recall: 0.8200000000000001, precision: 0.8205128205128205, f_beta: 0.8199279711884755
train: step: 221, loss: 0.11758354306221008, acc: 0.84, recall: 0.84, precision: 0.8405448717948718, f_beta: 0.8399359743897559
train: step: 222, loss: 0.13818013668060303, acc: 0.79, recall: 0.79, precision: 0.7901160464185675, f_beta: 0.78997899789979
train: step: 223, loss: 0.1270686537027359, acc: 0.83, recall: 0.83, precision: 0.8301320528211285, f_beta: 0.8299829982998299
train: step: 224, loss: 0.1349281370639801, acc: 0.83, recall: 0.83, precision: 0.8301320528211285, f_beta: 0.8299829982998299
train: step: 225, loss: 0.126803457736969, acc: 0.8, recall: 0.8, precision: 0.8078817733990147, f_beta: 0.7987117552334944
train: step: 226, loss: 0.10716983675956726, acc: 0.86, recall: 0.86, precision: 0.8623188405797102, f_beta: 0.859775641025641
train: step: 227, loss: 0.09039903432130814, acc: 0.89, recall: 0.89, precision: 0.89015606242497, f_beta: 0.88998899889989
train: step: 228, loss: 0.12041802704334259, acc: 0.85, recall: 0.85, precision: 0.851264552388599, f_beta: 0.8498648783905516
train: step: 229, loss: 0.12040092796087265, acc: 0.86, recall: 0.86, precision: 0.8694581280788177, f_beta: 0.8590982286634461
train: step: 230, loss: 0.1806921809911728, acc: 0.77, recall: 0.77, precision: 0.7701080432172869, f_beta: 0.7699769976997699
train: step: 231, loss: 0.05523545295000076, acc: 0.92, recall: 0.92, precision: 0.92, f_beta: 0.92
train: step: 232, loss: 0.10022802650928497, acc: 0.83, recall: 0.8300000000000001, precision: 0.8365973072215422, f_beta: 0.8291628982011858
train: step: 233, loss: 0.11858869343996048, acc: 0.86, recall: 0.8600000000000001, precision: 0.8652597402597402, f_beta: 0.8594941790445605
train: step: 234, loss: 0.09987180680036545, acc: 0.84, recall: 0.84, precision: 0.8405448717948718, f_beta: 0.8399359743897559
train: step: 235, loss: 0.10265155881643295, acc: 0.85, recall: 0.85, precision: 0.851264552388599, f_beta: 0.8498648783905516
train: step: 236, loss: 0.10503306984901428, acc: 0.86, recall: 0.86, precision: 0.8623188405797102, f_beta: 0.859775641025641
train: step: 237, loss: 0.10335268080234528, acc: 0.85, recall: 0.8500000000000001, precision: 0.8569971440228478, f_beta: 0.8492613807657521
train: step: 238, loss: 0.09744150936603546, acc: 0.85, recall: 0.85, precision: 0.851264552388599, f_beta: 0.8498648783905516
train: step: 239, loss: 0.1360786259174347, acc: 0.82, recall: 0.8200000000000001, precision: 0.822061191626409, f_beta: 0.8197115384615385
train: step: 240, loss: 0.11905800551176071, acc: 0.83, recall: 0.83, precision: 0.8301320528211285, f_beta: 0.8299829982998299
train: step: 241, loss: 0.10713597387075424, acc: 0.86, recall: 0.86, precision: 0.8694581280788177, f_beta: 0.8590982286634461
train: step: 242, loss: 0.04690951853990555, acc: 0.95, recall: 0.95, precision: 0.9501800720288115, f_beta: 0.9499949994999499
train: step: 243, loss: 0.1520553082227707, acc: 0.78, recall: 0.78, precision: 0.7818035426731079, f_beta: 0.7796474358974359
train: step: 244, loss: 0.055998530238866806, acc: 0.93, recall: 0.9299999999999999, precision: 0.9343434343434343, f_beta: 0.9298245614035088
train: step: 245, loss: 0.15742391347885132, acc: 0.79, recall: 0.79, precision: 0.7957976336189311, f_beta: 0.7889659330720531
train: step: 246, loss: 0.09015798568725586, acc: 0.87, recall: 0.87, precision: 0.8713368125250903, f_beta: 0.8698828946051447
train: step: 247, loss: 0.12782682478427887, acc: 0.83, recall: 0.8300000000000001, precision: 0.8333333333333334, f_beta: 0.8295739348370927
train: step: 248, loss: 0.09674759209156036, acc: 0.87, recall: 0.87, precision: 0.8713368125250903, f_beta: 0.8698828946051447
train: step: 249, loss: 0.20495426654815674, acc: 0.82, recall: 0.8200000000000001, precision: 0.8205128205128205, f_beta: 0.8199279711884755
train: step: 250, loss: 0.1424798220396042, acc: 0.82, recall: 0.8200000000000001, precision: 0.822061191626409, f_beta: 0.8197115384615385
train: step: 251, loss: 0.11802847683429718, acc: 0.85, recall: 0.85, precision: 0.8501400560224089, f_beta: 0.84998499849985
train: step: 252, loss: 0.1179983913898468, acc: 0.87, recall: 0.87, precision: 0.8737373737373737, f_beta: 0.8696741854636592
train: step: 253, loss: 0.1408814638853073, acc: 0.81, recall: 0.81, precision: 0.8111200321156162, f_beta: 0.8098288459613652
train: step: 254, loss: 0.11043735593557358, acc: 0.84, recall: 0.8400000000000001, precision: 0.8489326765188834, f_beta: 0.8389694041867956
train: step: 255, loss: 0.07236486673355103, acc: 0.91, recall: 0.91, precision: 0.9101640656262505, f_beta: 0.90999099909991
train: step: 256, loss: 0.12456686794757843, acc: 0.83, recall: 0.8300000000000001, precision: 0.8410500206696983, f_beta: 0.8286117552172598
train: step: 257, loss: 0.09803539514541626, acc: 0.87, recall: 0.87, precision: 0.8888188314417822, f_beta: 0.868407733576273
train: step: 258, loss: 0.09649568796157837, acc: 0.86, recall: 0.86, precision: 0.8605769230769231, f_beta: 0.8599439775910365
train: step: 259, loss: 0.13024400174617767, acc: 0.81, recall: 0.81, precision: 0.8161974704202366, f_beta: 0.8090644156366193
train: step: 260, loss: 0.11831752955913544, acc: 0.84, recall: 0.84, precision: 0.8405448717948718, f_beta: 0.8399359743897559
train: step: 261, loss: 0.10461153090000153, acc: 0.84, recall: 0.84, precision: 0.84, f_beta: 0.8399999999999999
train: step: 262, loss: 0.07435666769742966, acc: 0.9, recall: 0.9, precision: 0.9006410256410255, f_beta: 0.8999599839935974
train: step: 263, loss: 0.14054130017757416, acc: 0.8, recall: 0.8, precision: 0.8078817733990147, f_beta: 0.7987117552334944
train: step: 264, loss: 0.08954861015081406, acc: 0.89, recall: 0.89, precision: 0.8914090726615818, f_beta: 0.8899009108197378
train: step: 265, loss: 0.1527954339981079, acc: 0.74, recall: 0.74, precision: 0.7604166666666667, f_beta: 0.7348021215830274
train: step: 266, loss: 0.08280788362026215, acc: 0.89, recall: 0.8899999999999999, precision: 0.8977968176254589, f_beta: 0.8894583458948848
train: step: 267, loss: 0.10202240943908691, acc: 0.85, recall: 0.85, precision: 0.8501400560224089, f_beta: 0.84998499849985
train: step: 268, loss: 0.12786178290843964, acc: 0.85, recall: 0.85, precision: 0.8501400560224089, f_beta: 0.84998499849985
train: step: 269, loss: 0.13263669610023499, acc: 0.88, recall: 0.88, precision: 0.8824476650563606, f_beta: 0.8798076923076923
train: step: 270, loss: 0.12207412719726562, acc: 0.82, recall: 0.8200000000000001, precision: 0.8246753246753247, f_beta: 0.8193496587715776
train: step: 271, loss: 0.13888095319271088, acc: 0.81, recall: 0.81, precision: 0.8131313131313131, f_beta: 0.8095238095238095
train: step: 272, loss: 0.3294755518436432, acc: 0.77, recall: 0.77, precision: 0.7895752895752897, f_beta: 0.7660461804495982
train: step: 273, loss: 0.14270542562007904, acc: 0.8, recall: 0.8, precision: 0.8078817733990147, f_beta: 0.7987117552334944
train: step: 274, loss: 0.16353237628936768, acc: 0.76, recall: 0.76, precision: 0.7616747181964574, f_beta: 0.7596153846153846
train: step: 275, loss: 0.16889835894107819, acc: 0.77, recall: 0.77, precision: 0.7967032967032968, f_beta: 0.7647058823529411
train: step: 276, loss: 0.10346082597970963, acc: 0.86, recall: 0.8600000000000001, precision: 0.8652597402597402, f_beta: 0.8594941790445605
train: step: 277, loss: 0.10670879483222961, acc: 0.87, recall: 0.87, precision: 0.8773969808241534, f_beta: 0.8693598633303186
train: step: 278, loss: 0.11520292609930038, acc: 0.85, recall: 0.85, precision: 0.851264552388599, f_beta: 0.8498648783905516
train: step: 279, loss: 0.10615295171737671, acc: 0.89, recall: 0.8899999999999999, precision: 0.8939393939393939, f_beta: 0.8897243107769424
train: step: 280, loss: 0.14075683057308197, acc: 0.8, recall: 0.8, precision: 0.8125, f_beta: 0.797979797979798
train: step: 281, loss: 0.14356862008571625, acc: 0.82, recall: 0.8200000000000001, precision: 0.8333333333333334, f_beta: 0.8181818181818181
train: step: 282, loss: 0.11235881596803665, acc: 0.84, recall: 0.84, precision: 0.84, f_beta: 0.8399999999999999
train: step: 283, loss: 0.18950162827968597, acc: 0.84, recall: 0.84, precision: 0.8541666666666667, f_beta: 0.8383838383838385
train: step: 284, loss: 0.10042329132556915, acc: 0.86, recall: 0.86, precision: 0.8623188405797102, f_beta: 0.859775641025641
train: step: 285, loss: 0.08904507756233215, acc: 0.91, recall: 0.9099999999999999, precision: 0.9181966544267646, f_beta: 0.9095568284594513
train: step: 286, loss: 0.12399263679981232, acc: 0.78, recall: 0.78, precision: 0.8038194444444444, f_beta: 0.7756017951856385
train: step: 287, loss: 0.10496833175420761, acc: 0.87, recall: 0.87, precision: 0.8713368125250903, f_beta: 0.8698828946051447
train: step: 288, loss: 0.10629201680421829, acc: 0.84, recall: 0.84, precision: 0.8607809847198642, f_beta: 0.8376623376623376
train: step: 289, loss: 0.12050960212945938, acc: 0.83, recall: 0.8300000000000001, precision: 0.8333333333333334, f_beta: 0.8295739348370927
train: step: 290, loss: 0.13789939880371094, acc: 0.81, recall: 0.81, precision: 0.8131313131313131, f_beta: 0.8095238095238095
train: step: 291, loss: 0.09631309658288956, acc: 0.9, recall: 0.8999999999999999, precision: 0.9058441558441559, f_beta: 0.8996386993175431
train: step: 292, loss: 0.10098337382078171, acc: 0.87, recall: 0.87, precision: 0.8773969808241534, f_beta: 0.8693598633303186
train: step: 293, loss: 0.12347362190485, acc: 0.83, recall: 0.8300000000000001, precision: 0.8410500206696983, f_beta: 0.8286117552172598
train: step: 294, loss: 0.06951311975717545, acc: 0.9, recall: 0.9, precision: 0.9, f_beta: 0.9
train: step: 295, loss: 0.0855458453297615, acc: 0.91, recall: 0.91, precision: 0.9101640656262505, f_beta: 0.90999099909991
train: step: 296, loss: 0.11388006061315536, acc: 0.82, recall: 0.82, precision: 0.8395585738539898, f_beta: 0.8173701298701298
train: step: 297, loss: 0.11256446689367294, acc: 0.84, recall: 0.84, precision: 0.84, f_beta: 0.8399999999999999
train: step: 298, loss: 0.13095945119857788, acc: 0.84, recall: 0.8400000000000001, precision: 0.8421900161030595, f_beta: 0.8397435897435896
train: step: 299, loss: 0.14602619409561157, acc: 0.82, recall: 0.8200000000000001, precision: 0.822061191626409, f_beta: 0.8197115384615385
train: step: 300, loss: 0.13007134199142456, acc: 0.81, recall: 0.81, precision: 0.8257671290458175, f_beta: 0.8076728413807065
train: step: 301, loss: 0.06689649820327759, acc: 0.92, recall: 0.9199999999999999, precision: 0.9206730769230769, f_beta: 0.919967987194878
train: step: 302, loss: 0.11412674188613892, acc: 0.85, recall: 0.8500000000000001, precision: 0.8535353535353536, f_beta: 0.849624060150376
train: step: 303, loss: 0.13133950531482697, acc: 0.88, recall: 0.88, precision: 0.8824476650563606, f_beta: 0.8798076923076923
train: step: 304, loss: 0.11488300561904907, acc: 0.88, recall: 0.8799999999999999, precision: 0.885551948051948, f_beta: 0.8795664391810518
train: step: 305, loss: 0.09083747863769531, acc: 0.9, recall: 0.9, precision: 0.9006410256410255, f_beta: 0.8999599839935974
train: step: 306, loss: 0.08124103397130966, acc: 0.9, recall: 0.9, precision: 0.9006410256410255, f_beta: 0.8999599839935974
train: step: 307, loss: 0.09794985502958298, acc: 0.86, recall: 0.86, precision: 0.86, f_beta: 0.8599999999999999
train: step: 308, loss: 0.1145063191652298, acc: 0.84, recall: 0.84, precision: 0.84, f_beta: 0.8399999999999999
train: step: 309, loss: 0.15300717949867249, acc: 0.8, recall: 0.8, precision: 0.8043831168831169, f_beta: 0.7992773986350863
train: step: 310, loss: 0.13307346403598785, acc: 0.81, recall: 0.81, precision: 0.8111200321156162, f_beta: 0.8098288459613652
train: step: 311, loss: 0.1259583979845047, acc: 0.87, recall: 0.87, precision: 0.8737373737373737, f_beta: 0.8696741854636592
train: step: 312, loss: 0.1512449085712433, acc: 0.79, recall: 0.79, precision: 0.7997106242248864, f_beta: 0.7882851093860268
train: step: 313, loss: 0.13809604942798615, acc: 0.83, recall: 0.83, precision: 0.8301320528211285, f_beta: 0.8299829982998299
train: step: 314, loss: 0.12740762531757355, acc: 0.81, recall: 0.81, precision: 0.8257671290458175, f_beta: 0.8076728413807065
train: step: 315, loss: 0.061214372515678406, acc: 0.91, recall: 0.9099999999999999, precision: 0.9141414141414141, f_beta: 0.9097744360902256
train: step: 316, loss: 0.12406989932060242, acc: 0.84, recall: 0.84, precision: 0.8405448717948718, f_beta: 0.8399359743897559
train: step: 317, loss: 0.10656316578388214, acc: 0.87, recall: 0.87, precision: 0.8737373737373737, f_beta: 0.8696741854636592
train: step: 318, loss: 0.1604623794555664, acc: 0.78, recall: 0.78, precision: 0.7840909090909092, f_beta: 0.779205138498595
train: step: 319, loss: 0.09491491317749023, acc: 0.88, recall: 0.88, precision: 0.8806089743589745, f_beta: 0.879951980792317
train: step: 320, loss: 0.10730575770139694, acc: 0.86, recall: 0.8600000000000001, precision: 0.8652597402597402, f_beta: 0.8594941790445605
train: step: 321, loss: 0.12356747686862946, acc: 0.83, recall: 0.83, precision: 0.8301320528211285, f_beta: 0.8299829982998299
train: step: 322, loss: 0.09304536879062653, acc: 0.89, recall: 0.8899999999999999, precision: 0.8939393939393939, f_beta: 0.8897243107769424
train: step: 323, loss: 0.1785181611776352, acc: 0.74, recall: 0.74, precision: 0.7403846153846154, f_beta: 0.7398959583833533
train: step: 324, loss: 0.07959678024053574, acc: 0.89, recall: 0.8899999999999999, precision: 0.8939393939393939, f_beta: 0.8897243107769424
train: step: 325, loss: 0.09756950289011002, acc: 0.86, recall: 0.8600000000000001, precision: 0.8652597402597402, f_beta: 0.8594941790445605
train: step: 326, loss: 0.14334510266780853, acc: 0.82, recall: 0.8200000000000001, precision: 0.8284072249589491, f_beta: 0.818840579710145
train: step: 327, loss: 0.07097276300191879, acc: 0.91, recall: 0.9099999999999999, precision: 0.9114813327980731, f_beta: 0.9099189270343309
train: step: 328, loss: 0.1115129366517067, acc: 0.84, recall: 0.84, precision: 0.8405448717948718, f_beta: 0.8399359743897559
train: step: 329, loss: 0.06868748366832733, acc: 0.92, recall: 0.92, precision: 0.92, f_beta: 0.92
train: step: 330, loss: 0.07311098277568817, acc: 0.9, recall: 0.8999999999999999, precision: 0.9058441558441559, f_beta: 0.8996386993175431
train: step: 331, loss: 0.14546021819114685, acc: 0.79, recall: 0.79, precision: 0.7901160464185675, f_beta: 0.78997899789979
train: step: 332, loss: 0.05933018773794174, acc: 0.9, recall: 0.9, precision: 0.9006410256410255, f_beta: 0.8999599839935974
train: step: 333, loss: 0.11891188472509384, acc: 0.85, recall: 0.8500000000000001, precision: 0.8535353535353536, f_beta: 0.849624060150376
train: step: 334, loss: 0.0849156379699707, acc: 0.86, recall: 0.86, precision: 0.86, f_beta: 0.8599999999999999
train: step: 335, loss: 0.05543169379234314, acc: 0.92, recall: 0.9199999999999999, precision: 0.9206730769230769, f_beta: 0.919967987194878
train: step: 336, loss: 0.1403721421957016, acc: 0.83, recall: 0.83, precision: 0.8301320528211285, f_beta: 0.8299829982998299
train: step: 337, loss: 0.12035392969846725, acc: 0.84, recall: 0.84, precision: 0.84, f_beta: 0.8399999999999999
train: step: 338, loss: 0.10880973190069199, acc: 0.87, recall: 0.87, precision: 0.8701480592236894, f_beta: 0.8699869986998701
train: step: 339, loss: 0.12264939397573471, acc: 0.8, recall: 0.8, precision: 0.8043831168831169, f_beta: 0.7992773986350863
train: step: 340, loss: 0.09469639509916306, acc: 0.87, recall: 0.87, precision: 0.8701480592236894, f_beta: 0.8699869986998701
train: step: 341, loss: 0.08305101096630096, acc: 0.93, recall: 0.9299999999999999, precision: 0.9315535929345644, f_beta: 0.9299369432489241
train: step: 342, loss: 0.09146805107593536, acc: 0.88, recall: 0.88, precision: 0.8806089743589745, f_beta: 0.879951980792317
train: step: 343, loss: 0.10766860842704773, acc: 0.84, recall: 0.8400000000000001, precision: 0.8449675324675325, f_beta: 0.839421918908069
train: step: 344, loss: 0.03838386386632919, acc: 0.95, recall: 0.95, precision: 0.9501800720288115, f_beta: 0.9499949994999499
train: step: 345, loss: 0.05966930463910103, acc: 0.91, recall: 0.9099999999999999, precision: 0.9181966544267646, f_beta: 0.9095568284594513
train: step: 346, loss: 0.1060723215341568, acc: 0.88, recall: 0.88, precision: 0.8806089743589745, f_beta: 0.879951980792317
train: step: 347, loss: 0.09708507359027863, acc: 0.83, recall: 0.8300000000000001, precision: 0.8311922922521076, f_beta: 0.8298468621759585
train: step: 348, loss: 0.11968346685171127, acc: 0.84, recall: 0.84, precision: 0.8405448717948718, f_beta: 0.8399359743897559
train: step: 349, loss: 0.09448354691267014, acc: 0.86, recall: 0.86, precision: 0.8623188405797102, f_beta: 0.859775641025641
train: step: 350, loss: 0.11799917370080948, acc: 0.85, recall: 0.85, precision: 0.8617197188921042, f_beta: 0.8487750781328762
train: step: 351, loss: 0.13972130417823792, acc: 0.84, recall: 0.8400000000000001, precision: 0.8489326765188834, f_beta: 0.8389694041867956
train: step: 352, loss: 0.0900472104549408, acc: 0.86, recall: 0.86, precision: 0.8623188405797102, f_beta: 0.859775641025641
train: step: 353, loss: 0.09369520097970963, acc: 0.88, recall: 0.8799999999999999, precision: 0.885551948051948, f_beta: 0.8795664391810518
train: step: 354, loss: 0.08942443877458572, acc: 0.87, recall: 0.87, precision: 0.8713368125250903, f_beta: 0.8698828946051447
train: step: 355, loss: 0.1252235621213913, acc: 0.82, recall: 0.8200000000000001, precision: 0.822061191626409, f_beta: 0.8197115384615385
train: step: 356, loss: 0.09504369646310806, acc: 0.88, recall: 0.88, precision: 0.8806089743589745, f_beta: 0.879951980792317
train: step: 357, loss: 0.08445369452238083, acc: 0.89, recall: 0.89, precision: 0.8914090726615818, f_beta: 0.8899009108197378
train: step: 358, loss: 0.0923418253660202, acc: 0.86, recall: 0.86, precision: 0.8623188405797102, f_beta: 0.859775641025641
train: step: 359, loss: 0.10053917020559311, acc: 0.88, recall: 0.88, precision: 0.88, f_beta: 0.88
train: step: 360, loss: 0.11775092780590057, acc: 0.84, recall: 0.8400000000000001, precision: 0.8421900161030595, f_beta: 0.8397435897435896
train: step: 361, loss: 0.11675677448511124, acc: 0.84, recall: 0.84, precision: 0.8689236111111112, f_beta: 0.8368013055895552
train: step: 362, loss: 0.06549624353647232, acc: 0.92, recall: 0.9199999999999999, precision: 0.9227053140096618, f_beta: 0.9198717948717948
train: step: 363, loss: 0.0688248872756958, acc: 0.93, recall: 0.9299999999999999, precision: 0.930172068827531, f_beta: 0.9299929992999301
train: step: 364, loss: 0.09174264967441559, acc: 0.91, recall: 0.91, precision: 0.9101640656262505, f_beta: 0.90999099909991
train: step: 365, loss: 0.0860968753695488, acc: 0.89, recall: 0.8899999999999999, precision: 0.8939393939393939, f_beta: 0.8897243107769424
train: step: 366, loss: 0.10545690357685089, acc: 0.85, recall: 0.85, precision: 0.8501400560224089, f_beta: 0.84998499849985
train: step: 367, loss: 0.12110166251659393, acc: 0.86, recall: 0.8600000000000001, precision: 0.8652597402597402, f_beta: 0.8594941790445605
train: step: 368, loss: 0.07970990240573883, acc: 0.91, recall: 0.91, precision: 0.9101640656262505, f_beta: 0.90999099909991
train: step: 369, loss: 0.09453828632831573, acc: 0.84, recall: 0.84, precision: 0.8405448717948718, f_beta: 0.8399359743897559
train: step: 370, loss: 0.1009330302476883, acc: 0.83, recall: 0.8300000000000001, precision: 0.8410500206696983, f_beta: 0.8286117552172598
train: step: 371, loss: 0.0582013763487339, acc: 0.93, recall: 0.9299999999999999, precision: 0.9315535929345644, f_beta: 0.9299369432489241
train: step: 372, loss: 0.09760700911283493, acc: 0.87, recall: 0.87, precision: 0.8701480592236894, f_beta: 0.8699869986998701
train: step: 373, loss: 0.16189761459827423, acc: 0.79, recall: 0.79, precision: 0.7997106242248864, f_beta: 0.7882851093860268
train: step: 374, loss: 0.10976701974868774, acc: 0.81, recall: 0.81, precision: 0.810124049619848, f_beta: 0.8099809980998101
train: step: 375, loss: 0.13709516823291779, acc: 0.8, recall: 0.8, precision: 0.8004807692307692, f_beta: 0.7999199679871949
train: step: 376, loss: 0.09442057460546494, acc: 0.89, recall: 0.8899999999999999, precision: 0.8939393939393939, f_beta: 0.8897243107769424
train: step: 377, loss: 0.11906280368566513, acc: 0.81, recall: 0.81, precision: 0.8131313131313131, f_beta: 0.8095238095238095
train: step: 378, loss: 0.0967448428273201, acc: 0.88, recall: 0.88, precision: 0.88, f_beta: 0.88
train: step: 379, loss: 0.10009279102087021, acc: 0.84, recall: 0.8400000000000001, precision: 0.8421900161030595, f_beta: 0.8397435897435896
train: step: 380, loss: 0.09109598398208618, acc: 0.86, recall: 0.8600000000000001, precision: 0.8652597402597402, f_beta: 0.8594941790445605
train: step: 381, loss: 0.08252760767936707, acc: 0.9, recall: 0.8999999999999999, precision: 0.9058441558441559, f_beta: 0.8996386993175431
train: step: 382, loss: 0.10345491766929626, acc: 0.84, recall: 0.8400000000000001, precision: 0.8421900161030595, f_beta: 0.8397435897435896
train: step: 383, loss: 0.09998444467782974, acc: 0.89, recall: 0.89, precision: 0.89015606242497, f_beta: 0.88998899889989
train: step: 384, loss: 0.09381280839443207, acc: 0.88, recall: 0.88, precision: 0.88, f_beta: 0.88
train: step: 385, loss: 0.1084175780415535, acc: 0.85, recall: 0.85, precision: 0.8501400560224089, f_beta: 0.84998499849985
train: step: 386, loss: 0.11972782760858536, acc: 0.84, recall: 0.8400000000000001, precision: 0.8421900161030595, f_beta: 0.8397435897435896
train: step: 387, loss: 0.11089546233415604, acc: 0.84, recall: 0.8400000000000001, precision: 0.8489326765188834, f_beta: 0.8389694041867956
train: step: 388, loss: 0.12096738815307617, acc: 0.84, recall: 0.8400000000000001, precision: 0.8489326765188834, f_beta: 0.8389694041867956
train: step: 389, loss: 0.07969753444194794, acc: 0.88, recall: 0.8799999999999999, precision: 0.885551948051948, f_beta: 0.8795664391810518
train: step: 390, loss: 0.13554182648658752, acc: 0.8, recall: 0.8, precision: 0.8, f_beta: 0.8000000000000002
train: step: 391, loss: 0.0705704465508461, acc: 0.91, recall: 0.91, precision: 0.9101640656262505, f_beta: 0.90999099909991
train: step: 392, loss: 0.1258551925420761, acc: 0.82, recall: 0.8200000000000001, precision: 0.85650623885918, f_beta: 0.8152709359605911
train: step: 393, loss: 0.09684497863054276, acc: 0.85, recall: 0.85, precision: 0.8501400560224089, f_beta: 0.84998499849985
train: step: 394, loss: 0.059247393161058426, acc: 0.92, recall: 0.9199999999999999, precision: 0.9227053140096618, f_beta: 0.9198717948717948
train: step: 395, loss: 0.16586455702781677, acc: 0.79, recall: 0.79, precision: 0.7929292929292929, f_beta: 0.7894736842105263
train: step: 396, loss: 0.10790450125932693, acc: 0.87, recall: 0.87, precision: 0.8701480592236894, f_beta: 0.8699869986998701
train: step: 397, loss: 0.14906929433345795, acc: 0.79, recall: 0.79, precision: 0.7901160464185675, f_beta: 0.78997899789979
train: step: 398, loss: 0.11656155437231064, acc: 0.9, recall: 0.8999999999999999, precision: 0.9025764895330113, f_beta: 0.8998397435897436
train: step: 399, loss: 0.04693073406815529, acc: 0.94, recall: 0.94, precision: 0.9428341384863124, f_beta: 0.9399038461538463
train: step: 400, loss: 0.07712879031896591, acc: 0.9, recall: 0.9, precision: 0.9006410256410255, f_beta: 0.8999599839935974
train: step: 401, loss: 0.12672396004199982, acc: 0.81, recall: 0.81, precision: 0.810124049619848, f_beta: 0.8099809980998101
train: step: 402, loss: 0.08710721880197525, acc: 0.89, recall: 0.89, precision: 0.89015606242497, f_beta: 0.88998899889989
train: step: 403, loss: 0.1372659057378769, acc: 0.8, recall: 0.8, precision: 0.8004807692307692, f_beta: 0.7999199679871949
train: step: 404, loss: 0.09012287110090256, acc: 0.89, recall: 0.8899999999999999, precision: 0.8977968176254589, f_beta: 0.8894583458948848
train: step: 405, loss: 0.08997568488121033, acc: 0.87, recall: 0.87, precision: 0.8773969808241534, f_beta: 0.8693598633303186
train: step: 406, loss: 0.106361024081707, acc: 0.84, recall: 0.8400000000000001, precision: 0.8449675324675325, f_beta: 0.839421918908069
train: step: 407, loss: 0.07444717735052109, acc: 0.9, recall: 0.8999999999999999, precision: 0.9025764895330113, f_beta: 0.8998397435897436
train: step: 408, loss: 0.06408901512622833, acc: 0.93, recall: 0.9299999999999999, precision: 0.930172068827531, f_beta: 0.9299929992999301
train: step: 409, loss: 0.1106347143650055, acc: 0.83, recall: 0.83, precision: 0.8301320528211285, f_beta: 0.8299829982998299
train: step: 410, loss: 0.04479946568608284, acc: 0.95, recall: 0.95, precision: 0.9501800720288115, f_beta: 0.9499949994999499
train: step: 411, loss: 0.12347663193941116, acc: 0.83, recall: 0.83, precision: 0.8301320528211285, f_beta: 0.8299829982998299
train: step: 412, loss: 0.08319742977619171, acc: 0.91, recall: 0.9099999999999999, precision: 0.9114813327980731, f_beta: 0.9099189270343309
train: step: 413, loss: 0.09336000680923462, acc: 0.86, recall: 0.86, precision: 0.8694581280788177, f_beta: 0.8590982286634461
train: step: 414, loss: 0.16660627722740173, acc: 0.79, recall: 0.79, precision: 0.7901160464185675, f_beta: 0.78997899789979
train: step: 415, loss: 0.09907501935958862, acc: 0.88, recall: 0.88, precision: 0.8806089743589745, f_beta: 0.879951980792317
train: step: 416, loss: 0.10817638039588928, acc: 0.86, recall: 0.86, precision: 0.8605769230769231, f_beta: 0.8599439775910365
train: step: 417, loss: 0.11161205917596817, acc: 0.87, recall: 0.87, precision: 0.8713368125250903, f_beta: 0.8698828946051447
train: step: 418, loss: 0.07948198169469833, acc: 0.89, recall: 0.8899999999999999, precision: 0.8939393939393939, f_beta: 0.8897243107769424
train: step: 419, loss: 0.10696443915367126, acc: 0.86, recall: 0.86, precision: 0.8694581280788177, f_beta: 0.8590982286634461
train: step: 420, loss: 0.11868619173765182, acc: 0.83, recall: 0.83, precision: 0.8301320528211285, f_beta: 0.8299829982998299
train: step: 421, loss: 0.26105085015296936, acc: 0.84, recall: 0.84, precision: 0.84, f_beta: 0.8399999999999999
train: step: 422, loss: 0.058451004326343536, acc: 0.93, recall: 0.9299999999999999, precision: 0.9343434343434343, f_beta: 0.9298245614035088
train: step: 423, loss: 0.07953234761953354, acc: 0.92, recall: 0.9199999999999999, precision: 0.9227053140096618, f_beta: 0.9198717948717948
train: step: 424, loss: 0.08574098348617554, acc: 0.86, recall: 0.86, precision: 0.8623188405797102, f_beta: 0.859775641025641
train: step: 425, loss: 0.11904004961252213, acc: 0.82, recall: 0.82, precision: 0.82, f_beta: 0.82
train: step: 426, loss: 0.08567214012145996, acc: 0.87, recall: 0.87, precision: 0.8737373737373737, f_beta: 0.8696741854636592
train: step: 427, loss: 0.09761039912700653, acc: 0.86, recall: 0.86, precision: 0.86, f_beta: 0.8599999999999999
train: step: 428, loss: 0.11966385692358017, acc: 0.83, recall: 0.83, precision: 0.8301320528211285, f_beta: 0.8299829982998299
train: step: 429, loss: 0.08268611878156662, acc: 0.89, recall: 0.8899999999999999, precision: 0.8977968176254589, f_beta: 0.8894583458948848
train: step: 430, loss: 0.13693416118621826, acc: 0.82, recall: 0.82, precision: 0.82, f_beta: 0.82
train: step: 431, loss: 0.1019158735871315, acc: 0.85, recall: 0.85, precision: 0.8617197188921042, f_beta: 0.8487750781328762
train: step: 432, loss: 0.11545651406049728, acc: 0.86, recall: 0.86, precision: 0.8605769230769231, f_beta: 0.8599439775910365
train: step: 433, loss: 0.1251496821641922, acc: 0.86, recall: 0.86, precision: 0.8623188405797102, f_beta: 0.859775641025641
train: step: 434, loss: 0.09590413421392441, acc: 0.89, recall: 0.89, precision: 0.89015606242497, f_beta: 0.88998899889989
train: step: 435, loss: 0.10778834670782089, acc: 0.87, recall: 0.87, precision: 0.8713368125250903, f_beta: 0.8698828946051447
train: step: 436, loss: 0.12727540731430054, acc: 0.8, recall: 0.8, precision: 0.8019323671497585, f_beta: 0.7996794871794872
train: step: 437, loss: 0.10493236780166626, acc: 0.86, recall: 0.8600000000000001, precision: 0.8652597402597402, f_beta: 0.8594941790445605
train: step: 438, loss: 0.08442947268486023, acc: 0.91, recall: 0.9099999999999999, precision: 0.9141414141414141, f_beta: 0.9097744360902256
train: step: 439, loss: 0.10075651109218597, acc: 0.86, recall: 0.86, precision: 0.86, f_beta: 0.8599999999999999
train: step: 440, loss: 0.0850297063589096, acc: 0.9, recall: 0.8999999999999999, precision: 0.9058441558441559, f_beta: 0.8996386993175431
train: step: 441, loss: 0.09453695267438889, acc: 0.89, recall: 0.89, precision: 0.89015606242497, f_beta: 0.88998899889989
train: step: 442, loss: 0.07956010848283768, acc: 0.89, recall: 0.89, precision: 0.8914090726615818, f_beta: 0.8899009108197378
train: step: 443, loss: 0.09164667129516602, acc: 0.87, recall: 0.87, precision: 0.8713368125250903, f_beta: 0.8698828946051447
train: step: 444, loss: 0.10108187049627304, acc: 0.88, recall: 0.88, precision: 0.88, f_beta: 0.88
train: step: 445, loss: 0.12411350011825562, acc: 0.85, recall: 0.8500000000000001, precision: 0.8569971440228478, f_beta: 0.8492613807657521
train: step: 446, loss: 0.13236598670482635, acc: 0.85, recall: 0.85, precision: 0.8501400560224089, f_beta: 0.84998499849985
train: step: 447, loss: 0.10040653496980667, acc: 0.89, recall: 0.89, precision: 0.8914090726615818, f_beta: 0.8899009108197378
train: step: 448, loss: 0.10097449272871017, acc: 0.88, recall: 0.88, precision: 0.88, f_beta: 0.88
train: step: 449, loss: 0.08819825947284698, acc: 0.86, recall: 0.8600000000000001, precision: 0.8652597402597402, f_beta: 0.8594941790445605
train: step: 450, loss: 0.09391286969184875, acc: 0.89, recall: 0.89, precision: 0.89015606242497, f_beta: 0.88998899889989
train: step: 451, loss: 0.09180568903684616, acc: 0.88, recall: 0.8799999999999999, precision: 0.885551948051948, f_beta: 0.8795664391810518
train: step: 452, loss: 0.09271647781133652, acc: 0.89, recall: 0.89, precision: 0.8914090726615818, f_beta: 0.8899009108197378
train: step: 453, loss: 0.10765277594327927, acc: 0.84, recall: 0.8400000000000001, precision: 0.8421900161030595, f_beta: 0.8397435897435896
train: step: 454, loss: 0.11408981680870056, acc: 0.83, recall: 0.83, precision: 0.8301320528211285, f_beta: 0.8299829982998299
train: step: 455, loss: 0.11343919485807419, acc: 0.84, recall: 0.84, precision: 0.8405448717948718, f_beta: 0.8399359743897559
train: step: 456, loss: 0.10508231818675995, acc: 0.88, recall: 0.88, precision: 0.8806089743589745, f_beta: 0.879951980792317
train: step: 457, loss: 0.0660499855875969, acc: 0.91, recall: 0.91, precision: 0.9101640656262505, f_beta: 0.90999099909991
train: step: 458, loss: 0.11107350140810013, acc: 0.86, recall: 0.86, precision: 0.86, f_beta: 0.8599999999999999
train: step: 459, loss: 0.11164978891611099, acc: 0.88, recall: 0.88, precision: 0.88, f_beta: 0.88
train: step: 460, loss: 0.13078351318836212, acc: 0.85, recall: 0.85, precision: 0.867801597309794, f_beta: 0.848162769511084
train: step: 461, loss: 0.07405167818069458, acc: 0.89, recall: 0.89, precision: 0.89015606242497, f_beta: 0.88998899889989
train: step: 462, loss: 0.05766400322318077, acc: 0.94, recall: 0.94, precision: 0.9428341384863124, f_beta: 0.9399038461538463
train: step: 463, loss: 0.10194138437509537, acc: 0.86, recall: 0.86, precision: 0.86, f_beta: 0.8599999999999999
train: step: 464, loss: 0.1320495903491974, acc: 0.84, recall: 0.84, precision: 0.84, f_beta: 0.8399999999999999
train: step: 465, loss: 0.11567211151123047, acc: 0.86, recall: 0.86, precision: 0.86, f_beta: 0.8599999999999999
train: step: 466, loss: 0.1316409707069397, acc: 0.81, recall: 0.81, precision: 0.8131313131313131, f_beta: 0.8095238095238095
train: step: 467, loss: 0.11195854097604752, acc: 0.85, recall: 0.8500000000000001, precision: 0.8569971440228478, f_beta: 0.8492613807657521
train: step: 468, loss: 0.10259267687797546, acc: 0.84, recall: 0.8400000000000001, precision: 0.8421900161030595, f_beta: 0.8397435897435896
train: step: 469, loss: 0.09878459572792053, acc: 0.86, recall: 0.86, precision: 0.8605769230769231, f_beta: 0.8599439775910365
train: step: 470, loss: 0.05113214626908302, acc: 0.94, recall: 0.94, precision: 0.9428341384863124, f_beta: 0.9399038461538463
train: step: 471, loss: 0.08515629917383194, acc: 0.89, recall: 0.89, precision: 0.8914090726615818, f_beta: 0.8899009108197378
train: step: 472, loss: 0.111026331782341, acc: 0.84, recall: 0.84, precision: 0.8405448717948718, f_beta: 0.8399359743897559
train: step: 473, loss: 0.09728792309761047, acc: 0.85, recall: 0.85, precision: 0.8501400560224089, f_beta: 0.84998499849985
train: step: 474, loss: 0.039554450660943985, acc: 0.96, recall: 0.96, precision: 0.9607371794871795, f_beta: 0.959983993597439
train: step: 475, loss: 0.10052288323640823, acc: 0.89, recall: 0.89, precision: 0.9030591153369161, f_beta: 0.8891017239641092
train: step: 476, loss: 0.10297553986310959, acc: 0.86, recall: 0.86, precision: 0.86, f_beta: 0.8599999999999999
train: step: 477, loss: 0.07907387614250183, acc: 0.87, recall: 0.87, precision: 0.8701480592236894, f_beta: 0.8699869986998701
train: step: 478, loss: 0.110785111784935, acc: 0.84, recall: 0.8400000000000001, precision: 0.8489326765188834, f_beta: 0.8389694041867956
train: step: 479, loss: 0.10750152915716171, acc: 0.85, recall: 0.85, precision: 0.851264552388599, f_beta: 0.8498648783905516
train: step: 480, loss: 0.08105272054672241, acc: 0.88, recall: 0.88, precision: 0.8824476650563606, f_beta: 0.8798076923076923
train: step: 481, loss: 0.11981933563947678, acc: 0.83, recall: 0.8300000000000001, precision: 0.8365973072215422, f_beta: 0.8291628982011858
train: step: 482, loss: 0.09859517961740494, acc: 0.86, recall: 0.8600000000000001, precision: 0.8652597402597402, f_beta: 0.8594941790445605
train: step: 483, loss: 0.09254902601242065, acc: 0.86, recall: 0.86, precision: 0.8605769230769231, f_beta: 0.8599439775910365
train: step: 484, loss: 0.09284836053848267, acc: 0.89, recall: 0.89, precision: 0.8914090726615818, f_beta: 0.8899009108197378
train: step: 485, loss: 0.09833437949419022, acc: 0.85, recall: 0.85, precision: 0.8501400560224089, f_beta: 0.84998499849985
train: step: 486, loss: 0.07722163945436478, acc: 0.9, recall: 0.8999999999999999, precision: 0.9025764895330113, f_beta: 0.8998397435897436
train: step: 487, loss: 0.1066240668296814, acc: 0.86, recall: 0.86, precision: 0.875, f_beta: 0.8585858585858586
train: step: 488, loss: 0.05783126875758171, acc: 0.91, recall: 0.91, precision: 0.9101640656262505, f_beta: 0.90999099909991
train: step: 489, loss: 0.08047941327095032, acc: 0.9, recall: 0.8999999999999999, precision: 0.9025764895330113, f_beta: 0.8998397435897436
train: step: 490, loss: 0.08596418052911758, acc: 0.9, recall: 0.9, precision: 0.9006410256410255, f_beta: 0.8999599839935974
train: step: 491, loss: 0.07518922537565231, acc: 0.92, recall: 0.9199999999999999, precision: 0.9206730769230769, f_beta: 0.919967987194878
train: step: 492, loss: 0.04281296208500862, acc: 0.95, recall: 0.95, precision: 0.9501800720288115, f_beta: 0.9499949994999499
train: step: 493, loss: 0.11252955347299576, acc: 0.84, recall: 0.84, precision: 0.8405448717948718, f_beta: 0.8399359743897559
train: step: 494, loss: 0.10839462280273438, acc: 0.85, recall: 0.8500000000000001, precision: 0.8535353535353536, f_beta: 0.849624060150376
train: step: 495, loss: 0.09277653694152832, acc: 0.9, recall: 0.9, precision: 0.9, f_beta: 0.9
train: step: 496, loss: 0.07125713676214218, acc: 0.91, recall: 0.9099999999999999, precision: 0.9114813327980731, f_beta: 0.9099189270343309
train: step: 497, loss: 0.10909993201494217, acc: 0.83, recall: 0.8300000000000001, precision: 0.8333333333333334, f_beta: 0.8295739348370927
train: step: 498, loss: 0.11538659781217575, acc: 0.84, recall: 0.84, precision: 0.8405448717948718, f_beta: 0.8399359743897559
train: step: 499, loss: 0.05376867204904556, acc: 0.93, recall: 0.9299999999999999, precision: 0.9343434343434343, f_beta: 0.9298245614035088
train: step: 500, loss: 0.09393537789583206, acc: 0.86, recall: 0.8600000000000001, precision: 0.8652597402597402, f_beta: 0.8594941790445605
train: step: 501, loss: 0.10429108887910843, acc: 0.87, recall: 0.87, precision: 0.8737373737373737, f_beta: 0.8696741854636592
train: step: 502, loss: 0.10230056941509247, acc: 0.87, recall: 0.87, precision: 0.8701480592236894, f_beta: 0.8699869986998701
train: step: 503, loss: 0.0578378289937973, acc: 0.92, recall: 0.92, precision: 0.92, f_beta: 0.92
train: step: 504, loss: 0.09177079796791077, acc: 0.86, recall: 0.86, precision: 0.8694581280788177, f_beta: 0.8590982286634461
train: step: 505, loss: 0.08850423991680145, acc: 0.91, recall: 0.9099999999999999, precision: 0.9114813327980731, f_beta: 0.9099189270343309
train: step: 506, loss: 0.09723163396120071, acc: 0.87, recall: 0.87, precision: 0.8773969808241534, f_beta: 0.8693598633303186
train: step: 507, loss: 0.08401380479335785, acc: 0.89, recall: 0.89, precision: 0.8914090726615818, f_beta: 0.8899009108197378
train: step: 508, loss: 0.07553336769342422, acc: 0.91, recall: 0.9099999999999999, precision: 0.9114813327980731, f_beta: 0.9099189270343309
train: step: 509, loss: 0.07382556796073914, acc: 0.89, recall: 0.8899999999999999, precision: 0.8939393939393939, f_beta: 0.8897243107769424
train: step: 510, loss: 0.10211696475744247, acc: 0.88, recall: 0.88, precision: 0.8806089743589745, f_beta: 0.879951980792317
train: step: 511, loss: 0.11047334969043732, acc: 0.86, recall: 0.86, precision: 0.8623188405797102, f_beta: 0.859775641025641
train: step: 512, loss: 0.094635508954525, acc: 0.86, recall: 0.86, precision: 0.8605769230769231, f_beta: 0.8599439775910365
train: step: 513, loss: 0.0897931456565857, acc: 0.89, recall: 0.89, precision: 0.8914090726615818, f_beta: 0.8899009108197378
train: step: 514, loss: 0.11270309239625931, acc: 0.88, recall: 0.88, precision: 0.88, f_beta: 0.88
train: step: 515, loss: 0.11890609562397003, acc: 0.86, recall: 0.86, precision: 0.8623188405797102, f_beta: 0.859775641025641
train: step: 516, loss: 0.08307752013206482, acc: 0.89, recall: 0.89, precision: 0.89015606242497, f_beta: 0.88998899889989
train: step: 517, loss: 0.0994776040315628, acc: 0.87, recall: 0.87, precision: 0.8701480592236894, f_beta: 0.8699869986998701
train: step: 518, loss: 0.10921373218297958, acc: 0.87, recall: 0.87, precision: 0.8701480592236894, f_beta: 0.8699869986998701
train: step: 519, loss: 0.06429153680801392, acc: 0.92, recall: 0.9199999999999999, precision: 0.9206730769230769, f_beta: 0.919967987194878
train: step: 520, loss: 0.11040551960468292, acc: 0.85, recall: 0.85, precision: 0.8501400560224089, f_beta: 0.84998499849985
train: step: 521, loss: 0.08908478915691376, acc: 0.88, recall: 0.88, precision: 0.8806089743589745, f_beta: 0.879951980792317
train: step: 522, loss: 0.117863729596138, acc: 0.85, recall: 0.85, precision: 0.8501400560224089, f_beta: 0.84998499849985
train: step: 523, loss: 0.14265793561935425, acc: 0.89, recall: 0.89, precision: 0.8914090726615818, f_beta: 0.8899009108197378
train: step: 524, loss: 0.055800534784793854, acc: 0.94, recall: 0.94, precision: 0.9428341384863124, f_beta: 0.9399038461538463
train: step: 525, loss: 0.0979795902967453, acc: 0.87, recall: 0.87, precision: 0.8737373737373737, f_beta: 0.8696741854636592
train: step: 526, loss: 0.10312417894601822, acc: 0.9, recall: 0.8999999999999999, precision: 0.9058441558441559, f_beta: 0.8996386993175431
train: step: 527, loss: 0.06456202268600464, acc: 0.9, recall: 0.8999999999999999, precision: 0.9025764895330113, f_beta: 0.8998397435897436
train: step: 528, loss: 0.07520532608032227, acc: 0.9, recall: 0.9, precision: 0.9006410256410255, f_beta: 0.8999599839935974
train: step: 529, loss: 0.09852157533168793, acc: 0.86, recall: 0.86, precision: 0.8694581280788177, f_beta: 0.8590982286634461
train: step: 530, loss: 0.08722791075706482, acc: 0.9, recall: 0.8999999999999999, precision: 0.9058441558441559, f_beta: 0.8996386993175431
train: step: 531, loss: 0.08691667765378952, acc: 0.89, recall: 0.8899999999999999, precision: 0.8977968176254589, f_beta: 0.8894583458948848
train: step: 532, loss: 0.14968332648277283, acc: 0.78, recall: 0.78, precision: 0.780448717948718, f_beta: 0.7799119647859143
train: step: 533, loss: 0.08232039213180542, acc: 0.9, recall: 0.9, precision: 0.9, f_beta: 0.9
train: step: 534, loss: 0.13667938113212585, acc: 0.87, recall: 0.87, precision: 0.8713368125250903, f_beta: 0.8698828946051447
train: step: 535, loss: 0.06303814798593521, acc: 0.91, recall: 0.9099999999999999, precision: 0.9141414141414141, f_beta: 0.9097744360902256
train: step: 536, loss: 0.07279544323682785, acc: 0.92, recall: 0.9199999999999999, precision: 0.9227053140096618, f_beta: 0.9198717948717948
train: step: 537, loss: 0.11147895455360413, acc: 0.83, recall: 0.83, precision: 0.8467843631778058, f_beta: 0.8279178054458953
train: step: 538, loss: 0.15162675082683563, acc: 0.85, recall: 0.85, precision: 0.8501400560224089, f_beta: 0.84998499849985
train: step: 539, loss: 0.14735586941242218, acc: 0.77, recall: 0.77, precision: 0.8278290432248665, f_beta: 0.7593890574327858
train: step: 540, loss: 0.16110141575336456, acc: 0.81, recall: 0.81, precision: 0.8324753324753325, f_beta: 0.8067338012409724
train: step: 541, loss: 0.0887860357761383, acc: 0.91, recall: 0.9099999999999999, precision: 0.9181966544267646, f_beta: 0.9095568284594513
train: step: 542, loss: 0.12020624428987503, acc: 0.86, recall: 0.86, precision: 0.875, f_beta: 0.8585858585858586
train: step: 543, loss: 0.08872245997190475, acc: 0.88, recall: 0.88, precision: 0.88, f_beta: 0.88
train: step: 544, loss: 0.1172538697719574, acc: 0.84, recall: 0.8400000000000001, precision: 0.8421900161030595, f_beta: 0.8397435897435896
train: step: 545, loss: 0.08851203322410583, acc: 0.88, recall: 0.88, precision: 0.8824476650563606, f_beta: 0.8798076923076923
train: step: 546, loss: 0.1265772581100464, acc: 0.88, recall: 0.88, precision: 0.8806089743589745, f_beta: 0.879951980792317
train: step: 547, loss: 0.06640477478504181, acc: 0.91, recall: 0.91, precision: 0.9101640656262505, f_beta: 0.90999099909991
train: step: 548, loss: 0.09939271956682205, acc: 0.87, recall: 0.87, precision: 0.8701480592236894, f_beta: 0.8699869986998701
train: step: 549, loss: 0.10361862182617188, acc: 0.87, recall: 0.87, precision: 0.8773969808241534, f_beta: 0.8693598633303186
train: step: 550, loss: 0.11463617533445358, acc: 0.85, recall: 0.85, precision: 0.8501400560224089, f_beta: 0.84998499849985
train: step: 551, loss: 0.10333671420812607, acc: 0.86, recall: 0.86, precision: 0.8694581280788177, f_beta: 0.8590982286634461
train: step: 552, loss: 0.06304802000522614, acc: 0.93, recall: 0.9299999999999999, precision: 0.930172068827531, f_beta: 0.9299929992999301
train: step: 553, loss: 0.0947088971734047, acc: 0.88, recall: 0.88, precision: 0.8958333333333333, f_beta: 0.8787878787878789
train: step: 554, loss: 0.10080251842737198, acc: 0.88, recall: 0.88, precision: 0.9032258064516129, f_beta: 0.8782467532467533
train: step: 555, loss: 0.08398059755563736, acc: 0.94, recall: 0.94, precision: 0.9407051282051282, f_beta: 0.9399759903961584
train: step: 556, loss: 0.1424480378627777, acc: 0.8, recall: 0.8, precision: 0.8043831168831169, f_beta: 0.7992773986350863
train: step: 557, loss: 0.09648397564888, acc: 0.89, recall: 0.89, precision: 0.9098360655737705, f_beta: 0.8886526976414617
train: step: 558, loss: 0.11473484337329865, acc: 0.88, recall: 0.88, precision: 0.8806089743589745, f_beta: 0.879951980792317
train: step: 559, loss: 0.2803012430667877, acc: 0.85, recall: 0.8500000000000001, precision: 0.8569971440228478, f_beta: 0.8492613807657521
train: step: 560, loss: 0.07705827057361603, acc: 0.91, recall: 0.9099999999999999, precision: 0.9114813327980731, f_beta: 0.9099189270343309
train: step: 561, loss: 0.13327746093273163, acc: 0.84, recall: 0.8400000000000001, precision: 0.8449675324675325, f_beta: 0.839421918908069
train: step: 562, loss: 0.07227431237697601, acc: 0.93, recall: 0.9299999999999999, precision: 0.930172068827531, f_beta: 0.9299929992999301
train: step: 563, loss: 0.0864693820476532, acc: 0.87, recall: 0.87, precision: 0.8737373737373737, f_beta: 0.8696741854636592
train: step: 564, loss: 0.09349223971366882, acc: 0.85, recall: 0.85, precision: 0.867801597309794, f_beta: 0.848162769511084
train: step: 565, loss: 0.09822273999452591, acc: 0.9, recall: 0.8999999999999999, precision: 0.9025764895330113, f_beta: 0.8998397435897436
train: step: 566, loss: 0.078848697245121, acc: 0.89, recall: 0.89, precision: 0.9030591153369161, f_beta: 0.8891017239641092
train: step: 567, loss: 0.08227328956127167, acc: 0.89, recall: 0.8899999999999999, precision: 0.8939393939393939, f_beta: 0.8897243107769424
train: step: 568, loss: 0.10591476410627365, acc: 0.85, recall: 0.85, precision: 0.851264552388599, f_beta: 0.8498648783905516
train: step: 569, loss: 0.049069568514823914, acc: 0.94, recall: 0.94, precision: 0.94, f_beta: 0.94
train: step: 570, loss: 0.04914448782801628, acc: 0.93, recall: 0.9299999999999999, precision: 0.9343434343434343, f_beta: 0.9298245614035088
train: step: 571, loss: 0.059646014124155045, acc: 0.93, recall: 0.9299999999999999, precision: 0.9315535929345644, f_beta: 0.9299369432489241
train: step: 572, loss: 0.08869671076536179, acc: 0.9, recall: 0.9, precision: 0.9006410256410255, f_beta: 0.8999599839935974
train: step: 573, loss: 0.08493679016828537, acc: 0.89, recall: 0.8899999999999999, precision: 0.8977968176254589, f_beta: 0.8894583458948848
train: step: 574, loss: 0.07840131968259811, acc: 0.9, recall: 0.8999999999999999, precision: 0.9058441558441559, f_beta: 0.8996386993175431
train: step: 575, loss: 0.04855667054653168, acc: 0.92, recall: 0.9199999999999999, precision: 0.9206730769230769, f_beta: 0.919967987194878
train: step: 576, loss: 0.05767643824219704, acc: 0.92, recall: 0.9199999999999999, precision: 0.9206730769230769, f_beta: 0.919967987194878
train: step: 577, loss: 0.11725917458534241, acc: 0.85, recall: 0.85, precision: 0.8617197188921042, f_beta: 0.8487750781328762
train: step: 578, loss: 0.0673663541674614, acc: 0.91, recall: 0.91, precision: 0.9101640656262505, f_beta: 0.90999099909991
train: step: 579, loss: 0.05643434822559357, acc: 0.93, recall: 0.9299999999999999, precision: 0.930172068827531, f_beta: 0.9299929992999301
train: step: 580, loss: 0.05601397156715393, acc: 0.93, recall: 0.9299999999999999, precision: 0.9315535929345644, f_beta: 0.9299369432489241
train: step: 581, loss: 0.06338268518447876, acc: 0.9, recall: 0.8999999999999999, precision: 0.9058441558441559, f_beta: 0.8996386993175431
train: step: 582, loss: 0.07326265424489975, acc: 0.9, recall: 0.9, precision: 0.9006410256410255, f_beta: 0.8999599839935974
train: step: 583, loss: 0.09532992541790009, acc: 0.86, recall: 0.86, precision: 0.86, f_beta: 0.8599999999999999
train: step: 584, loss: 0.09333425760269165, acc: 0.87, recall: 0.87, precision: 0.8713368125250903, f_beta: 0.8698828946051447
train: step: 585, loss: 0.07051097601652145, acc: 0.92, recall: 0.92, precision: 0.92, f_beta: 0.92
train: step: 586, loss: 0.06254363059997559, acc: 0.92, recall: 0.92, precision: 0.92, f_beta: 0.92
train: step: 587, loss: 0.03946274518966675, acc: 0.96, recall: 0.96, precision: 0.962962962962963, f_beta: 0.9599358974358974
train: step: 588, loss: 0.14723581075668335, acc: 0.81, recall: 0.81, precision: 0.8161974704202366, f_beta: 0.8090644156366193
train: step: 589, loss: 0.194803848862648, acc: 0.82, recall: 0.8200000000000001, precision: 0.8246753246753247, f_beta: 0.8193496587715776
train: step: 590, loss: 0.09753183275461197, acc: 0.88, recall: 0.8799999999999999, precision: 0.885551948051948, f_beta: 0.8795664391810518
train: step: 591, loss: 0.0791594535112381, acc: 0.88, recall: 0.88, precision: 0.88, f_beta: 0.88
train: step: 592, loss: 0.08012990653514862, acc: 0.89, recall: 0.89, precision: 0.89015606242497, f_beta: 0.88998899889989
train: step: 593, loss: 0.12694598734378815, acc: 0.85, recall: 0.8500000000000001, precision: 0.8535353535353536, f_beta: 0.849624060150376
train: step: 594, loss: 0.13543444871902466, acc: 0.84, recall: 0.8400000000000001, precision: 0.8489326765188834, f_beta: 0.8389694041867956
train: step: 595, loss: 0.08471828699111938, acc: 0.9, recall: 0.8999999999999999, precision: 0.9058441558441559, f_beta: 0.8996386993175431
train: step: 596, loss: 0.10568336397409439, acc: 0.88, recall: 0.88, precision: 0.8958333333333333, f_beta: 0.8787878787878789
train: step: 597, loss: 0.08606184273958206, acc: 0.89, recall: 0.8899999999999999, precision: 0.8939393939393939, f_beta: 0.8897243107769424
train: step: 598, loss: 0.1040099635720253, acc: 0.87, recall: 0.87, precision: 0.8701480592236894, f_beta: 0.8699869986998701
train: step: 599, loss: 0.13527795672416687, acc: 0.82, recall: 0.8200000000000001, precision: 0.822061191626409, f_beta: 0.8197115384615385
train: step: 600, loss: 0.04622161015868187, acc: 0.93, recall: 0.9299999999999999, precision: 0.930172068827531, f_beta: 0.9299929992999301
train: step: 601, loss: 0.12532368302345276, acc: 0.82, recall: 0.8200000000000001, precision: 0.8205128205128205, f_beta: 0.8199279711884755
train: step: 602, loss: 0.11789388954639435, acc: 0.84, recall: 0.84, precision: 0.8405448717948718, f_beta: 0.8399359743897559
train: step: 603, loss: 0.10151218622922897, acc: 0.89, recall: 0.8899999999999999, precision: 0.8939393939393939, f_beta: 0.8897243107769424
train: step: 604, loss: 0.15199370682239532, acc: 0.79, recall: 0.79, precision: 0.7957976336189311, f_beta: 0.7889659330720531
train: step: 605, loss: 0.10114826261997223, acc: 0.86, recall: 0.86, precision: 0.86, f_beta: 0.8599999999999999
train: step: 606, loss: 0.09885429590940475, acc: 0.92, recall: 0.9199999999999999, precision: 0.9227053140096618, f_beta: 0.9198717948717948
train: step: 607, loss: 0.09942152351140976, acc: 0.86, recall: 0.86, precision: 0.8605769230769231, f_beta: 0.8599439775910365
train: step: 608, loss: 0.08559431880712509, acc: 0.88, recall: 0.8799999999999999, precision: 0.885551948051948, f_beta: 0.8795664391810518
train: step: 609, loss: 0.1335790753364563, acc: 0.86, recall: 0.86, precision: 0.8605769230769231, f_beta: 0.8599439775910365
train: step: 610, loss: 0.10789632797241211, acc: 0.85, recall: 0.85, precision: 0.851264552388599, f_beta: 0.8498648783905516
train: step: 611, loss: 0.09502939879894257, acc: 0.86, recall: 0.86, precision: 0.8605769230769231, f_beta: 0.8599439775910365
train: step: 612, loss: 0.08024095743894577, acc: 0.89, recall: 0.89, precision: 0.89015606242497, f_beta: 0.88998899889989
train: step: 613, loss: 0.1262248456478119, acc: 0.83, recall: 0.83, precision: 0.8301320528211285, f_beta: 0.8299829982998299
train: step: 614, loss: 0.08048797398805618, acc: 0.91, recall: 0.9099999999999999, precision: 0.9114813327980731, f_beta: 0.9099189270343309
train: step: 615, loss: 0.09477812051773071, acc: 0.92, recall: 0.9199999999999999, precision: 0.9206730769230769, f_beta: 0.919967987194878
train: step: 616, loss: 0.15459689497947693, acc: 0.83, recall: 0.8300000000000001, precision: 0.8410500206696983, f_beta: 0.8286117552172598
train: step: 617, loss: 0.06778087466955185, acc: 0.9, recall: 0.9, precision: 0.9, f_beta: 0.9
train: step: 618, loss: 0.10581611841917038, acc: 0.87, recall: 0.87, precision: 0.8737373737373737, f_beta: 0.8696741854636592
train: step: 619, loss: 0.08723869174718857, acc: 0.89, recall: 0.8899999999999999, precision: 0.8939393939393939, f_beta: 0.8897243107769424
train: step: 620, loss: 0.07867782562971115, acc: 0.89, recall: 0.8899999999999999, precision: 0.8939393939393939, f_beta: 0.8897243107769424
train: step: 621, loss: 0.04209443926811218, acc: 0.96, recall: 0.96, precision: 0.9607371794871795, f_beta: 0.959983993597439
train: step: 622, loss: 0.12281707674264908, acc: 0.85, recall: 0.85, precision: 0.851264552388599, f_beta: 0.8498648783905516
train: step: 623, loss: 0.12314070016145706, acc: 0.83, recall: 0.8300000000000001, precision: 0.8311922922521076, f_beta: 0.8298468621759585
train: step: 624, loss: 0.09936235100030899, acc: 0.86, recall: 0.86, precision: 0.8820033955857385, f_beta: 0.8579545454545454
train: step: 625, loss: 0.13163217902183533, acc: 0.81, recall: 0.81, precision: 0.8111200321156162, f_beta: 0.8098288459613652
train: step: 626, loss: 0.057595930993556976, acc: 0.93, recall: 0.9299999999999999, precision: 0.9315535929345644, f_beta: 0.9299369432489241
train: step: 627, loss: 0.08321857452392578, acc: 0.88, recall: 0.88, precision: 0.8958333333333333, f_beta: 0.8787878787878789
train: step: 628, loss: 0.0524321086704731, acc: 0.93, recall: 0.9299999999999999, precision: 0.9343434343434343, f_beta: 0.9298245614035088
train: step: 629, loss: 0.09385574609041214, acc: 0.85, recall: 0.8500000000000001, precision: 0.8535353535353536, f_beta: 0.849624060150376
train: step: 630, loss: 0.09775171428918839, acc: 0.86, recall: 0.86, precision: 0.8623188405797102, f_beta: 0.859775641025641
train: step: 631, loss: 0.07884714007377625, acc: 0.9, recall: 0.9, precision: 0.9006410256410255, f_beta: 0.8999599839935974
train: step: 632, loss: 0.06577818840742111, acc: 0.93, recall: 0.9299999999999999, precision: 0.9315535929345644, f_beta: 0.9299369432489241
train: step: 633, loss: 0.08839163184165955, acc: 0.89, recall: 0.89, precision: 0.8914090726615818, f_beta: 0.8899009108197378
train: step: 634, loss: 0.08970609307289124, acc: 0.9, recall: 0.8999999999999999, precision: 0.9058441558441559, f_beta: 0.8996386993175431
train: step: 635, loss: 0.10128964483737946, acc: 0.86, recall: 0.8600000000000001, precision: 0.8652597402597402, f_beta: 0.8594941790445605
train: step: 636, loss: 0.16276611387729645, acc: 0.77, recall: 0.77, precision: 0.805291723202171, f_beta: 0.7631551848419318
train: step: 637, loss: 0.08918633311986923, acc: 0.9, recall: 0.9, precision: 0.9006410256410255, f_beta: 0.8999599839935974
train: step: 638, loss: 0.35030239820480347, acc: 0.87, recall: 0.87, precision: 0.8701480592236894, f_beta: 0.8699869986998701
train: step: 639, loss: 0.17826305329799652, acc: 0.85, recall: 0.85, precision: 0.8753753753753755, f_beta: 0.8474214220323468
train: step: 640, loss: 0.0844937115907669, acc: 0.9, recall: 0.9, precision: 0.9166666666666667, f_beta: 0.898989898989899
train: step: 641, loss: 0.08950233459472656, acc: 0.89, recall: 0.89, precision: 0.9030591153369161, f_beta: 0.8891017239641092
train: step: 642, loss: 0.06284481287002563, acc: 0.92, recall: 0.92, precision: 0.92, f_beta: 0.92
train: step: 643, loss: 0.06382069736719131, acc: 0.92, recall: 0.9199999999999999, precision: 0.9227053140096618, f_beta: 0.9198717948717948
train: step: 644, loss: 0.07337536662817001, acc: 0.92, recall: 0.9199999999999999, precision: 0.9206730769230769, f_beta: 0.919967987194878
train: step: 645, loss: 0.08149978518486023, acc: 0.89, recall: 0.8899999999999999, precision: 0.8977968176254589, f_beta: 0.8894583458948848
train: step: 646, loss: 0.1472121626138687, acc: 0.88, recall: 0.88, precision: 0.8806089743589745, f_beta: 0.879951980792317
train: step: 647, loss: 0.06622307747602463, acc: 0.92, recall: 0.9199999999999999, precision: 0.9206730769230769, f_beta: 0.919967987194878
train: step: 648, loss: 0.11770670115947723, acc: 0.82, recall: 0.82, precision: 0.82, f_beta: 0.82
train: step: 649, loss: 0.09385132789611816, acc: 0.88, recall: 0.88, precision: 0.8806089743589745, f_beta: 0.879951980792317
train: step: 650, loss: 0.10068825632333755, acc: 0.9, recall: 0.8999999999999999, precision: 0.9058441558441559, f_beta: 0.8996386993175431
train: step: 651, loss: 0.06004030629992485, acc: 0.91, recall: 0.9099999999999999, precision: 0.9141414141414141, f_beta: 0.9097744360902256
train: step: 652, loss: 0.08574429154396057, acc: 0.89, recall: 0.89, precision: 0.8914090726615818, f_beta: 0.8899009108197378
train: step: 653, loss: 0.0626351609826088, acc: 0.92, recall: 0.9199999999999999, precision: 0.9206730769230769, f_beta: 0.919967987194878
train: step: 654, loss: 0.08689947426319122, acc: 0.88, recall: 0.8799999999999999, precision: 0.885551948051948, f_beta: 0.8795664391810518
train: step: 655, loss: 0.12072096019983292, acc: 0.85, recall: 0.8500000000000001, precision: 0.8569971440228478, f_beta: 0.8492613807657521
train: step: 656, loss: 0.08664678782224655, acc: 0.88, recall: 0.88, precision: 0.88, f_beta: 0.88
train: step: 657, loss: 0.12483858317136765, acc: 0.87, recall: 0.87, precision: 0.8701480592236894, f_beta: 0.8699869986998701
train: step: 658, loss: 0.08295879513025284, acc: 0.89, recall: 0.8899999999999999, precision: 0.8977968176254589, f_beta: 0.8894583458948848
train: step: 659, loss: 0.05199598893523216, acc: 0.94, recall: 0.94, precision: 0.9428341384863124, f_beta: 0.9399038461538463
train: step: 660, loss: 0.09317228198051453, acc: 0.87, recall: 0.87, precision: 0.8701480592236894, f_beta: 0.8699869986998701
train: step: 661, loss: 0.06085081025958061, acc: 0.92, recall: 0.9199999999999999, precision: 0.9227053140096618, f_beta: 0.9198717948717948
train: step: 662, loss: 0.08817768841981888, acc: 0.87, recall: 0.87, precision: 0.8701480592236894, f_beta: 0.8699869986998701
train: step: 663, loss: 0.14039257168769836, acc: 0.79, recall: 0.79, precision: 0.7997106242248864, f_beta: 0.7882851093860268
train: step: 664, loss: 0.07698221504688263, acc: 0.9, recall: 0.8999999999999999, precision: 0.9025764895330113, f_beta: 0.8998397435897436
train: step: 665, loss: 0.09053024649620056, acc: 0.9, recall: 0.9, precision: 0.9006410256410255, f_beta: 0.8999599839935974
train: step: 666, loss: 0.12119222432374954, acc: 0.81, recall: 0.81, precision: 0.8131313131313131, f_beta: 0.8095238095238095
train: step: 667, loss: 0.09899219870567322, acc: 0.89, recall: 0.89, precision: 0.89015606242497, f_beta: 0.88998899889989
train: step: 668, loss: 0.0810224711894989, acc: 0.92, recall: 0.92, precision: 0.92, f_beta: 0.92
train: step: 669, loss: 0.09359270334243774, acc: 0.89, recall: 0.89, precision: 0.89015606242497, f_beta: 0.88998899889989
train: step: 670, loss: 0.07650479674339294, acc: 0.9, recall: 0.9, precision: 0.9, f_beta: 0.9
train: step: 671, loss: 0.06286512315273285, acc: 0.93, recall: 0.9299999999999999, precision: 0.930172068827531, f_beta: 0.9299929992999301
train: step: 672, loss: 0.06859887391328812, acc: 0.91, recall: 0.9099999999999999, precision: 0.9114813327980731, f_beta: 0.9099189270343309
train: step: 673, loss: 0.09081243723630905, acc: 0.89, recall: 0.8899999999999999, precision: 0.8977968176254589, f_beta: 0.8894583458948848
train: step: 674, loss: 0.06464126706123352, acc: 0.91, recall: 0.9099999999999999, precision: 0.9181966544267646, f_beta: 0.9095568284594513
train: step: 675, loss: 0.060741934925317764, acc: 0.93, recall: 0.9299999999999999, precision: 0.9315535929345644, f_beta: 0.9299369432489241
train: step: 676, loss: 0.05011439323425293, acc: 0.94, recall: 0.94, precision: 0.9407051282051282, f_beta: 0.9399759903961584
train: step: 677, loss: 0.09059534221887589, acc: 0.87, recall: 0.87, precision: 0.8701480592236894, f_beta: 0.8699869986998701
train: step: 678, loss: 0.09137168526649475, acc: 0.87, recall: 0.87, precision: 0.8713368125250903, f_beta: 0.8698828946051447
train: step: 679, loss: 0.07690281420946121, acc: 0.9, recall: 0.9, precision: 0.9006410256410255, f_beta: 0.8999599839935974
train: step: 680, loss: 0.08768680691719055, acc: 0.91, recall: 0.91, precision: 0.9101640656262505, f_beta: 0.90999099909991
train: step: 681, loss: 0.10525644570589066, acc: 0.86, recall: 0.86, precision: 0.8623188405797102, f_beta: 0.859775641025641
train: step: 682, loss: 0.06799742579460144, acc: 0.9, recall: 0.9, precision: 0.9, f_beta: 0.9
train: step: 683, loss: 0.03544631972908974, acc: 0.95, recall: 0.95, precision: 0.9516258530710557, f_beta: 0.949954959463517
train: step: 684, loss: 0.09983406960964203, acc: 0.87, recall: 0.87, precision: 0.8737373737373737, f_beta: 0.8696741854636592
train: step: 685, loss: 0.13009053468704224, acc: 0.83, recall: 0.8300000000000001, precision: 0.8333333333333334, f_beta: 0.8295739348370927
train: step: 686, loss: 0.0852547436952591, acc: 0.88, recall: 0.8799999999999999, precision: 0.885551948051948, f_beta: 0.8795664391810518
train: step: 687, loss: 0.10564561933279037, acc: 0.85, recall: 0.8500000000000001, precision: 0.8569971440228478, f_beta: 0.8492613807657521
train: step: 688, loss: 0.07623632252216339, acc: 0.91, recall: 0.91, precision: 0.9101640656262505, f_beta: 0.90999099909991
train: step: 689, loss: 0.0780162587761879, acc: 0.91, recall: 0.9099999999999999, precision: 0.9114813327980731, f_beta: 0.9099189270343309
train: step: 690, loss: 0.07245403528213501, acc: 0.91, recall: 0.9099999999999999, precision: 0.9181966544267646, f_beta: 0.9095568284594513
train: step: 691, loss: 0.069634348154068, acc: 0.92, recall: 0.9199999999999999, precision: 0.9261363636363636, f_beta: 0.9197109594540345
train: step: 692, loss: 0.12354326248168945, acc: 0.84, recall: 0.84, precision: 0.84, f_beta: 0.8399999999999999
train: step: 693, loss: 0.049286648631095886, acc: 0.94, recall: 0.94, precision: 0.9428341384863124, f_beta: 0.9399038461538463
train: step: 694, loss: 0.05753971263766289, acc: 0.93, recall: 0.9299999999999999, precision: 0.9315535929345644, f_beta: 0.9299369432489241
train: step: 695, loss: 0.05646095797419548, acc: 0.94, recall: 0.94, precision: 0.9407051282051282, f_beta: 0.9399759903961584
train: step: 696, loss: 0.07001152634620667, acc: 0.91, recall: 0.91, precision: 0.9101640656262505, f_beta: 0.90999099909991
train: step: 697, loss: 0.05797608196735382, acc: 0.91, recall: 0.9099999999999999, precision: 0.9141414141414141, f_beta: 0.9097744360902256
train: step: 698, loss: 0.10644793510437012, acc: 0.82, recall: 0.8200000000000001, precision: 0.8205128205128205, f_beta: 0.8199279711884755
train: step: 699, loss: 0.11542389541864395, acc: 0.83, recall: 0.8300000000000001, precision: 0.8311922922521076, f_beta: 0.8298468621759585
train: step: 700, loss: 0.09551729261875153, acc: 0.88, recall: 0.8799999999999999, precision: 0.885551948051948, f_beta: 0.8795664391810518
train: step: 701, loss: 0.09173668920993805, acc: 0.88, recall: 0.88, precision: 0.88, f_beta: 0.88
train: step: 702, loss: 0.07532454282045364, acc: 0.89, recall: 0.8899999999999999, precision: 0.8939393939393939, f_beta: 0.8897243107769424
train: step: 703, loss: 0.09744195640087128, acc: 0.88, recall: 0.88, precision: 0.88, f_beta: 0.88
train: step: 704, loss: 0.09844591468572617, acc: 0.87, recall: 0.87, precision: 0.8701480592236894, f_beta: 0.8699869986998701
train: step: 705, loss: 0.04992568865418434, acc: 0.93, recall: 0.9299999999999999, precision: 0.9315535929345644, f_beta: 0.9299369432489241
train: step: 706, loss: 0.053452033549547195, acc: 0.95, recall: 0.95, precision: 0.9501800720288115, f_beta: 0.9499949994999499
train: step: 707, loss: 0.09031742811203003, acc: 0.89, recall: 0.8899999999999999, precision: 0.8939393939393939, f_beta: 0.8897243107769424
train: step: 708, loss: 0.10083211958408356, acc: 0.87, recall: 0.87, precision: 0.8713368125250903, f_beta: 0.8698828946051447
train: step: 709, loss: 0.10387370735406876, acc: 0.83, recall: 0.8300000000000001, precision: 0.8365973072215422, f_beta: 0.8291628982011858
train: step: 710, loss: 0.09529747068881989, acc: 0.88, recall: 0.88, precision: 0.8824476650563606, f_beta: 0.8798076923076923
train: step: 711, loss: 0.06532479077577591, acc: 0.91, recall: 0.91, precision: 0.9101640656262505, f_beta: 0.90999099909991
train: step: 712, loss: 0.08558658510446548, acc: 0.9, recall: 0.9, precision: 0.9, f_beta: 0.9
train: step: 713, loss: 0.11693271994590759, acc: 0.84, recall: 0.8400000000000001, precision: 0.8421900161030595, f_beta: 0.8397435897435896
train: step: 714, loss: 0.03438244387507439, acc: 0.96, recall: 0.96, precision: 0.9607371794871795, f_beta: 0.959983993597439
train: step: 715, loss: 0.06167687103152275, acc: 0.92, recall: 0.9199999999999999, precision: 0.9261363636363636, f_beta: 0.9197109594540345
train: step: 716, loss: 0.0717414915561676, acc: 0.93, recall: 0.9299999999999999, precision: 0.9315535929345644, f_beta: 0.9299369432489241
train: step: 717, loss: 0.07233744114637375, acc: 0.9, recall: 0.9, precision: 0.9, f_beta: 0.9
train: step: 718, loss: 0.06693127751350403, acc: 0.91, recall: 0.9099999999999999, precision: 0.9141414141414141, f_beta: 0.9097744360902256
train: step: 719, loss: 0.05241955816745758, acc: 0.93, recall: 0.9299999999999999, precision: 0.9315535929345644, f_beta: 0.9299369432489241
train: step: 720, loss: 0.11263061314821243, acc: 0.82, recall: 0.8200000000000001, precision: 0.8284072249589491, f_beta: 0.818840579710145
train: step: 721, loss: 0.10354124009609222, acc: 0.85, recall: 0.8500000000000001, precision: 0.8535353535353536, f_beta: 0.849624060150376
train: step: 722, loss: 0.04483219236135483, acc: 0.96, recall: 0.96, precision: 0.962962962962963, f_beta: 0.9599358974358974
train: step: 723, loss: 0.030908260494470596, acc: 0.95, recall: 0.95, precision: 0.9501800720288115, f_beta: 0.9499949994999499
train: step: 724, loss: 0.07412751019001007, acc: 0.91, recall: 0.9099999999999999, precision: 0.9114813327980731, f_beta: 0.9099189270343309
train: step: 725, loss: 0.049875907599925995, acc: 0.95, recall: 0.95, precision: 0.9501800720288115, f_beta: 0.9499949994999499
train: step: 726, loss: 0.05719367414712906, acc: 0.92, recall: 0.92, precision: 0.92, f_beta: 0.92
train: step: 727, loss: 0.0882456973195076, acc: 0.87, recall: 0.87, precision: 0.8713368125250903, f_beta: 0.8698828946051447
train: step: 728, loss: 0.12712159752845764, acc: 0.86, recall: 0.86, precision: 0.86, f_beta: 0.8599999999999999
train: step: 729, loss: 0.09937003999948502, acc: 0.85, recall: 0.85, precision: 0.8501400560224089, f_beta: 0.84998499849985
train: step: 730, loss: 0.06743795424699783, acc: 0.93, recall: 0.9299999999999999, precision: 0.9343434343434343, f_beta: 0.9298245614035088
train: step: 731, loss: 0.1079494059085846, acc: 0.86, recall: 0.8600000000000001, precision: 0.8652597402597402, f_beta: 0.8594941790445605
train: step: 732, loss: 0.07547152787446976, acc: 0.92, recall: 0.9199999999999999, precision: 0.9206730769230769, f_beta: 0.919967987194878
train: step: 733, loss: 0.08751561492681503, acc: 0.89, recall: 0.8899999999999999, precision: 0.8939393939393939, f_beta: 0.8897243107769424
train: step: 734, loss: 0.035692621022462845, acc: 0.96, recall: 0.96, precision: 0.9607371794871795, f_beta: 0.959983993597439
train: step: 735, loss: 0.06986547261476517, acc: 0.9, recall: 0.9, precision: 0.9006410256410255, f_beta: 0.8999599839935974
train: step: 736, loss: 0.04803876578807831, acc: 0.92, recall: 0.9199999999999999, precision: 0.9206730769230769, f_beta: 0.919967987194878
train: step: 737, loss: 0.08500232547521591, acc: 0.9, recall: 0.8999999999999999, precision: 0.9058441558441559, f_beta: 0.8996386993175431
train: step: 738, loss: 0.05814383178949356, acc: 0.92, recall: 0.9199999999999999, precision: 0.9206730769230769, f_beta: 0.919967987194878
train: step: 739, loss: 0.06868329644203186, acc: 0.91, recall: 0.91, precision: 0.9101640656262505, f_beta: 0.90999099909991
train: step: 740, loss: 0.08232925087213516, acc: 0.87, recall: 0.87, precision: 0.8713368125250903, f_beta: 0.8698828946051447
train: step: 741, loss: 0.06015937030315399, acc: 0.91, recall: 0.91, precision: 0.9101640656262505, f_beta: 0.90999099909991
train: step: 742, loss: 0.06560599058866501, acc: 0.92, recall: 0.9199999999999999, precision: 0.9206730769230769, f_beta: 0.919967987194878
train: step: 743, loss: 0.04958067834377289, acc: 0.93, recall: 0.9299999999999999, precision: 0.930172068827531, f_beta: 0.9299929992999301
train: step: 744, loss: 0.049658436328172684, acc: 0.94, recall: 0.94, precision: 0.9428341384863124, f_beta: 0.9399038461538463
train: step: 745, loss: 0.09858093410730362, acc: 0.92, recall: 0.9199999999999999, precision: 0.9206730769230769, f_beta: 0.919967987194878
train: step: 746, loss: 0.04036592319607735, acc: 0.95, recall: 0.95, precision: 0.9501800720288115, f_beta: 0.9499949994999499
train: step: 747, loss: 0.08819610625505447, acc: 0.89, recall: 0.89, precision: 0.8914090726615818, f_beta: 0.8899009108197378
train: step: 748, loss: 0.1186557412147522, acc: 0.86, recall: 0.86, precision: 0.8605769230769231, f_beta: 0.8599439775910365
train: step: 749, loss: 0.03258794918656349, acc: 0.97, recall: 0.97, precision: 0.9716981132075472, f_beta: 0.9699729756781104
train: step: 750, loss: 0.06788415461778641, acc: 0.93, recall: 0.9299999999999999, precision: 0.930172068827531, f_beta: 0.9299929992999301
train: step: 751, loss: 0.11897985637187958, acc: 0.86, recall: 0.86, precision: 0.8605769230769231, f_beta: 0.8599439775910365
train: step: 752, loss: 0.0908505842089653, acc: 0.9, recall: 0.9, precision: 0.9006410256410255, f_beta: 0.8999599839935974
train: step: 753, loss: 0.08601158857345581, acc: 0.88, recall: 0.88, precision: 0.88, f_beta: 0.88
train: step: 754, loss: 0.0760321319103241, acc: 0.91, recall: 0.9099999999999999, precision: 0.9114813327980731, f_beta: 0.9099189270343309
train: step: 755, loss: 0.0770954117178917, acc: 0.91, recall: 0.91, precision: 0.9101640656262505, f_beta: 0.90999099909991
train: step: 756, loss: 0.09762571007013321, acc: 0.87, recall: 0.87, precision: 0.8713368125250903, f_beta: 0.8698828946051447
train: step: 757, loss: 0.11293572187423706, acc: 0.84, recall: 0.8400000000000001, precision: 0.8449675324675325, f_beta: 0.839421918908069
train: step: 758, loss: 0.054992809891700745, acc: 0.92, recall: 0.92, precision: 0.92, f_beta: 0.92
train: step: 759, loss: 0.041959792375564575, acc: 0.95, recall: 0.95, precision: 0.9501800720288115, f_beta: 0.9499949994999499
train: step: 760, loss: 0.05125908553600311, acc: 0.96, recall: 0.96, precision: 0.9607371794871795, f_beta: 0.959983993597439
train: step: 761, loss: 0.03802775591611862, acc: 0.96, recall: 0.96, precision: 0.9607371794871795, f_beta: 0.959983993597439
train: step: 762, loss: 0.06531725823879242, acc: 0.93, recall: 0.9299999999999999, precision: 0.930172068827531, f_beta: 0.9299929992999301
train: step: 763, loss: 0.06168956682085991, acc: 0.92, recall: 0.92, precision: 0.92, f_beta: 0.92
train: step: 764, loss: 0.07070553302764893, acc: 0.89, recall: 0.89, precision: 0.89015606242497, f_beta: 0.88998899889989
train: step: 765, loss: 0.053109388798475266, acc: 0.94, recall: 0.94, precision: 0.94, f_beta: 0.94
train: step: 766, loss: 0.1419813185930252, acc: 0.8, recall: 0.8, precision: 0.8019323671497585, f_beta: 0.7996794871794872
train: step: 767, loss: 0.09660959243774414, acc: 0.87, recall: 0.87, precision: 0.8713368125250903, f_beta: 0.8698828946051447
train: step: 768, loss: 0.08025984466075897, acc: 0.91, recall: 0.91, precision: 0.9101640656262505, f_beta: 0.90999099909991
train: step: 769, loss: 0.07675305753946304, acc: 0.91, recall: 0.9099999999999999, precision: 0.9114813327980731, f_beta: 0.9099189270343309
train: step: 770, loss: 0.05655061826109886, acc: 0.92, recall: 0.9199999999999999, precision: 0.9206730769230769, f_beta: 0.919967987194878
train: step: 771, loss: 0.06897907704114914, acc: 0.91, recall: 0.91, precision: 0.9101640656262505, f_beta: 0.90999099909991
train: step: 772, loss: 0.12163688987493515, acc: 0.84, recall: 0.8400000000000001, precision: 0.8421900161030595, f_beta: 0.8397435897435896
train: step: 773, loss: 0.09833091497421265, acc: 0.86, recall: 0.86, precision: 0.8623188405797102, f_beta: 0.859775641025641
train: step: 774, loss: 0.10398535430431366, acc: 0.85, recall: 0.85, precision: 0.8501400560224089, f_beta: 0.84998499849985
train: step: 775, loss: 0.07591613382101059, acc: 0.89, recall: 0.8899999999999999, precision: 0.8977968176254589, f_beta: 0.8894583458948848
train: step: 776, loss: 0.08767219632863998, acc: 0.87, recall: 0.87, precision: 0.8737373737373737, f_beta: 0.8696741854636592
train: step: 777, loss: 0.05842118337750435, acc: 0.91, recall: 0.91, precision: 0.9101640656262505, f_beta: 0.90999099909991
train: step: 778, loss: 0.059196289628744125, acc: 0.92, recall: 0.92, precision: 0.92, f_beta: 0.92
train: step: 779, loss: 0.06651125103235245, acc: 0.9, recall: 0.8999999999999999, precision: 0.9058441558441559, f_beta: 0.8996386993175431
train: step: 780, loss: 0.08239820599555969, acc: 0.87, recall: 0.87, precision: 0.8773969808241534, f_beta: 0.8693598633303186
train: step: 781, loss: 0.07703983038663864, acc: 0.86, recall: 0.86, precision: 0.8694581280788177, f_beta: 0.8590982286634461
train: step: 782, loss: 0.06610429286956787, acc: 0.93, recall: 0.9299999999999999, precision: 0.930172068827531, f_beta: 0.9299929992999301
train: step: 783, loss: 0.0908684954047203, acc: 0.87, recall: 0.87, precision: 0.8823894171145101, f_beta: 0.8689384010484928
train: step: 784, loss: 0.08250594884157181, acc: 0.91, recall: 0.9099999999999999, precision: 0.9181966544267646, f_beta: 0.9095568284594513
train: step: 785, loss: 0.1394088864326477, acc: 0.87, recall: 0.87, precision: 0.8701480592236894, f_beta: 0.8699869986998701
train: step: 786, loss: 0.05402245372533798, acc: 0.93, recall: 0.9299999999999999, precision: 0.930172068827531, f_beta: 0.9299929992999301
train: step: 787, loss: 0.10910605639219284, acc: 0.84, recall: 0.8400000000000001, precision: 0.8421900161030595, f_beta: 0.8397435897435896
train: step: 788, loss: 0.0999799445271492, acc: 0.89, recall: 0.89, precision: 0.89015606242497, f_beta: 0.88998899889989
train: step: 789, loss: 0.08467299491167068, acc: 0.88, recall: 0.88, precision: 0.8899835796387521, f_beta: 0.8792270531400966
train: step: 790, loss: 0.14034023880958557, acc: 0.81, recall: 0.81, precision: 0.8203803224472923, f_beta: 0.8084484323016432
train: step: 791, loss: 0.10494398325681686, acc: 0.88, recall: 0.88, precision: 0.8806089743589745, f_beta: 0.879951980792317
train: step: 792, loss: 0.09996639937162399, acc: 0.85, recall: 0.85, precision: 0.867801597309794, f_beta: 0.848162769511084
train: step: 793, loss: 0.08084636926651001, acc: 0.9, recall: 0.9, precision: 0.9, f_beta: 0.9
train: step: 794, loss: 0.05131295323371887, acc: 0.96, recall: 0.96, precision: 0.962962962962963, f_beta: 0.9599358974358974
train: step: 795, loss: 0.05976615101099014, acc: 0.93, recall: 0.9299999999999999, precision: 0.930172068827531, f_beta: 0.9299929992999301
train: step: 796, loss: 0.07738714665174484, acc: 0.89, recall: 0.89, precision: 0.8914090726615818, f_beta: 0.8899009108197378
train: step: 797, loss: 0.1440959870815277, acc: 0.9, recall: 0.9, precision: 0.9006410256410255, f_beta: 0.8999599839935974
train: step: 798, loss: 0.10808660835027695, acc: 0.85, recall: 0.85, precision: 0.8753753753753755, f_beta: 0.8474214220323468
train: step: 799, loss: 0.08607067167758942, acc: 0.87, recall: 0.87, precision: 0.8701480592236894, f_beta: 0.8699869986998701
train: step: 800, loss: 0.2759864330291748, acc: 0.83, recall: 0.8300000000000001, precision: 0.8311922922521076, f_beta: 0.8298468621759585
train: step: 801, loss: 0.10729117691516876, acc: 0.88, recall: 0.88, precision: 0.8958333333333333, f_beta: 0.8787878787878789
train: step: 802, loss: 0.06399831175804138, acc: 0.91, recall: 0.9099999999999999, precision: 0.9181966544267646, f_beta: 0.9095568284594513
train: step: 803, loss: 0.10568803548812866, acc: 0.85, recall: 0.85, precision: 0.8501400560224089, f_beta: 0.84998499849985
train: step: 804, loss: 0.09253492206335068, acc: 0.87, recall: 0.87, precision: 0.8823894171145101, f_beta: 0.8689384010484928
train: step: 805, loss: 0.10366186499595642, acc: 0.9, recall: 0.9, precision: 0.9166666666666667, f_beta: 0.898989898989899
train: step: 806, loss: 0.07640677690505981, acc: 0.91, recall: 0.9099999999999999, precision: 0.923728813559322, f_beta: 0.9092650468797258
train: step: 807, loss: 0.048162512481212616, acc: 0.94, recall: 0.94, precision: 0.9428341384863124, f_beta: 0.9399038461538463
train: step: 808, loss: 0.053491342812776566, acc: 0.93, recall: 0.9299999999999999, precision: 0.930172068827531, f_beta: 0.9299929992999301
train: step: 809, loss: 0.10894749313592911, acc: 0.87, recall: 0.87, precision: 0.8737373737373737, f_beta: 0.8696741854636592
train: step: 810, loss: 0.04652954265475273, acc: 0.94, recall: 0.94, precision: 0.94, f_beta: 0.94
train: step: 811, loss: 0.04963959753513336, acc: 0.94, recall: 0.94, precision: 0.94, f_beta: 0.94
train: step: 812, loss: 0.07245998829603195, acc: 0.91, recall: 0.91, precision: 0.9101640656262505, f_beta: 0.90999099909991
train: step: 813, loss: 0.07717051357030869, acc: 0.9, recall: 0.8999999999999999, precision: 0.9025764895330113, f_beta: 0.8998397435897436
train: step: 814, loss: 0.08546176552772522, acc: 0.87, recall: 0.87, precision: 0.8773969808241534, f_beta: 0.8693598633303186
train: step: 815, loss: 0.06902183592319489, acc: 0.9, recall: 0.9, precision: 0.9006410256410255, f_beta: 0.8999599839935974
train: step: 816, loss: 0.06423633545637131, acc: 0.91, recall: 0.9099999999999999, precision: 0.9141414141414141, f_beta: 0.9097744360902256
train: step: 817, loss: 0.09930335730314255, acc: 0.87, recall: 0.87, precision: 0.8701480592236894, f_beta: 0.8699869986998701
train: step: 818, loss: 0.14257539808750153, acc: 0.8, recall: 0.8, precision: 0.8004807692307692, f_beta: 0.7999199679871949
train: step: 819, loss: 0.07699626684188843, acc: 0.89, recall: 0.89, precision: 0.89015606242497, f_beta: 0.88998899889989
train: step: 820, loss: 0.05071944370865822, acc: 0.94, recall: 0.94, precision: 0.9428341384863124, f_beta: 0.9399038461538463
train: step: 821, loss: 0.10346684604883194, acc: 0.96, recall: 0.96, precision: 0.9607371794871795, f_beta: 0.959983993597439
train: step: 822, loss: 0.10858502984046936, acc: 0.85, recall: 0.85, precision: 0.851264552388599, f_beta: 0.8498648783905516
train: step: 823, loss: 0.12326120585203171, acc: 0.88, recall: 0.8799999999999999, precision: 0.885551948051948, f_beta: 0.8795664391810518
train: step: 824, loss: 0.07331875711679459, acc: 0.93, recall: 0.9299999999999999, precision: 0.930172068827531, f_beta: 0.9299929992999301
train: step: 825, loss: 0.1189514622092247, acc: 0.83, recall: 0.8300000000000001, precision: 0.8365973072215422, f_beta: 0.8291628982011858
train: step: 826, loss: 0.09148269891738892, acc: 0.89, recall: 0.8899999999999999, precision: 0.8977968176254589, f_beta: 0.8894583458948848
train: step: 827, loss: 0.08454009890556335, acc: 0.89, recall: 0.8899999999999999, precision: 0.8939393939393939, f_beta: 0.8897243107769424
train: step: 828, loss: 0.12243311107158661, acc: 0.86, recall: 0.86, precision: 0.86, f_beta: 0.8599999999999999
train: step: 829, loss: 0.09440050274133682, acc: 0.88, recall: 0.88, precision: 0.8899835796387521, f_beta: 0.8792270531400966
train: step: 830, loss: 0.13148802518844604, acc: 0.83, recall: 0.8300000000000001, precision: 0.8333333333333334, f_beta: 0.8295739348370927
train: step: 831, loss: 0.10407859832048416, acc: 0.86, recall: 0.86, precision: 0.86, f_beta: 0.8599999999999999
train: step: 832, loss: 0.1288151741027832, acc: 0.89, recall: 0.8899999999999999, precision: 0.8977968176254589, f_beta: 0.8894583458948848
train: step: 833, loss: 0.1348072737455368, acc: 0.91, recall: 0.9099999999999999, precision: 0.9141414141414141, f_beta: 0.9097744360902256
train: step: 834, loss: 0.09652011841535568, acc: 0.88, recall: 0.88, precision: 0.8824476650563606, f_beta: 0.8798076923076923
train: step: 835, loss: 0.07142610847949982, acc: 0.91, recall: 0.9099999999999999, precision: 0.9181966544267646, f_beta: 0.9095568284594513
train: step: 836, loss: 0.06604462116956711, acc: 0.89, recall: 0.8899999999999999, precision: 0.8939393939393939, f_beta: 0.8897243107769424
train: step: 837, loss: 0.08740296959877014, acc: 0.89, recall: 0.89, precision: 0.9098360655737705, f_beta: 0.8886526976414617
train: step: 838, loss: 0.08753766119480133, acc: 0.88, recall: 0.88, precision: 0.8806089743589745, f_beta: 0.879951980792317
train: step: 839, loss: 0.12652412056922913, acc: 0.85, recall: 0.85, precision: 0.8617197188921042, f_beta: 0.8487750781328762
train: step: 840, loss: 0.1468517929315567, acc: 0.88, recall: 0.88, precision: 0.8824476650563606, f_beta: 0.8798076923076923
train: step: 841, loss: 0.0915621668100357, acc: 0.89, recall: 0.8899999999999999, precision: 0.8977968176254589, f_beta: 0.8894583458948848
train: step: 842, loss: 0.09892383217811584, acc: 0.86, recall: 0.86, precision: 0.8623188405797102, f_beta: 0.859775641025641
train: step: 843, loss: 0.08009542524814606, acc: 0.91, recall: 0.9099999999999999, precision: 0.9141414141414141, f_beta: 0.9097744360902256
train: step: 844, loss: 0.0582859143614769, acc: 0.92, recall: 0.9199999999999999, precision: 0.9310344827586207, f_beta: 0.9194847020933978
train: step: 845, loss: 0.0975835770368576, acc: 0.85, recall: 0.8500000000000001, precision: 0.8535353535353536, f_beta: 0.849624060150376
train: step: 846, loss: 0.104674331843853, acc: 0.85, recall: 0.85, precision: 0.8501400560224089, f_beta: 0.84998499849985
train: step: 847, loss: 0.07271786779165268, acc: 0.91, recall: 0.9099999999999999, precision: 0.9141414141414141, f_beta: 0.9097744360902256
train: step: 848, loss: 0.10339407622814178, acc: 0.87, recall: 0.87, precision: 0.8737373737373737, f_beta: 0.8696741854636592
train: step: 849, loss: 0.06812448054552078, acc: 0.92, recall: 0.92, precision: 0.92, f_beta: 0.92
train: step: 850, loss: 0.08286745101213455, acc: 0.89, recall: 0.8899999999999999, precision: 0.8939393939393939, f_beta: 0.8897243107769424
train: step: 851, loss: 0.06868342310190201, acc: 0.92, recall: 0.9199999999999999, precision: 0.9206730769230769, f_beta: 0.919967987194878
train: step: 852, loss: 0.0736396312713623, acc: 0.92, recall: 0.9199999999999999, precision: 0.9206730769230769, f_beta: 0.919967987194878
train: step: 853, loss: 0.08183134347200394, acc: 0.87, recall: 0.87, precision: 0.8713368125250903, f_beta: 0.8698828946051447
train: step: 854, loss: 0.08985159546136856, acc: 0.89, recall: 0.8899999999999999, precision: 0.8939393939393939, f_beta: 0.8897243107769424
train: step: 855, loss: 0.044749803841114044, acc: 0.92, recall: 0.9199999999999999, precision: 0.9227053140096618, f_beta: 0.9198717948717948
train: step: 856, loss: 0.09777875989675522, acc: 0.86, recall: 0.86, precision: 0.8623188405797102, f_beta: 0.859775641025641
train: step: 857, loss: 0.06457508355379105, acc: 0.92, recall: 0.92, precision: 0.92, f_beta: 0.92
train: step: 858, loss: 0.05789518356323242, acc: 0.91, recall: 0.91, precision: 0.9101640656262505, f_beta: 0.90999099909991
train: step: 859, loss: 0.11957816779613495, acc: 0.86, recall: 0.86, precision: 0.86, f_beta: 0.8599999999999999
train: step: 860, loss: 0.06222185119986534, acc: 0.91, recall: 0.91, precision: 0.9101640656262505, f_beta: 0.90999099909991
train: step: 861, loss: 0.05099618062376976, acc: 0.94, recall: 0.94, precision: 0.9428341384863124, f_beta: 0.9399038461538463
train: step: 862, loss: 0.09950217604637146, acc: 0.88, recall: 0.8799999999999999, precision: 0.885551948051948, f_beta: 0.8795664391810518
train: step: 863, loss: 0.08538089692592621, acc: 0.86, recall: 0.86, precision: 0.8605769230769231, f_beta: 0.8599439775910365
train: step: 864, loss: 0.044785164296627045, acc: 0.94, recall: 0.94, precision: 0.9428341384863124, f_beta: 0.9399038461538463
train: step: 865, loss: 0.0504172220826149, acc: 0.94, recall: 0.94, precision: 0.9407051282051282, f_beta: 0.9399759903961584
train: step: 866, loss: 0.05341658741235733, acc: 0.92, recall: 0.92, precision: 0.92, f_beta: 0.92
train: step: 867, loss: 0.07351513206958771, acc: 0.91, recall: 0.9099999999999999, precision: 0.9114813327980731, f_beta: 0.9099189270343309
train: step: 868, loss: 0.08040319383144379, acc: 0.87, recall: 0.87, precision: 0.8713368125250903, f_beta: 0.8698828946051447
train: step: 869, loss: 0.05141167715191841, acc: 0.94, recall: 0.94, precision: 0.9428341384863124, f_beta: 0.9399038461538463
train: step: 870, loss: 0.08024634420871735, acc: 0.86, recall: 0.86, precision: 0.875, f_beta: 0.8585858585858586
train: step: 871, loss: 0.10043696314096451, acc: 0.85, recall: 0.85, precision: 0.8501400560224089, f_beta: 0.84998499849985
train: step: 872, loss: 0.05734402686357498, acc: 0.94, recall: 0.94, precision: 0.94, f_beta: 0.94
train: step: 873, loss: 0.07153011113405228, acc: 0.91, recall: 0.91, precision: 0.9101640656262505, f_beta: 0.90999099909991
train: step: 874, loss: 0.07088176906108856, acc: 0.9, recall: 0.8999999999999999, precision: 0.9058441558441559, f_beta: 0.8996386993175431
train: step: 875, loss: 0.0845271646976471, acc: 0.89, recall: 0.89, precision: 0.8914090726615818, f_beta: 0.8899009108197378
train: step: 876, loss: 0.08312718570232391, acc: 0.89, recall: 0.89, precision: 0.89015606242497, f_beta: 0.88998899889989
train: step: 877, loss: 0.07595527917146683, acc: 0.92, recall: 0.92, precision: 0.92, f_beta: 0.92
train: step: 878, loss: 0.03594629466533661, acc: 0.97, recall: 0.97, precision: 0.9716981132075472, f_beta: 0.9699729756781104
train: step: 879, loss: 0.10142681747674942, acc: 0.85, recall: 0.85, precision: 0.8753753753753755, f_beta: 0.8474214220323468
train: step: 880, loss: 0.09277495741844177, acc: 0.87, recall: 0.87, precision: 0.8713368125250903, f_beta: 0.8698828946051447
train: step: 881, loss: 0.08643338829278946, acc: 0.86, recall: 0.86, precision: 0.8694581280788177, f_beta: 0.8590982286634461
train: step: 882, loss: 0.08217313885688782, acc: 0.89, recall: 0.89, precision: 0.89015606242497, f_beta: 0.88998899889989
train: step: 883, loss: 0.0705994963645935, acc: 0.92, recall: 0.9199999999999999, precision: 0.9206730769230769, f_beta: 0.919967987194878
train: step: 884, loss: 0.08614692836999893, acc: 0.92, recall: 0.9199999999999999, precision: 0.9206730769230769, f_beta: 0.919967987194878
train: step: 885, loss: 0.05816478282213211, acc: 0.94, recall: 0.94, precision: 0.94, f_beta: 0.94
train: step: 886, loss: 0.1051306501030922, acc: 0.88, recall: 0.88, precision: 0.8806089743589745, f_beta: 0.879951980792317
train: step: 887, loss: 0.05797669291496277, acc: 0.93, recall: 0.9299999999999999, precision: 0.9315535929345644, f_beta: 0.9299369432489241
train: step: 888, loss: 0.09662477672100067, acc: 0.89, recall: 0.89, precision: 0.8914090726615818, f_beta: 0.8899009108197378
train: step: 889, loss: 0.11170189082622528, acc: 0.85, recall: 0.85, precision: 0.8501400560224089, f_beta: 0.84998499849985
train: step: 890, loss: 0.11705619841814041, acc: 0.85, recall: 0.85, precision: 0.851264552388599, f_beta: 0.8498648783905516
train: step: 891, loss: 0.13248586654663086, acc: 0.84, recall: 0.8400000000000001, precision: 0.8421900161030595, f_beta: 0.8397435897435896
train: step: 892, loss: 0.0811481848359108, acc: 0.91, recall: 0.9099999999999999, precision: 0.9114813327980731, f_beta: 0.9099189270343309
train: step: 893, loss: 0.09013976156711578, acc: 0.9, recall: 0.9, precision: 0.9006410256410255, f_beta: 0.8999599839935974
train: step: 894, loss: 0.06392582505941391, acc: 0.9, recall: 0.9, precision: 0.9, f_beta: 0.9
train: step: 895, loss: 0.06335705518722534, acc: 0.91, recall: 0.91, precision: 0.9101640656262505, f_beta: 0.90999099909991
train: step: 896, loss: 0.10663767904043198, acc: 0.86, recall: 0.86, precision: 0.86, f_beta: 0.8599999999999999
train: step: 897, loss: 0.0916639119386673, acc: 0.87, recall: 0.87, precision: 0.8888188314417822, f_beta: 0.868407733576273
train: step: 898, loss: 0.1107330247759819, acc: 0.86, recall: 0.86, precision: 0.8623188405797102, f_beta: 0.859775641025641
train: step: 899, loss: 0.06857147812843323, acc: 0.93, recall: 0.9299999999999999, precision: 0.930172068827531, f_beta: 0.9299929992999301
train: step: 900, loss: 0.07574489712715149, acc: 0.89, recall: 0.89, precision: 0.8914090726615818, f_beta: 0.8899009108197378
train: step: 901, loss: 0.04080647602677345, acc: 0.94, recall: 0.94, precision: 0.94, f_beta: 0.94
train: step: 902, loss: 0.05836724117398262, acc: 0.93, recall: 0.9299999999999999, precision: 0.9315535929345644, f_beta: 0.9299369432489241
train: step: 903, loss: 0.0911521166563034, acc: 0.89, recall: 0.89, precision: 0.89015606242497, f_beta: 0.88998899889989
train: step: 904, loss: 0.07731030136346817, acc: 0.88, recall: 0.88, precision: 0.8958333333333333, f_beta: 0.8787878787878789
train: step: 905, loss: 0.039940182119607925, acc: 0.97, recall: 0.97, precision: 0.9716981132075472, f_beta: 0.9699729756781104
train: step: 906, loss: 0.19566601514816284, acc: 0.83, recall: 0.83, precision: 0.8301320528211285, f_beta: 0.8299829982998299
train: step: 907, loss: 0.10770919173955917, acc: 0.85, recall: 0.85, precision: 0.8501400560224089, f_beta: 0.84998499849985
train: step: 908, loss: 0.050381869077682495, acc: 0.95, recall: 0.95, precision: 0.9545454545454546, f_beta: 0.949874686716792
train: step: 909, loss: 0.0904783234000206, acc: 0.9, recall: 0.9, precision: 0.9006410256410255, f_beta: 0.8999599839935974
train: step: 910, loss: 0.0958520695567131, acc: 0.88, recall: 0.88, precision: 0.8824476650563606, f_beta: 0.8798076923076923
train: step: 911, loss: 0.1016872227191925, acc: 0.86, recall: 0.8600000000000001, precision: 0.8652597402597402, f_beta: 0.8594941790445605
train: step: 912, loss: 0.07131446152925491, acc: 0.9, recall: 0.9, precision: 0.9006410256410255, f_beta: 0.8999599839935974
train: step: 913, loss: 0.052085667848587036, acc: 0.94, recall: 0.94, precision: 0.9428341384863124, f_beta: 0.9399038461538463
train: step: 914, loss: 0.04163946583867073, acc: 0.94, recall: 0.94, precision: 0.9428341384863124, f_beta: 0.9399038461538463
train: step: 915, loss: 0.08162630349397659, acc: 0.89, recall: 0.89, precision: 0.8914090726615818, f_beta: 0.8899009108197378
train: step: 916, loss: 0.15579481422901154, acc: 0.82, recall: 0.8200000000000001, precision: 0.8333333333333334, f_beta: 0.8181818181818181
train: step: 917, loss: 0.13833536207675934, acc: 0.87, recall: 0.87, precision: 0.8713368125250903, f_beta: 0.8698828946051447
train: step: 918, loss: 0.111297607421875, acc: 0.86, recall: 0.86, precision: 0.8605769230769231, f_beta: 0.8599439775910365
train: step: 919, loss: 0.10655930638313293, acc: 0.88, recall: 0.8799999999999999, precision: 0.885551948051948, f_beta: 0.8795664391810518
train: step: 920, loss: 0.05252986028790474, acc: 0.93, recall: 0.9299999999999999, precision: 0.9385964912280702, f_beta: 0.9296553110240177
train: step: 921, loss: 0.09673336893320084, acc: 0.87, recall: 0.87, precision: 0.8823894171145101, f_beta: 0.8689384010484928
train: step: 922, loss: 0.1347346305847168, acc: 0.82, recall: 0.82, precision: 0.8395585738539898, f_beta: 0.8173701298701298
train: step: 923, loss: 0.08767852932214737, acc: 0.87, recall: 0.87, precision: 0.8823894171145101, f_beta: 0.8689384010484928
train: step: 924, loss: 0.07326528429985046, acc: 0.9, recall: 0.8999999999999999, precision: 0.9105090311986863, f_beta: 0.8993558776167472
train: step: 925, loss: 0.05309189856052399, acc: 0.95, recall: 0.95, precision: 0.9516258530710557, f_beta: 0.949954959463517
train: step: 926, loss: 0.13601423799991608, acc: 0.83, recall: 0.83, precision: 0.8301320528211285, f_beta: 0.8299829982998299
train: step: 927, loss: 0.09427686780691147, acc: 0.85, recall: 0.85, precision: 0.8617197188921042, f_beta: 0.8487750781328762
train: step: 928, loss: 0.05443623661994934, acc: 0.93, recall: 0.9299999999999999, precision: 0.930172068827531, f_beta: 0.9299929992999301
train: step: 929, loss: 0.07898109406232834, acc: 0.9, recall: 0.8999999999999999, precision: 0.9058441558441559, f_beta: 0.8996386993175431
train: step: 930, loss: 0.042004361748695374, acc: 0.96, recall: 0.96, precision: 0.962962962962963, f_beta: 0.9599358974358974
train: step: 931, loss: 0.08342943340539932, acc: 0.91, recall: 0.91, precision: 0.9101640656262505, f_beta: 0.90999099909991
train: step: 932, loss: 0.11886471509933472, acc: 0.85, recall: 0.85, precision: 0.851264552388599, f_beta: 0.8498648783905516
train: step: 933, loss: 0.11560386419296265, acc: 0.87, recall: 0.87, precision: 0.8773969808241534, f_beta: 0.8693598633303186
train: step: 934, loss: 0.08888528496026993, acc: 0.86, recall: 0.86, precision: 0.875, f_beta: 0.8585858585858586
train: step: 935, loss: 0.08313087373971939, acc: 0.9, recall: 0.8999999999999999, precision: 0.9105090311986863, f_beta: 0.8993558776167472
train: step: 936, loss: 0.11352647095918655, acc: 0.85, recall: 0.8500000000000001, precision: 0.8569971440228478, f_beta: 0.8492613807657521
train: step: 937, loss: 0.07627639174461365, acc: 0.89, recall: 0.8899999999999999, precision: 0.8939393939393939, f_beta: 0.8897243107769424
train: step: 938, loss: 0.08388204872608185, acc: 0.87, recall: 0.87, precision: 0.8773969808241534, f_beta: 0.8693598633303186
train: step: 939, loss: 0.12004174292087555, acc: 0.83, recall: 0.8300000000000001, precision: 0.8410500206696983, f_beta: 0.8286117552172598
train: step: 940, loss: 0.06990120559930801, acc: 0.91, recall: 0.9099999999999999, precision: 0.9114813327980731, f_beta: 0.9099189270343309
train: step: 941, loss: 0.0737110897898674, acc: 0.91, recall: 0.9099999999999999, precision: 0.9181966544267646, f_beta: 0.9095568284594513
train: step: 942, loss: 0.06431728601455688, acc: 0.94, recall: 0.94, precision: 0.9428341384863124, f_beta: 0.9399038461538463
train: step: 943, loss: 0.08323893696069717, acc: 0.9, recall: 0.9, precision: 0.9006410256410255, f_beta: 0.8999599839935974
train: step: 944, loss: 0.10355167090892792, acc: 0.84, recall: 0.84, precision: 0.8405448717948718, f_beta: 0.8399359743897559
train: step: 945, loss: 0.05818003788590431, acc: 0.92, recall: 0.9199999999999999, precision: 0.9206730769230769, f_beta: 0.919967987194878
train: step: 946, loss: 0.1026889905333519, acc: 0.86, recall: 0.86, precision: 0.875, f_beta: 0.8585858585858586
train: step: 947, loss: 0.07154465466737747, acc: 0.92, recall: 0.9199999999999999, precision: 0.9227053140096618, f_beta: 0.9198717948717948
train: step: 948, loss: 0.06308059394359589, acc: 0.93, recall: 0.9299999999999999, precision: 0.930172068827531, f_beta: 0.9299929992999301
train: step: 949, loss: 0.04812907055020332, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 950, loss: 0.06625758111476898, acc: 0.91, recall: 0.9099999999999999, precision: 0.9141414141414141, f_beta: 0.9097744360902256
train: step: 951, loss: 0.07517730444669724, acc: 0.91, recall: 0.9099999999999999, precision: 0.9114813327980731, f_beta: 0.9099189270343309
train: step: 952, loss: 0.07836906611919403, acc: 0.88, recall: 0.88, precision: 0.8806089743589745, f_beta: 0.879951980792317
train: step: 953, loss: 0.07517831772565842, acc: 0.88, recall: 0.88, precision: 0.8824476650563606, f_beta: 0.8798076923076923
train: step: 954, loss: 0.08286012709140778, acc: 0.88, recall: 0.88, precision: 0.8806089743589745, f_beta: 0.879951980792317
train: step: 955, loss: 0.0672568753361702, acc: 0.96, recall: 0.96, precision: 0.9607371794871795, f_beta: 0.959983993597439
train: step: 956, loss: 0.057044707238674164, acc: 0.91, recall: 0.91, precision: 0.9101640656262505, f_beta: 0.90999099909991
train: step: 957, loss: 0.05442758649587631, acc: 0.95, recall: 0.95, precision: 0.9516258530710557, f_beta: 0.949954959463517
train: step: 958, loss: 0.08042697608470917, acc: 0.91, recall: 0.91, precision: 0.9101640656262505, f_beta: 0.90999099909991
train: step: 959, loss: 0.09526042640209198, acc: 0.85, recall: 0.85, precision: 0.851264552388599, f_beta: 0.8498648783905516
train: step: 960, loss: 0.13119438290596008, acc: 0.83, recall: 0.8300000000000001, precision: 0.8311922922521076, f_beta: 0.8298468621759585
train: step: 961, loss: 0.07898406684398651, acc: 0.9, recall: 0.8999999999999999, precision: 0.9025764895330113, f_beta: 0.8998397435897436
train: step: 962, loss: 0.07905980944633484, acc: 0.9, recall: 0.9, precision: 0.9, f_beta: 0.9
train: step: 963, loss: 0.06010822206735611, acc: 0.94, recall: 0.94, precision: 0.94, f_beta: 0.94
train: step: 964, loss: 0.060795459896326065, acc: 0.91, recall: 0.91, precision: 0.9101640656262505, f_beta: 0.90999099909991
train: step: 965, loss: 0.08723250031471252, acc: 0.88, recall: 0.88, precision: 0.8899835796387521, f_beta: 0.8792270531400966
train: step: 966, loss: 0.05142970010638237, acc: 0.94, recall: 0.94, precision: 0.9464285714285714, f_beta: 0.939783219590526
train: step: 967, loss: 0.041022732853889465, acc: 0.96, recall: 0.96, precision: 0.96, f_beta: 0.96
train: step: 968, loss: 0.05640966072678566, acc: 0.95, recall: 0.95, precision: 0.9516258530710557, f_beta: 0.949954959463517
train: step: 969, loss: 0.043085698038339615, acc: 0.93, recall: 0.9299999999999999, precision: 0.930172068827531, f_beta: 0.9299929992999301
train: step: 970, loss: 0.07212453335523605, acc: 0.89, recall: 0.8899999999999999, precision: 0.8939393939393939, f_beta: 0.8897243107769424
train: step: 971, loss: 0.0749133750796318, acc: 0.9, recall: 0.8999999999999999, precision: 0.9025764895330113, f_beta: 0.8998397435897436
train: step: 972, loss: 0.0972239226102829, acc: 0.88, recall: 0.8799999999999999, precision: 0.885551948051948, f_beta: 0.8795664391810518
train: step: 973, loss: 0.07217849045991898, acc: 0.89, recall: 0.89, precision: 0.9030591153369161, f_beta: 0.8891017239641092
train: step: 974, loss: 0.1280783861875534, acc: 0.79, recall: 0.79, precision: 0.7910477719791249, f_beta: 0.7898108297467722
train: step: 975, loss: 0.04349943995475769, acc: 0.94, recall: 0.94, precision: 0.9407051282051282, f_beta: 0.9399759903961584
train: step: 976, loss: 0.12747342884540558, acc: 0.83, recall: 0.8300000000000001, precision: 0.8311922922521076, f_beta: 0.8298468621759585
train: step: 977, loss: 0.07410430163145065, acc: 0.9, recall: 0.8999999999999999, precision: 0.9058441558441559, f_beta: 0.8996386993175431
train: step: 978, loss: 0.07477882504463196, acc: 0.91, recall: 0.91, precision: 0.9101640656262505, f_beta: 0.90999099909991
train: step: 979, loss: 0.05208726227283478, acc: 0.91, recall: 0.91, precision: 0.9101640656262505, f_beta: 0.90999099909991
train: step: 980, loss: 0.05381517484784126, acc: 0.95, recall: 0.95, precision: 0.9516258530710557, f_beta: 0.949954959463517
train: step: 981, loss: 0.09822593629360199, acc: 0.9, recall: 0.8999999999999999, precision: 0.9025764895330113, f_beta: 0.8998397435897436
train: step: 982, loss: 0.05086442083120346, acc: 0.94, recall: 0.94, precision: 0.94, f_beta: 0.94
train: step: 983, loss: 0.07151266932487488, acc: 0.9, recall: 0.9, precision: 0.9006410256410255, f_beta: 0.8999599839935974
train: step: 984, loss: 0.08930648863315582, acc: 0.9, recall: 0.9, precision: 0.9006410256410255, f_beta: 0.8999599839935974
train: step: 985, loss: 0.07752075046300888, acc: 0.92, recall: 0.92, precision: 0.92, f_beta: 0.92
train: step: 986, loss: 0.05950987711548805, acc: 0.94, recall: 0.94, precision: 0.94, f_beta: 0.94
train: step: 987, loss: 0.053440943360328674, acc: 0.95, recall: 0.95, precision: 0.9501800720288115, f_beta: 0.9499949994999499
train: step: 988, loss: 0.07782799005508423, acc: 0.92, recall: 0.9199999999999999, precision: 0.9261363636363636, f_beta: 0.9197109594540345
train: step: 989, loss: 0.0760851576924324, acc: 0.9, recall: 0.9, precision: 0.9006410256410255, f_beta: 0.8999599839935974
train: step: 990, loss: 0.02095101587474346, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 991, loss: 0.04977474734187126, acc: 0.96, recall: 0.96, precision: 0.9607371794871795, f_beta: 0.959983993597439
train: step: 992, loss: 0.09035863727331161, acc: 0.88, recall: 0.88, precision: 0.8899835796387521, f_beta: 0.8792270531400966
train: step: 993, loss: 0.06208869814872742, acc: 0.91, recall: 0.9099999999999999, precision: 0.9141414141414141, f_beta: 0.9097744360902256
train: step: 994, loss: 0.09016373753547668, acc: 0.87, recall: 0.87, precision: 0.8713368125250903, f_beta: 0.8698828946051447
train: step: 995, loss: 0.042540665715932846, acc: 0.93, recall: 0.9299999999999999, precision: 0.9315535929345644, f_beta: 0.9299369432489241
train: step: 996, loss: 0.057551849633455276, acc: 0.92, recall: 0.9199999999999999, precision: 0.9206730769230769, f_beta: 0.919967987194878
train: step: 997, loss: 0.07634822279214859, acc: 0.92, recall: 0.9199999999999999, precision: 0.9206730769230769, f_beta: 0.919967987194878
train: step: 998, loss: 0.09272841364145279, acc: 0.9, recall: 0.9, precision: 0.9, f_beta: 0.9
train: step: 999, loss: 0.05929730832576752, acc: 0.92, recall: 0.92, precision: 0.92, f_beta: 0.92
train: step: 1000, loss: 0.046215519309043884, acc: 0.95, recall: 0.95, precision: 0.9501800720288115, f_beta: 0.9499949994999499
train: step: 1001, loss: 0.06158054992556572, acc: 0.93, recall: 0.9299999999999999, precision: 0.930172068827531, f_beta: 0.9299929992999301
train: step: 1002, loss: 0.06488069891929626, acc: 0.92, recall: 0.92, precision: 0.92, f_beta: 0.92
train: step: 1003, loss: 0.08675365149974823, acc: 0.89, recall: 0.89, precision: 0.8914090726615818, f_beta: 0.8899009108197378
train: step: 1004, loss: 0.0731559693813324, acc: 0.88, recall: 0.88, precision: 0.88, f_beta: 0.88
train: step: 1005, loss: 0.10274059325456619, acc: 0.87, recall: 0.87, precision: 0.8701480592236894, f_beta: 0.8699869986998701
train: step: 1006, loss: 0.058492496609687805, acc: 0.91, recall: 0.9099999999999999, precision: 0.9114813327980731, f_beta: 0.9099189270343309
train: step: 1007, loss: 0.09478776901960373, acc: 0.88, recall: 0.88, precision: 0.8824476650563606, f_beta: 0.8798076923076923
train: step: 1008, loss: 0.07556484639644623, acc: 0.9, recall: 0.8999999999999999, precision: 0.9025764895330113, f_beta: 0.8998397435897436
train: step: 1009, loss: 0.10317827016115189, acc: 0.85, recall: 0.85, precision: 0.8501400560224089, f_beta: 0.84998499849985
train: step: 1010, loss: 0.03183973208069801, acc: 0.95, recall: 0.95, precision: 0.9501800720288115, f_beta: 0.9499949994999499
train: step: 1011, loss: 0.05796154960989952, acc: 0.92, recall: 0.9199999999999999, precision: 0.9206730769230769, f_beta: 0.919967987194878
train: step: 1012, loss: 0.031568557024002075, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 1013, loss: 0.0688464492559433, acc: 0.91, recall: 0.91, precision: 0.9101640656262505, f_beta: 0.90999099909991
train: step: 1014, loss: 0.08325520157814026, acc: 0.87, recall: 0.87, precision: 0.8888188314417822, f_beta: 0.868407733576273
train: step: 1015, loss: 0.07526426762342453, acc: 0.91, recall: 0.91, precision: 0.9101640656262505, f_beta: 0.90999099909991
train: step: 1016, loss: 0.1109127327799797, acc: 0.86, recall: 0.86, precision: 0.8623188405797102, f_beta: 0.859775641025641
train: step: 1017, loss: 0.0815848633646965, acc: 0.89, recall: 0.89, precision: 0.89015606242497, f_beta: 0.88998899889989
train: step: 1018, loss: 0.051276903599500656, acc: 0.95, recall: 0.95, precision: 0.9501800720288115, f_beta: 0.9499949994999499
train: step: 1019, loss: 0.08484556525945663, acc: 0.91, recall: 0.9099999999999999, precision: 0.9181966544267646, f_beta: 0.9095568284594513
train: step: 1020, loss: 0.056565847247838974, acc: 0.91, recall: 0.9099999999999999, precision: 0.9114813327980731, f_beta: 0.9099189270343309
train: step: 1021, loss: 0.05032166466116905, acc: 0.95, recall: 0.95, precision: 0.9545454545454546, f_beta: 0.949874686716792
train: step: 1022, loss: 0.03286329284310341, acc: 0.95, recall: 0.95, precision: 0.9545454545454546, f_beta: 0.949874686716792
train: step: 1023, loss: 0.058217503130435944, acc: 0.93, recall: 0.9299999999999999, precision: 0.930172068827531, f_beta: 0.9299929992999301
train: step: 1024, loss: 0.08095934242010117, acc: 0.89, recall: 0.8899999999999999, precision: 0.8939393939393939, f_beta: 0.8897243107769424
train: step: 1025, loss: 0.024737326428294182, acc: 0.96, recall: 0.96, precision: 0.9607371794871795, f_beta: 0.959983993597439
train: step: 1026, loss: 0.07836733013391495, acc: 0.9, recall: 0.9, precision: 0.9006410256410255, f_beta: 0.8999599839935974
train: step: 1027, loss: 0.06966743618249893, acc: 0.91, recall: 0.9099999999999999, precision: 0.9114813327980731, f_beta: 0.9099189270343309
train: step: 1028, loss: 0.0808616429567337, acc: 0.9, recall: 0.9, precision: 0.9006410256410255, f_beta: 0.8999599839935974
train: step: 1029, loss: 0.07163895666599274, acc: 0.91, recall: 0.9099999999999999, precision: 0.9181966544267646, f_beta: 0.9095568284594513
train: step: 1030, loss: 0.04464320093393326, acc: 0.93, recall: 0.9299999999999999, precision: 0.930172068827531, f_beta: 0.9299929992999301
train: step: 1031, loss: 0.07821512222290039, acc: 0.91, recall: 0.9099999999999999, precision: 0.9114813327980731, f_beta: 0.9099189270343309
train: step: 1032, loss: 0.056807875633239746, acc: 0.92, recall: 0.9199999999999999, precision: 0.9206730769230769, f_beta: 0.919967987194878
train: step: 1033, loss: 0.08671566098928452, acc: 0.89, recall: 0.89, precision: 0.89015606242497, f_beta: 0.88998899889989
train: step: 1034, loss: 0.08892005681991577, acc: 0.89, recall: 0.89, precision: 0.89015606242497, f_beta: 0.88998899889989
train: step: 1035, loss: 0.08438184857368469, acc: 0.92, recall: 0.9199999999999999, precision: 0.9261363636363636, f_beta: 0.9197109594540345
train: step: 1036, loss: 0.09409347176551819, acc: 0.85, recall: 0.85, precision: 0.851264552388599, f_beta: 0.8498648783905516
train: step: 1037, loss: 0.07835758477449417, acc: 0.9, recall: 0.8999999999999999, precision: 0.9025764895330113, f_beta: 0.8998397435897436
train: step: 1038, loss: 0.06245802715420723, acc: 0.91, recall: 0.9099999999999999, precision: 0.9141414141414141, f_beta: 0.9097744360902256
train: step: 1039, loss: 0.037031807005405426, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 1040, loss: 0.10838470607995987, acc: 0.88, recall: 0.88, precision: 0.8824476650563606, f_beta: 0.8798076923076923
train: step: 1041, loss: 0.08333612233400345, acc: 0.89, recall: 0.8899999999999999, precision: 0.8939393939393939, f_beta: 0.8897243107769424
train: step: 1042, loss: 0.07783278077840805, acc: 0.92, recall: 0.9199999999999999, precision: 0.9227053140096618, f_beta: 0.9198717948717948
train: step: 1043, loss: 0.05329226329922676, acc: 0.92, recall: 0.9199999999999999, precision: 0.9206730769230769, f_beta: 0.919967987194878
train: step: 1044, loss: 0.07111669331789017, acc: 0.89, recall: 0.89, precision: 0.8914090726615818, f_beta: 0.8899009108197378
train: step: 1045, loss: 0.04862510785460472, acc: 0.96, recall: 0.96, precision: 0.9607371794871795, f_beta: 0.959983993597439
train: step: 1046, loss: 0.0529150664806366, acc: 0.93, recall: 0.9299999999999999, precision: 0.930172068827531, f_beta: 0.9299929992999301
train: step: 1047, loss: 0.10774926096200943, acc: 0.87, recall: 0.87, precision: 0.8701480592236894, f_beta: 0.8699869986998701
train: step: 1048, loss: 0.05420670658349991, acc: 0.93, recall: 0.9299999999999999, precision: 0.9315535929345644, f_beta: 0.9299369432489241
train: step: 1049, loss: 0.10071416199207306, acc: 0.91, recall: 0.9099999999999999, precision: 0.9181966544267646, f_beta: 0.9095568284594513
train: step: 1050, loss: 0.06514798849821091, acc: 0.92, recall: 0.92, precision: 0.92, f_beta: 0.92
train: step: 1051, loss: 0.0620310939848423, acc: 0.92, recall: 0.92, precision: 0.92, f_beta: 0.92
train: step: 1052, loss: 0.1291971504688263, acc: 0.83, recall: 0.83, precision: 0.8301320528211285, f_beta: 0.8299829982998299
train: step: 1053, loss: 0.10821186751127243, acc: 0.86, recall: 0.8600000000000001, precision: 0.8652597402597402, f_beta: 0.8594941790445605
train: step: 1054, loss: 0.09683160483837128, acc: 0.86, recall: 0.86, precision: 0.890625, f_beta: 0.8572011423908608
train: step: 1055, loss: 0.10208907723426819, acc: 0.82, recall: 0.8200000000000001, precision: 0.8246753246753247, f_beta: 0.8193496587715776
train: step: 1056, loss: 0.06070387363433838, acc: 0.92, recall: 0.9199999999999999, precision: 0.9261363636363636, f_beta: 0.9197109594540345
train: step: 1057, loss: 0.06944864988327026, acc: 0.89, recall: 0.89, precision: 0.8914090726615818, f_beta: 0.8899009108197378
train: step: 1058, loss: 0.03339862450957298, acc: 0.98, recall: 0.98, precision: 0.98, f_beta: 0.98
train: step: 1059, loss: 0.08290085196495056, acc: 0.88, recall: 0.88, precision: 0.8824476650563606, f_beta: 0.8798076923076923
train: step: 1060, loss: 0.057720478624105453, acc: 0.94, recall: 0.94, precision: 0.9407051282051282, f_beta: 0.9399759903961584
train: step: 1061, loss: 0.12474512308835983, acc: 0.84, recall: 0.8400000000000001, precision: 0.8421900161030595, f_beta: 0.8397435897435896
train: step: 1062, loss: 0.027302663773298264, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 1063, loss: 0.09985092282295227, acc: 0.87, recall: 0.87, precision: 0.8888188314417822, f_beta: 0.868407733576273
train: step: 1064, loss: 0.0329773873090744, acc: 0.95, recall: 0.95, precision: 0.9501800720288115, f_beta: 0.9499949994999499
train: step: 1065, loss: 0.13103128969669342, acc: 0.84, recall: 0.84, precision: 0.8405448717948718, f_beta: 0.8399359743897559
train: step: 1066, loss: 0.029526472091674805, acc: 0.97, recall: 0.97, precision: 0.9716981132075472, f_beta: 0.9699729756781104
train: step: 1067, loss: 0.04856247082352638, acc: 0.95, recall: 0.95, precision: 0.9501800720288115, f_beta: 0.9499949994999499
train: step: 1068, loss: 0.10883747041225433, acc: 0.86, recall: 0.8600000000000001, precision: 0.8652597402597402, f_beta: 0.8594941790445605
train: step: 1069, loss: 0.07015207409858704, acc: 0.95, recall: 0.95, precision: 0.9516258530710557, f_beta: 0.949954959463517
train: step: 1070, loss: 0.047720443457365036, acc: 0.94, recall: 0.94, precision: 0.9407051282051282, f_beta: 0.9399759903961584
train: step: 1071, loss: 0.0612313449382782, acc: 0.92, recall: 0.9199999999999999, precision: 0.9261363636363636, f_beta: 0.9197109594540345
train: step: 1072, loss: 0.0821182057261467, acc: 0.87, recall: 0.87, precision: 0.8773969808241534, f_beta: 0.8693598633303186
train: step: 1073, loss: 0.1040450781583786, acc: 0.86, recall: 0.86, precision: 0.8605769230769231, f_beta: 0.8599439775910365
train: step: 1074, loss: 0.08070112019777298, acc: 0.89, recall: 0.89, precision: 0.89015606242497, f_beta: 0.88998899889989
train: step: 1075, loss: 0.07424583286046982, acc: 0.92, recall: 0.9199999999999999, precision: 0.9206730769230769, f_beta: 0.919967987194878
train: step: 1076, loss: 0.09281101822853088, acc: 0.88, recall: 0.8799999999999999, precision: 0.885551948051948, f_beta: 0.8795664391810518
train: step: 1077, loss: 0.08747375756502151, acc: 0.88, recall: 0.8799999999999999, precision: 0.885551948051948, f_beta: 0.8795664391810518
train: step: 1078, loss: 0.05412217974662781, acc: 0.94, recall: 0.94, precision: 0.94, f_beta: 0.94
train: step: 1079, loss: 0.07122260332107544, acc: 0.9, recall: 0.9, precision: 0.9006410256410255, f_beta: 0.8999599839935974
train: step: 1080, loss: 0.0720054879784584, acc: 0.9, recall: 0.9, precision: 0.9006410256410255, f_beta: 0.8999599839935974
train: step: 1081, loss: 0.05739384517073631, acc: 0.93, recall: 0.9299999999999999, precision: 0.9343434343434343, f_beta: 0.9298245614035088
train: step: 1082, loss: 0.055605269968509674, acc: 0.93, recall: 0.9299999999999999, precision: 0.9315535929345644, f_beta: 0.9299369432489241
train: step: 1083, loss: 0.0524124950170517, acc: 0.94, recall: 0.94, precision: 0.94, f_beta: 0.94
train: step: 1084, loss: 0.0686420425772667, acc: 0.93, recall: 0.9299999999999999, precision: 0.930172068827531, f_beta: 0.9299929992999301
train: step: 1085, loss: 0.05780795216560364, acc: 0.94, recall: 0.94, precision: 0.9407051282051282, f_beta: 0.9399759903961584
train: step: 1086, loss: 0.09475602954626083, acc: 0.88, recall: 0.88, precision: 0.8806089743589745, f_beta: 0.879951980792317
train: step: 1087, loss: 0.062401264905929565, acc: 0.93, recall: 0.9299999999999999, precision: 0.9343434343434343, f_beta: 0.9298245614035088
train: step: 1088, loss: 0.05498964712023735, acc: 0.93, recall: 0.9299999999999999, precision: 0.9315535929345644, f_beta: 0.9299369432489241
train: step: 1089, loss: 0.07489092648029327, acc: 0.92, recall: 0.9199999999999999, precision: 0.9206730769230769, f_beta: 0.919967987194878
train: step: 1090, loss: 0.07524852454662323, acc: 0.91, recall: 0.9099999999999999, precision: 0.9114813327980731, f_beta: 0.9099189270343309
train: step: 1091, loss: 0.061574697494506836, acc: 0.94, recall: 0.94, precision: 0.9428341384863124, f_beta: 0.9399038461538463
train: step: 1092, loss: 0.08584730327129364, acc: 0.91, recall: 0.91, precision: 0.9101640656262505, f_beta: 0.90999099909991
train: step: 1093, loss: 0.068474680185318, acc: 0.91, recall: 0.91, precision: 0.9101640656262505, f_beta: 0.90999099909991
train: step: 1094, loss: 0.09998634457588196, acc: 0.87, recall: 0.87, precision: 0.8773969808241534, f_beta: 0.8693598633303186
train: step: 1095, loss: 0.0783703625202179, acc: 0.92, recall: 0.9199999999999999, precision: 0.9206730769230769, f_beta: 0.919967987194878
train: step: 1096, loss: 0.06116079166531563, acc: 0.94, recall: 0.94, precision: 0.94, f_beta: 0.94
train: step: 1097, loss: 0.04352966323494911, acc: 0.95, recall: 0.95, precision: 0.9501800720288115, f_beta: 0.9499949994999499
train: step: 1098, loss: 0.03891966864466667, acc: 0.95, recall: 0.95, precision: 0.9516258530710557, f_beta: 0.949954959463517
train: step: 1099, loss: 0.042199283838272095, acc: 0.95, recall: 0.95, precision: 0.9516258530710557, f_beta: 0.949954959463517
train: step: 1100, loss: 0.07722848653793335, acc: 0.89, recall: 0.89, precision: 0.8914090726615818, f_beta: 0.8899009108197378
train: step: 1101, loss: 0.0763840600848198, acc: 0.91, recall: 0.9099999999999999, precision: 0.9114813327980731, f_beta: 0.9099189270343309
train: step: 1102, loss: 0.06925924122333527, acc: 0.93, recall: 0.9299999999999999, precision: 0.930172068827531, f_beta: 0.9299929992999301
train: step: 1103, loss: 0.08721614629030228, acc: 0.9, recall: 0.8999999999999999, precision: 0.9025764895330113, f_beta: 0.8998397435897436
train: step: 1104, loss: 0.060271263122558594, acc: 0.93, recall: 0.9299999999999999, precision: 0.9315535929345644, f_beta: 0.9299369432489241
train: step: 1105, loss: 0.06079816445708275, acc: 0.93, recall: 0.9299999999999999, precision: 0.930172068827531, f_beta: 0.9299929992999301
train: step: 1106, loss: 0.10297456383705139, acc: 0.87, recall: 0.87, precision: 0.8737373737373737, f_beta: 0.8696741854636592
train: step: 1107, loss: 0.06848761439323425, acc: 0.9, recall: 0.9, precision: 0.9006410256410255, f_beta: 0.8999599839935974
train: step: 1108, loss: 0.04303663223981857, acc: 0.94, recall: 0.94, precision: 0.9407051282051282, f_beta: 0.9399759903961584
train: step: 1109, loss: 0.07485061883926392, acc: 0.89, recall: 0.89, precision: 0.8914090726615818, f_beta: 0.8899009108197378
train: step: 1110, loss: 0.08927999436855316, acc: 0.89, recall: 0.8899999999999999, precision: 0.8939393939393939, f_beta: 0.8897243107769424
train: step: 1111, loss: 0.07801744341850281, acc: 0.9, recall: 0.8999999999999999, precision: 0.9058441558441559, f_beta: 0.8996386993175431
train: step: 1112, loss: 0.08669840544462204, acc: 0.9, recall: 0.8999999999999999, precision: 0.9058441558441559, f_beta: 0.8996386993175431
train: step: 1113, loss: 0.10361146926879883, acc: 0.88, recall: 0.88, precision: 0.8824476650563606, f_beta: 0.8798076923076923
train: step: 1114, loss: 0.043159857392311096, acc: 0.96, recall: 0.96, precision: 0.962962962962963, f_beta: 0.9599358974358974
train: step: 1115, loss: 0.0906192809343338, acc: 0.86, recall: 0.86, precision: 0.86, f_beta: 0.8599999999999999
train: step: 1116, loss: 0.04814556986093521, acc: 0.93, recall: 0.9299999999999999, precision: 0.9315535929345644, f_beta: 0.9299369432489241
train: step: 1117, loss: 0.07585284113883972, acc: 0.91, recall: 0.9099999999999999, precision: 0.9141414141414141, f_beta: 0.9097744360902256
train: step: 1118, loss: 0.029563292860984802, acc: 0.96, recall: 0.96, precision: 0.9607371794871795, f_beta: 0.959983993597439
train: step: 1119, loss: 0.052989207208156586, acc: 0.94, recall: 0.94, precision: 0.9407051282051282, f_beta: 0.9399759903961584
train: step: 1120, loss: 0.09534762054681778, acc: 0.88, recall: 0.88, precision: 0.8958333333333333, f_beta: 0.8787878787878789
train: step: 1121, loss: 0.06382755935192108, acc: 0.93, recall: 0.9299999999999999, precision: 0.930172068827531, f_beta: 0.9299929992999301
train: step: 1122, loss: 0.07627109438180923, acc: 0.92, recall: 0.9199999999999999, precision: 0.9206730769230769, f_beta: 0.919967987194878
train: step: 1123, loss: 0.05983562767505646, acc: 0.92, recall: 0.9199999999999999, precision: 0.9227053140096618, f_beta: 0.9198717948717948
train: step: 1124, loss: 0.04353304207324982, acc: 0.94, recall: 0.94, precision: 0.9464285714285714, f_beta: 0.939783219590526
train: step: 1125, loss: 0.035537272691726685, acc: 0.96, recall: 0.96, precision: 0.96, f_beta: 0.96
train: step: 1126, loss: 0.06772569566965103, acc: 0.9, recall: 0.9, precision: 0.9, f_beta: 0.9
train: step: 1127, loss: 0.042208097875118256, acc: 0.94, recall: 0.94, precision: 0.9428341384863124, f_beta: 0.9399038461538463
train: step: 1128, loss: 0.06682305037975311, acc: 0.92, recall: 0.9199999999999999, precision: 0.9206730769230769, f_beta: 0.919967987194878
train: step: 1129, loss: 0.06870004534721375, acc: 0.93, recall: 0.9299999999999999, precision: 0.9315535929345644, f_beta: 0.9299369432489241
train: step: 1130, loss: 0.062472205609083176, acc: 0.92, recall: 0.92, precision: 0.92, f_beta: 0.92
train: step: 1131, loss: 0.07858376204967499, acc: 0.88, recall: 0.8799999999999999, precision: 0.885551948051948, f_beta: 0.8795664391810518
train: step: 1132, loss: 0.14561930298805237, acc: 0.8, recall: 0.8, precision: 0.8183361629881154, f_beta: 0.797077922077922
train: step: 1133, loss: 0.06571469455957413, acc: 0.92, recall: 0.9199999999999999, precision: 0.9261363636363636, f_beta: 0.9197109594540345
train: step: 1134, loss: 0.04098837822675705, acc: 0.96, recall: 0.96, precision: 0.96, f_beta: 0.96
train: step: 1135, loss: 0.06545890867710114, acc: 0.93, recall: 0.9299999999999999, precision: 0.930172068827531, f_beta: 0.9299929992999301
train: step: 1136, loss: 0.08336887508630753, acc: 0.89, recall: 0.89, precision: 0.89015606242497, f_beta: 0.88998899889989
train: step: 1137, loss: 0.05656203255057335, acc: 0.92, recall: 0.9199999999999999, precision: 0.9206730769230769, f_beta: 0.919967987194878
train: step: 1138, loss: 0.12522348761558533, acc: 0.83, recall: 0.83, precision: 0.8301320528211285, f_beta: 0.8299829982998299
train: step: 1139, loss: 0.05628344416618347, acc: 0.93, recall: 0.9299999999999999, precision: 0.9315535929345644, f_beta: 0.9299369432489241
train: step: 1140, loss: 0.07173598557710648, acc: 0.91, recall: 0.9099999999999999, precision: 0.9141414141414141, f_beta: 0.9097744360902256
train: step: 1141, loss: 0.0649716705083847, acc: 0.93, recall: 0.9299999999999999, precision: 0.930172068827531, f_beta: 0.9299929992999301
train: step: 1142, loss: 0.07759536802768707, acc: 0.89, recall: 0.8899999999999999, precision: 0.8977968176254589, f_beta: 0.8894583458948848
train: step: 1143, loss: 0.07684101164340973, acc: 0.89, recall: 0.89, precision: 0.8914090726615818, f_beta: 0.8899009108197378
train: step: 1144, loss: 0.029327373951673508, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 1145, loss: 0.05726545676589012, acc: 0.93, recall: 0.9299999999999999, precision: 0.930172068827531, f_beta: 0.9299929992999301
train: step: 1146, loss: 0.07091541588306427, acc: 0.89, recall: 0.89, precision: 0.8914090726615818, f_beta: 0.8899009108197378
train: step: 1147, loss: 0.03813369572162628, acc: 0.96, recall: 0.96, precision: 0.9607371794871795, f_beta: 0.959983993597439
train: step: 1148, loss: 0.03970945253968239, acc: 0.94, recall: 0.94, precision: 0.94, f_beta: 0.94
train: step: 1149, loss: 0.05698844790458679, acc: 0.92, recall: 0.9199999999999999, precision: 0.9227053140096618, f_beta: 0.9198717948717948
train: step: 1150, loss: 0.0587146170437336, acc: 0.94, recall: 0.94, precision: 0.9428341384863124, f_beta: 0.9399038461538463
train: step: 1151, loss: 0.04600739851593971, acc: 0.94, recall: 0.94, precision: 0.9407051282051282, f_beta: 0.9399759903961584
train: step: 1152, loss: 0.06444766372442245, acc: 0.93, recall: 0.9299999999999999, precision: 0.9343434343434343, f_beta: 0.9298245614035088
train: step: 1153, loss: 0.07253103703260422, acc: 0.91, recall: 0.91, precision: 0.9101640656262505, f_beta: 0.90999099909991
train: step: 1154, loss: 0.03840659558773041, acc: 0.96, recall: 0.96, precision: 0.96, f_beta: 0.96
train: step: 1155, loss: 0.07395767420530319, acc: 0.91, recall: 0.91, precision: 0.9101640656262505, f_beta: 0.90999099909991
train: step: 1156, loss: 0.07702723890542984, acc: 0.88, recall: 0.88, precision: 0.8824476650563606, f_beta: 0.8798076923076923
train: step: 1157, loss: 0.05204518884420395, acc: 0.93, recall: 0.9299999999999999, precision: 0.930172068827531, f_beta: 0.9299929992999301
train: step: 1158, loss: 0.07152486592531204, acc: 0.92, recall: 0.9199999999999999, precision: 0.9310344827586207, f_beta: 0.9194847020933978
train: step: 1159, loss: 0.0734369233250618, acc: 0.91, recall: 0.91, precision: 0.9101640656262505, f_beta: 0.90999099909991
train: step: 1160, loss: 0.09273473918437958, acc: 0.87, recall: 0.87, precision: 0.8701480592236894, f_beta: 0.8699869986998701
train: step: 1161, loss: 0.037806034088134766, acc: 0.95, recall: 0.95, precision: 0.9516258530710557, f_beta: 0.949954959463517
train: step: 1162, loss: 0.042235538363456726, acc: 0.94, recall: 0.94, precision: 0.9407051282051282, f_beta: 0.9399759903961584
train: step: 1163, loss: 0.1054171621799469, acc: 0.87, recall: 0.87, precision: 0.8701480592236894, f_beta: 0.8699869986998701
train: step: 1164, loss: 0.07230524718761444, acc: 0.91, recall: 0.9099999999999999, precision: 0.9141414141414141, f_beta: 0.9097744360902256
train: step: 1165, loss: 0.04401268810033798, acc: 0.96, recall: 0.96, precision: 0.962962962962963, f_beta: 0.9599358974358974
train: step: 1166, loss: 0.11651329696178436, acc: 0.89, recall: 0.89, precision: 0.89015606242497, f_beta: 0.88998899889989
train: step: 1167, loss: 0.07750990986824036, acc: 0.9, recall: 0.9, precision: 0.9006410256410255, f_beta: 0.8999599839935974
train: step: 1168, loss: 0.09185027331113815, acc: 0.89, recall: 0.89, precision: 0.89015606242497, f_beta: 0.88998899889989
train: step: 1169, loss: 0.04359544813632965, acc: 0.95, recall: 0.95, precision: 0.9501800720288115, f_beta: 0.9499949994999499
train: step: 1170, loss: 0.046941060572862625, acc: 0.95, recall: 0.95, precision: 0.9501800720288115, f_beta: 0.9499949994999499
train: step: 1171, loss: 0.013395078480243683, acc: 0.98, recall: 0.98, precision: 0.9807692307692308, f_beta: 0.9799919967987194
train: step: 1172, loss: 0.06914662569761276, acc: 0.9, recall: 0.8999999999999999, precision: 0.9058441558441559, f_beta: 0.8996386993175431
train: step: 1173, loss: 0.05278057977557182, acc: 0.92, recall: 0.9199999999999999, precision: 0.9206730769230769, f_beta: 0.919967987194878
train: step: 1174, loss: 0.1171058639883995, acc: 0.85, recall: 0.85, precision: 0.851264552388599, f_beta: 0.8498648783905516
train: step: 1175, loss: 0.07918515056371689, acc: 0.9, recall: 0.9, precision: 0.9, f_beta: 0.9
train: step: 1176, loss: 0.04560096561908722, acc: 0.93, recall: 0.9299999999999999, precision: 0.930172068827531, f_beta: 0.9299929992999301
train: step: 1177, loss: 0.07900495827198029, acc: 0.91, recall: 0.9099999999999999, precision: 0.9114813327980731, f_beta: 0.9099189270343309
train: step: 1178, loss: 0.05579032748937607, acc: 0.92, recall: 0.9199999999999999, precision: 0.9227053140096618, f_beta: 0.9198717948717948
train: step: 1179, loss: 0.10117121785879135, acc: 0.88, recall: 0.88, precision: 0.8824476650563606, f_beta: 0.8798076923076923
train: step: 1180, loss: 0.04494199901819229, acc: 0.94, recall: 0.94, precision: 0.9428341384863124, f_beta: 0.9399038461538463
train: step: 1181, loss: 0.08088324218988419, acc: 0.9, recall: 0.9, precision: 0.9, f_beta: 0.9
train: step: 1182, loss: 0.044384609907865524, acc: 0.93, recall: 0.9299999999999999, precision: 0.9315535929345644, f_beta: 0.9299369432489241
train: step: 1183, loss: 0.042035799473524094, acc: 0.93, recall: 0.9299999999999999, precision: 0.9343434343434343, f_beta: 0.9298245614035088
train: step: 1184, loss: 0.04457686468958855, acc: 0.93, recall: 0.9299999999999999, precision: 0.930172068827531, f_beta: 0.9299929992999301
train: step: 1185, loss: 0.053900815546512604, acc: 0.93, recall: 0.9299999999999999, precision: 0.930172068827531, f_beta: 0.9299929992999301
train: step: 1186, loss: 0.04562949016690254, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 1187, loss: 0.08882532268762589, acc: 0.9, recall: 0.9, precision: 0.9, f_beta: 0.9
train: step: 1188, loss: 0.11496803164482117, acc: 0.87, recall: 0.87, precision: 0.8737373737373737, f_beta: 0.8696741854636592
train: step: 1189, loss: 0.09353961795568466, acc: 0.9, recall: 0.8999999999999999, precision: 0.9025764895330113, f_beta: 0.8998397435897436
train: step: 1190, loss: 0.09357777237892151, acc: 0.86, recall: 0.86, precision: 0.86, f_beta: 0.8599999999999999
train: step: 1191, loss: 0.10106195509433746, acc: 0.88, recall: 0.88, precision: 0.8806089743589745, f_beta: 0.879951980792317
train: step: 1192, loss: 0.0693141520023346, acc: 0.91, recall: 0.9099999999999999, precision: 0.9114813327980731, f_beta: 0.9099189270343309
train: step: 1193, loss: 0.042950209230184555, acc: 0.93, recall: 0.9299999999999999, precision: 0.9315535929345644, f_beta: 0.9299369432489241
train: step: 1194, loss: 0.040772389620542526, acc: 0.94, recall: 0.94, precision: 0.9407051282051282, f_beta: 0.9399759903961584
train: step: 1195, loss: 0.05987391620874405, acc: 0.93, recall: 0.9299999999999999, precision: 0.9343434343434343, f_beta: 0.9298245614035088
train: step: 1196, loss: 0.03847578912973404, acc: 0.94, recall: 0.94, precision: 0.9428341384863124, f_beta: 0.9399038461538463
train: step: 1197, loss: 0.06274683028459549, acc: 0.9, recall: 0.8999999999999999, precision: 0.9025764895330113, f_beta: 0.8998397435897436
train: step: 1198, loss: 0.06853926926851273, acc: 0.89, recall: 0.89, precision: 0.89015606242497, f_beta: 0.88998899889989
train: step: 1199, loss: 0.06608647108078003, acc: 0.92, recall: 0.9199999999999999, precision: 0.9206730769230769, f_beta: 0.919967987194878
train: step: 1200, loss: 0.09456192702054977, acc: 0.87, recall: 0.87, precision: 0.8701480592236894, f_beta: 0.8699869986998701
train: step: 1201, loss: 0.10631265491247177, acc: 0.84, recall: 0.8400000000000001, precision: 0.8449675324675325, f_beta: 0.839421918908069
train: step: 1202, loss: 0.05862977355718613, acc: 0.94, recall: 0.94, precision: 0.94, f_beta: 0.94
train: step: 1203, loss: 0.05776653438806534, acc: 0.91, recall: 0.9099999999999999, precision: 0.9114813327980731, f_beta: 0.9099189270343309
train: step: 1204, loss: 0.07772758603096008, acc: 0.91, recall: 0.9099999999999999, precision: 0.9141414141414141, f_beta: 0.9097744360902256
train: step: 1205, loss: 0.03137233108282089, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 1206, loss: 0.07726448029279709, acc: 0.9, recall: 0.9, precision: 0.9006410256410255, f_beta: 0.8999599839935974
train: step: 1207, loss: 0.05443616956472397, acc: 0.92, recall: 0.92, precision: 0.92, f_beta: 0.92
train: step: 1208, loss: 0.05470610782504082, acc: 0.93, recall: 0.9299999999999999, precision: 0.9343434343434343, f_beta: 0.9298245614035088
train: step: 1209, loss: 0.04194117709994316, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 1210, loss: 0.04536332190036774, acc: 0.94, recall: 0.94, precision: 0.9464285714285714, f_beta: 0.939783219590526
train: step: 1211, loss: 0.0396900475025177, acc: 0.96, recall: 0.96, precision: 0.962962962962963, f_beta: 0.9599358974358974
train: step: 1212, loss: 0.04984922707080841, acc: 0.92, recall: 0.9199999999999999, precision: 0.9261363636363636, f_beta: 0.9197109594540345
train: step: 1213, loss: 0.08022035658359528, acc: 0.88, recall: 0.88, precision: 0.8824476650563606, f_beta: 0.8798076923076923
train: step: 1214, loss: 0.08810032159090042, acc: 0.88, recall: 0.8799999999999999, precision: 0.885551948051948, f_beta: 0.8795664391810518
train: step: 1215, loss: 0.07145952433347702, acc: 0.89, recall: 0.89, precision: 0.89015606242497, f_beta: 0.88998899889989
train: step: 1216, loss: 0.040708694607019424, acc: 0.95, recall: 0.95, precision: 0.9516258530710557, f_beta: 0.949954959463517
train: step: 1217, loss: 0.03978176414966583, acc: 0.96, recall: 0.96, precision: 0.9607371794871795, f_beta: 0.959983993597439
train: step: 1218, loss: 0.07166950404644012, acc: 0.91, recall: 0.9099999999999999, precision: 0.9181966544267646, f_beta: 0.9095568284594513
train: step: 1219, loss: 0.06630220264196396, acc: 0.9, recall: 0.8999999999999999, precision: 0.9025764895330113, f_beta: 0.8998397435897436
train: step: 1220, loss: 0.04821881651878357, acc: 0.93, recall: 0.9299999999999999, precision: 0.930172068827531, f_beta: 0.9299929992999301
train: step: 1221, loss: 0.05610884726047516, acc: 0.94, recall: 0.94, precision: 0.94, f_beta: 0.94
train: step: 1222, loss: 0.0753481462597847, acc: 0.91, recall: 0.9099999999999999, precision: 0.9114813327980731, f_beta: 0.9099189270343309
train: step: 1223, loss: 0.0640370100736618, acc: 0.91, recall: 0.9099999999999999, precision: 0.9181966544267646, f_beta: 0.9095568284594513
train: step: 1224, loss: 0.03955104202032089, acc: 0.95, recall: 0.95, precision: 0.9516258530710557, f_beta: 0.949954959463517
train: step: 1225, loss: 0.054538678377866745, acc: 0.94, recall: 0.94, precision: 0.94, f_beta: 0.94
train: step: 1226, loss: 0.08046427369117737, acc: 0.9, recall: 0.8999999999999999, precision: 0.9025764895330113, f_beta: 0.8998397435897436
train: step: 1227, loss: 0.046922966837882996, acc: 0.92, recall: 0.92, precision: 0.92, f_beta: 0.92
train: step: 1228, loss: 0.05053836479783058, acc: 0.94, recall: 0.94, precision: 0.94, f_beta: 0.94
train: step: 1229, loss: 0.03596304729580879, acc: 0.95, recall: 0.95, precision: 0.9516258530710557, f_beta: 0.949954959463517
train: step: 1230, loss: 0.04911850765347481, acc: 0.94, recall: 0.94, precision: 0.9428341384863124, f_beta: 0.9399038461538463
train: step: 1231, loss: 0.03414440527558327, acc: 0.98, recall: 0.98, precision: 0.98, f_beta: 0.98
train: step: 1232, loss: 0.07419589161872864, acc: 0.89, recall: 0.89, precision: 0.89015606242497, f_beta: 0.88998899889989
train: step: 1233, loss: 0.06488203257322311, acc: 0.92, recall: 0.9199999999999999, precision: 0.9227053140096618, f_beta: 0.9198717948717948
train: step: 1234, loss: 0.04391266033053398, acc: 0.93, recall: 0.9299999999999999, precision: 0.9315535929345644, f_beta: 0.9299369432489241
train: step: 1235, loss: 0.04340317100286484, acc: 0.95, recall: 0.95, precision: 0.9501800720288115, f_beta: 0.9499949994999499
train: step: 1236, loss: 0.09000210464000702, acc: 0.87, recall: 0.87, precision: 0.8701480592236894, f_beta: 0.8699869986998701
train: step: 1237, loss: 0.10302270203828812, acc: 0.86, recall: 0.86, precision: 0.8694581280788177, f_beta: 0.8590982286634461
train: step: 1238, loss: 0.05457159876823425, acc: 0.93, recall: 0.9299999999999999, precision: 0.930172068827531, f_beta: 0.9299929992999301
train: step: 1239, loss: 0.09155994653701782, acc: 0.87, recall: 0.87, precision: 0.8701480592236894, f_beta: 0.8699869986998701
train: step: 1240, loss: 0.03744332864880562, acc: 0.95, recall: 0.95, precision: 0.9545454545454546, f_beta: 0.949874686716792
train: step: 1241, loss: 0.061873242259025574, acc: 0.91, recall: 0.91, precision: 0.9101640656262505, f_beta: 0.90999099909991
train: step: 1242, loss: 0.05383948236703873, acc: 0.93, recall: 0.9299999999999999, precision: 0.9315535929345644, f_beta: 0.9299369432489241
train: step: 1243, loss: 0.07753550261259079, acc: 0.91, recall: 0.9099999999999999, precision: 0.9114813327980731, f_beta: 0.9099189270343309
train: step: 1244, loss: 0.07325576990842819, acc: 0.9, recall: 0.9, precision: 0.9, f_beta: 0.9
train: step: 1245, loss: 0.058863963931798935, acc: 0.92, recall: 0.9199999999999999, precision: 0.9206730769230769, f_beta: 0.919967987194878
train: step: 1246, loss: 0.07319607585668564, acc: 0.9, recall: 0.9, precision: 0.9, f_beta: 0.9
train: step: 1247, loss: 0.05738157406449318, acc: 0.92, recall: 0.9199999999999999, precision: 0.9206730769230769, f_beta: 0.919967987194878
train: step: 1248, loss: 0.028864460065960884, acc: 0.98, recall: 0.98, precision: 0.9807692307692308, f_beta: 0.9799919967987194
train: step: 1249, loss: 0.07010675221681595, acc: 0.91, recall: 0.9099999999999999, precision: 0.9114813327980731, f_beta: 0.9099189270343309
train: step: 1250, loss: 0.0896347239613533, acc: 0.9, recall: 0.9, precision: 0.9, f_beta: 0.9
train: step: 1251, loss: 0.06841395795345306, acc: 0.92, recall: 0.9199999999999999, precision: 0.9206730769230769, f_beta: 0.919967987194878
train: step: 1252, loss: 0.0792497843503952, acc: 0.89, recall: 0.89, precision: 0.8914090726615818, f_beta: 0.8899009108197378
train: step: 1253, loss: 0.07567542046308517, acc: 0.88, recall: 0.88, precision: 0.8806089743589745, f_beta: 0.879951980792317
train: step: 1254, loss: 0.06149471178650856, acc: 0.93, recall: 0.9299999999999999, precision: 0.930172068827531, f_beta: 0.9299929992999301
train: step: 1255, loss: 0.12448333948850632, acc: 0.85, recall: 0.85, precision: 0.851264552388599, f_beta: 0.8498648783905516
train: step: 1256, loss: 0.09552931785583496, acc: 0.9, recall: 0.8999999999999999, precision: 0.9058441558441559, f_beta: 0.8996386993175431
train: step: 1257, loss: 0.11556616425514221, acc: 0.86, recall: 0.8600000000000001, precision: 0.8652597402597402, f_beta: 0.8594941790445605
train: step: 1258, loss: 0.05218754708766937, acc: 0.95, recall: 0.95, precision: 0.9516258530710557, f_beta: 0.949954959463517
train: step: 1259, loss: 0.06466776877641678, acc: 0.92, recall: 0.9199999999999999, precision: 0.9206730769230769, f_beta: 0.919967987194878
train: step: 1260, loss: 0.019541751593351364, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 1261, loss: 0.04341091215610504, acc: 0.96, recall: 0.96, precision: 0.96, f_beta: 0.96
train: step: 1262, loss: 0.0858856737613678, acc: 0.9, recall: 0.8999999999999999, precision: 0.9025764895330113, f_beta: 0.8998397435897436
train: step: 1263, loss: 0.06965478509664536, acc: 0.9, recall: 0.8999999999999999, precision: 0.9025764895330113, f_beta: 0.8998397435897436
train: step: 1264, loss: 0.08183921128511429, acc: 0.89, recall: 0.8899999999999999, precision: 0.8939393939393939, f_beta: 0.8897243107769424
train: step: 1265, loss: 0.06414973735809326, acc: 0.94, recall: 0.94, precision: 0.9464285714285714, f_beta: 0.939783219590526
train: step: 1266, loss: 0.05406169965863228, acc: 0.92, recall: 0.9199999999999999, precision: 0.9206730769230769, f_beta: 0.919967987194878
train: step: 1267, loss: 0.061336107552051544, acc: 0.94, recall: 0.94, precision: 0.9428341384863124, f_beta: 0.9399038461538463
train: step: 1268, loss: 0.06567662954330444, acc: 0.92, recall: 0.9199999999999999, precision: 0.9227053140096618, f_beta: 0.9198717948717948
train: step: 1269, loss: 0.034991536289453506, acc: 0.96, recall: 0.96, precision: 0.9607371794871795, f_beta: 0.959983993597439
train: step: 1270, loss: 0.01310056447982788, acc: 0.99, recall: 0.99, precision: 0.9901960784313726, f_beta: 0.98999899989999
train: step: 1271, loss: 0.05084027722477913, acc: 0.94, recall: 0.94, precision: 0.94, f_beta: 0.94
train: step: 1272, loss: 0.07715149968862534, acc: 0.91, recall: 0.91, precision: 0.9101640656262505, f_beta: 0.90999099909991
train: step: 1273, loss: 0.07206837832927704, acc: 0.89, recall: 0.8899999999999999, precision: 0.8977968176254589, f_beta: 0.8894583458948848
train: step: 1274, loss: 0.06445761024951935, acc: 0.92, recall: 0.9199999999999999, precision: 0.9261363636363636, f_beta: 0.9197109594540345
train: step: 1275, loss: 0.050711628049612045, acc: 0.95, recall: 0.95, precision: 0.9516258530710557, f_beta: 0.949954959463517
train: step: 1276, loss: 0.04134393110871315, acc: 0.95, recall: 0.95, precision: 0.9545454545454546, f_beta: 0.949874686716792
train: step: 1277, loss: 0.035524558275938034, acc: 0.95, recall: 0.95, precision: 0.9545454545454546, f_beta: 0.949874686716792
train: step: 1278, loss: 0.04545009508728981, acc: 0.94, recall: 0.94, precision: 0.9428341384863124, f_beta: 0.9399038461538463
train: step: 1279, loss: 0.06665346771478653, acc: 0.93, recall: 0.9299999999999999, precision: 0.930172068827531, f_beta: 0.9299929992999301
train: step: 1280, loss: 0.08118950575590134, acc: 0.9, recall: 0.9, precision: 0.9006410256410255, f_beta: 0.8999599839935974
train: step: 1281, loss: 0.10276754200458527, acc: 0.88, recall: 0.88, precision: 0.8824476650563606, f_beta: 0.8798076923076923
train: step: 1282, loss: 0.09354566782712936, acc: 0.87, recall: 0.87, precision: 0.8713368125250903, f_beta: 0.8698828946051447
train: step: 1283, loss: 0.051880840212106705, acc: 0.95, recall: 0.95, precision: 0.9501800720288115, f_beta: 0.9499949994999499
train: step: 1284, loss: 0.05480950325727463, acc: 0.93, recall: 0.9299999999999999, precision: 0.930172068827531, f_beta: 0.9299929992999301
train: step: 1285, loss: 0.0823105052113533, acc: 0.88, recall: 0.88, precision: 0.8806089743589745, f_beta: 0.879951980792317
train: step: 1286, loss: 0.09491617232561111, acc: 0.86, recall: 0.86, precision: 0.8605769230769231, f_beta: 0.8599439775910365
train: step: 1287, loss: 0.02096436731517315, acc: 0.98, recall: 0.98, precision: 0.9807692307692308, f_beta: 0.9799919967987194
train: step: 1288, loss: 0.033436812460422516, acc: 0.96, recall: 0.96, precision: 0.9607371794871795, f_beta: 0.959983993597439
train: step: 1289, loss: 0.07767406105995178, acc: 0.9, recall: 0.8999999999999999, precision: 0.9058441558441559, f_beta: 0.8996386993175431
train: step: 1290, loss: 0.09173959493637085, acc: 0.88, recall: 0.88, precision: 0.8824476650563606, f_beta: 0.8798076923076923
train: step: 1291, loss: 0.07198501378297806, acc: 0.91, recall: 0.9099999999999999, precision: 0.9141414141414141, f_beta: 0.9097744360902256
train: step: 1292, loss: 0.08837135136127472, acc: 0.91, recall: 0.9099999999999999, precision: 0.9114813327980731, f_beta: 0.9099189270343309
train: step: 1293, loss: 0.07437409460544586, acc: 0.9, recall: 0.9, precision: 0.9006410256410255, f_beta: 0.8999599839935974
train: step: 1294, loss: 0.06851336359977722, acc: 0.92, recall: 0.9199999999999999, precision: 0.9206730769230769, f_beta: 0.919967987194878
train: step: 1295, loss: 0.07475143671035767, acc: 0.92, recall: 0.9199999999999999, precision: 0.9206730769230769, f_beta: 0.919967987194878
train: step: 1296, loss: 0.03783304616808891, acc: 0.94, recall: 0.94, precision: 0.9428341384863124, f_beta: 0.9399038461538463
train: step: 1297, loss: 0.04868757724761963, acc: 0.94, recall: 0.94, precision: 0.9407051282051282, f_beta: 0.9399759903961584
train: step: 1298, loss: 0.05259513854980469, acc: 0.95, recall: 0.95, precision: 0.9501800720288115, f_beta: 0.9499949994999499
train: step: 1299, loss: 0.05382281914353371, acc: 0.93, recall: 0.9299999999999999, precision: 0.930172068827531, f_beta: 0.9299929992999301
train: step: 1300, loss: 0.018964858725667, acc: 0.99, recall: 0.99, precision: 0.9901960784313726, f_beta: 0.98999899989999
train: step: 1301, loss: 0.05550249665975571, acc: 0.93, recall: 0.9299999999999999, precision: 0.9315535929345644, f_beta: 0.9299369432489241
train: step: 1302, loss: 0.06627307087182999, acc: 0.91, recall: 0.9099999999999999, precision: 0.9114813327980731, f_beta: 0.9099189270343309
train: step: 1303, loss: 0.05732506886124611, acc: 0.91, recall: 0.91, precision: 0.9101640656262505, f_beta: 0.90999099909991
train: step: 1304, loss: 0.07902394980192184, acc: 0.9, recall: 0.8999999999999999, precision: 0.9058441558441559, f_beta: 0.8996386993175431
train: step: 1305, loss: 0.04076475650072098, acc: 0.95, recall: 0.95, precision: 0.9501800720288115, f_beta: 0.9499949994999499
train: step: 1306, loss: 0.04810125380754471, acc: 0.92, recall: 0.9199999999999999, precision: 0.9206730769230769, f_beta: 0.919967987194878
train: step: 1307, loss: 0.14080727100372314, acc: 0.82, recall: 0.8200000000000001, precision: 0.8246753246753247, f_beta: 0.8193496587715776
train: step: 1308, loss: 0.04942880943417549, acc: 0.93, recall: 0.9299999999999999, precision: 0.930172068827531, f_beta: 0.9299929992999301
train: step: 1309, loss: 0.03738464042544365, acc: 0.95, recall: 0.95, precision: 0.9516258530710557, f_beta: 0.949954959463517
train: step: 1310, loss: 0.029263591393828392, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 1311, loss: 0.02846083603799343, acc: 0.98, recall: 0.98, precision: 0.98, f_beta: 0.98
train: step: 1312, loss: 0.06447445601224899, acc: 0.92, recall: 0.9199999999999999, precision: 0.9206730769230769, f_beta: 0.919967987194878
train: step: 1313, loss: 0.10240164399147034, acc: 0.86, recall: 0.86, precision: 0.8623188405797102, f_beta: 0.859775641025641
train: step: 1314, loss: 0.07978523522615433, acc: 0.89, recall: 0.89, precision: 0.89015606242497, f_beta: 0.88998899889989
train: step: 1315, loss: 0.06768066436052322, acc: 0.9, recall: 0.8999999999999999, precision: 0.9058441558441559, f_beta: 0.8996386993175431
train: step: 1316, loss: 0.06609167903661728, acc: 0.91, recall: 0.91, precision: 0.9101640656262505, f_beta: 0.90999099909991
train: step: 1317, loss: 0.08497080206871033, acc: 0.87, recall: 0.87, precision: 0.8701480592236894, f_beta: 0.8699869986998701
train: step: 1318, loss: 0.09214325249195099, acc: 0.84, recall: 0.84, precision: 0.8405448717948718, f_beta: 0.8399359743897559
train: step: 1319, loss: 0.039262741804122925, acc: 0.96, recall: 0.96, precision: 0.9607371794871795, f_beta: 0.959983993597439
train: step: 1320, loss: 0.023441070690751076, acc: 0.98, recall: 0.98, precision: 0.98, f_beta: 0.98
train: step: 1321, loss: 0.07630424201488495, acc: 0.92, recall: 0.92, precision: 0.92, f_beta: 0.92
train: step: 1322, loss: 0.05319705978035927, acc: 0.92, recall: 0.9199999999999999, precision: 0.9206730769230769, f_beta: 0.919967987194878
train: step: 1323, loss: 0.04839349910616875, acc: 0.96, recall: 0.96, precision: 0.9607371794871795, f_beta: 0.959983993597439
train: step: 1324, loss: 0.06645011901855469, acc: 0.92, recall: 0.9199999999999999, precision: 0.9227053140096618, f_beta: 0.9198717948717948
train: step: 1325, loss: 0.07327470928430557, acc: 0.91, recall: 0.9099999999999999, precision: 0.9114813327980731, f_beta: 0.9099189270343309
train: step: 1326, loss: 0.03997271507978439, acc: 0.94, recall: 0.94, precision: 0.9407051282051282, f_beta: 0.9399759903961584
train: step: 1327, loss: 0.061257123947143555, acc: 0.93, recall: 0.9299999999999999, precision: 0.9315535929345644, f_beta: 0.9299369432489241
train: step: 1328, loss: 0.058516860008239746, acc: 0.92, recall: 0.92, precision: 0.92, f_beta: 0.92
train: step: 1329, loss: 0.05039491504430771, acc: 0.93, recall: 0.9299999999999999, precision: 0.9315535929345644, f_beta: 0.9299369432489241
train: step: 1330, loss: 0.09117067605257034, acc: 0.89, recall: 0.8899999999999999, precision: 0.8939393939393939, f_beta: 0.8897243107769424
train: step: 1331, loss: 0.06979357451200485, acc: 0.89, recall: 0.89, precision: 0.8914090726615818, f_beta: 0.8899009108197378
train: step: 1332, loss: 0.0373147577047348, acc: 0.95, recall: 0.95, precision: 0.9501800720288115, f_beta: 0.9499949994999499
train: step: 1333, loss: 0.08399220556020737, acc: 0.87, recall: 0.87, precision: 0.8713368125250903, f_beta: 0.8698828946051447
train: step: 1334, loss: 0.07497414946556091, acc: 0.89, recall: 0.89, precision: 0.9030591153369161, f_beta: 0.8891017239641092
train: step: 1335, loss: 0.054254382848739624, acc: 0.93, recall: 0.9299999999999999, precision: 0.9315535929345644, f_beta: 0.9299369432489241
train: step: 1336, loss: 0.08318941295146942, acc: 0.89, recall: 0.89, precision: 0.89015606242497, f_beta: 0.88998899889989
train: step: 1337, loss: 0.08992986381053925, acc: 0.89, recall: 0.89, precision: 0.89015606242497, f_beta: 0.88998899889989
train: step: 1338, loss: 0.0859210193157196, acc: 0.9, recall: 0.9, precision: 0.9, f_beta: 0.9
train: step: 1339, loss: 0.04480712115764618, acc: 0.95, recall: 0.95, precision: 0.9545454545454546, f_beta: 0.949874686716792
train: step: 1340, loss: 0.07844596356153488, acc: 0.89, recall: 0.89, precision: 0.8914090726615818, f_beta: 0.8899009108197378
train: step: 1341, loss: 0.06645999103784561, acc: 0.91, recall: 0.91, precision: 0.9101640656262505, f_beta: 0.90999099909991
train: step: 1342, loss: 0.05186457186937332, acc: 0.93, recall: 0.9299999999999999, precision: 0.930172068827531, f_beta: 0.9299929992999301
train: step: 1343, loss: 0.04870570823550224, acc: 0.95, recall: 0.95, precision: 0.9501800720288115, f_beta: 0.9499949994999499
train: step: 1344, loss: 0.04504753276705742, acc: 0.93, recall: 0.9299999999999999, precision: 0.930172068827531, f_beta: 0.9299929992999301
train: step: 1345, loss: 0.07942261546850204, acc: 0.88, recall: 0.88, precision: 0.88, f_beta: 0.88
train: step: 1346, loss: 0.0538046658039093, acc: 0.92, recall: 0.9199999999999999, precision: 0.9261363636363636, f_beta: 0.9197109594540345
train: step: 1347, loss: 0.11965503543615341, acc: 0.86, recall: 0.86, precision: 0.8605769230769231, f_beta: 0.8599439775910365
train: step: 1348, loss: 0.04906821623444557, acc: 0.95, recall: 0.95, precision: 0.9516258530710557, f_beta: 0.949954959463517
train: step: 1349, loss: 0.05793648585677147, acc: 0.92, recall: 0.9199999999999999, precision: 0.9227053140096618, f_beta: 0.9198717948717948
train: step: 1350, loss: 0.039869602769613266, acc: 0.93, recall: 0.9299999999999999, precision: 0.9315535929345644, f_beta: 0.9299369432489241
train: step: 1351, loss: 0.0931117832660675, acc: 0.87, recall: 0.87, precision: 0.8701480592236894, f_beta: 0.8699869986998701
train: step: 1352, loss: 0.09851453453302383, acc: 0.87, recall: 0.87, precision: 0.8701480592236894, f_beta: 0.8699869986998701
train: step: 1353, loss: 0.04663847014307976, acc: 0.93, recall: 0.9299999999999999, precision: 0.930172068827531, f_beta: 0.9299929992999301
train: step: 1354, loss: 0.044573795050382614, acc: 0.96, recall: 0.96, precision: 0.9607371794871795, f_beta: 0.959983993597439
train: step: 1355, loss: 0.04933549091219902, acc: 0.95, recall: 0.95, precision: 0.9516258530710557, f_beta: 0.949954959463517
train: step: 1356, loss: 0.034713707864284515, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 1357, loss: 0.06458671391010284, acc: 0.9, recall: 0.9, precision: 0.9, f_beta: 0.9
train: step: 1358, loss: 0.04378339648246765, acc: 0.95, recall: 0.95, precision: 0.9545454545454546, f_beta: 0.949874686716792
train: step: 1359, loss: 0.0696139708161354, acc: 0.92, recall: 0.9199999999999999, precision: 0.9206730769230769, f_beta: 0.919967987194878
train: step: 1360, loss: 0.07096926867961884, acc: 0.9, recall: 0.8999999999999999, precision: 0.9025764895330113, f_beta: 0.8998397435897436
train: step: 1361, loss: 0.053939007222652435, acc: 0.92, recall: 0.9199999999999999, precision: 0.9206730769230769, f_beta: 0.919967987194878
train: step: 1362, loss: 0.06924828141927719, acc: 0.92, recall: 0.9199999999999999, precision: 0.9206730769230769, f_beta: 0.919967987194878
train: step: 1363, loss: 0.04617565497756004, acc: 0.94, recall: 0.94, precision: 0.9407051282051282, f_beta: 0.9399759903961584
train: step: 1364, loss: 0.05927438661456108, acc: 0.93, recall: 0.9299999999999999, precision: 0.930172068827531, f_beta: 0.9299929992999301
train: step: 1365, loss: 0.10053827613592148, acc: 0.89, recall: 0.8899999999999999, precision: 0.8939393939393939, f_beta: 0.8897243107769424
train: step: 1366, loss: 0.04528672993183136, acc: 0.94, recall: 0.94, precision: 0.9428341384863124, f_beta: 0.9399038461538463
train: step: 1367, loss: 0.06732222437858582, acc: 0.91, recall: 0.9099999999999999, precision: 0.9141414141414141, f_beta: 0.9097744360902256
train: step: 1368, loss: 0.03789137303829193, acc: 0.96, recall: 0.96, precision: 0.962962962962963, f_beta: 0.9599358974358974
train: step: 1369, loss: 0.09366706758737564, acc: 0.88, recall: 0.88, precision: 0.8899835796387521, f_beta: 0.8792270531400966
train: step: 1370, loss: 0.060079410672187805, acc: 0.93, recall: 0.9299999999999999, precision: 0.930172068827531, f_beta: 0.9299929992999301
train: step: 1371, loss: 0.046425770968198776, acc: 0.93, recall: 0.9299999999999999, precision: 0.930172068827531, f_beta: 0.9299929992999301
train: step: 1372, loss: 0.10173793137073517, acc: 0.86, recall: 0.86, precision: 0.8694581280788177, f_beta: 0.8590982286634461
train: step: 1373, loss: 0.04819231107831001, acc: 0.93, recall: 0.9299999999999999, precision: 0.930172068827531, f_beta: 0.9299929992999301
train: step: 1374, loss: 0.08929526060819626, acc: 0.9, recall: 0.8999999999999999, precision: 0.9105090311986863, f_beta: 0.8993558776167472
train: step: 1375, loss: 0.024556737393140793, acc: 0.97, recall: 0.97, precision: 0.9716981132075472, f_beta: 0.9699729756781104
train: step: 1376, loss: 0.040621317923069, acc: 0.95, recall: 0.95, precision: 0.9501800720288115, f_beta: 0.9499949994999499
train: step: 1377, loss: 0.0562887005507946, acc: 0.93, recall: 0.9299999999999999, precision: 0.9315535929345644, f_beta: 0.9299369432489241
train: step: 1378, loss: 0.10173770785331726, acc: 0.89, recall: 0.89, precision: 0.8914090726615818, f_beta: 0.8899009108197378
train: step: 1379, loss: 0.03767877444624901, acc: 0.95, recall: 0.95, precision: 0.9516258530710557, f_beta: 0.949954959463517
train: step: 1380, loss: 0.05620250850915909, acc: 0.93, recall: 0.9299999999999999, precision: 0.9315535929345644, f_beta: 0.9299369432489241
train: step: 1381, loss: 0.0405997671186924, acc: 0.96, recall: 0.96, precision: 0.962962962962963, f_beta: 0.9599358974358974
train: step: 1382, loss: 0.0778619721531868, acc: 0.9, recall: 0.8999999999999999, precision: 0.9025764895330113, f_beta: 0.8998397435897436
train: step: 1383, loss: 0.07215853035449982, acc: 0.9, recall: 0.9, precision: 0.9006410256410255, f_beta: 0.8999599839935974
train: step: 1384, loss: 0.02958662435412407, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 1385, loss: 0.07796864956617355, acc: 0.89, recall: 0.89, precision: 0.89015606242497, f_beta: 0.88998899889989
train: step: 1386, loss: 0.06301682442426682, acc: 0.93, recall: 0.9299999999999999, precision: 0.930172068827531, f_beta: 0.9299929992999301
train: step: 1387, loss: 0.0400141142308712, acc: 0.95, recall: 0.95, precision: 0.9516258530710557, f_beta: 0.949954959463517
train: step: 1388, loss: 0.07556585967540741, acc: 0.9, recall: 0.9, precision: 0.9, f_beta: 0.9
train: step: 1389, loss: 0.036328379064798355, acc: 0.98, recall: 0.98, precision: 0.98, f_beta: 0.98
train: step: 1390, loss: 0.03160737454891205, acc: 0.96, recall: 0.96, precision: 0.9607371794871795, f_beta: 0.959983993597439
train: step: 1391, loss: 0.07801323384046555, acc: 0.88, recall: 0.88, precision: 0.8824476650563606, f_beta: 0.8798076923076923
train: step: 1392, loss: 0.038181763142347336, acc: 0.96, recall: 0.96, precision: 0.96, f_beta: 0.96
train: step: 1393, loss: 0.07067149877548218, acc: 0.9, recall: 0.8999999999999999, precision: 0.9025764895330113, f_beta: 0.8998397435897436
train: step: 1394, loss: 0.08572905510663986, acc: 0.88, recall: 0.88, precision: 0.88, f_beta: 0.88
train: step: 1395, loss: 0.06331657618284225, acc: 0.91, recall: 0.9099999999999999, precision: 0.9141414141414141, f_beta: 0.9097744360902256
train: step: 1396, loss: 0.06770692765712738, acc: 0.91, recall: 0.9099999999999999, precision: 0.9181966544267646, f_beta: 0.9095568284594513
train: step: 1397, loss: 0.06064503639936447, acc: 0.92, recall: 0.9199999999999999, precision: 0.9206730769230769, f_beta: 0.919967987194878
train: step: 1398, loss: 0.03548561409115791, acc: 0.95, recall: 0.95, precision: 0.9501800720288115, f_beta: 0.9499949994999499
train: step: 1399, loss: 0.10036899894475937, acc: 0.87, recall: 0.87, precision: 0.8713368125250903, f_beta: 0.8698828946051447
train: step: 1400, loss: 0.042116858065128326, acc: 0.96, recall: 0.96, precision: 0.9607371794871795, f_beta: 0.959983993597439
train: step: 1401, loss: 0.03947226703166962, acc: 0.95, recall: 0.95, precision: 0.9516258530710557, f_beta: 0.949954959463517
train: step: 1402, loss: 0.04419592767953873, acc: 0.94, recall: 0.94, precision: 0.9407051282051282, f_beta: 0.9399759903961584
train: step: 1403, loss: 0.0468306839466095, acc: 0.95, recall: 0.95, precision: 0.9501800720288115, f_beta: 0.9499949994999499
train: step: 1404, loss: 0.06276526302099228, acc: 0.92, recall: 0.9199999999999999, precision: 0.9206730769230769, f_beta: 0.919967987194878
train: step: 1405, loss: 0.07454240322113037, acc: 0.9, recall: 0.9, precision: 0.9006410256410255, f_beta: 0.8999599839935974
train: step: 1406, loss: 0.06537668406963348, acc: 0.9, recall: 0.9, precision: 0.9006410256410255, f_beta: 0.8999599839935974
train: step: 1407, loss: 0.0756959542632103, acc: 0.91, recall: 0.9099999999999999, precision: 0.9114813327980731, f_beta: 0.9099189270343309
train: step: 1408, loss: 0.03792749345302582, acc: 0.96, recall: 0.96, precision: 0.9607371794871795, f_beta: 0.959983993597439
train: step: 1409, loss: 0.08442382514476776, acc: 0.88, recall: 0.88, precision: 0.8806089743589745, f_beta: 0.879951980792317
train: step: 1410, loss: 0.04577971622347832, acc: 0.93, recall: 0.9299999999999999, precision: 0.9315535929345644, f_beta: 0.9299369432489241
train: step: 1411, loss: 0.06546402722597122, acc: 0.91, recall: 0.9099999999999999, precision: 0.9181966544267646, f_beta: 0.9095568284594513
train: step: 1412, loss: 0.061014119535684586, acc: 0.92, recall: 0.9199999999999999, precision: 0.9310344827586207, f_beta: 0.9194847020933978
train: step: 1413, loss: 0.041561633348464966, acc: 0.92, recall: 0.9199999999999999, precision: 0.9206730769230769, f_beta: 0.919967987194878
train: step: 1414, loss: 0.045491237193346024, acc: 0.95, recall: 0.95, precision: 0.9501800720288115, f_beta: 0.9499949994999499
train: step: 1415, loss: 0.09168478101491928, acc: 0.89, recall: 0.89, precision: 0.89015606242497, f_beta: 0.88998899889989
train: step: 1416, loss: 0.07296919077634811, acc: 0.92, recall: 0.9199999999999999, precision: 0.9206730769230769, f_beta: 0.919967987194878
train: step: 1417, loss: 0.07577026635408401, acc: 0.92, recall: 0.9199999999999999, precision: 0.9206730769230769, f_beta: 0.919967987194878
train: step: 1418, loss: 0.04228062555193901, acc: 0.94, recall: 0.94, precision: 0.9407051282051282, f_beta: 0.9399759903961584
train: step: 1419, loss: 0.07669748365879059, acc: 0.91, recall: 0.9099999999999999, precision: 0.9181966544267646, f_beta: 0.9095568284594513
train: step: 1420, loss: 0.05839068815112114, acc: 0.93, recall: 0.9299999999999999, precision: 0.930172068827531, f_beta: 0.9299929992999301
train: step: 1421, loss: 0.08436066657304764, acc: 0.93, recall: 0.9299999999999999, precision: 0.9315535929345644, f_beta: 0.9299369432489241
train: step: 1422, loss: 0.02298806980252266, acc: 0.97, recall: 0.97, precision: 0.9716981132075472, f_beta: 0.9699729756781104
train: step: 1423, loss: 0.05118205398321152, acc: 0.93, recall: 0.9299999999999999, precision: 0.9343434343434343, f_beta: 0.9298245614035088
train: step: 1424, loss: 0.019978955388069153, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 1425, loss: 0.04352256655693054, acc: 0.96, recall: 0.96, precision: 0.96, f_beta: 0.96
train: step: 1426, loss: 0.03627404198050499, acc: 0.94, recall: 0.94, precision: 0.9407051282051282, f_beta: 0.9399759903961584
train: step: 1427, loss: 0.05059722438454628, acc: 0.94, recall: 0.94, precision: 0.9464285714285714, f_beta: 0.939783219590526
train: step: 1428, loss: 0.07245329022407532, acc: 0.9, recall: 0.9, precision: 0.9166666666666667, f_beta: 0.898989898989899
train: step: 1429, loss: 0.10755471885204315, acc: 0.88, recall: 0.88, precision: 0.88, f_beta: 0.88
train: step: 1430, loss: 0.0774611085653305, acc: 0.91, recall: 0.91, precision: 0.9101640656262505, f_beta: 0.90999099909991
train: step: 1431, loss: 0.04462512955069542, acc: 0.95, recall: 0.95, precision: 0.9501800720288115, f_beta: 0.9499949994999499
train: step: 1432, loss: 0.08317732065916061, acc: 0.89, recall: 0.89, precision: 0.89015606242497, f_beta: 0.88998899889989
train: step: 1433, loss: 0.03906700015068054, acc: 0.96, recall: 0.96, precision: 0.96, f_beta: 0.96
train: step: 1434, loss: 0.06220105290412903, acc: 0.89, recall: 0.89, precision: 0.89015606242497, f_beta: 0.88998899889989
train: step: 1435, loss: 0.028660602867603302, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 1436, loss: 0.04590095579624176, acc: 0.94, recall: 0.94, precision: 0.9407051282051282, f_beta: 0.9399759903961584
train: step: 1437, loss: 0.07601642608642578, acc: 0.93, recall: 0.9299999999999999, precision: 0.930172068827531, f_beta: 0.9299929992999301
train: step: 1438, loss: 0.06279897689819336, acc: 0.91, recall: 0.9099999999999999, precision: 0.9141414141414141, f_beta: 0.9097744360902256
train: step: 1439, loss: 0.0705147534608841, acc: 0.93, recall: 0.9299999999999999, precision: 0.930172068827531, f_beta: 0.9299929992999301
train: step: 1440, loss: 0.06259456276893616, acc: 0.93, recall: 0.9299999999999999, precision: 0.930172068827531, f_beta: 0.9299929992999301
train: step: 1441, loss: 0.0535515695810318, acc: 0.95, recall: 0.95, precision: 0.9545454545454546, f_beta: 0.949874686716792
train: step: 1442, loss: 0.07209596782922745, acc: 0.91, recall: 0.9099999999999999, precision: 0.9141414141414141, f_beta: 0.9097744360902256
train: step: 1443, loss: 0.05789358541369438, acc: 0.93, recall: 0.9299999999999999, precision: 0.930172068827531, f_beta: 0.9299929992999301
train: step: 1444, loss: 0.09122596681118011, acc: 0.88, recall: 0.88, precision: 0.88, f_beta: 0.88
train: step: 1445, loss: 0.06958354264497757, acc: 0.91, recall: 0.91, precision: 0.9101640656262505, f_beta: 0.90999099909991
train: step: 1446, loss: 0.06866873800754547, acc: 0.9, recall: 0.8999999999999999, precision: 0.9025764895330113, f_beta: 0.8998397435897436
train: step: 1447, loss: 0.07631917297840118, acc: 0.88, recall: 0.88, precision: 0.8806089743589745, f_beta: 0.879951980792317
train: step: 1448, loss: 0.02837137132883072, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 1449, loss: 0.033337537199258804, acc: 0.95, recall: 0.95, precision: 0.9501800720288115, f_beta: 0.9499949994999499
train: step: 1450, loss: 0.06380794942378998, acc: 0.92, recall: 0.9199999999999999, precision: 0.9261363636363636, f_beta: 0.9197109594540345
train: step: 1451, loss: 0.04185865446925163, acc: 0.95, recall: 0.95, precision: 0.9501800720288115, f_beta: 0.9499949994999499
train: step: 1452, loss: 0.029306579381227493, acc: 0.97, recall: 0.97, precision: 0.9716981132075472, f_beta: 0.9699729756781104
train: step: 1453, loss: 0.04084280505776405, acc: 0.96, recall: 0.96, precision: 0.962962962962963, f_beta: 0.9599358974358974
train: step: 1454, loss: 0.11536842584609985, acc: 0.85, recall: 0.85, precision: 0.8617197188921042, f_beta: 0.8487750781328762
train: step: 1455, loss: 0.04627872630953789, acc: 0.93, recall: 0.9299999999999999, precision: 0.9315535929345644, f_beta: 0.9299369432489241
train: step: 1456, loss: 0.06066889315843582, acc: 0.92, recall: 0.92, precision: 0.92, f_beta: 0.92
train: step: 1457, loss: 0.034243207424879074, acc: 0.95, recall: 0.95, precision: 0.9501800720288115, f_beta: 0.9499949994999499
train: step: 1458, loss: 0.08371452242136002, acc: 0.88, recall: 0.88, precision: 0.88, f_beta: 0.88
train: step: 1459, loss: 0.0967215821146965, acc: 0.86, recall: 0.86, precision: 0.875, f_beta: 0.8585858585858586
train: step: 1460, loss: 0.023358823731541634, acc: 0.97, recall: 0.97, precision: 0.9716981132075472, f_beta: 0.9699729756781104
train: step: 1461, loss: 0.08097049593925476, acc: 0.89, recall: 0.89, precision: 0.8914090726615818, f_beta: 0.8899009108197378
train: step: 1462, loss: 0.05693105608224869, acc: 0.93, recall: 0.9299999999999999, precision: 0.9315535929345644, f_beta: 0.9299369432489241
train: step: 1463, loss: 0.051909517496824265, acc: 0.93, recall: 0.9299999999999999, precision: 0.930172068827531, f_beta: 0.9299929992999301
train: step: 1464, loss: 0.04128114879131317, acc: 0.95, recall: 0.95, precision: 0.9501800720288115, f_beta: 0.9499949994999499
train: step: 1465, loss: 0.06987931579351425, acc: 0.91, recall: 0.9099999999999999, precision: 0.9141414141414141, f_beta: 0.9097744360902256
train: step: 1466, loss: 0.08564935624599457, acc: 0.87, recall: 0.87, precision: 0.8701480592236894, f_beta: 0.8699869986998701
train: step: 1467, loss: 0.057279568165540695, acc: 0.93, recall: 0.9299999999999999, precision: 0.9385964912280702, f_beta: 0.9296553110240177
train: step: 1468, loss: 0.08171441406011581, acc: 0.9, recall: 0.9, precision: 0.9, f_beta: 0.9
train: step: 1469, loss: 0.05142588168382645, acc: 0.95, recall: 0.95, precision: 0.9501800720288115, f_beta: 0.9499949994999499
train: step: 1470, loss: 0.061797402799129486, acc: 0.91, recall: 0.9099999999999999, precision: 0.9181966544267646, f_beta: 0.9095568284594513
train: step: 1471, loss: 0.05730394273996353, acc: 0.93, recall: 0.9299999999999999, precision: 0.930172068827531, f_beta: 0.9299929992999301
train: step: 1472, loss: 0.07001791149377823, acc: 0.92, recall: 0.9199999999999999, precision: 0.9227053140096618, f_beta: 0.9198717948717948
train: step: 1473, loss: 0.05659380927681923, acc: 0.92, recall: 0.9199999999999999, precision: 0.9206730769230769, f_beta: 0.919967987194878
train: step: 1474, loss: 0.07053601741790771, acc: 0.91, recall: 0.9099999999999999, precision: 0.9114813327980731, f_beta: 0.9099189270343309
train: step: 1475, loss: 0.06991895288228989, acc: 0.9, recall: 0.8999999999999999, precision: 0.9025764895330113, f_beta: 0.8998397435897436
train: step: 1476, loss: 0.08809880912303925, acc: 0.89, recall: 0.8899999999999999, precision: 0.8939393939393939, f_beta: 0.8897243107769424
train: step: 1477, loss: 0.032450273633003235, acc: 0.96, recall: 0.96, precision: 0.9607371794871795, f_beta: 0.959983993597439
train: step: 1478, loss: 0.06539120525121689, acc: 0.92, recall: 0.9199999999999999, precision: 0.9261363636363636, f_beta: 0.9197109594540345
train: step: 1479, loss: 0.04575543850660324, acc: 0.91, recall: 0.91, precision: 0.9101640656262505, f_beta: 0.90999099909991
train: step: 1480, loss: 0.05392061173915863, acc: 0.93, recall: 0.9299999999999999, precision: 0.930172068827531, f_beta: 0.9299929992999301
train: step: 1481, loss: 0.060477904975414276, acc: 0.92, recall: 0.9199999999999999, precision: 0.9261363636363636, f_beta: 0.9197109594540345
train: step: 1482, loss: 0.051786869764328, acc: 0.95, recall: 0.95, precision: 0.9501800720288115, f_beta: 0.9499949994999499
train: step: 1483, loss: 0.05809566378593445, acc: 0.92, recall: 0.92, precision: 0.92, f_beta: 0.92
train: step: 1484, loss: 0.0562848225235939, acc: 0.92, recall: 0.9199999999999999, precision: 0.9227053140096618, f_beta: 0.9198717948717948
train: step: 1485, loss: 0.08599517494440079, acc: 0.89, recall: 0.8899999999999999, precision: 0.8939393939393939, f_beta: 0.8897243107769424
train: step: 1486, loss: 0.060388270765542984, acc: 0.92, recall: 0.9199999999999999, precision: 0.9227053140096618, f_beta: 0.9198717948717948
train: step: 1487, loss: 0.04292912781238556, acc: 0.94, recall: 0.94, precision: 0.94, f_beta: 0.94
train: step: 1488, loss: 0.06373973935842514, acc: 0.92, recall: 0.9199999999999999, precision: 0.9227053140096618, f_beta: 0.9198717948717948
train: step: 1489, loss: 0.07680097222328186, acc: 0.92, recall: 0.9199999999999999, precision: 0.9206730769230769, f_beta: 0.919967987194878
train: step: 1490, loss: 0.04088924452662468, acc: 0.95, recall: 0.95, precision: 0.9501800720288115, f_beta: 0.9499949994999499
train: step: 1491, loss: 0.1053420826792717, acc: 0.85, recall: 0.85, precision: 0.851264552388599, f_beta: 0.8498648783905516
train: step: 1492, loss: 0.05864652618765831, acc: 0.93, recall: 0.9299999999999999, precision: 0.9315535929345644, f_beta: 0.9299369432489241
train: step: 1493, loss: 0.04404645040631294, acc: 0.96, recall: 0.96, precision: 0.96, f_beta: 0.96
train: step: 1494, loss: 0.08324623107910156, acc: 0.91, recall: 0.91, precision: 0.9101640656262505, f_beta: 0.90999099909991
train: step: 1495, loss: 0.04389771446585655, acc: 0.96, recall: 0.96, precision: 0.9607371794871795, f_beta: 0.959983993597439
train: step: 1496, loss: 0.04157554730772972, acc: 0.93, recall: 0.9299999999999999, precision: 0.930172068827531, f_beta: 0.9299929992999301
train: step: 1497, loss: 0.0408659353852272, acc: 0.96, recall: 0.96, precision: 0.96, f_beta: 0.96
train: step: 1498, loss: 0.04013471677899361, acc: 0.94, recall: 0.94, precision: 0.9407051282051282, f_beta: 0.9399759903961584
train: step: 1499, loss: 0.11283383518457413, acc: 0.86, recall: 0.86, precision: 0.86, f_beta: 0.8599999999999999
train: step: 1500, loss: 0.06489298492670059, acc: 0.91, recall: 0.91, precision: 0.9101640656262505, f_beta: 0.90999099909991
train: step: 1501, loss: 0.06691386550664902, acc: 0.9, recall: 0.8999999999999999, precision: 0.9025764895330113, f_beta: 0.8998397435897436
train: step: 1502, loss: 0.034064941108226776, acc: 0.96, recall: 0.96, precision: 0.96, f_beta: 0.96
train: step: 1503, loss: 0.09194071590900421, acc: 0.88, recall: 0.8799999999999999, precision: 0.885551948051948, f_beta: 0.8795664391810518
train: step: 1504, loss: 0.0883508026599884, acc: 0.87, recall: 0.87, precision: 0.8713368125250903, f_beta: 0.8698828946051447
train: step: 1505, loss: 0.033453237265348434, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 1506, loss: 0.05039302259683609, acc: 0.93, recall: 0.9299999999999999, precision: 0.9343434343434343, f_beta: 0.9298245614035088
train: step: 1507, loss: 0.051631178706884384, acc: 0.94, recall: 0.94, precision: 0.9407051282051282, f_beta: 0.9399759903961584
train: step: 1508, loss: 0.04357556253671646, acc: 0.94, recall: 0.94, precision: 0.9407051282051282, f_beta: 0.9399759903961584
train: step: 1509, loss: 0.08686722069978714, acc: 0.88, recall: 0.88, precision: 0.8824476650563606, f_beta: 0.8798076923076923
train: step: 1510, loss: 0.06553790718317032, acc: 0.93, recall: 0.9299999999999999, precision: 0.9315535929345644, f_beta: 0.9299369432489241
train: step: 1511, loss: 0.026927463710308075, acc: 0.95, recall: 0.95, precision: 0.9501800720288115, f_beta: 0.9499949994999499
train: step: 1512, loss: 0.04035435989499092, acc: 0.96, recall: 0.96, precision: 0.962962962962963, f_beta: 0.9599358974358974
train: step: 1513, loss: 0.07307522743940353, acc: 0.92, recall: 0.9199999999999999, precision: 0.9206730769230769, f_beta: 0.919967987194878
train: step: 1514, loss: 0.0646638423204422, acc: 0.92, recall: 0.9199999999999999, precision: 0.9206730769230769, f_beta: 0.919967987194878
train: step: 1515, loss: 0.05728757381439209, acc: 0.9, recall: 0.9, precision: 0.9006410256410255, f_beta: 0.8999599839935974
train: step: 1516, loss: 0.07269427180290222, acc: 0.89, recall: 0.8899999999999999, precision: 0.8939393939393939, f_beta: 0.8897243107769424
train: step: 1517, loss: 0.04930286481976509, acc: 0.95, recall: 0.95, precision: 0.9501800720288115, f_beta: 0.9499949994999499
train: step: 1518, loss: 0.05212579295039177, acc: 0.94, recall: 0.94, precision: 0.9407051282051282, f_beta: 0.9399759903961584
train: step: 1519, loss: 0.036491867154836655, acc: 0.95, recall: 0.95, precision: 0.9501800720288115, f_beta: 0.9499949994999499
train: step: 1520, loss: 0.037680648267269135, acc: 0.95, recall: 0.95, precision: 0.9501800720288115, f_beta: 0.9499949994999499
train: step: 1521, loss: 0.018336979672312737, acc: 0.99, recall: 0.99, precision: 0.9901960784313726, f_beta: 0.98999899989999
train: step: 1522, loss: 0.07020789384841919, acc: 0.93, recall: 0.9299999999999999, precision: 0.9315535929345644, f_beta: 0.9299369432489241
train: step: 1523, loss: 0.0278620645403862, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 1524, loss: 0.05787978321313858, acc: 0.94, recall: 0.94, precision: 0.9407051282051282, f_beta: 0.9399759903961584
train: step: 1525, loss: 0.07112742215394974, acc: 0.92, recall: 0.9199999999999999, precision: 0.9206730769230769, f_beta: 0.919967987194878
train: step: 1526, loss: 0.05239059403538704, acc: 0.92, recall: 0.9199999999999999, precision: 0.9206730769230769, f_beta: 0.919967987194878
train: step: 1527, loss: 0.0565066896378994, acc: 0.93, recall: 0.9299999999999999, precision: 0.9343434343434343, f_beta: 0.9298245614035088
train: step: 1528, loss: 0.06290346384048462, acc: 0.93, recall: 0.9299999999999999, precision: 0.9315535929345644, f_beta: 0.9299369432489241
train: step: 1529, loss: 0.08607250452041626, acc: 0.88, recall: 0.88, precision: 0.8824476650563606, f_beta: 0.8798076923076923
train: step: 1530, loss: 0.06501813232898712, acc: 0.92, recall: 0.9199999999999999, precision: 0.9206730769230769, f_beta: 0.919967987194878
train: step: 1531, loss: 0.05312971770763397, acc: 0.93, recall: 0.9299999999999999, precision: 0.9315535929345644, f_beta: 0.9299369432489241
train: step: 1532, loss: 0.029906529933214188, acc: 0.96, recall: 0.96, precision: 0.9607371794871795, f_beta: 0.959983993597439
train: step: 1533, loss: 0.010793461464345455, acc: 1.0, recall: 1.0, precision: 1.0, f_beta: 1.0
train: step: 1534, loss: 0.026307890191674232, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 1535, loss: 0.06516990065574646, acc: 0.93, recall: 0.9299999999999999, precision: 0.9343434343434343, f_beta: 0.9298245614035088
train: step: 1536, loss: 0.0736149251461029, acc: 0.91, recall: 0.91, precision: 0.9101640656262505, f_beta: 0.90999099909991
train: step: 1537, loss: 0.022757649421691895, acc: 0.98, recall: 0.98, precision: 0.98, f_beta: 0.98
train: step: 1538, loss: 0.05352410674095154, acc: 0.93, recall: 0.9299999999999999, precision: 0.9315535929345644, f_beta: 0.9299369432489241
train: step: 1539, loss: 0.10596302151679993, acc: 0.86, recall: 0.86, precision: 0.86, f_beta: 0.8599999999999999
train: step: 1540, loss: 0.033096447587013245, acc: 0.95, recall: 0.95, precision: 0.9501800720288115, f_beta: 0.9499949994999499
train: step: 1541, loss: 0.04504361003637314, acc: 0.95, recall: 0.95, precision: 0.9516258530710557, f_beta: 0.949954959463517
train: step: 1542, loss: 0.0226511899381876, acc: 0.98, recall: 0.98, precision: 0.98, f_beta: 0.98
train: step: 1543, loss: 0.03285966068506241, acc: 0.96, recall: 0.96, precision: 0.96, f_beta: 0.96
train: step: 1544, loss: 0.011019911617040634, acc: 0.99, recall: 0.99, precision: 0.9901960784313726, f_beta: 0.98999899989999
train: step: 1545, loss: 0.03375239670276642, acc: 0.96, recall: 0.96, precision: 0.96, f_beta: 0.96
train: step: 1546, loss: 0.04728641360998154, acc: 0.97, recall: 0.97, precision: 0.9716981132075472, f_beta: 0.9699729756781104
train: step: 1547, loss: 0.052471823990345, acc: 0.92, recall: 0.92, precision: 0.92, f_beta: 0.92
train: step: 1548, loss: 0.06421322375535965, acc: 0.91, recall: 0.91, precision: 0.9101640656262505, f_beta: 0.90999099909991
train: step: 1549, loss: 0.06826066970825195, acc: 0.91, recall: 0.91, precision: 0.9101640656262505, f_beta: 0.90999099909991
train: step: 1550, loss: 0.02226165682077408, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 1551, loss: 0.09372108429670334, acc: 0.89, recall: 0.89, precision: 0.89015606242497, f_beta: 0.88998899889989
train: step: 1552, loss: 0.0408037006855011, acc: 0.97, recall: 0.97, precision: 0.9716981132075472, f_beta: 0.9699729756781104
train: step: 1553, loss: 0.04650384932756424, acc: 0.94, recall: 0.94, precision: 0.9407051282051282, f_beta: 0.9399759903961584
train: step: 1554, loss: 0.024517716839909554, acc: 0.97, recall: 0.97, precision: 0.9716981132075472, f_beta: 0.9699729756781104
train: step: 1555, loss: 0.08694761991500854, acc: 0.91, recall: 0.9099999999999999, precision: 0.9141414141414141, f_beta: 0.9097744360902256
train: step: 1556, loss: 0.07632987201213837, acc: 0.92, recall: 0.9199999999999999, precision: 0.9227053140096618, f_beta: 0.9198717948717948
train: step: 1557, loss: 0.0553995706140995, acc: 0.93, recall: 0.9299999999999999, precision: 0.930172068827531, f_beta: 0.9299929992999301
train: step: 1558, loss: 0.08812496066093445, acc: 0.88, recall: 0.88, precision: 0.88, f_beta: 0.88
train: step: 1559, loss: 0.0711611807346344, acc: 0.91, recall: 0.91, precision: 0.9101640656262505, f_beta: 0.90999099909991
train: step: 1560, loss: 0.0477156825363636, acc: 0.94, recall: 0.94, precision: 0.9464285714285714, f_beta: 0.939783219590526
train: step: 1561, loss: 0.039718613028526306, acc: 0.96, recall: 0.96, precision: 0.9607371794871795, f_beta: 0.959983993597439
train: step: 1562, loss: 0.08094312995672226, acc: 0.88, recall: 0.88, precision: 0.8806089743589745, f_beta: 0.879951980792317
train: step: 1563, loss: 0.04225067049264908, acc: 0.94, recall: 0.94, precision: 0.9407051282051282, f_beta: 0.9399759903961584
train: step: 1564, loss: 0.05942896753549576, acc: 0.92, recall: 0.9199999999999999, precision: 0.9206730769230769, f_beta: 0.919967987194878
train: step: 1565, loss: 0.04997384548187256, acc: 0.94, recall: 0.94, precision: 0.94, f_beta: 0.94
train: step: 1566, loss: 0.05706131458282471, acc: 0.95, recall: 0.95, precision: 0.9501800720288115, f_beta: 0.9499949994999499
train: step: 1567, loss: 0.04299570992588997, acc: 0.95, recall: 0.95, precision: 0.9516258530710557, f_beta: 0.949954959463517
train: step: 1568, loss: 0.0417645163834095, acc: 0.96, recall: 0.96, precision: 0.96, f_beta: 0.96
train: step: 1569, loss: 0.052104540169239044, acc: 0.94, recall: 0.94, precision: 0.9407051282051282, f_beta: 0.9399759903961584
train: step: 1570, loss: 0.05375371873378754, acc: 0.94, recall: 0.94, precision: 0.94, f_beta: 0.94
train: step: 1571, loss: 0.030527295544743538, acc: 0.95, recall: 0.95, precision: 0.9501800720288115, f_beta: 0.9499949994999499
train: step: 1572, loss: 0.060212116688489914, acc: 0.93, recall: 0.9299999999999999, precision: 0.9315535929345644, f_beta: 0.9299369432489241
train: step: 1573, loss: 0.041686736047267914, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 1574, loss: 0.027635937556624413, acc: 0.95, recall: 0.95, precision: 0.9501800720288115, f_beta: 0.9499949994999499
train: step: 1575, loss: 0.04116615280508995, acc: 0.96, recall: 0.96, precision: 0.9607371794871795, f_beta: 0.959983993597439
train: step: 1576, loss: 0.05898623168468475, acc: 0.94, recall: 0.94, precision: 0.94, f_beta: 0.94
train: step: 1577, loss: 0.05123484134674072, acc: 0.95, recall: 0.95, precision: 0.9501800720288115, f_beta: 0.9499949994999499
train: step: 1578, loss: 0.027122922241687775, acc: 0.96, recall: 0.96, precision: 0.96, f_beta: 0.96
train: step: 1579, loss: 0.06898026168346405, acc: 0.9, recall: 0.8999999999999999, precision: 0.9058441558441559, f_beta: 0.8996386993175431
train: step: 1580, loss: 0.07782398164272308, acc: 0.89, recall: 0.89, precision: 0.8914090726615818, f_beta: 0.8899009108197378
train: step: 1581, loss: 0.05856738239526749, acc: 0.93, recall: 0.9299999999999999, precision: 0.9385964912280702, f_beta: 0.9296553110240177
train: step: 1582, loss: 0.03338618203997612, acc: 0.96, recall: 0.96, precision: 0.96, f_beta: 0.96
train: step: 1583, loss: 0.08001113682985306, acc: 0.9, recall: 0.9, precision: 0.9006410256410255, f_beta: 0.8999599839935974
train: step: 1584, loss: 0.04460976645350456, acc: 0.95, recall: 0.95, precision: 0.9545454545454546, f_beta: 0.949874686716792
train: step: 1585, loss: 0.05247565358877182, acc: 0.94, recall: 0.94, precision: 0.9407051282051282, f_beta: 0.9399759903961584
train: step: 1586, loss: 0.05212324112653732, acc: 0.95, recall: 0.95, precision: 0.9516258530710557, f_beta: 0.949954959463517
train: step: 1587, loss: 0.049338627606630325, acc: 0.94, recall: 0.94, precision: 0.9407051282051282, f_beta: 0.9399759903961584
train: step: 1588, loss: 0.02698614075779915, acc: 0.96, recall: 0.96, precision: 0.96, f_beta: 0.96
train: step: 1589, loss: 0.0441630519926548, acc: 0.95, recall: 0.95, precision: 0.9501800720288115, f_beta: 0.9499949994999499
train: step: 1590, loss: 0.09068118035793304, acc: 0.88, recall: 0.8799999999999999, precision: 0.885551948051948, f_beta: 0.8795664391810518
train: step: 1591, loss: 0.059508487582206726, acc: 0.93, recall: 0.9299999999999999, precision: 0.9315535929345644, f_beta: 0.9299369432489241
train: step: 1592, loss: 0.03322092071175575, acc: 0.95, recall: 0.95, precision: 0.9501800720288115, f_beta: 0.9499949994999499
train: step: 1593, loss: 0.06388664990663528, acc: 0.91, recall: 0.91, precision: 0.9101640656262505, f_beta: 0.90999099909991
train: step: 1594, loss: 0.07657663524150848, acc: 0.9, recall: 0.8999999999999999, precision: 0.9025764895330113, f_beta: 0.8998397435897436
train: step: 1595, loss: 0.03458501026034355, acc: 0.96, recall: 0.96, precision: 0.9607371794871795, f_beta: 0.959983993597439
train: step: 1596, loss: 0.0933493971824646, acc: 0.88, recall: 0.88, precision: 0.88, f_beta: 0.88
train: step: 1597, loss: 0.02894311212003231, acc: 0.96, recall: 0.96, precision: 0.9607371794871795, f_beta: 0.959983993597439
train: step: 1598, loss: 0.05783731862902641, acc: 0.91, recall: 0.91, precision: 0.9101640656262505, f_beta: 0.90999099909991
train: step: 1599, loss: 0.03295622020959854, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 1600, loss: 0.02159532532095909, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 1601, loss: 0.03998284414410591, acc: 0.94, recall: 0.94, precision: 0.94, f_beta: 0.94
train: step: 1602, loss: 0.054738081991672516, acc: 0.93, recall: 0.9299999999999999, precision: 0.930172068827531, f_beta: 0.9299929992999301
train: step: 1603, loss: 0.03665341064333916, acc: 0.95, recall: 0.95, precision: 0.9516258530710557, f_beta: 0.949954959463517
train: step: 1604, loss: 0.07449755817651749, acc: 0.9, recall: 0.9, precision: 0.9006410256410255, f_beta: 0.8999599839935974
train: step: 1605, loss: 0.07755855470895767, acc: 0.9, recall: 0.9, precision: 0.9006410256410255, f_beta: 0.8999599839935974
train: step: 1606, loss: 0.06298863142728806, acc: 0.93, recall: 0.9299999999999999, precision: 0.9343434343434343, f_beta: 0.9298245614035088
train: step: 1607, loss: 0.0895109474658966, acc: 0.91, recall: 0.9099999999999999, precision: 0.9141414141414141, f_beta: 0.9097744360902256
train: step: 1608, loss: 0.02406075783073902, acc: 0.96, recall: 0.96, precision: 0.96, f_beta: 0.96
train: step: 1609, loss: 0.08057615906000137, acc: 0.89, recall: 0.89, precision: 0.9098360655737705, f_beta: 0.8886526976414617
train: step: 1610, loss: 0.046846903860569, acc: 0.94, recall: 0.94, precision: 0.94, f_beta: 0.94
train: step: 1611, loss: 0.04213683307170868, acc: 0.94, recall: 0.94, precision: 0.9407051282051282, f_beta: 0.9399759903961584
train: step: 1612, loss: 0.11919903010129929, acc: 0.85, recall: 0.85, precision: 0.8501400560224089, f_beta: 0.84998499849985
train: step: 1613, loss: 0.03871891647577286, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 1614, loss: 0.02333996817469597, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 1615, loss: 0.04313291609287262, acc: 0.93, recall: 0.9299999999999999, precision: 0.930172068827531, f_beta: 0.9299929992999301
train: step: 1616, loss: 0.05074698105454445, acc: 0.93, recall: 0.9299999999999999, precision: 0.9315535929345644, f_beta: 0.9299369432489241
train: step: 1617, loss: 0.0437350831925869, acc: 0.95, recall: 0.95, precision: 0.9501800720288115, f_beta: 0.9499949994999499
train: step: 1618, loss: 0.04361271858215332, acc: 0.96, recall: 0.96, precision: 0.9607371794871795, f_beta: 0.959983993597439
train: step: 1619, loss: 0.06034480035305023, acc: 0.92, recall: 0.9199999999999999, precision: 0.9227053140096618, f_beta: 0.9198717948717948
train: step: 1620, loss: 0.052086830139160156, acc: 0.93, recall: 0.9299999999999999, precision: 0.930172068827531, f_beta: 0.9299929992999301
train: step: 1621, loss: 0.08284926414489746, acc: 0.9, recall: 0.8999999999999999, precision: 0.9058441558441559, f_beta: 0.8996386993175431
train: step: 1622, loss: 0.095120869576931, acc: 0.84, recall: 0.8400000000000001, precision: 0.8421900161030595, f_beta: 0.8397435897435896
train: step: 1623, loss: 0.02577916346490383, acc: 0.98, recall: 0.98, precision: 0.98, f_beta: 0.98
train: step: 1624, loss: 0.05147765204310417, acc: 0.95, recall: 0.95, precision: 0.9545454545454546, f_beta: 0.949874686716792
train: step: 1625, loss: 0.028763651847839355, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 1626, loss: 0.053315915167331696, acc: 0.92, recall: 0.9199999999999999, precision: 0.9206730769230769, f_beta: 0.919967987194878
train: step: 1627, loss: 0.041525859385728836, acc: 0.95, recall: 0.95, precision: 0.9501800720288115, f_beta: 0.9499949994999499
train: step: 1628, loss: 0.07972454279661179, acc: 0.91, recall: 0.91, precision: 0.9101640656262505, f_beta: 0.90999099909991
train: step: 1629, loss: 0.04947957023978233, acc: 0.94, recall: 0.94, precision: 0.9464285714285714, f_beta: 0.939783219590526
train: step: 1630, loss: 0.03662434592843056, acc: 0.96, recall: 0.96, precision: 0.96, f_beta: 0.96
train: step: 1631, loss: 0.027784772217273712, acc: 0.98, recall: 0.98, precision: 0.98, f_beta: 0.98
train: step: 1632, loss: 0.046771544963121414, acc: 0.95, recall: 0.95, precision: 0.9545454545454546, f_beta: 0.949874686716792
train: step: 1633, loss: 0.0891246572136879, acc: 0.87, recall: 0.87, precision: 0.8713368125250903, f_beta: 0.8698828946051447
train: step: 1634, loss: 0.0863628163933754, acc: 0.9, recall: 0.8999999999999999, precision: 0.9025764895330113, f_beta: 0.8998397435897436
train: step: 1635, loss: 0.03342197835445404, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 1636, loss: 0.06539390236139297, acc: 0.91, recall: 0.91, precision: 0.9101640656262505, f_beta: 0.90999099909991
train: step: 1637, loss: 0.020017391070723534, acc: 0.97, recall: 0.97, precision: 0.9716981132075472, f_beta: 0.9699729756781104
train: step: 1638, loss: 0.08300748467445374, acc: 0.91, recall: 0.9099999999999999, precision: 0.9114813327980731, f_beta: 0.9099189270343309
train: step: 1639, loss: 0.06585638225078583, acc: 0.9, recall: 0.8999999999999999, precision: 0.9025764895330113, f_beta: 0.8998397435897436
train: step: 1640, loss: 0.027201242744922638, acc: 0.96, recall: 0.96, precision: 0.9607371794871795, f_beta: 0.959983993597439
train: step: 1641, loss: 0.055593881756067276, acc: 0.94, recall: 0.94, precision: 0.9407051282051282, f_beta: 0.9399759903961584
train: step: 1642, loss: 0.02186533994972706, acc: 0.98, recall: 0.98, precision: 0.9807692307692308, f_beta: 0.9799919967987194
train: step: 1643, loss: 0.01327888946980238, acc: 0.99, recall: 0.99, precision: 0.9901960784313726, f_beta: 0.98999899989999
train: step: 1644, loss: 0.025953039526939392, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 1645, loss: 0.06211121380329132, acc: 0.92, recall: 0.92, precision: 0.92, f_beta: 0.92
train: step: 1646, loss: 0.05209213122725487, acc: 0.94, recall: 0.94, precision: 0.9407051282051282, f_beta: 0.9399759903961584
train: step: 1647, loss: 0.011925892904400826, acc: 0.99, recall: 0.99, precision: 0.9901960784313726, f_beta: 0.98999899989999
train: step: 1648, loss: 0.04624593257904053, acc: 0.93, recall: 0.9299999999999999, precision: 0.930172068827531, f_beta: 0.9299929992999301
train: step: 1649, loss: 0.03595694154500961, acc: 0.95, recall: 0.95, precision: 0.9501800720288115, f_beta: 0.9499949994999499
train: step: 1650, loss: 0.06816785782575607, acc: 0.94, recall: 0.94, precision: 0.9407051282051282, f_beta: 0.9399759903961584
train: step: 1651, loss: 0.05742236226797104, acc: 0.93, recall: 0.9299999999999999, precision: 0.9315535929345644, f_beta: 0.9299369432489241
train: step: 1652, loss: 0.0670909434556961, acc: 0.91, recall: 0.91, precision: 0.9101640656262505, f_beta: 0.90999099909991
train: step: 1653, loss: 0.03945455327630043, acc: 0.96, recall: 0.96, precision: 0.9607371794871795, f_beta: 0.959983993597439
train: step: 1654, loss: 0.045822057873010635, acc: 0.93, recall: 0.9299999999999999, precision: 0.9315535929345644, f_beta: 0.9299369432489241
train: step: 1655, loss: 0.07407373189926147, acc: 0.9, recall: 0.9, precision: 0.9, f_beta: 0.9
train: step: 1656, loss: 0.03425317257642746, acc: 0.96, recall: 0.96, precision: 0.9607371794871795, f_beta: 0.959983993597439
train: step: 1657, loss: 0.0434776209294796, acc: 0.94, recall: 0.94, precision: 0.9407051282051282, f_beta: 0.9399759903961584
train: step: 1658, loss: 0.02381342649459839, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 1659, loss: 0.017039883881807327, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 1660, loss: 0.09564682096242905, acc: 0.88, recall: 0.88, precision: 0.8824476650563606, f_beta: 0.8798076923076923
train: step: 1661, loss: 0.0708763599395752, acc: 0.9, recall: 0.9, precision: 0.9006410256410255, f_beta: 0.8999599839935974
train: step: 1662, loss: 0.08969852328300476, acc: 0.88, recall: 0.88, precision: 0.88, f_beta: 0.88
train: step: 1663, loss: 0.0678105354309082, acc: 0.9, recall: 0.9, precision: 0.9006410256410255, f_beta: 0.8999599839935974
train: step: 1664, loss: 0.09044072777032852, acc: 0.9, recall: 0.8999999999999999, precision: 0.9025764895330113, f_beta: 0.8998397435897436
train: step: 1665, loss: 0.04762732610106468, acc: 0.94, recall: 0.94, precision: 0.9464285714285714, f_beta: 0.939783219590526
train: step: 1666, loss: 0.05819499120116234, acc: 0.92, recall: 0.9199999999999999, precision: 0.9310344827586207, f_beta: 0.9194847020933978
train: step: 1667, loss: 0.030594084411859512, acc: 0.96, recall: 0.96, precision: 0.96, f_beta: 0.96
train: step: 1668, loss: 0.04456739500164986, acc: 0.95, recall: 0.95, precision: 0.9516258530710557, f_beta: 0.949954959463517
train: step: 1669, loss: 0.03967049717903137, acc: 0.94, recall: 0.94, precision: 0.9464285714285714, f_beta: 0.939783219590526
train: step: 1670, loss: 0.06495298445224762, acc: 0.89, recall: 0.89, precision: 0.89015606242497, f_beta: 0.88998899889989
train: step: 1671, loss: 0.08680898696184158, acc: 0.87, recall: 0.87, precision: 0.8713368125250903, f_beta: 0.8698828946051447
train: step: 1672, loss: 0.04552678018808365, acc: 0.95, recall: 0.95, precision: 0.9501800720288115, f_beta: 0.9499949994999499
train: step: 1673, loss: 0.05960548296570778, acc: 0.91, recall: 0.91, precision: 0.9101640656262505, f_beta: 0.90999099909991
train: step: 1674, loss: 0.052419401705265045, acc: 0.92, recall: 0.9199999999999999, precision: 0.9227053140096618, f_beta: 0.9198717948717948
train: step: 1675, loss: 0.05302966758608818, acc: 0.94, recall: 0.94, precision: 0.94, f_beta: 0.94
train: step: 1676, loss: 0.05939561873674393, acc: 0.92, recall: 0.9199999999999999, precision: 0.9227053140096618, f_beta: 0.9198717948717948
train: step: 1677, loss: 0.014969535171985626, acc: 0.99, recall: 0.99, precision: 0.9901960784313726, f_beta: 0.98999899989999
train: step: 1678, loss: 0.03543002903461456, acc: 0.96, recall: 0.96, precision: 0.962962962962963, f_beta: 0.9599358974358974
train: step: 1679, loss: 0.04141969978809357, acc: 0.95, recall: 0.95, precision: 0.9501800720288115, f_beta: 0.9499949994999499
train: step: 1680, loss: 0.05644498020410538, acc: 0.91, recall: 0.9099999999999999, precision: 0.9114813327980731, f_beta: 0.9099189270343309
train: step: 1681, loss: 0.05625278502702713, acc: 0.91, recall: 0.9099999999999999, precision: 0.9114813327980731, f_beta: 0.9099189270343309
train: step: 1682, loss: 0.05684760957956314, acc: 0.93, recall: 0.9299999999999999, precision: 0.930172068827531, f_beta: 0.9299929992999301
train: step: 1683, loss: 0.06787936389446259, acc: 0.92, recall: 0.9199999999999999, precision: 0.9261363636363636, f_beta: 0.9197109594540345
train: step: 1684, loss: 0.025508388876914978, acc: 0.98, recall: 0.98, precision: 0.98, f_beta: 0.98
train: step: 1685, loss: 0.05465839058160782, acc: 0.92, recall: 0.9199999999999999, precision: 0.9227053140096618, f_beta: 0.9198717948717948
train: step: 1686, loss: 0.1031460165977478, acc: 0.85, recall: 0.8500000000000001, precision: 0.8535353535353536, f_beta: 0.849624060150376
train: step: 1687, loss: 0.025692500174045563, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 1688, loss: 0.03666529804468155, acc: 0.97, recall: 0.97, precision: 0.9716981132075472, f_beta: 0.9699729756781104
train: step: 1689, loss: 0.06873297691345215, acc: 0.91, recall: 0.9099999999999999, precision: 0.9141414141414141, f_beta: 0.9097744360902256
train: step: 1690, loss: 0.046268343925476074, acc: 0.94, recall: 0.94, precision: 0.9407051282051282, f_beta: 0.9399759903961584
train: step: 1691, loss: 0.038248952478170395, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 1692, loss: 0.04782271385192871, acc: 0.95, recall: 0.95, precision: 0.9545454545454546, f_beta: 0.949874686716792
train: step: 1693, loss: 0.09203332662582397, acc: 0.89, recall: 0.89, precision: 0.89015606242497, f_beta: 0.88998899889989
train: step: 1694, loss: 0.05858980119228363, acc: 0.9, recall: 0.8999999999999999, precision: 0.9025764895330113, f_beta: 0.8998397435897436
train: step: 1695, loss: 0.04485094174742699, acc: 0.94, recall: 0.94, precision: 0.94, f_beta: 0.94
train: step: 1696, loss: 0.0356765016913414, acc: 0.96, recall: 0.96, precision: 0.962962962962963, f_beta: 0.9599358974358974
train: step: 1697, loss: 0.06810861825942993, acc: 0.91, recall: 0.9099999999999999, precision: 0.9114813327980731, f_beta: 0.9099189270343309
train: step: 1698, loss: 0.016536187380552292, acc: 0.99, recall: 0.99, precision: 0.9901960784313726, f_beta: 0.98999899989999
train: step: 1699, loss: 0.06957367062568665, acc: 0.92, recall: 0.9199999999999999, precision: 0.9261363636363636, f_beta: 0.9197109594540345
train: step: 1700, loss: 0.028958778828382492, acc: 0.96, recall: 0.96, precision: 0.962962962962963, f_beta: 0.9599358974358974
train: step: 1701, loss: 0.011696958914399147, acc: 0.99, recall: 0.99, precision: 0.9901960784313726, f_beta: 0.98999899989999
train: step: 1702, loss: 0.019899023696780205, acc: 0.96, recall: 0.96, precision: 0.9607371794871795, f_beta: 0.959983993597439
train: step: 1703, loss: 0.01236879825592041, acc: 0.99, recall: 0.99, precision: 0.9901960784313726, f_beta: 0.98999899989999
train: step: 1704, loss: 0.04536153003573418, acc: 0.95, recall: 0.95, precision: 0.9501800720288115, f_beta: 0.9499949994999499
train: step: 1705, loss: 0.0226274486631155, acc: 0.98, recall: 0.98, precision: 0.9807692307692308, f_beta: 0.9799919967987194
train: step: 1706, loss: 0.04953458905220032, acc: 0.94, recall: 0.94, precision: 0.9407051282051282, f_beta: 0.9399759903961584
train: step: 1707, loss: 0.07950326800346375, acc: 0.9, recall: 0.9, precision: 0.9006410256410255, f_beta: 0.8999599839935974
train: step: 1708, loss: 0.06856740266084671, acc: 0.91, recall: 0.9099999999999999, precision: 0.9181966544267646, f_beta: 0.9095568284594513
train: step: 1709, loss: 0.031710878014564514, acc: 0.96, recall: 0.96, precision: 0.9607371794871795, f_beta: 0.959983993597439
train: step: 1710, loss: 0.08536920696496964, acc: 0.88, recall: 0.88, precision: 0.8806089743589745, f_beta: 0.879951980792317
train: step: 1711, loss: 0.022487353533506393, acc: 0.98, recall: 0.98, precision: 0.9807692307692308, f_beta: 0.9799919967987194
train: step: 1712, loss: 0.05062370374798775, acc: 0.94, recall: 0.94, precision: 0.94, f_beta: 0.94
train: step: 1713, loss: 0.047650281339883804, acc: 0.94, recall: 0.94, precision: 0.9407051282051282, f_beta: 0.9399759903961584
train: step: 1714, loss: 0.03796330466866493, acc: 0.96, recall: 0.96, precision: 0.962962962962963, f_beta: 0.9599358974358974
train: step: 1715, loss: 0.035988517105579376, acc: 0.95, recall: 0.95, precision: 0.9516258530710557, f_beta: 0.949954959463517
train: step: 1716, loss: 0.03496982902288437, acc: 0.98, recall: 0.98, precision: 0.98, f_beta: 0.98
train: step: 1717, loss: 0.04638230428099632, acc: 0.93, recall: 0.9299999999999999, precision: 0.930172068827531, f_beta: 0.9299929992999301
train: step: 1718, loss: 0.0750894621014595, acc: 0.92, recall: 0.92, precision: 0.92, f_beta: 0.92
train: step: 1719, loss: 0.05331674963235855, acc: 0.94, recall: 0.94, precision: 0.9407051282051282, f_beta: 0.9399759903961584
train: step: 1720, loss: 0.03779632970690727, acc: 0.96, recall: 0.96, precision: 0.96, f_beta: 0.96
train: step: 1721, loss: 0.06430207937955856, acc: 0.92, recall: 0.9199999999999999, precision: 0.9310344827586207, f_beta: 0.9194847020933978
train: step: 1722, loss: 0.03286111354827881, acc: 0.96, recall: 0.96, precision: 0.96, f_beta: 0.96
train: step: 1723, loss: 0.10464365780353546, acc: 0.88, recall: 0.88, precision: 0.8824476650563606, f_beta: 0.8798076923076923
train: step: 1724, loss: 0.05168522894382477, acc: 0.94, recall: 0.94, precision: 0.9407051282051282, f_beta: 0.9399759903961584
train: step: 1725, loss: 0.048791199922561646, acc: 0.93, recall: 0.9299999999999999, precision: 0.9315535929345644, f_beta: 0.9299369432489241
train: step: 1726, loss: 0.0695088729262352, acc: 0.93, recall: 0.9299999999999999, precision: 0.930172068827531, f_beta: 0.9299929992999301
train: step: 1727, loss: 0.044699616730213165, acc: 0.95, recall: 0.95, precision: 0.9501800720288115, f_beta: 0.9499949994999499
train: step: 1728, loss: 0.052268996834754944, acc: 0.94, recall: 0.94, precision: 0.94, f_beta: 0.94
train: step: 1729, loss: 0.039426401257514954, acc: 0.95, recall: 0.95, precision: 0.9501800720288115, f_beta: 0.9499949994999499
train: step: 1730, loss: 0.08607491850852966, acc: 0.88, recall: 0.8799999999999999, precision: 0.885551948051948, f_beta: 0.8795664391810518
train: step: 1731, loss: 0.052889108657836914, acc: 0.92, recall: 0.9199999999999999, precision: 0.9206730769230769, f_beta: 0.919967987194878
train: step: 1732, loss: 0.04663001000881195, acc: 0.95, recall: 0.95, precision: 0.9501800720288115, f_beta: 0.9499949994999499
train: step: 1733, loss: 0.04971291869878769, acc: 0.95, recall: 0.95, precision: 0.9516258530710557, f_beta: 0.949954959463517
train: step: 1734, loss: 0.10090009868144989, acc: 0.87, recall: 0.87, precision: 0.8713368125250903, f_beta: 0.8698828946051447
train: step: 1735, loss: 0.02775692008435726, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 1736, loss: 0.05662301182746887, acc: 0.92, recall: 0.92, precision: 0.92, f_beta: 0.92
train: step: 1737, loss: 0.09504804760217667, acc: 0.86, recall: 0.8600000000000001, precision: 0.8652597402597402, f_beta: 0.8594941790445605
train: step: 1738, loss: 0.06991039216518402, acc: 0.93, recall: 0.9299999999999999, precision: 0.930172068827531, f_beta: 0.9299929992999301
train: step: 1739, loss: 0.058940544724464417, acc: 0.92, recall: 0.9199999999999999, precision: 0.9206730769230769, f_beta: 0.919967987194878
train: step: 1740, loss: 0.02816290780901909, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 1741, loss: 0.06162073090672493, acc: 0.91, recall: 0.91, precision: 0.9101640656262505, f_beta: 0.90999099909991
train: step: 1742, loss: 0.0411360077559948, acc: 0.95, recall: 0.95, precision: 0.9501800720288115, f_beta: 0.9499949994999499
train: step: 1743, loss: 0.0308192390948534, acc: 0.95, recall: 0.95, precision: 0.9516258530710557, f_beta: 0.949954959463517
train: step: 1744, loss: 0.07628580927848816, acc: 0.89, recall: 0.89, precision: 0.89015606242497, f_beta: 0.88998899889989
train: step: 1745, loss: 0.036262694746255875, acc: 0.96, recall: 0.96, precision: 0.96, f_beta: 0.96
train: step: 1746, loss: 0.05586862936615944, acc: 0.92, recall: 0.9199999999999999, precision: 0.9261363636363636, f_beta: 0.9197109594540345
train: step: 1747, loss: 0.07533053308725357, acc: 0.9, recall: 0.8999999999999999, precision: 0.9025764895330113, f_beta: 0.8998397435897436
train: step: 1748, loss: 0.1255282461643219, acc: 0.86, recall: 0.86, precision: 0.8623188405797102, f_beta: 0.859775641025641
train: step: 1749, loss: 0.037321485579013824, acc: 0.94, recall: 0.94, precision: 0.9407051282051282, f_beta: 0.9399759903961584
train: step: 1750, loss: 0.06390136480331421, acc: 0.9, recall: 0.8999999999999999, precision: 0.9025764895330113, f_beta: 0.8998397435897436
train: step: 1751, loss: 0.04166214168071747, acc: 0.96, recall: 0.96, precision: 0.9607371794871795, f_beta: 0.959983993597439
train: step: 1752, loss: 0.06482101231813431, acc: 0.91, recall: 0.91, precision: 0.9101640656262505, f_beta: 0.90999099909991
train: step: 1753, loss: 0.01415502279996872, acc: 0.98, recall: 0.98, precision: 0.98, f_beta: 0.98
train: step: 1754, loss: 0.056275319308042526, acc: 0.93, recall: 0.9299999999999999, precision: 0.9315535929345644, f_beta: 0.9299369432489241
train: step: 1755, loss: 0.06263858824968338, acc: 0.91, recall: 0.91, precision: 0.9101640656262505, f_beta: 0.90999099909991
train: step: 1756, loss: 0.03781731426715851, acc: 0.96, recall: 0.96, precision: 0.9607371794871795, f_beta: 0.959983993597439
train: step: 1757, loss: 0.05727633461356163, acc: 0.93, recall: 0.9299999999999999, precision: 0.930172068827531, f_beta: 0.9299929992999301
train: step: 1758, loss: 0.03449797257781029, acc: 0.96, recall: 0.96, precision: 0.9607371794871795, f_beta: 0.959983993597439
train: step: 1759, loss: 0.05556846782565117, acc: 0.93, recall: 0.9299999999999999, precision: 0.9315535929345644, f_beta: 0.9299369432489241
train: step: 1760, loss: 0.03724680468440056, acc: 0.95, recall: 0.95, precision: 0.9516258530710557, f_beta: 0.949954959463517
train: step: 1761, loss: 0.04661162197589874, acc: 0.94, recall: 0.94, precision: 0.9407051282051282, f_beta: 0.9399759903961584
train: step: 1762, loss: 0.05965769663453102, acc: 0.95, recall: 0.95, precision: 0.9501800720288115, f_beta: 0.9499949994999499
train: step: 1763, loss: 0.029798220843076706, acc: 0.96, recall: 0.96, precision: 0.9607371794871795, f_beta: 0.959983993597439
train: step: 1764, loss: 0.03996778652071953, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 1765, loss: 0.044384993612766266, acc: 0.93, recall: 0.9299999999999999, precision: 0.930172068827531, f_beta: 0.9299929992999301
train: step: 1766, loss: 0.05325448885560036, acc: 0.94, recall: 0.94, precision: 0.9407051282051282, f_beta: 0.9399759903961584
train: step: 1767, loss: 0.08344094455242157, acc: 0.9, recall: 0.9, precision: 0.9, f_beta: 0.9
train: step: 1768, loss: 0.05938081815838814, acc: 0.92, recall: 0.9199999999999999, precision: 0.9206730769230769, f_beta: 0.919967987194878
train: step: 1769, loss: 0.021831898018717766, acc: 0.98, recall: 0.98, precision: 0.9807692307692308, f_beta: 0.9799919967987194
train: step: 1770, loss: 0.05511617660522461, acc: 0.92, recall: 0.9199999999999999, precision: 0.9206730769230769, f_beta: 0.919967987194878
train: step: 1771, loss: 0.04859629273414612, acc: 0.92, recall: 0.9199999999999999, precision: 0.9227053140096618, f_beta: 0.9198717948717948
train: step: 1772, loss: 0.021711815148591995, acc: 0.98, recall: 0.98, precision: 0.98, f_beta: 0.98
train: step: 1773, loss: 0.07517507672309875, acc: 0.9, recall: 0.9, precision: 0.9006410256410255, f_beta: 0.8999599839935974
train: step: 1774, loss: 0.07155802845954895, acc: 0.91, recall: 0.9099999999999999, precision: 0.9114813327980731, f_beta: 0.9099189270343309
train: step: 1775, loss: 0.029034888371825218, acc: 0.97, recall: 0.97, precision: 0.9716981132075472, f_beta: 0.9699729756781104
train: step: 1776, loss: 0.07086797058582306, acc: 0.94, recall: 0.94, precision: 0.9407051282051282, f_beta: 0.9399759903961584
train: step: 1777, loss: 0.07110223919153214, acc: 0.92, recall: 0.9199999999999999, precision: 0.9206730769230769, f_beta: 0.919967987194878
train: step: 1778, loss: 0.043600212782621384, acc: 0.93, recall: 0.9299999999999999, precision: 0.930172068827531, f_beta: 0.9299929992999301
train: step: 1779, loss: 0.03918594866991043, acc: 0.94, recall: 0.94, precision: 0.9407051282051282, f_beta: 0.9399759903961584
train: step: 1780, loss: 0.04602108150720596, acc: 0.93, recall: 0.9299999999999999, precision: 0.930172068827531, f_beta: 0.9299929992999301
train: step: 1781, loss: 0.05284130945801735, acc: 0.95, recall: 0.95, precision: 0.9501800720288115, f_beta: 0.9499949994999499
train: step: 1782, loss: 0.03283517807722092, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 1783, loss: 0.06284257769584656, acc: 0.91, recall: 0.91, precision: 0.9101640656262505, f_beta: 0.90999099909991
train: step: 1784, loss: 0.07415799051523209, acc: 0.9, recall: 0.8999999999999999, precision: 0.9058441558441559, f_beta: 0.8996386993175431
train: step: 1785, loss: 0.06933920085430145, acc: 0.92, recall: 0.92, precision: 0.92, f_beta: 0.92
train: step: 1786, loss: 0.06455224007368088, acc: 0.93, recall: 0.9299999999999999, precision: 0.930172068827531, f_beta: 0.9299929992999301
train: step: 1787, loss: 0.08379548788070679, acc: 0.87, recall: 0.87, precision: 0.8773969808241534, f_beta: 0.8693598633303186
train: step: 1788, loss: 0.04654017463326454, acc: 0.94, recall: 0.94, precision: 0.94, f_beta: 0.94
train: step: 1789, loss: 0.03637724742293358, acc: 0.96, recall: 0.96, precision: 0.96, f_beta: 0.96
train: step: 1790, loss: 0.0344354547560215, acc: 0.96, recall: 0.96, precision: 0.9607371794871795, f_beta: 0.959983993597439
train: step: 1791, loss: 0.05180228874087334, acc: 0.95, recall: 0.95, precision: 0.9501800720288115, f_beta: 0.9499949994999499
train: step: 1792, loss: 0.024364251643419266, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 1793, loss: 0.07342354953289032, acc: 0.9, recall: 0.8999999999999999, precision: 0.9058441558441559, f_beta: 0.8996386993175431
train: step: 1794, loss: 0.04311162978410721, acc: 0.94, recall: 0.94, precision: 0.9407051282051282, f_beta: 0.9399759903961584
train: step: 1795, loss: 0.0442185178399086, acc: 0.94, recall: 0.94, precision: 0.9428341384863124, f_beta: 0.9399038461538463
train: step: 1796, loss: 0.035226739943027496, acc: 0.96, recall: 0.96, precision: 0.962962962962963, f_beta: 0.9599358974358974
train: step: 1797, loss: 0.03078773058950901, acc: 0.98, recall: 0.98, precision: 0.9807692307692308, f_beta: 0.9799919967987194
train: step: 1798, loss: 0.07493212819099426, acc: 0.89, recall: 0.89, precision: 0.89015606242497, f_beta: 0.88998899889989
train: step: 1799, loss: 0.04336240887641907, acc: 0.94, recall: 0.94, precision: 0.9407051282051282, f_beta: 0.9399759903961584
train: step: 1800, loss: 0.04198606312274933, acc: 0.96, recall: 0.96, precision: 0.96, f_beta: 0.96
train: step: 1801, loss: 0.0565284825861454, acc: 0.94, recall: 0.94, precision: 0.9428341384863124, f_beta: 0.9399038461538463
train: step: 1802, loss: 0.043509941548109055, acc: 0.94, recall: 0.94, precision: 0.9407051282051282, f_beta: 0.9399759903961584
train: step: 1803, loss: 0.02335195243358612, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 1804, loss: 0.023293467238545418, acc: 0.98, recall: 0.98, precision: 0.9807692307692308, f_beta: 0.9799919967987194
train: step: 1805, loss: 0.0386803038418293, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 1806, loss: 0.014387565664947033, acc: 0.99, recall: 0.99, precision: 0.9901960784313726, f_beta: 0.98999899989999
train: step: 1807, loss: 0.06826473772525787, acc: 0.91, recall: 0.91, precision: 0.9101640656262505, f_beta: 0.90999099909991
train: step: 1808, loss: 0.05444898456335068, acc: 0.93, recall: 0.9299999999999999, precision: 0.930172068827531, f_beta: 0.9299929992999301
train: step: 1809, loss: 0.06729666888713837, acc: 0.92, recall: 0.9199999999999999, precision: 0.9206730769230769, f_beta: 0.919967987194878
train: step: 1810, loss: 0.02519841678440571, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 1811, loss: 0.04492722079157829, acc: 0.96, recall: 0.96, precision: 0.9607371794871795, f_beta: 0.959983993597439
train: step: 1812, loss: 0.041970644146203995, acc: 0.95, recall: 0.95, precision: 0.9545454545454546, f_beta: 0.949874686716792
train: step: 1813, loss: 0.04555989429354668, acc: 0.94, recall: 0.94, precision: 0.94, f_beta: 0.94
train: step: 1814, loss: 0.0098092807456851, acc: 0.99, recall: 0.99, precision: 0.9901960784313726, f_beta: 0.98999899989999
train: step: 1815, loss: 0.06566239148378372, acc: 0.92, recall: 0.9199999999999999, precision: 0.9227053140096618, f_beta: 0.9198717948717948
train: step: 1816, loss: 0.07311779260635376, acc: 0.9, recall: 0.9, precision: 0.9006410256410255, f_beta: 0.8999599839935974
train: step: 1817, loss: 0.017909366637468338, acc: 0.98, recall: 0.98, precision: 0.98, f_beta: 0.98
train: step: 1818, loss: 0.04128311946988106, acc: 0.94, recall: 0.94, precision: 0.94, f_beta: 0.94
train: step: 1819, loss: 0.059912990778684616, acc: 0.92, recall: 0.9199999999999999, precision: 0.9206730769230769, f_beta: 0.919967987194878
train: step: 1820, loss: 0.03407859429717064, acc: 0.96, recall: 0.96, precision: 0.9607371794871795, f_beta: 0.959983993597439
train: step: 1821, loss: 0.048091717064380646, acc: 0.94, recall: 0.94, precision: 0.9464285714285714, f_beta: 0.939783219590526
train: step: 1822, loss: 0.06796547770500183, acc: 0.92, recall: 0.92, precision: 0.92, f_beta: 0.92
train: step: 1823, loss: 0.04455910623073578, acc: 0.95, recall: 0.95, precision: 0.9501800720288115, f_beta: 0.9499949994999499
train: step: 1824, loss: 0.0047688912600278854, acc: 1.0, recall: 1.0, precision: 1.0, f_beta: 1.0
train: step: 1825, loss: 0.05774332955479622, acc: 0.93, recall: 0.9299999999999999, precision: 0.930172068827531, f_beta: 0.9299929992999301
train: step: 1826, loss: 0.025982830673456192, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 1827, loss: 0.038484200835227966, acc: 0.95, recall: 0.95, precision: 0.9516258530710557, f_beta: 0.949954959463517
train: step: 1828, loss: 0.051541198045015335, acc: 0.93, recall: 0.9299999999999999, precision: 0.9315535929345644, f_beta: 0.9299369432489241
train: step: 1829, loss: 0.05631985515356064, acc: 0.93, recall: 0.9299999999999999, precision: 0.930172068827531, f_beta: 0.9299929992999301
train: step: 1830, loss: 0.08876283466815948, acc: 0.89, recall: 0.8899999999999999, precision: 0.8939393939393939, f_beta: 0.8897243107769424
train: step: 1831, loss: 0.07271584868431091, acc: 0.91, recall: 0.91, precision: 0.9101640656262505, f_beta: 0.90999099909991
train: step: 1832, loss: 0.05053448677062988, acc: 0.92, recall: 0.92, precision: 0.92, f_beta: 0.92
train: step: 1833, loss: 0.04159555584192276, acc: 0.95, recall: 0.95, precision: 0.9516258530710557, f_beta: 0.949954959463517
train: step: 1834, loss: 0.06953761726617813, acc: 0.92, recall: 0.9199999999999999, precision: 0.9206730769230769, f_beta: 0.919967987194878
train: step: 1835, loss: 0.05307071655988693, acc: 0.93, recall: 0.9299999999999999, precision: 0.930172068827531, f_beta: 0.9299929992999301
train: step: 1836, loss: 0.0838422179222107, acc: 0.92, recall: 0.9199999999999999, precision: 0.9227053140096618, f_beta: 0.9198717948717948
train: step: 1837, loss: 0.042938232421875, acc: 0.95, recall: 0.95, precision: 0.9516258530710557, f_beta: 0.949954959463517
train: step: 1838, loss: 0.05765926465392113, acc: 0.93, recall: 0.9299999999999999, precision: 0.9385964912280702, f_beta: 0.9296553110240177
train: step: 1839, loss: 0.06410935521125793, acc: 0.9, recall: 0.9, precision: 0.9006410256410255, f_beta: 0.8999599839935974
train: step: 1840, loss: 0.07761868834495544, acc: 0.9, recall: 0.8999999999999999, precision: 0.9058441558441559, f_beta: 0.8996386993175431
train: step: 1841, loss: 0.034954071044921875, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 1842, loss: 0.04083056375384331, acc: 0.96, recall: 0.96, precision: 0.96, f_beta: 0.96
train: step: 1843, loss: 0.02563750557601452, acc: 0.96, recall: 0.96, precision: 0.9607371794871795, f_beta: 0.959983993597439
train: step: 1844, loss: 0.028994498774409294, acc: 0.97, recall: 0.97, precision: 0.9716981132075472, f_beta: 0.9699729756781104
train: step: 1845, loss: 0.04617270827293396, acc: 0.95, recall: 0.95, precision: 0.9501800720288115, f_beta: 0.9499949994999499
train: step: 1846, loss: 0.04041554778814316, acc: 0.96, recall: 0.96, precision: 0.962962962962963, f_beta: 0.9599358974358974
train: step: 1847, loss: 0.09224874526262283, acc: 0.87, recall: 0.87, precision: 0.8701480592236894, f_beta: 0.8699869986998701
train: step: 1848, loss: 0.09348836541175842, acc: 0.89, recall: 0.8899999999999999, precision: 0.8977968176254589, f_beta: 0.8894583458948848
train: step: 1849, loss: 0.04550715908408165, acc: 0.94, recall: 0.94, precision: 0.9428341384863124, f_beta: 0.9399038461538463
train: step: 1850, loss: 0.03181526064872742, acc: 0.96, recall: 0.96, precision: 0.9607371794871795, f_beta: 0.959983993597439
train: step: 1851, loss: 0.07220396399497986, acc: 0.91, recall: 0.9099999999999999, precision: 0.9114813327980731, f_beta: 0.9099189270343309
train: step: 1852, loss: 0.047915201634168625, acc: 0.93, recall: 0.9299999999999999, precision: 0.9343434343434343, f_beta: 0.9298245614035088
train: step: 1853, loss: 0.020996201783418655, acc: 0.98, recall: 0.98, precision: 0.9807692307692308, f_beta: 0.9799919967987194
train: step: 1854, loss: 0.05024155601859093, acc: 0.94, recall: 0.94, precision: 0.9407051282051282, f_beta: 0.9399759903961584
train: step: 1855, loss: 0.031054005026817322, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 1856, loss: 0.04411672428250313, acc: 0.95, recall: 0.95, precision: 0.9501800720288115, f_beta: 0.9499949994999499
train: step: 1857, loss: 0.04612582176923752, acc: 0.94, recall: 0.94, precision: 0.94, f_beta: 0.94
train: step: 1858, loss: 0.06647124141454697, acc: 0.9, recall: 0.9, precision: 0.9006410256410255, f_beta: 0.8999599839935974
train: step: 1859, loss: 0.02648846060037613, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 1860, loss: 0.03711613267660141, acc: 0.96, recall: 0.96, precision: 0.9607371794871795, f_beta: 0.959983993597439
train: step: 1861, loss: 0.1132625862956047, acc: 0.86, recall: 0.86, precision: 0.8605769230769231, f_beta: 0.8599439775910365
train: step: 1862, loss: 0.06821440160274506, acc: 0.9, recall: 0.9, precision: 0.9006410256410255, f_beta: 0.8999599839935974
train: step: 1863, loss: 0.04607991874217987, acc: 0.95, recall: 0.95, precision: 0.9501800720288115, f_beta: 0.9499949994999499
train: step: 1864, loss: 0.059524402022361755, acc: 0.92, recall: 0.9199999999999999, precision: 0.9227053140096618, f_beta: 0.9198717948717948
train: step: 1865, loss: 0.04728342220187187, acc: 0.93, recall: 0.9299999999999999, precision: 0.9343434343434343, f_beta: 0.9298245614035088
train: step: 1866, loss: 0.05748728662729263, acc: 0.94, recall: 0.94, precision: 0.9464285714285714, f_beta: 0.939783219590526
train: step: 1867, loss: 0.06315944343805313, acc: 0.92, recall: 0.9199999999999999, precision: 0.9310344827586207, f_beta: 0.9194847020933978
train: step: 1868, loss: 0.064403235912323, acc: 0.91, recall: 0.9099999999999999, precision: 0.9114813327980731, f_beta: 0.9099189270343309
train: step: 1869, loss: 0.011252357624471188, acc: 0.98, recall: 0.98, precision: 0.98, f_beta: 0.98
train: step: 1870, loss: 0.026592502370476723, acc: 0.97, recall: 0.97, precision: 0.9716981132075472, f_beta: 0.9699729756781104
train: step: 1871, loss: 0.020388254895806313, acc: 0.98, recall: 0.98, precision: 0.98, f_beta: 0.98
train: step: 1872, loss: 0.057209666818380356, acc: 0.93, recall: 0.9299999999999999, precision: 0.930172068827531, f_beta: 0.9299929992999301
train: step: 1873, loss: 0.022988634184002876, acc: 0.98, recall: 0.98, precision: 0.9807692307692308, f_beta: 0.9799919967987194
train: step: 1874, loss: 0.03375392407178879, acc: 0.96, recall: 0.96, precision: 0.96, f_beta: 0.96
train: step: 1875, loss: 0.061093129217624664, acc: 0.94, recall: 0.94, precision: 0.9464285714285714, f_beta: 0.939783219590526
train: step: 1876, loss: 0.023087941110134125, acc: 0.98, recall: 0.98, precision: 0.9807692307692308, f_beta: 0.9799919967987194
train: step: 1877, loss: 0.03758413344621658, acc: 0.95, recall: 0.95, precision: 0.9545454545454546, f_beta: 0.949874686716792
train: step: 1878, loss: 0.06425359100103378, acc: 0.92, recall: 0.9199999999999999, precision: 0.9261363636363636, f_beta: 0.9197109594540345
train: step: 1879, loss: 0.02379581518471241, acc: 0.98, recall: 0.98, precision: 0.98, f_beta: 0.98
train: step: 1880, loss: 0.07356522232294083, acc: 0.91, recall: 0.9099999999999999, precision: 0.9114813327980731, f_beta: 0.9099189270343309
train: step: 1881, loss: 0.036449506878852844, acc: 0.96, recall: 0.96, precision: 0.9607371794871795, f_beta: 0.959983993597439
train: step: 1882, loss: 0.04008208215236664, acc: 0.95, recall: 0.95, precision: 0.9545454545454546, f_beta: 0.949874686716792
train: step: 1883, loss: 0.06773141771554947, acc: 0.91, recall: 0.9099999999999999, precision: 0.9141414141414141, f_beta: 0.9097744360902256
train: step: 1884, loss: 0.08052653074264526, acc: 0.9, recall: 0.8999999999999999, precision: 0.9025764895330113, f_beta: 0.8998397435897436
train: step: 1885, loss: 0.09393173456192017, acc: 0.88, recall: 0.88, precision: 0.88, f_beta: 0.88
train: step: 1886, loss: 0.028435859829187393, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 1887, loss: 0.04256194084882736, acc: 0.95, recall: 0.95, precision: 0.9501800720288115, f_beta: 0.9499949994999499
train: step: 1888, loss: 0.07450895011425018, acc: 0.89, recall: 0.89, precision: 0.8914090726615818, f_beta: 0.8899009108197378
train: step: 1889, loss: 0.04905590042471886, acc: 0.95, recall: 0.95, precision: 0.9501800720288115, f_beta: 0.9499949994999499
train: step: 1890, loss: 0.0705978199839592, acc: 0.9, recall: 0.9, precision: 0.9006410256410255, f_beta: 0.8999599839935974
train: step: 1891, loss: 0.029319582507014275, acc: 0.97, recall: 0.97, precision: 0.9716981132075472, f_beta: 0.9699729756781104
train: step: 1892, loss: 0.032921381294727325, acc: 0.95, recall: 0.95, precision: 0.9516258530710557, f_beta: 0.949954959463517
train: step: 1893, loss: 0.0350428968667984, acc: 0.96, recall: 0.96, precision: 0.9607371794871795, f_beta: 0.959983993597439
train: step: 1894, loss: 0.052509356290102005, acc: 0.93, recall: 0.9299999999999999, precision: 0.930172068827531, f_beta: 0.9299929992999301
train: step: 1895, loss: 0.00973724015057087, acc: 0.99, recall: 0.99, precision: 0.9901960784313726, f_beta: 0.98999899989999
train: step: 1896, loss: 0.00996603537350893, acc: 0.99, recall: 0.99, precision: 0.9901960784313726, f_beta: 0.98999899989999
train: step: 1897, loss: 0.03877680003643036, acc: 0.96, recall: 0.96, precision: 0.96, f_beta: 0.96
train: step: 1898, loss: 0.04010287672281265, acc: 0.95, recall: 0.95, precision: 0.9501800720288115, f_beta: 0.9499949994999499
train: step: 1899, loss: 0.05745670199394226, acc: 0.92, recall: 0.92, precision: 0.92, f_beta: 0.92
train: step: 1900, loss: 0.07543111592531204, acc: 0.89, recall: 0.89, precision: 0.8914090726615818, f_beta: 0.8899009108197378
train: step: 1901, loss: 0.025318671017885208, acc: 0.96, recall: 0.96, precision: 0.96, f_beta: 0.96
train: step: 1902, loss: 0.030774330720305443, acc: 0.98, recall: 0.98, precision: 0.9807692307692308, f_beta: 0.9799919967987194
train: step: 1903, loss: 0.03633608669042587, acc: 0.96, recall: 0.96, precision: 0.9607371794871795, f_beta: 0.959983993597439
train: step: 1904, loss: 0.02974356897175312, acc: 0.96, recall: 0.96, precision: 0.9607371794871795, f_beta: 0.959983993597439
train: step: 1905, loss: 0.05439716950058937, acc: 0.95, recall: 0.95, precision: 0.9516258530710557, f_beta: 0.949954959463517
train: step: 1906, loss: 0.049019813537597656, acc: 0.95, recall: 0.95, precision: 0.9516258530710557, f_beta: 0.949954959463517
train: step: 1907, loss: 0.06084863096475601, acc: 0.9, recall: 0.9, precision: 0.9006410256410255, f_beta: 0.8999599839935974
train: step: 1908, loss: 0.04570736736059189, acc: 0.94, recall: 0.94, precision: 0.94, f_beta: 0.94
train: step: 1909, loss: 0.07092757523059845, acc: 0.92, recall: 0.9199999999999999, precision: 0.9227053140096618, f_beta: 0.9198717948717948
train: step: 1910, loss: 0.060763608664274216, acc: 0.92, recall: 0.9199999999999999, precision: 0.9206730769230769, f_beta: 0.919967987194878
train: step: 1911, loss: 0.03773341700434685, acc: 0.96, recall: 0.96, precision: 0.9607371794871795, f_beta: 0.959983993597439
train: step: 1912, loss: 0.06795543432235718, acc: 0.92, recall: 0.9199999999999999, precision: 0.9206730769230769, f_beta: 0.919967987194878
train: step: 1913, loss: 0.03209884464740753, acc: 0.96, recall: 0.96, precision: 0.9607371794871795, f_beta: 0.959983993597439
train: step: 1914, loss: 0.034083615988492966, acc: 0.96, recall: 0.96, precision: 0.962962962962963, f_beta: 0.9599358974358974
train: step: 1915, loss: 0.06129820644855499, acc: 0.92, recall: 0.9199999999999999, precision: 0.9206730769230769, f_beta: 0.919967987194878
train: step: 1916, loss: 0.06719228625297546, acc: 0.92, recall: 0.9199999999999999, precision: 0.9206730769230769, f_beta: 0.919967987194878
train: step: 1917, loss: 0.06826900690793991, acc: 0.91, recall: 0.91, precision: 0.9101640656262505, f_beta: 0.90999099909991
train: step: 1918, loss: 0.09166166186332703, acc: 0.87, recall: 0.87, precision: 0.8701480592236894, f_beta: 0.8699869986998701
train: step: 1919, loss: 0.07199747860431671, acc: 0.92, recall: 0.9199999999999999, precision: 0.9206730769230769, f_beta: 0.919967987194878
train: step: 1920, loss: 0.031043173745274544, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 1921, loss: 0.05886527895927429, acc: 0.92, recall: 0.9199999999999999, precision: 0.9227053140096618, f_beta: 0.9198717948717948
train: step: 1922, loss: 0.06055659428238869, acc: 0.93, recall: 0.9299999999999999, precision: 0.9315535929345644, f_beta: 0.9299369432489241
train: step: 1923, loss: 0.06579436361789703, acc: 0.92, recall: 0.92, precision: 0.92, f_beta: 0.92
train: step: 1924, loss: 0.06727734208106995, acc: 0.9, recall: 0.9, precision: 0.9, f_beta: 0.9
train: step: 1925, loss: 0.05125325173139572, acc: 0.93, recall: 0.9299999999999999, precision: 0.9315535929345644, f_beta: 0.9299369432489241
train: step: 1926, loss: 0.018418999388813972, acc: 0.99, recall: 0.99, precision: 0.9901960784313726, f_beta: 0.98999899989999
train: step: 1927, loss: 0.02884267270565033, acc: 0.96, recall: 0.96, precision: 0.962962962962963, f_beta: 0.9599358974358974
train: step: 1928, loss: 0.06397507339715958, acc: 0.92, recall: 0.92, precision: 0.92, f_beta: 0.92
train: step: 1929, loss: 0.05691085383296013, acc: 0.94, recall: 0.94, precision: 0.94, f_beta: 0.94
train: step: 1930, loss: 0.02574748545885086, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 1931, loss: 0.08554217964410782, acc: 0.91, recall: 0.9099999999999999, precision: 0.9114813327980731, f_beta: 0.9099189270343309
train: step: 1932, loss: 0.05437434837222099, acc: 0.93, recall: 0.9299999999999999, precision: 0.930172068827531, f_beta: 0.9299929992999301
train: step: 1933, loss: 0.0338035449385643, acc: 0.96, recall: 0.96, precision: 0.9607371794871795, f_beta: 0.959983993597439
train: step: 1934, loss: 0.02407095953822136, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 1935, loss: 0.054643720388412476, acc: 0.93, recall: 0.9299999999999999, precision: 0.930172068827531, f_beta: 0.9299929992999301
train: step: 1936, loss: 0.07408006489276886, acc: 0.9, recall: 0.8999999999999999, precision: 0.9105090311986863, f_beta: 0.8993558776167472
train: step: 1937, loss: 0.04115164279937744, acc: 0.94, recall: 0.94, precision: 0.9428341384863124, f_beta: 0.9399038461538463
train: step: 1938, loss: 0.09383146464824677, acc: 0.89, recall: 0.89, precision: 0.89015606242497, f_beta: 0.88998899889989
train: step: 1939, loss: 0.04855107143521309, acc: 0.93, recall: 0.9299999999999999, precision: 0.9315535929345644, f_beta: 0.9299369432489241
train: step: 1940, loss: 0.008110364899039268, acc: 0.99, recall: 0.99, precision: 0.9901960784313726, f_beta: 0.98999899989999
train: step: 1941, loss: 0.052797336131334305, acc: 0.93, recall: 0.9299999999999999, precision: 0.9315535929345644, f_beta: 0.9299369432489241
train: step: 1942, loss: 0.05567086115479469, acc: 0.94, recall: 0.94, precision: 0.94, f_beta: 0.94
train: step: 1943, loss: 0.07713974267244339, acc: 0.91, recall: 0.9099999999999999, precision: 0.923728813559322, f_beta: 0.9092650468797258
train: step: 1944, loss: 0.04385588318109512, acc: 0.95, recall: 0.95, precision: 0.9516258530710557, f_beta: 0.949954959463517
train: step: 1945, loss: 0.027800900861620903, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 1946, loss: 0.054545510560274124, acc: 0.94, recall: 0.94, precision: 0.9428341384863124, f_beta: 0.9399038461538463
train: step: 1947, loss: 0.08026579767465591, acc: 0.91, recall: 0.9099999999999999, precision: 0.9114813327980731, f_beta: 0.9099189270343309
train: step: 1948, loss: 0.037292808294296265, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 1949, loss: 0.02433626726269722, acc: 0.96, recall: 0.96, precision: 0.9607371794871795, f_beta: 0.959983993597439
train: step: 1950, loss: 0.021730313077569008, acc: 0.97, recall: 0.97, precision: 0.9716981132075472, f_beta: 0.9699729756781104
train: step: 1951, loss: 0.0677311047911644, acc: 0.92, recall: 0.9199999999999999, precision: 0.9261363636363636, f_beta: 0.9197109594540345
train: step: 1952, loss: 0.0566258542239666, acc: 0.92, recall: 0.9199999999999999, precision: 0.9206730769230769, f_beta: 0.919967987194878
train: step: 1953, loss: 0.06843771785497665, acc: 0.93, recall: 0.9299999999999999, precision: 0.930172068827531, f_beta: 0.9299929992999301
train: step: 1954, loss: 0.08157861977815628, acc: 0.9, recall: 0.8999999999999999, precision: 0.9025764895330113, f_beta: 0.8998397435897436
train: step: 1955, loss: 0.07374253123998642, acc: 0.91, recall: 0.9099999999999999, precision: 0.9114813327980731, f_beta: 0.9099189270343309
train: step: 1956, loss: 0.05188000574707985, acc: 0.94, recall: 0.94, precision: 0.9407051282051282, f_beta: 0.9399759903961584
train: step: 1957, loss: 0.06921549141407013, acc: 0.9, recall: 0.9, precision: 0.9, f_beta: 0.9
train: step: 1958, loss: 0.018209584057331085, acc: 0.99, recall: 0.99, precision: 0.9901960784313726, f_beta: 0.98999899989999
train: step: 1959, loss: 0.03500412032008171, acc: 0.95, recall: 0.95, precision: 0.9516258530710557, f_beta: 0.949954959463517
train: step: 1960, loss: 0.055287789553403854, acc: 0.92, recall: 0.92, precision: 0.92, f_beta: 0.92
train: step: 1961, loss: 0.09287633746862411, acc: 0.89, recall: 0.89, precision: 0.89015606242497, f_beta: 0.88998899889989
train: step: 1962, loss: 0.05829741805791855, acc: 0.93, recall: 0.9299999999999999, precision: 0.930172068827531, f_beta: 0.9299929992999301
train: step: 1963, loss: 0.039214421063661575, acc: 0.95, recall: 0.95, precision: 0.9516258530710557, f_beta: 0.949954959463517
train: step: 1964, loss: 0.06742361932992935, acc: 0.92, recall: 0.9199999999999999, precision: 0.9261363636363636, f_beta: 0.9197109594540345
train: step: 1965, loss: 0.08660755306482315, acc: 0.89, recall: 0.89, precision: 0.8914090726615818, f_beta: 0.8899009108197378
train: step: 1966, loss: 0.028376206755638123, acc: 0.97, recall: 0.97, precision: 0.9716981132075472, f_beta: 0.9699729756781104
train: step: 1967, loss: 0.10552564263343811, acc: 0.86, recall: 0.8600000000000001, precision: 0.8652597402597402, f_beta: 0.8594941790445605
train: step: 1968, loss: 0.07042013108730316, acc: 0.9, recall: 0.8999999999999999, precision: 0.9025764895330113, f_beta: 0.8998397435897436
train: step: 1969, loss: 0.024140339344739914, acc: 0.98, recall: 0.98, precision: 0.9807692307692308, f_beta: 0.9799919967987194
train: step: 1970, loss: 0.06555216759443283, acc: 0.92, recall: 0.9199999999999999, precision: 0.9227053140096618, f_beta: 0.9198717948717948
train: step: 1971, loss: 0.06033922731876373, acc: 0.92, recall: 0.9199999999999999, precision: 0.9206730769230769, f_beta: 0.919967987194878
train: step: 1972, loss: 0.05235360190272331, acc: 0.94, recall: 0.94, precision: 0.9428341384863124, f_beta: 0.9399038461538463
train: step: 1973, loss: 0.06041061505675316, acc: 0.93, recall: 0.9299999999999999, precision: 0.930172068827531, f_beta: 0.9299929992999301
train: step: 1974, loss: 0.09038332849740982, acc: 0.89, recall: 0.8899999999999999, precision: 0.8939393939393939, f_beta: 0.8897243107769424
train: step: 1975, loss: 0.06836219877004623, acc: 0.91, recall: 0.9099999999999999, precision: 0.9114813327980731, f_beta: 0.9099189270343309
train: step: 1976, loss: 0.07592464238405228, acc: 0.9, recall: 0.9, precision: 0.9, f_beta: 0.9
train: step: 1977, loss: 0.07217860221862793, acc: 0.91, recall: 0.91, precision: 0.9101640656262505, f_beta: 0.90999099909991
train: step: 1978, loss: 0.03635682165622711, acc: 0.95, recall: 0.95, precision: 0.9545454545454546, f_beta: 0.949874686716792
train: step: 1979, loss: 0.059547435492277145, acc: 0.92, recall: 0.9199999999999999, precision: 0.9310344827586207, f_beta: 0.9194847020933978
train: step: 1980, loss: 0.06078344210982323, acc: 0.92, recall: 0.92, precision: 0.92, f_beta: 0.92
train: step: 1981, loss: 0.047793351113796234, acc: 0.95, recall: 0.95, precision: 0.9545454545454546, f_beta: 0.949874686716792
train: step: 1982, loss: 0.0789768397808075, acc: 0.91, recall: 0.9099999999999999, precision: 0.9141414141414141, f_beta: 0.9097744360902256
train: step: 1983, loss: 0.06979288905858994, acc: 0.89, recall: 0.89, precision: 0.8914090726615818, f_beta: 0.8899009108197378
train: step: 1984, loss: 0.04970933869481087, acc: 0.94, recall: 0.94, precision: 0.94, f_beta: 0.94
train: step: 1985, loss: 0.04582536593079567, acc: 0.96, recall: 0.96, precision: 0.9607371794871795, f_beta: 0.959983993597439
train: step: 1986, loss: 0.05102751404047012, acc: 0.95, recall: 0.95, precision: 0.9516258530710557, f_beta: 0.949954959463517
train: step: 1987, loss: 0.03892135247588158, acc: 0.94, recall: 0.94, precision: 0.9428341384863124, f_beta: 0.9399038461538463
train: step: 1988, loss: 0.05209548398852348, acc: 0.94, recall: 0.94, precision: 0.9464285714285714, f_beta: 0.939783219590526
train: step: 1989, loss: 0.03896937519311905, acc: 0.96, recall: 0.96, precision: 0.96, f_beta: 0.96
train: step: 1990, loss: 0.07740024477243423, acc: 0.93, recall: 0.9299999999999999, precision: 0.9343434343434343, f_beta: 0.9298245614035088
train: step: 1991, loss: 0.03737025335431099, acc: 0.95, recall: 0.95, precision: 0.9516258530710557, f_beta: 0.949954959463517
train: step: 1992, loss: 0.03482753783464432, acc: 0.96, recall: 0.96, precision: 0.9607371794871795, f_beta: 0.959983993597439
train: step: 1993, loss: 0.04058826342225075, acc: 0.96, recall: 0.96, precision: 0.9607371794871795, f_beta: 0.959983993597439
train: step: 1994, loss: 0.050109073519706726, acc: 0.96, recall: 0.96, precision: 0.96, f_beta: 0.96
train: step: 1995, loss: 0.03167256340384483, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 1996, loss: 0.06433208286762238, acc: 0.91, recall: 0.9099999999999999, precision: 0.9114813327980731, f_beta: 0.9099189270343309
train: step: 1997, loss: 0.021568570286035538, acc: 0.97, recall: 0.97, precision: 0.9716981132075472, f_beta: 0.9699729756781104
train: step: 1998, loss: 0.004976388067007065, acc: 1.0, recall: 1.0, precision: 1.0, f_beta: 1.0
train: step: 1999, loss: 0.03631211817264557, acc: 0.94, recall: 0.94, precision: 0.9464285714285714, f_beta: 0.939783219590526
train: step: 2000, loss: 0.03651704266667366, acc: 0.95, recall: 0.95, precision: 0.9516258530710557, f_beta: 0.949954959463517
train: step: 2001, loss: 0.04871156066656113, acc: 0.95, recall: 0.95, precision: 0.9501800720288115, f_beta: 0.9499949994999499
train: step: 2002, loss: 0.047833215445280075, acc: 0.93, recall: 0.9299999999999999, precision: 0.930172068827531, f_beta: 0.9299929992999301
train: step: 2003, loss: 0.03179044649004936, acc: 0.96, recall: 0.96, precision: 0.9607371794871795, f_beta: 0.959983993597439
train: step: 2004, loss: 0.06340990960597992, acc: 0.91, recall: 0.91, precision: 0.9101640656262505, f_beta: 0.90999099909991
train: step: 2005, loss: 0.04715447500348091, acc: 0.94, recall: 0.94, precision: 0.9407051282051282, f_beta: 0.9399759903961584
train: step: 2006, loss: 0.028107900172472, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 2007, loss: 0.0844656378030777, acc: 0.89, recall: 0.89, precision: 0.89015606242497, f_beta: 0.88998899889989
train: step: 2008, loss: 0.023625021800398827, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 2009, loss: 0.025290265679359436, acc: 0.98, recall: 0.98, precision: 0.98, f_beta: 0.98
train: step: 2010, loss: 0.05807794630527496, acc: 0.93, recall: 0.9299999999999999, precision: 0.930172068827531, f_beta: 0.9299929992999301
train: step: 2011, loss: 0.02383509650826454, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 2012, loss: 0.07787885516881943, acc: 0.9, recall: 0.8999999999999999, precision: 0.9025764895330113, f_beta: 0.8998397435897436
train: step: 2013, loss: 0.023450331762433052, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 2014, loss: 0.028092145919799805, acc: 0.95, recall: 0.95, precision: 0.9516258530710557, f_beta: 0.949954959463517
train: step: 2015, loss: 0.045655976980924606, acc: 0.95, recall: 0.95, precision: 0.9501800720288115, f_beta: 0.9499949994999499
train: step: 2016, loss: 0.06612928211688995, acc: 0.92, recall: 0.9199999999999999, precision: 0.9206730769230769, f_beta: 0.919967987194878
train: step: 2017, loss: 0.05651520565152168, acc: 0.94, recall: 0.94, precision: 0.94, f_beta: 0.94
train: step: 2018, loss: 0.027046702802181244, acc: 0.97, recall: 0.97, precision: 0.9716981132075472, f_beta: 0.9699729756781104
train: step: 2019, loss: 0.05588272958993912, acc: 0.93, recall: 0.9299999999999999, precision: 0.9315535929345644, f_beta: 0.9299369432489241
train: step: 2020, loss: 0.04038180410861969, acc: 0.94, recall: 0.94, precision: 0.9407051282051282, f_beta: 0.9399759903961584
train: step: 2021, loss: 0.10459110885858536, acc: 0.85, recall: 0.85, precision: 0.8617197188921042, f_beta: 0.8487750781328762
train: step: 2022, loss: 0.05079708248376846, acc: 0.94, recall: 0.94, precision: 0.94, f_beta: 0.94
train: step: 2023, loss: 0.023327212780714035, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 2024, loss: 0.032817840576171875, acc: 0.96, recall: 0.96, precision: 0.9607371794871795, f_beta: 0.959983993597439
train: step: 2025, loss: 0.035622064024209976, acc: 0.96, recall: 0.96, precision: 0.9607371794871795, f_beta: 0.959983993597439
train: step: 2026, loss: 0.04928700998425484, acc: 0.94, recall: 0.94, precision: 0.9407051282051282, f_beta: 0.9399759903961584
train: step: 2027, loss: 0.06102975830435753, acc: 0.92, recall: 0.9199999999999999, precision: 0.9310344827586207, f_beta: 0.9194847020933978
train: step: 2028, loss: 0.05257464572787285, acc: 0.92, recall: 0.9199999999999999, precision: 0.9206730769230769, f_beta: 0.919967987194878
train: step: 2029, loss: 0.06601489335298538, acc: 0.92, recall: 0.9199999999999999, precision: 0.9261363636363636, f_beta: 0.9197109594540345
train: step: 2030, loss: 0.06620091199874878, acc: 0.91, recall: 0.9099999999999999, precision: 0.9114813327980731, f_beta: 0.9099189270343309
train: step: 2031, loss: 0.059196386486291885, acc: 0.93, recall: 0.9299999999999999, precision: 0.9315535929345644, f_beta: 0.9299369432489241
train: step: 2032, loss: 0.027683200314641, acc: 0.97, recall: 0.97, precision: 0.9716981132075472, f_beta: 0.9699729756781104
train: step: 2033, loss: 0.023622680455446243, acc: 0.97, recall: 0.97, precision: 0.9716981132075472, f_beta: 0.9699729756781104
train: step: 2034, loss: 0.07889177650213242, acc: 0.9, recall: 0.9, precision: 0.9006410256410255, f_beta: 0.8999599839935974
train: step: 2035, loss: 0.10425467789173126, acc: 0.86, recall: 0.86, precision: 0.8623188405797102, f_beta: 0.859775641025641
train: step: 2036, loss: 0.05282004177570343, acc: 0.92, recall: 0.9199999999999999, precision: 0.9227053140096618, f_beta: 0.9198717948717948
train: step: 2037, loss: 0.05156687647104263, acc: 0.94, recall: 0.94, precision: 0.9407051282051282, f_beta: 0.9399759903961584
train: step: 2038, loss: 0.05187094584107399, acc: 0.94, recall: 0.94, precision: 0.9407051282051282, f_beta: 0.9399759903961584
train: step: 2039, loss: 0.06097234785556793, acc: 0.94, recall: 0.94, precision: 0.9428341384863124, f_beta: 0.9399038461538463
train: step: 2040, loss: 0.03362234681844711, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 2041, loss: 0.0690915659070015, acc: 0.91, recall: 0.9099999999999999, precision: 0.9114813327980731, f_beta: 0.9099189270343309
train: step: 2042, loss: 0.05249987542629242, acc: 0.93, recall: 0.9299999999999999, precision: 0.930172068827531, f_beta: 0.9299929992999301
train: step: 2043, loss: 0.04473046213388443, acc: 0.95, recall: 0.95, precision: 0.9516258530710557, f_beta: 0.949954959463517
train: step: 2044, loss: 0.05269097164273262, acc: 0.93, recall: 0.9299999999999999, precision: 0.9343434343434343, f_beta: 0.9298245614035088
train: step: 2045, loss: 0.029259812086820602, acc: 0.96, recall: 0.96, precision: 0.9607371794871795, f_beta: 0.959983993597439
train: step: 2046, loss: 0.07070400565862656, acc: 0.91, recall: 0.9099999999999999, precision: 0.9181966544267646, f_beta: 0.9095568284594513
train: step: 2047, loss: 0.030155997723340988, acc: 0.96, recall: 0.96, precision: 0.9607371794871795, f_beta: 0.959983993597439
train: step: 2048, loss: 0.04289046302437782, acc: 0.96, recall: 0.96, precision: 0.962962962962963, f_beta: 0.9599358974358974
train: step: 2049, loss: 0.06796960532665253, acc: 0.92, recall: 0.9199999999999999, precision: 0.9261363636363636, f_beta: 0.9197109594540345
train: step: 2050, loss: 0.01899128220975399, acc: 0.98, recall: 0.98, precision: 0.9807692307692308, f_beta: 0.9799919967987194
train: step: 2051, loss: 0.04405033215880394, acc: 0.94, recall: 0.94, precision: 0.9428341384863124, f_beta: 0.9399038461538463
train: step: 2052, loss: 0.03995271772146225, acc: 0.95, recall: 0.95, precision: 0.9516258530710557, f_beta: 0.949954959463517
train: step: 2053, loss: 0.024359244853258133, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 2054, loss: 0.034989044070243835, acc: 0.95, recall: 0.95, precision: 0.9501800720288115, f_beta: 0.9499949994999499
train: step: 2055, loss: 0.048269763588905334, acc: 0.94, recall: 0.94, precision: 0.9428341384863124, f_beta: 0.9399038461538463
train: step: 2056, loss: 0.03776547685265541, acc: 0.94, recall: 0.94, precision: 0.9428341384863124, f_beta: 0.9399038461538463
train: step: 2057, loss: 0.04063601419329643, acc: 0.95, recall: 0.95, precision: 0.9501800720288115, f_beta: 0.9499949994999499
train: step: 2058, loss: 0.05123709514737129, acc: 0.94, recall: 0.94, precision: 0.9407051282051282, f_beta: 0.9399759903961584
train: step: 2059, loss: 0.01887456886470318, acc: 0.98, recall: 0.98, precision: 0.98, f_beta: 0.98
train: step: 2060, loss: 0.06389439851045609, acc: 0.93, recall: 0.9299999999999999, precision: 0.9315535929345644, f_beta: 0.9299369432489241
train: step: 2061, loss: 0.0817834883928299, acc: 0.9, recall: 0.9, precision: 0.9006410256410255, f_beta: 0.8999599839935974
train: step: 2062, loss: 0.042952872812747955, acc: 0.95, recall: 0.95, precision: 0.9516258530710557, f_beta: 0.949954959463517
train: step: 2063, loss: 0.03668886050581932, acc: 0.95, recall: 0.95, precision: 0.9516258530710557, f_beta: 0.949954959463517
train: step: 2064, loss: 0.062313783913850784, acc: 0.93, recall: 0.9299999999999999, precision: 0.930172068827531, f_beta: 0.9299929992999301
train: step: 2065, loss: 0.009384416043758392, acc: 0.99, recall: 0.99, precision: 0.9901960784313726, f_beta: 0.98999899989999
train: step: 2066, loss: 0.021671220660209656, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 2067, loss: 0.03456912189722061, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 2068, loss: 0.07813767343759537, acc: 0.91, recall: 0.91, precision: 0.9101640656262505, f_beta: 0.90999099909991
train: step: 2069, loss: 0.04392557591199875, acc: 0.95, recall: 0.95, precision: 0.9516258530710557, f_beta: 0.949954959463517
train: step: 2070, loss: 0.030515024438500404, acc: 0.96, recall: 0.96, precision: 0.9607371794871795, f_beta: 0.959983993597439
train: step: 2071, loss: 0.06877370178699493, acc: 0.89, recall: 0.89, precision: 0.8914090726615818, f_beta: 0.8899009108197378
train: step: 2072, loss: 0.043856263160705566, acc: 0.94, recall: 0.94, precision: 0.9428341384863124, f_beta: 0.9399038461538463
train: step: 2073, loss: 0.07920214533805847, acc: 0.92, recall: 0.9199999999999999, precision: 0.9206730769230769, f_beta: 0.919967987194878
train: step: 2074, loss: 0.07628824561834335, acc: 0.9, recall: 0.9, precision: 0.9, f_beta: 0.9
train: step: 2075, loss: 0.05819071829319, acc: 0.92, recall: 0.9199999999999999, precision: 0.9261363636363636, f_beta: 0.9197109594540345
train: step: 2076, loss: 0.012021026574075222, acc: 0.99, recall: 0.99, precision: 0.9901960784313726, f_beta: 0.98999899989999
train: step: 2077, loss: 0.06952030956745148, acc: 0.92, recall: 0.92, precision: 0.92, f_beta: 0.92
train: step: 2078, loss: 0.07159055024385452, acc: 0.91, recall: 0.9099999999999999, precision: 0.9181966544267646, f_beta: 0.9095568284594513
train: step: 2079, loss: 0.014031248167157173, acc: 0.98, recall: 0.98, precision: 0.98, f_beta: 0.98
train: step: 2080, loss: 0.06489113718271255, acc: 0.93, recall: 0.9299999999999999, precision: 0.930172068827531, f_beta: 0.9299929992999301
train: step: 2081, loss: 0.051918067038059235, acc: 0.93, recall: 0.9299999999999999, precision: 0.9315535929345644, f_beta: 0.9299369432489241
train: step: 2082, loss: 0.04885021224617958, acc: 0.94, recall: 0.94, precision: 0.9407051282051282, f_beta: 0.9399759903961584
train: step: 2083, loss: 0.06989637762308121, acc: 0.9, recall: 0.8999999999999999, precision: 0.9058441558441559, f_beta: 0.8996386993175431
train: step: 2084, loss: 0.05836467817425728, acc: 0.92, recall: 0.9199999999999999, precision: 0.9206730769230769, f_beta: 0.919967987194878
train: step: 2085, loss: 0.05950409919023514, acc: 0.91, recall: 0.91, precision: 0.9101640656262505, f_beta: 0.90999099909991
train: step: 2086, loss: 0.021031269803643227, acc: 0.96, recall: 0.96, precision: 0.9607371794871795, f_beta: 0.959983993597439
train: step: 2087, loss: 0.017793428152799606, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 2088, loss: 0.011460565030574799, acc: 0.99, recall: 0.99, precision: 0.9901960784313726, f_beta: 0.98999899989999
train: step: 2089, loss: 0.04704699665307999, acc: 0.95, recall: 0.95, precision: 0.9545454545454546, f_beta: 0.949874686716792
train: step: 2090, loss: 0.05274778977036476, acc: 0.92, recall: 0.9199999999999999, precision: 0.9206730769230769, f_beta: 0.919967987194878
train: step: 2091, loss: 0.03955823555588722, acc: 0.96, recall: 0.96, precision: 0.962962962962963, f_beta: 0.9599358974358974
train: step: 2092, loss: 0.04375714436173439, acc: 0.96, recall: 0.96, precision: 0.9607371794871795, f_beta: 0.959983993597439
train: step: 2093, loss: 0.05817826837301254, acc: 0.94, recall: 0.94, precision: 0.9428341384863124, f_beta: 0.9399038461538463
train: step: 2094, loss: 0.0638083964586258, acc: 0.91, recall: 0.9099999999999999, precision: 0.9141414141414141, f_beta: 0.9097744360902256
train: step: 2095, loss: 0.013854878023266792, acc: 0.98, recall: 0.98, precision: 0.9807692307692308, f_beta: 0.9799919967987194
train: step: 2096, loss: 0.0892997682094574, acc: 0.9, recall: 0.9, precision: 0.9, f_beta: 0.9
train: step: 2097, loss: 0.08078933507204056, acc: 0.89, recall: 0.8899999999999999, precision: 0.8977968176254589, f_beta: 0.8894583458948848
train: step: 2098, loss: 0.023188380524516106, acc: 0.97, recall: 0.97, precision: 0.9716981132075472, f_beta: 0.9699729756781104
train: step: 2099, loss: 0.0901070386171341, acc: 0.87, recall: 0.87, precision: 0.8737373737373737, f_beta: 0.8696741854636592
train: step: 2100, loss: 0.03329981118440628, acc: 0.95, recall: 0.95, precision: 0.9501800720288115, f_beta: 0.9499949994999499
train: step: 2101, loss: 0.026746716350317, acc: 0.97, recall: 0.97, precision: 0.9716981132075472, f_beta: 0.9699729756781104
train: step: 2102, loss: 0.033544596284627914, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 2103, loss: 0.047232672572135925, acc: 0.92, recall: 0.9199999999999999, precision: 0.9227053140096618, f_beta: 0.9198717948717948
train: step: 2104, loss: 0.031112728640437126, acc: 0.95, recall: 0.95, precision: 0.9516258530710557, f_beta: 0.949954959463517
train: step: 2105, loss: 0.03142794221639633, acc: 0.97, recall: 0.97, precision: 0.9716981132075472, f_beta: 0.9699729756781104
train: step: 2106, loss: 0.05324668809771538, acc: 0.96, recall: 0.96, precision: 0.96, f_beta: 0.96
train: step: 2107, loss: 0.07694989442825317, acc: 0.91, recall: 0.91, precision: 0.9101640656262505, f_beta: 0.90999099909991
train: step: 2108, loss: 0.021761521697044373, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 2109, loss: 0.032559022307395935, acc: 0.96, recall: 0.96, precision: 0.9607371794871795, f_beta: 0.959983993597439
train: step: 2110, loss: 0.04648357257246971, acc: 0.96, recall: 0.96, precision: 0.9607371794871795, f_beta: 0.959983993597439
train: step: 2111, loss: 0.015077861957252026, acc: 0.99, recall: 0.99, precision: 0.9901960784313726, f_beta: 0.98999899989999
train: step: 2112, loss: 0.055840395390987396, acc: 0.94, recall: 0.94, precision: 0.9428341384863124, f_beta: 0.9399038461538463
train: step: 2113, loss: 0.05722276121377945, acc: 0.92, recall: 0.9199999999999999, precision: 0.9227053140096618, f_beta: 0.9198717948717948
train: step: 2114, loss: 0.034068480134010315, acc: 0.96, recall: 0.96, precision: 0.9607371794871795, f_beta: 0.959983993597439
train: step: 2115, loss: 0.08281700313091278, acc: 0.91, recall: 0.9099999999999999, precision: 0.9114813327980731, f_beta: 0.9099189270343309
train: step: 2116, loss: 0.07137610763311386, acc: 0.91, recall: 0.9099999999999999, precision: 0.9114813327980731, f_beta: 0.9099189270343309
train: step: 2117, loss: 0.01883774623274803, acc: 0.98, recall: 0.98, precision: 0.98, f_beta: 0.98
train: step: 2118, loss: 0.048353057354688644, acc: 0.94, recall: 0.94, precision: 0.9464285714285714, f_beta: 0.939783219590526
train: step: 2119, loss: 0.04254572093486786, acc: 0.95, recall: 0.95, precision: 0.9501800720288115, f_beta: 0.9499949994999499
train: step: 2120, loss: 0.06217060983181, acc: 0.93, recall: 0.9299999999999999, precision: 0.9315535929345644, f_beta: 0.9299369432489241
train: step: 2121, loss: 0.033263709396123886, acc: 0.95, recall: 0.95, precision: 0.9501800720288115, f_beta: 0.9499949994999499
train: step: 2122, loss: 0.041959505528211594, acc: 0.95, recall: 0.95, precision: 0.9545454545454546, f_beta: 0.949874686716792
train: step: 2123, loss: 0.03182964399456978, acc: 0.96, recall: 0.96, precision: 0.96, f_beta: 0.96
train: step: 2124, loss: 0.02928146906197071, acc: 0.95, recall: 0.95, precision: 0.9516258530710557, f_beta: 0.949954959463517
train: step: 2125, loss: 0.032965391874313354, acc: 0.97, recall: 0.97, precision: 0.9716981132075472, f_beta: 0.9699729756781104
train: step: 2126, loss: 0.03242606297135353, acc: 0.95, recall: 0.95, precision: 0.9516258530710557, f_beta: 0.949954959463517
train: step: 2127, loss: 0.018495285883545876, acc: 0.99, recall: 0.99, precision: 0.9901960784313726, f_beta: 0.98999899989999
train: step: 2128, loss: 0.027993569150567055, acc: 0.96, recall: 0.96, precision: 0.96, f_beta: 0.96
train: step: 2129, loss: 0.0421392098069191, acc: 0.95, recall: 0.95, precision: 0.9501800720288115, f_beta: 0.9499949994999499
train: step: 2130, loss: 0.04560593143105507, acc: 0.93, recall: 0.9299999999999999, precision: 0.930172068827531, f_beta: 0.9299929992999301
train: step: 2131, loss: 0.06292039901018143, acc: 0.93, recall: 0.9299999999999999, precision: 0.9315535929345644, f_beta: 0.9299369432489241
train: step: 2132, loss: 0.0784221813082695, acc: 0.91, recall: 0.9099999999999999, precision: 0.9141414141414141, f_beta: 0.9097744360902256
train: step: 2133, loss: 0.051823247224092484, acc: 0.94, recall: 0.94, precision: 0.9407051282051282, f_beta: 0.9399759903961584
train: step: 2134, loss: 0.049745287746191025, acc: 0.93, recall: 0.9299999999999999, precision: 0.9315535929345644, f_beta: 0.9299369432489241
train: step: 2135, loss: 0.022372202947735786, acc: 0.98, recall: 0.98, precision: 0.98, f_beta: 0.98
train: step: 2136, loss: 0.014520876109600067, acc: 0.99, recall: 0.99, precision: 0.9901960784313726, f_beta: 0.98999899989999
train: step: 2137, loss: 0.03572262451052666, acc: 0.95, recall: 0.95, precision: 0.9516258530710557, f_beta: 0.949954959463517
train: step: 2138, loss: 0.03433726355433464, acc: 0.94, recall: 0.94, precision: 0.9407051282051282, f_beta: 0.9399759903961584
train: step: 2139, loss: 0.07556348294019699, acc: 0.91, recall: 0.91, precision: 0.9101640656262505, f_beta: 0.90999099909991
train: step: 2140, loss: 0.060074333101511, acc: 0.92, recall: 0.9199999999999999, precision: 0.9206730769230769, f_beta: 0.919967987194878
train: step: 2141, loss: 0.03295455873012543, acc: 0.96, recall: 0.96, precision: 0.9607371794871795, f_beta: 0.959983993597439
train: step: 2142, loss: 0.053150177001953125, acc: 0.92, recall: 0.9199999999999999, precision: 0.9227053140096618, f_beta: 0.9198717948717948
train: step: 2143, loss: 0.04127294942736626, acc: 0.93, recall: 0.9299999999999999, precision: 0.9315535929345644, f_beta: 0.9299369432489241
train: step: 2144, loss: 0.03388486057519913, acc: 0.96, recall: 0.96, precision: 0.9607371794871795, f_beta: 0.959983993597439
train: step: 2145, loss: 0.029399670660495758, acc: 0.96, recall: 0.96, precision: 0.9607371794871795, f_beta: 0.959983993597439
train: step: 2146, loss: 0.059076614677906036, acc: 0.92, recall: 0.92, precision: 0.92, f_beta: 0.92
train: step: 2147, loss: 0.05289212614297867, acc: 0.93, recall: 0.9299999999999999, precision: 0.9315535929345644, f_beta: 0.9299369432489241
train: step: 2148, loss: 0.030027994886040688, acc: 0.95, recall: 0.95, precision: 0.9516258530710557, f_beta: 0.949954959463517
train: step: 2149, loss: 0.030470771715044975, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 2150, loss: 0.07004091143608093, acc: 0.91, recall: 0.91, precision: 0.9101640656262505, f_beta: 0.90999099909991
train: step: 2151, loss: 0.0499894805252552, acc: 0.94, recall: 0.94, precision: 0.9428341384863124, f_beta: 0.9399038461538463
train: step: 2152, loss: 0.03025618940591812, acc: 0.96, recall: 0.96, precision: 0.9607371794871795, f_beta: 0.959983993597439
train: step: 2153, loss: 0.017155343666672707, acc: 0.99, recall: 0.99, precision: 0.9901960784313726, f_beta: 0.98999899989999
train: step: 2154, loss: 0.06789448112249374, acc: 0.92, recall: 0.9199999999999999, precision: 0.9261363636363636, f_beta: 0.9197109594540345
train: step: 2155, loss: 0.013526246882975101, acc: 0.98, recall: 0.98, precision: 0.98, f_beta: 0.98
train: step: 2156, loss: 0.050724636763334274, acc: 0.93, recall: 0.9299999999999999, precision: 0.930172068827531, f_beta: 0.9299929992999301
train: step: 2157, loss: 0.047390710562467575, acc: 0.94, recall: 0.94, precision: 0.9428341384863124, f_beta: 0.9399038461538463
train: step: 2158, loss: 0.044839173555374146, acc: 0.94, recall: 0.94, precision: 0.9407051282051282, f_beta: 0.9399759903961584
train: step: 2159, loss: 0.038581617176532745, acc: 0.96, recall: 0.96, precision: 0.9607371794871795, f_beta: 0.959983993597439
train: step: 2160, loss: 0.05490344017744064, acc: 0.91, recall: 0.9099999999999999, precision: 0.9114813327980731, f_beta: 0.9099189270343309
train: step: 2161, loss: 0.02528141438961029, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 2162, loss: 0.05413118377327919, acc: 0.92, recall: 0.92, precision: 0.92, f_beta: 0.92
train: step: 2163, loss: 0.04994833841919899, acc: 0.94, recall: 0.94, precision: 0.9407051282051282, f_beta: 0.9399759903961584
train: step: 2164, loss: 0.05389415845274925, acc: 0.93, recall: 0.9299999999999999, precision: 0.9343434343434343, f_beta: 0.9298245614035088
train: step: 2165, loss: 0.039050325751304626, acc: 0.95, recall: 0.95, precision: 0.9516258530710557, f_beta: 0.949954959463517
train: step: 2166, loss: 0.036270130425691605, acc: 0.95, recall: 0.95, precision: 0.9501800720288115, f_beta: 0.9499949994999499
train: step: 2167, loss: 0.06975562870502472, acc: 0.92, recall: 0.92, precision: 0.92, f_beta: 0.92
train: step: 2168, loss: 0.022918445989489555, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 2169, loss: 0.017626509070396423, acc: 0.98, recall: 0.98, precision: 0.98, f_beta: 0.98
train: step: 2170, loss: 0.041598305106163025, acc: 0.95, recall: 0.95, precision: 0.9501800720288115, f_beta: 0.9499949994999499
train: step: 2171, loss: 0.028101177886128426, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 2172, loss: 0.031074969097971916, acc: 0.96, recall: 0.96, precision: 0.9607371794871795, f_beta: 0.959983993597439
train: step: 2173, loss: 0.06511475890874863, acc: 0.91, recall: 0.9099999999999999, precision: 0.9114813327980731, f_beta: 0.9099189270343309
train: step: 2174, loss: 0.03219950199127197, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 2175, loss: 0.04053368419408798, acc: 0.95, recall: 0.95, precision: 0.9501800720288115, f_beta: 0.9499949994999499
train: step: 2176, loss: 0.030134981498122215, acc: 0.95, recall: 0.95, precision: 0.9516258530710557, f_beta: 0.949954959463517
train: step: 2177, loss: 0.09015380591154099, acc: 0.88, recall: 0.88, precision: 0.8806089743589745, f_beta: 0.879951980792317
train: step: 2178, loss: 0.00862133502960205, acc: 0.99, recall: 0.99, precision: 0.9901960784313726, f_beta: 0.98999899989999
train: step: 2179, loss: 0.0461869053542614, acc: 0.94, recall: 0.94, precision: 0.94, f_beta: 0.94
train: step: 2180, loss: 0.04234485700726509, acc: 0.95, recall: 0.95, precision: 0.9516258530710557, f_beta: 0.949954959463517
train: step: 2181, loss: 0.08667243272066116, acc: 0.87, recall: 0.87, precision: 0.8773969808241534, f_beta: 0.8693598633303186
train: step: 2182, loss: 0.040296152234077454, acc: 0.95, recall: 0.95, precision: 0.9501800720288115, f_beta: 0.9499949994999499
train: step: 2183, loss: 0.06914763152599335, acc: 0.91, recall: 0.9099999999999999, precision: 0.9114813327980731, f_beta: 0.9099189270343309
train: step: 2184, loss: 0.07739180326461792, acc: 0.89, recall: 0.89, precision: 0.8914090726615818, f_beta: 0.8899009108197378
train: step: 2185, loss: 0.028886141255497932, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 2186, loss: 0.028713442385196686, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 2187, loss: 0.06600996851921082, acc: 0.94, recall: 0.94, precision: 0.9428341384863124, f_beta: 0.9399038461538463
train: step: 2188, loss: 0.050125427544116974, acc: 0.94, recall: 0.94, precision: 0.9407051282051282, f_beta: 0.9399759903961584
train: step: 2189, loss: 0.023407168686389923, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 2190, loss: 0.033857379108667374, acc: 0.94, recall: 0.94, precision: 0.9407051282051282, f_beta: 0.9399759903961584
train: step: 2191, loss: 0.07585830986499786, acc: 0.91, recall: 0.9099999999999999, precision: 0.9114813327980731, f_beta: 0.9099189270343309
train: step: 2192, loss: 0.04605637863278389, acc: 0.95, recall: 0.95, precision: 0.9516258530710557, f_beta: 0.949954959463517
train: step: 2193, loss: 0.0527358241379261, acc: 0.95, recall: 0.95, precision: 0.9516258530710557, f_beta: 0.949954959463517
train: step: 2194, loss: 0.047760412096977234, acc: 0.94, recall: 0.94, precision: 0.9407051282051282, f_beta: 0.9399759903961584
train: step: 2195, loss: 0.05582032352685928, acc: 0.93, recall: 0.9299999999999999, precision: 0.9315535929345644, f_beta: 0.9299369432489241
train: step: 2196, loss: 0.05290822684764862, acc: 0.93, recall: 0.9299999999999999, precision: 0.9343434343434343, f_beta: 0.9298245614035088
train: step: 2197, loss: 0.06067679449915886, acc: 0.91, recall: 0.9099999999999999, precision: 0.9114813327980731, f_beta: 0.9099189270343309
train: step: 2198, loss: 0.08734738081693649, acc: 0.89, recall: 0.8899999999999999, precision: 0.8939393939393939, f_beta: 0.8897243107769424
train: step: 2199, loss: 0.04298423230648041, acc: 0.95, recall: 0.95, precision: 0.9501800720288115, f_beta: 0.9499949994999499
train: step: 2200, loss: 0.019418656826019287, acc: 0.98, recall: 0.98, precision: 0.98, f_beta: 0.98
train: step: 2201, loss: 0.049204111099243164, acc: 0.94, recall: 0.94, precision: 0.9428341384863124, f_beta: 0.9399038461538463
train: step: 2202, loss: 0.03369416669011116, acc: 0.96, recall: 0.96, precision: 0.9607371794871795, f_beta: 0.959983993597439
train: step: 2203, loss: 0.013555342331528664, acc: 0.99, recall: 0.99, precision: 0.9901960784313726, f_beta: 0.98999899989999
train: step: 2204, loss: 0.021576251834630966, acc: 0.98, recall: 0.98, precision: 0.98, f_beta: 0.98
train: step: 2205, loss: 0.06593955308198929, acc: 0.92, recall: 0.9199999999999999, precision: 0.9261363636363636, f_beta: 0.9197109594540345
train: step: 2206, loss: 0.036219943314790726, acc: 0.96, recall: 0.96, precision: 0.9607371794871795, f_beta: 0.959983993597439
train: step: 2207, loss: 0.039759594947099686, acc: 0.95, recall: 0.95, precision: 0.9501800720288115, f_beta: 0.9499949994999499
train: step: 2208, loss: 0.04447504132986069, acc: 0.94, recall: 0.94, precision: 0.9407051282051282, f_beta: 0.9399759903961584
train: step: 2209, loss: 0.03402811661362648, acc: 0.96, recall: 0.96, precision: 0.9607371794871795, f_beta: 0.959983993597439
train: step: 2210, loss: 0.030253468081355095, acc: 0.95, recall: 0.95, precision: 0.9516258530710557, f_beta: 0.949954959463517
train: step: 2211, loss: 0.035523153841495514, acc: 0.94, recall: 0.94, precision: 0.94, f_beta: 0.94
train: step: 2212, loss: 0.09690175950527191, acc: 0.88, recall: 0.88, precision: 0.88, f_beta: 0.88
train: step: 2213, loss: 0.02001482993364334, acc: 0.98, recall: 0.98, precision: 0.98, f_beta: 0.98
train: step: 2214, loss: 0.060942307114601135, acc: 0.93, recall: 0.9299999999999999, precision: 0.9343434343434343, f_beta: 0.9298245614035088
train: step: 2215, loss: 0.06558871269226074, acc: 0.92, recall: 0.92, precision: 0.92, f_beta: 0.92
train: step: 2216, loss: 0.05347762256860733, acc: 0.94, recall: 0.94, precision: 0.94, f_beta: 0.94
train: step: 2217, loss: 0.061230048537254333, acc: 0.92, recall: 0.9199999999999999, precision: 0.9227053140096618, f_beta: 0.9198717948717948
train: step: 2218, loss: 0.04261002689599991, acc: 0.95, recall: 0.95, precision: 0.9501800720288115, f_beta: 0.9499949994999499
train: step: 2219, loss: 0.06415138393640518, acc: 0.93, recall: 0.9299999999999999, precision: 0.9385964912280702, f_beta: 0.9296553110240177
train: step: 2220, loss: 0.02438466250896454, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 2221, loss: 0.04042770341038704, acc: 0.96, recall: 0.96, precision: 0.9607371794871795, f_beta: 0.959983993597439
train: step: 2222, loss: 0.0743032842874527, acc: 0.92, recall: 0.9199999999999999, precision: 0.9227053140096618, f_beta: 0.9198717948717948
train: step: 2223, loss: 0.054160118103027344, acc: 0.94, recall: 0.94, precision: 0.9407051282051282, f_beta: 0.9399759903961584
train: step: 2224, loss: 0.041052866727113724, acc: 0.94, recall: 0.94, precision: 0.9407051282051282, f_beta: 0.9399759903961584
train: step: 2225, loss: 0.0875561535358429, acc: 0.89, recall: 0.8899999999999999, precision: 0.8939393939393939, f_beta: 0.8897243107769424
train: step: 2226, loss: 0.040202390402555466, acc: 0.94, recall: 0.94, precision: 0.94, f_beta: 0.94
train: step: 2227, loss: 0.0403844490647316, acc: 0.95, recall: 0.95, precision: 0.9501800720288115, f_beta: 0.9499949994999499
train: step: 2228, loss: 0.03624735772609711, acc: 0.95, recall: 0.95, precision: 0.9516258530710557, f_beta: 0.949954959463517
train: step: 2229, loss: 0.021598685532808304, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 2230, loss: 0.031366005539894104, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 2231, loss: 0.08004476875066757, acc: 0.89, recall: 0.89, precision: 0.8914090726615818, f_beta: 0.8899009108197378
train: step: 2232, loss: 0.051977477967739105, acc: 0.93, recall: 0.9299999999999999, precision: 0.9343434343434343, f_beta: 0.9298245614035088
train: step: 2233, loss: 0.0322154201567173, acc: 0.97, recall: 0.97, precision: 0.9716981132075472, f_beta: 0.9699729756781104
train: step: 2234, loss: 0.040954455733299255, acc: 0.94, recall: 0.94, precision: 0.9407051282051282, f_beta: 0.9399759903961584
train: step: 2235, loss: 0.06649546325206757, acc: 0.93, recall: 0.9299999999999999, precision: 0.9315535929345644, f_beta: 0.9299369432489241
train: step: 2236, loss: 0.050419993698596954, acc: 0.93, recall: 0.9299999999999999, precision: 0.9315535929345644, f_beta: 0.9299369432489241
train: step: 2237, loss: 0.05460092052817345, acc: 0.91, recall: 0.9099999999999999, precision: 0.9114813327980731, f_beta: 0.9099189270343309
train: step: 2238, loss: 0.020238399505615234, acc: 0.98, recall: 0.98, precision: 0.98, f_beta: 0.98
train: step: 2239, loss: 0.031682614237070084, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 2240, loss: 0.06961120665073395, acc: 0.92, recall: 0.92, precision: 0.92, f_beta: 0.92
train: step: 2241, loss: 0.04896397888660431, acc: 0.94, recall: 0.94, precision: 0.9407051282051282, f_beta: 0.9399759903961584
train: step: 2242, loss: 0.046483803540468216, acc: 0.94, recall: 0.94, precision: 0.9407051282051282, f_beta: 0.9399759903961584
train: step: 2243, loss: 0.06662488728761673, acc: 0.91, recall: 0.9099999999999999, precision: 0.9181966544267646, f_beta: 0.9095568284594513
train: step: 2244, loss: 0.02878611348569393, acc: 0.95, recall: 0.95, precision: 0.9501800720288115, f_beta: 0.9499949994999499
train: step: 2245, loss: 0.05367989093065262, acc: 0.92, recall: 0.9199999999999999, precision: 0.9261363636363636, f_beta: 0.9197109594540345
train: step: 2246, loss: 0.008145379833877087, acc: 0.99, recall: 0.99, precision: 0.9901960784313726, f_beta: 0.98999899989999
train: step: 2247, loss: 0.012196301482617855, acc: 0.99, recall: 0.99, precision: 0.9901960784313726, f_beta: 0.98999899989999
train: step: 2248, loss: 0.05628226324915886, acc: 0.93, recall: 0.9299999999999999, precision: 0.9315535929345644, f_beta: 0.9299369432489241
train: step: 2249, loss: 0.020994029939174652, acc: 0.97, recall: 0.97, precision: 0.9716981132075472, f_beta: 0.9699729756781104
train: step: 2250, loss: 0.031816378235816956, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 2251, loss: 0.044149771332740784, acc: 0.96, recall: 0.96, precision: 0.9607371794871795, f_beta: 0.959983993597439
train: step: 2252, loss: 0.06480371952056885, acc: 0.92, recall: 0.9199999999999999, precision: 0.9206730769230769, f_beta: 0.919967987194878
train: step: 2253, loss: 0.04794268682599068, acc: 0.94, recall: 0.94, precision: 0.9428341384863124, f_beta: 0.9399038461538463
train: step: 2254, loss: 0.02424122765660286, acc: 0.98, recall: 0.98, precision: 0.98, f_beta: 0.98
train: step: 2255, loss: 0.03766273334622383, acc: 0.95, recall: 0.95, precision: 0.9501800720288115, f_beta: 0.9499949994999499
train: step: 2256, loss: 0.05144599452614784, acc: 0.94, recall: 0.94, precision: 0.9428341384863124, f_beta: 0.9399038461538463
train: step: 2257, loss: 0.028293287381529808, acc: 0.96, recall: 0.96, precision: 0.962962962962963, f_beta: 0.9599358974358974
train: step: 2258, loss: 0.04260055720806122, acc: 0.95, recall: 0.95, precision: 0.9501800720288115, f_beta: 0.9499949994999499
train: step: 2259, loss: 0.045853063464164734, acc: 0.92, recall: 0.9199999999999999, precision: 0.9206730769230769, f_beta: 0.919967987194878
train: step: 2260, loss: 0.04466332495212555, acc: 0.94, recall: 0.94, precision: 0.9428341384863124, f_beta: 0.9399038461538463
train: step: 2261, loss: 0.018528705462813377, acc: 0.98, recall: 0.98, precision: 0.98, f_beta: 0.98
train: step: 2262, loss: 0.04275994375348091, acc: 0.95, recall: 0.95, precision: 0.9501800720288115, f_beta: 0.9499949994999499
train: step: 2263, loss: 0.04266275465488434, acc: 0.96, recall: 0.96, precision: 0.9607371794871795, f_beta: 0.959983993597439
train: step: 2264, loss: 0.03472784906625748, acc: 0.96, recall: 0.96, precision: 0.9607371794871795, f_beta: 0.959983993597439
train: step: 2265, loss: 0.025169482454657555, acc: 0.96, recall: 0.96, precision: 0.96, f_beta: 0.96
train: step: 2266, loss: 0.01055796816945076, acc: 1.0, recall: 1.0, precision: 1.0, f_beta: 1.0
train: step: 2267, loss: 0.06153251603245735, acc: 0.92, recall: 0.9199999999999999, precision: 0.9227053140096618, f_beta: 0.9198717948717948
train: step: 2268, loss: 0.03816007077693939, acc: 0.96, recall: 0.96, precision: 0.9607371794871795, f_beta: 0.959983993597439
train: step: 2269, loss: 0.060591474175453186, acc: 0.94, recall: 0.94, precision: 0.9407051282051282, f_beta: 0.9399759903961584
train: step: 2270, loss: 0.06259173899888992, acc: 0.93, recall: 0.9299999999999999, precision: 0.9315535929345644, f_beta: 0.9299369432489241
train: step: 2271, loss: 0.04047473520040512, acc: 0.95, recall: 0.95, precision: 0.9501800720288115, f_beta: 0.9499949994999499
train: step: 2272, loss: 0.017441248521208763, acc: 0.98, recall: 0.98, precision: 0.9807692307692308, f_beta: 0.9799919967987194
train: step: 2273, loss: 0.02917267382144928, acc: 0.96, recall: 0.96, precision: 0.9607371794871795, f_beta: 0.959983993597439
train: step: 2274, loss: 0.05127435177564621, acc: 0.95, recall: 0.95, precision: 0.9501800720288115, f_beta: 0.9499949994999499
train: step: 2275, loss: 0.008858763612806797, acc: 0.99, recall: 0.99, precision: 0.9901960784313726, f_beta: 0.98999899989999
train: step: 2276, loss: 0.056161995977163315, acc: 0.93, recall: 0.9299999999999999, precision: 0.930172068827531, f_beta: 0.9299929992999301
train: step: 2277, loss: 0.02256098762154579, acc: 0.98, recall: 0.98, precision: 0.98, f_beta: 0.98
train: step: 2278, loss: 0.05204840749502182, acc: 0.94, recall: 0.94, precision: 0.9407051282051282, f_beta: 0.9399759903961584
train: step: 2279, loss: 0.05967753380537033, acc: 0.92, recall: 0.9199999999999999, precision: 0.9227053140096618, f_beta: 0.9198717948717948
train: step: 2280, loss: 0.06233025714755058, acc: 0.91, recall: 0.9099999999999999, precision: 0.9114813327980731, f_beta: 0.9099189270343309
train: step: 2281, loss: 0.049986597150564194, acc: 0.95, recall: 0.95, precision: 0.9516258530710557, f_beta: 0.949954959463517
train: step: 2282, loss: 0.0675874873995781, acc: 0.91, recall: 0.91, precision: 0.9101640656262505, f_beta: 0.90999099909991
train: step: 2283, loss: 0.018419761210680008, acc: 0.98, recall: 0.98, precision: 0.9807692307692308, f_beta: 0.9799919967987194
train: step: 2284, loss: 0.03895359858870506, acc: 0.94, recall: 0.94, precision: 0.94, f_beta: 0.94
train: step: 2285, loss: 0.02214333601295948, acc: 0.98, recall: 0.98, precision: 0.98, f_beta: 0.98
train: step: 2286, loss: 0.034370653331279755, acc: 0.95, recall: 0.95, precision: 0.9501800720288115, f_beta: 0.9499949994999499
train: step: 2287, loss: 0.05326585844159126, acc: 0.93, recall: 0.9299999999999999, precision: 0.930172068827531, f_beta: 0.9299929992999301
train: step: 2288, loss: 0.06376541405916214, acc: 0.92, recall: 0.92, precision: 0.92, f_beta: 0.92
train: step: 2289, loss: 0.044446468353271484, acc: 0.96, recall: 0.96, precision: 0.96, f_beta: 0.96
train: step: 2290, loss: 0.04838861525058746, acc: 0.93, recall: 0.9299999999999999, precision: 0.930172068827531, f_beta: 0.9299929992999301
train: step: 2291, loss: 0.042541395872831345, acc: 0.95, recall: 0.95, precision: 0.9516258530710557, f_beta: 0.949954959463517
train: step: 2292, loss: 0.03705120459198952, acc: 0.95, recall: 0.95, precision: 0.9501800720288115, f_beta: 0.9499949994999499
train: step: 2293, loss: 0.0388282909989357, acc: 0.96, recall: 0.96, precision: 0.96, f_beta: 0.96
train: step: 2294, loss: 0.024767892435193062, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 2295, loss: 0.04999575763940811, acc: 0.94, recall: 0.94, precision: 0.94, f_beta: 0.94
train: step: 2296, loss: 0.04334959387779236, acc: 0.96, recall: 0.96, precision: 0.96, f_beta: 0.96
train: step: 2297, loss: 0.03753889724612236, acc: 0.95, recall: 0.95, precision: 0.9501800720288115, f_beta: 0.9499949994999499
train: step: 2298, loss: 0.06154065951704979, acc: 0.91, recall: 0.9099999999999999, precision: 0.9181966544267646, f_beta: 0.9095568284594513
train: step: 2299, loss: 0.057507313787937164, acc: 0.94, recall: 0.94, precision: 0.94, f_beta: 0.94
train: step: 2300, loss: 0.03216082975268364, acc: 0.96, recall: 0.96, precision: 0.9607371794871795, f_beta: 0.959983993597439
train: step: 2301, loss: 0.05259416997432709, acc: 0.91, recall: 0.9099999999999999, precision: 0.9141414141414141, f_beta: 0.9097744360902256
train: step: 2302, loss: 0.06775947660207748, acc: 0.91, recall: 0.9099999999999999, precision: 0.9181966544267646, f_beta: 0.9095568284594513
train: step: 2303, loss: 0.024982351809740067, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 2304, loss: 0.06366963684558868, acc: 0.93, recall: 0.9299999999999999, precision: 0.9315535929345644, f_beta: 0.9299369432489241
train: step: 2305, loss: 0.047029267996549606, acc: 0.95, recall: 0.95, precision: 0.9501800720288115, f_beta: 0.9499949994999499
train: step: 2306, loss: 0.04051615670323372, acc: 0.96, recall: 0.96, precision: 0.9607371794871795, f_beta: 0.959983993597439
train: step: 2307, loss: 0.05487783998250961, acc: 0.93, recall: 0.9299999999999999, precision: 0.930172068827531, f_beta: 0.9299929992999301
train: step: 2308, loss: 0.04122816398739815, acc: 0.94, recall: 0.94, precision: 0.9464285714285714, f_beta: 0.939783219590526
train: step: 2309, loss: 0.009520766325294971, acc: 0.99, recall: 0.99, precision: 0.9901960784313726, f_beta: 0.98999899989999
train: step: 2310, loss: 0.08493010699748993, acc: 0.88, recall: 0.88, precision: 0.88, f_beta: 0.88
train: step: 2311, loss: 0.052082132548093796, acc: 0.93, recall: 0.9299999999999999, precision: 0.9315535929345644, f_beta: 0.9299369432489241
train: step: 2312, loss: 0.021602049469947815, acc: 0.96, recall: 0.96, precision: 0.96, f_beta: 0.96
train: step: 2313, loss: 0.07427337020635605, acc: 0.93, recall: 0.9299999999999999, precision: 0.9315535929345644, f_beta: 0.9299369432489241
train: step: 2314, loss: 0.04172712191939354, acc: 0.95, recall: 0.95, precision: 0.9501800720288115, f_beta: 0.9499949994999499
train: step: 2315, loss: 0.07184123247861862, acc: 0.92, recall: 0.92, precision: 0.92, f_beta: 0.92
train: step: 2316, loss: 0.030409179627895355, acc: 0.96, recall: 0.96, precision: 0.9607371794871795, f_beta: 0.959983993597439
train: step: 2317, loss: 0.04036075621843338, acc: 0.95, recall: 0.95, precision: 0.9501800720288115, f_beta: 0.9499949994999499
train: step: 2318, loss: 0.04778769612312317, acc: 0.93, recall: 0.9299999999999999, precision: 0.930172068827531, f_beta: 0.9299929992999301
train: step: 2319, loss: 0.049185365438461304, acc: 0.94, recall: 0.94, precision: 0.9407051282051282, f_beta: 0.9399759903961584
train: step: 2320, loss: 0.03820795193314552, acc: 0.94, recall: 0.94, precision: 0.94, f_beta: 0.94
train: step: 2321, loss: 0.06766384840011597, acc: 0.92, recall: 0.9199999999999999, precision: 0.9261363636363636, f_beta: 0.9197109594540345
train: step: 2322, loss: 0.037608567625284195, acc: 0.96, recall: 0.96, precision: 0.96, f_beta: 0.96
train: step: 2323, loss: 0.03166145831346512, acc: 0.95, recall: 0.95, precision: 0.9516258530710557, f_beta: 0.949954959463517
train: step: 2324, loss: 0.08262667059898376, acc: 0.92, recall: 0.92, precision: 0.92, f_beta: 0.92
train: step: 2325, loss: 0.03356911987066269, acc: 0.96, recall: 0.96, precision: 0.9607371794871795, f_beta: 0.959983993597439
train: step: 2326, loss: 0.05155990645289421, acc: 0.93, recall: 0.9299999999999999, precision: 0.930172068827531, f_beta: 0.9299929992999301
train: step: 2327, loss: 0.0248117595911026, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 2328, loss: 0.06831290572881699, acc: 0.89, recall: 0.8899999999999999, precision: 0.8939393939393939, f_beta: 0.8897243107769424
train: step: 2329, loss: 0.07442810386419296, acc: 0.89, recall: 0.89, precision: 0.8914090726615818, f_beta: 0.8899009108197378
train: step: 2330, loss: 0.04593301936984062, acc: 0.95, recall: 0.95, precision: 0.9501800720288115, f_beta: 0.9499949994999499
train: step: 2331, loss: 0.04642164334654808, acc: 0.95, recall: 0.95, precision: 0.9516258530710557, f_beta: 0.949954959463517
train: step: 2332, loss: 0.03946719691157341, acc: 0.95, recall: 0.95, precision: 0.9501800720288115, f_beta: 0.9499949994999499
train: step: 2333, loss: 0.034021541476249695, acc: 0.95, recall: 0.95, precision: 0.9501800720288115, f_beta: 0.9499949994999499
train: step: 2334, loss: 0.048627518117427826, acc: 0.93, recall: 0.9299999999999999, precision: 0.930172068827531, f_beta: 0.9299929992999301
train: step: 2335, loss: 0.04104459285736084, acc: 0.93, recall: 0.9299999999999999, precision: 0.930172068827531, f_beta: 0.9299929992999301
train: step: 2336, loss: 0.027803057804703712, acc: 0.96, recall: 0.96, precision: 0.962962962962963, f_beta: 0.9599358974358974
train: step: 2337, loss: 0.025555090978741646, acc: 0.98, recall: 0.98, precision: 0.98, f_beta: 0.98
train: step: 2338, loss: 0.05371049791574478, acc: 0.94, recall: 0.94, precision: 0.9407051282051282, f_beta: 0.9399759903961584
train: step: 2339, loss: 0.054719939827919006, acc: 0.93, recall: 0.9299999999999999, precision: 0.930172068827531, f_beta: 0.9299929992999301
train: step: 2340, loss: 0.03091905638575554, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 2341, loss: 0.06874394416809082, acc: 0.93, recall: 0.9299999999999999, precision: 0.9315535929345644, f_beta: 0.9299369432489241
train: step: 2342, loss: 0.04251289367675781, acc: 0.95, recall: 0.95, precision: 0.9501800720288115, f_beta: 0.9499949994999499
train: step: 2343, loss: 0.01401373278349638, acc: 0.99, recall: 0.99, precision: 0.9901960784313726, f_beta: 0.98999899989999
train: step: 2344, loss: 0.04051193222403526, acc: 0.96, recall: 0.96, precision: 0.96, f_beta: 0.96
train: step: 2345, loss: 0.012865987606346607, acc: 0.99, recall: 0.99, precision: 0.9901960784313726, f_beta: 0.98999899989999
train: step: 2346, loss: 0.047274697571992874, acc: 0.94, recall: 0.94, precision: 0.9428341384863124, f_beta: 0.9399038461538463
train: step: 2347, loss: 0.0629708543419838, acc: 0.91, recall: 0.91, precision: 0.9101640656262505, f_beta: 0.90999099909991
train: step: 2348, loss: 0.010749232955276966, acc: 0.99, recall: 0.99, precision: 0.9901960784313726, f_beta: 0.98999899989999
train: step: 2349, loss: 0.06093653663992882, acc: 0.91, recall: 0.91, precision: 0.9101640656262505, f_beta: 0.90999099909991
train: step: 2350, loss: 0.05616661533713341, acc: 0.92, recall: 0.9199999999999999, precision: 0.9227053140096618, f_beta: 0.9198717948717948
train: step: 2351, loss: 0.08202119916677475, acc: 0.89, recall: 0.89, precision: 0.8914090726615818, f_beta: 0.8899009108197378
train: step: 2352, loss: 0.0296065304428339, acc: 0.96, recall: 0.96, precision: 0.9607371794871795, f_beta: 0.959983993597439
train: step: 2353, loss: 0.025780759751796722, acc: 0.96, recall: 0.96, precision: 0.962962962962963, f_beta: 0.9599358974358974
train: step: 2354, loss: 0.04592052102088928, acc: 0.94, recall: 0.94, precision: 0.9407051282051282, f_beta: 0.9399759903961584
train: step: 2355, loss: 0.031969986855983734, acc: 0.95, recall: 0.95, precision: 0.9501800720288115, f_beta: 0.9499949994999499
train: step: 2356, loss: 0.019795386120676994, acc: 0.98, recall: 0.98, precision: 0.9807692307692308, f_beta: 0.9799919967987194
train: step: 2357, loss: 0.06505505740642548, acc: 0.92, recall: 0.92, precision: 0.92, f_beta: 0.92
train: step: 2358, loss: 0.028527231886982918, acc: 0.96, recall: 0.96, precision: 0.9607371794871795, f_beta: 0.959983993597439
train: step: 2359, loss: 0.12680497765541077, acc: 0.84, recall: 0.84, precision: 0.8405448717948718, f_beta: 0.8399359743897559
train: step: 2360, loss: 0.04369937255978584, acc: 0.95, recall: 0.95, precision: 0.9501800720288115, f_beta: 0.9499949994999499
train: step: 2361, loss: 0.02251785434782505, acc: 0.96, recall: 0.96, precision: 0.9607371794871795, f_beta: 0.959983993597439
train: step: 2362, loss: 0.01077266689389944, acc: 0.99, recall: 0.99, precision: 0.9901960784313726, f_beta: 0.98999899989999
train: step: 2363, loss: 0.03483330458402634, acc: 0.94, recall: 0.94, precision: 0.9407051282051282, f_beta: 0.9399759903961584
train: step: 2364, loss: 0.043158043175935745, acc: 0.96, recall: 0.96, precision: 0.962962962962963, f_beta: 0.9599358974358974
train: step: 2365, loss: 0.042379483580589294, acc: 0.96, recall: 0.96, precision: 0.962962962962963, f_beta: 0.9599358974358974
train: step: 2366, loss: 0.04140976071357727, acc: 0.95, recall: 0.95, precision: 0.9501800720288115, f_beta: 0.9499949994999499
train: step: 2367, loss: 0.022730842232704163, acc: 0.97, recall: 0.97, precision: 0.9716981132075472, f_beta: 0.9699729756781104
train: step: 2368, loss: 0.026385610923171043, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 2369, loss: 0.049208518117666245, acc: 0.91, recall: 0.91, precision: 0.9101640656262505, f_beta: 0.90999099909991
train: step: 2370, loss: 0.04965446516871452, acc: 0.93, recall: 0.9299999999999999, precision: 0.930172068827531, f_beta: 0.9299929992999301
train: step: 2371, loss: 0.03761819750070572, acc: 0.95, recall: 0.95, precision: 0.9516258530710557, f_beta: 0.949954959463517
train: step: 2372, loss: 0.04672527313232422, acc: 0.93, recall: 0.9299999999999999, precision: 0.930172068827531, f_beta: 0.9299929992999301
train: step: 2373, loss: 0.07703721523284912, acc: 0.89, recall: 0.89, precision: 0.8914090726615818, f_beta: 0.8899009108197378
train: step: 2374, loss: 0.04506111145019531, acc: 0.92, recall: 0.9199999999999999, precision: 0.9261363636363636, f_beta: 0.9197109594540345
train: step: 2375, loss: 0.05944879353046417, acc: 0.93, recall: 0.9299999999999999, precision: 0.930172068827531, f_beta: 0.9299929992999301
train: step: 2376, loss: 0.07739372551441193, acc: 0.9, recall: 0.9, precision: 0.9006410256410255, f_beta: 0.8999599839935974
train: step: 2377, loss: 0.027400270104408264, acc: 0.97, recall: 0.97, precision: 0.9716981132075472, f_beta: 0.9699729756781104
train: step: 2378, loss: 0.041520338505506516, acc: 0.96, recall: 0.96, precision: 0.96, f_beta: 0.96
train: step: 2379, loss: 0.054215870797634125, acc: 0.92, recall: 0.9199999999999999, precision: 0.9206730769230769, f_beta: 0.919967987194878
train: step: 2380, loss: 0.0836268961429596, acc: 0.91, recall: 0.91, precision: 0.9101640656262505, f_beta: 0.90999099909991
train: step: 2381, loss: 0.06931077688932419, acc: 0.91, recall: 0.9099999999999999, precision: 0.9141414141414141, f_beta: 0.9097744360902256
train: step: 2382, loss: 0.05848363786935806, acc: 0.91, recall: 0.9099999999999999, precision: 0.9141414141414141, f_beta: 0.9097744360902256
train: step: 2383, loss: 0.02640124410390854, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 2384, loss: 0.02955283597111702, acc: 0.96, recall: 0.96, precision: 0.962962962962963, f_beta: 0.9599358974358974
train: step: 2385, loss: 0.06400725245475769, acc: 0.93, recall: 0.9299999999999999, precision: 0.930172068827531, f_beta: 0.9299929992999301
train: step: 2386, loss: 0.054435692727565765, acc: 0.95, recall: 0.95, precision: 0.9501800720288115, f_beta: 0.9499949994999499
train: step: 2387, loss: 0.0505223274230957, acc: 0.94, recall: 0.94, precision: 0.94, f_beta: 0.94
train: step: 2388, loss: 0.02641635574400425, acc: 0.96, recall: 0.96, precision: 0.962962962962963, f_beta: 0.9599358974358974
train: step: 2389, loss: 0.05979883670806885, acc: 0.92, recall: 0.9199999999999999, precision: 0.9227053140096618, f_beta: 0.9198717948717948
train: step: 2390, loss: 0.0537928007543087, acc: 0.93, recall: 0.9299999999999999, precision: 0.9315535929345644, f_beta: 0.9299369432489241
train: step: 2391, loss: 0.014786946587264538, acc: 0.98, recall: 0.98, precision: 0.9807692307692308, f_beta: 0.9799919967987194
train: step: 2392, loss: 0.04718269780278206, acc: 0.95, recall: 0.95, precision: 0.9501800720288115, f_beta: 0.9499949994999499
train: step: 2393, loss: 0.012791097164154053, acc: 0.99, recall: 0.99, precision: 0.9901960784313726, f_beta: 0.98999899989999
train: step: 2394, loss: 0.05259169638156891, acc: 0.93, recall: 0.9299999999999999, precision: 0.930172068827531, f_beta: 0.9299929992999301
train: step: 2395, loss: 0.012279453687369823, acc: 0.99, recall: 0.99, precision: 0.9901960784313726, f_beta: 0.98999899989999
train: step: 2396, loss: 0.03372878208756447, acc: 0.95, recall: 0.95, precision: 0.9516258530710557, f_beta: 0.949954959463517
train: step: 2397, loss: 0.04838648438453674, acc: 0.94, recall: 0.94, precision: 0.9407051282051282, f_beta: 0.9399759903961584
train: step: 2398, loss: 0.006243634968996048, acc: 0.99, recall: 0.99, precision: 0.9901960784313726, f_beta: 0.98999899989999
train: step: 2399, loss: 0.033604107797145844, acc: 0.96, recall: 0.96, precision: 0.962962962962963, f_beta: 0.9599358974358974
train: step: 2400, loss: 0.06083102151751518, acc: 0.92, recall: 0.9199999999999999, precision: 0.9261363636363636, f_beta: 0.9197109594540345
train: step: 2401, loss: 0.10441169142723083, acc: 0.85, recall: 0.85, precision: 0.8501400560224089, f_beta: 0.84998499849985
train: step: 2402, loss: 0.057611074298620224, acc: 0.92, recall: 0.92, precision: 0.92, f_beta: 0.92
train: step: 2403, loss: 0.04901380091905594, acc: 0.96, recall: 0.96, precision: 0.9607371794871795, f_beta: 0.959983993597439
train: step: 2404, loss: 0.0318976528942585, acc: 0.95, recall: 0.95, precision: 0.9516258530710557, f_beta: 0.949954959463517
train: step: 2405, loss: 0.035372115671634674, acc: 0.96, recall: 0.96, precision: 0.962962962962963, f_beta: 0.9599358974358974
train: step: 2406, loss: 0.05439884588122368, acc: 0.93, recall: 0.9299999999999999, precision: 0.9315535929345644, f_beta: 0.9299369432489241
train: step: 2407, loss: 0.032700229436159134, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 2408, loss: 0.044844795018434525, acc: 0.94, recall: 0.94, precision: 0.9464285714285714, f_beta: 0.939783219590526
train: step: 2409, loss: 0.052023906260728836, acc: 0.94, recall: 0.94, precision: 0.94, f_beta: 0.94
train: step: 2410, loss: 0.02831151708960533, acc: 0.96, recall: 0.96, precision: 0.96, f_beta: 0.96
train: step: 2411, loss: 0.028612280264496803, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 2412, loss: 0.030605027452111244, acc: 0.96, recall: 0.96, precision: 0.9607371794871795, f_beta: 0.959983993597439
train: step: 2413, loss: 0.06804531812667847, acc: 0.91, recall: 0.9099999999999999, precision: 0.9114813327980731, f_beta: 0.9099189270343309
train: step: 2414, loss: 0.0247653778642416, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 2415, loss: 0.04805988818407059, acc: 0.95, recall: 0.95, precision: 0.9516258530710557, f_beta: 0.949954959463517
train: step: 2416, loss: 0.05264013633131981, acc: 0.93, recall: 0.9299999999999999, precision: 0.930172068827531, f_beta: 0.9299929992999301
train: step: 2417, loss: 0.06766239553689957, acc: 0.92, recall: 0.9199999999999999, precision: 0.9206730769230769, f_beta: 0.919967987194878
train: step: 2418, loss: 0.02788456529378891, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 2419, loss: 0.01599690690636635, acc: 0.98, recall: 0.98, precision: 0.9807692307692308, f_beta: 0.9799919967987194
train: step: 2420, loss: 0.03881707414984703, acc: 0.96, recall: 0.96, precision: 0.96, f_beta: 0.96
train: step: 2421, loss: 0.027077417820692062, acc: 0.96, recall: 0.96, precision: 0.96, f_beta: 0.96
train: step: 2422, loss: 0.05045755207538605, acc: 0.92, recall: 0.9199999999999999, precision: 0.9227053140096618, f_beta: 0.9198717948717948
train: step: 2423, loss: 0.062006521970033646, acc: 0.93, recall: 0.9299999999999999, precision: 0.930172068827531, f_beta: 0.9299929992999301
train: step: 2424, loss: 0.0804838016629219, acc: 0.9, recall: 0.9, precision: 0.9006410256410255, f_beta: 0.8999599839935974
train: step: 2425, loss: 0.06230469048023224, acc: 0.92, recall: 0.9199999999999999, precision: 0.9206730769230769, f_beta: 0.919967987194878
train: step: 2426, loss: 0.07210975140333176, acc: 0.9, recall: 0.9, precision: 0.9006410256410255, f_beta: 0.8999599839935974
train: step: 2427, loss: 0.014465314336121082, acc: 0.99, recall: 0.99, precision: 0.9901960784313726, f_beta: 0.98999899989999
train: step: 2428, loss: 0.0296101626008749, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 2429, loss: 0.03883330896496773, acc: 0.95, recall: 0.95, precision: 0.9501800720288115, f_beta: 0.9499949994999499
train: step: 2430, loss: 0.03591516986489296, acc: 0.95, recall: 0.95, precision: 0.9501800720288115, f_beta: 0.9499949994999499
train: step: 2431, loss: 0.030940838158130646, acc: 0.97, recall: 0.97, precision: 0.9716981132075472, f_beta: 0.9699729756781104
train: step: 2432, loss: 0.021475372835993767, acc: 0.99, recall: 0.99, precision: 0.9901960784313726, f_beta: 0.98999899989999
train: step: 2433, loss: 0.044750288128852844, acc: 0.94, recall: 0.94, precision: 0.9428341384863124, f_beta: 0.9399038461538463
train: step: 2434, loss: 0.04875867813825607, acc: 0.95, recall: 0.95, precision: 0.9516258530710557, f_beta: 0.949954959463517
train: step: 2435, loss: 0.04012099280953407, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 2436, loss: 0.024902349337935448, acc: 0.97, recall: 0.97, precision: 0.9716981132075472, f_beta: 0.9699729756781104
train: step: 2437, loss: 0.025139903649687767, acc: 0.97, recall: 0.97, precision: 0.9716981132075472, f_beta: 0.9699729756781104
train: step: 2438, loss: 0.010226555168628693, acc: 0.99, recall: 0.99, precision: 0.9901960784313726, f_beta: 0.98999899989999
train: step: 2439, loss: 0.05093880742788315, acc: 0.93, recall: 0.9299999999999999, precision: 0.9315535929345644, f_beta: 0.9299369432489241
train: step: 2440, loss: 0.036180779337882996, acc: 0.96, recall: 0.96, precision: 0.9607371794871795, f_beta: 0.959983993597439
train: step: 2441, loss: 0.039653655141592026, acc: 0.95, recall: 0.95, precision: 0.9545454545454546, f_beta: 0.949874686716792
train: step: 2442, loss: 0.07869651168584824, acc: 0.91, recall: 0.9099999999999999, precision: 0.9114813327980731, f_beta: 0.9099189270343309
train: step: 2443, loss: 0.01686888001859188, acc: 0.98, recall: 0.98, precision: 0.98, f_beta: 0.98
train: step: 2444, loss: 0.053333770483732224, acc: 0.95, recall: 0.95, precision: 0.9516258530710557, f_beta: 0.949954959463517
train: step: 2445, loss: 0.04773864150047302, acc: 0.95, recall: 0.95, precision: 0.9501800720288115, f_beta: 0.9499949994999499
train: step: 2446, loss: 0.06734541058540344, acc: 0.91, recall: 0.91, precision: 0.9101640656262505, f_beta: 0.90999099909991
train: step: 2447, loss: 0.06067933142185211, acc: 0.92, recall: 0.9199999999999999, precision: 0.9206730769230769, f_beta: 0.919967987194878
train: step: 2448, loss: 0.06476210802793503, acc: 0.94, recall: 0.94, precision: 0.94, f_beta: 0.94
train: step: 2449, loss: 0.04000478237867355, acc: 0.95, recall: 0.95, precision: 0.9501800720288115, f_beta: 0.9499949994999499
train: step: 2450, loss: 0.07660999894142151, acc: 0.9, recall: 0.9, precision: 0.9006410256410255, f_beta: 0.8999599839935974
train: step: 2451, loss: 0.038123589009046555, acc: 0.94, recall: 0.94, precision: 0.9407051282051282, f_beta: 0.9399759903961584
train: step: 2452, loss: 0.07513083517551422, acc: 0.91, recall: 0.9099999999999999, precision: 0.9181966544267646, f_beta: 0.9095568284594513
train: step: 2453, loss: 0.01611120067536831, acc: 0.98, recall: 0.98, precision: 0.9807692307692308, f_beta: 0.9799919967987194
train: step: 2454, loss: 0.06058930233120918, acc: 0.92, recall: 0.9199999999999999, precision: 0.9206730769230769, f_beta: 0.919967987194878
train: step: 2455, loss: 0.04501236975193024, acc: 0.93, recall: 0.9299999999999999, precision: 0.930172068827531, f_beta: 0.9299929992999301
train: step: 2456, loss: 0.07736140489578247, acc: 0.91, recall: 0.91, precision: 0.9101640656262505, f_beta: 0.90999099909991
train: step: 2457, loss: 0.04094013571739197, acc: 0.94, recall: 0.94, precision: 0.9407051282051282, f_beta: 0.9399759903961584
train: step: 2458, loss: 0.045193348079919815, acc: 0.95, recall: 0.95, precision: 0.9501800720288115, f_beta: 0.9499949994999499
train: step: 2459, loss: 0.03809511289000511, acc: 0.96, recall: 0.96, precision: 0.9607371794871795, f_beta: 0.959983993597439
train: step: 2460, loss: 0.026389071717858315, acc: 0.96, recall: 0.96, precision: 0.9607371794871795, f_beta: 0.959983993597439
train: step: 2461, loss: 0.02094096690416336, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 2462, loss: 0.03570803999900818, acc: 0.95, recall: 0.95, precision: 0.9516258530710557, f_beta: 0.949954959463517
train: step: 2463, loss: 0.031312476843595505, acc: 0.97, recall: 0.97, precision: 0.9716981132075472, f_beta: 0.9699729756781104
train: step: 2464, loss: 0.03652849793434143, acc: 0.95, recall: 0.95, precision: 0.9501800720288115, f_beta: 0.9499949994999499
train: step: 2465, loss: 0.050190310925245285, acc: 0.93, recall: 0.9299999999999999, precision: 0.9315535929345644, f_beta: 0.9299369432489241
train: step: 2466, loss: 0.036574240773916245, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 2467, loss: 0.04720805212855339, acc: 0.93, recall: 0.9299999999999999, precision: 0.930172068827531, f_beta: 0.9299929992999301
train: step: 2468, loss: 0.049524858593940735, acc: 0.93, recall: 0.9299999999999999, precision: 0.930172068827531, f_beta: 0.9299929992999301
train: step: 2469, loss: 0.03251434862613678, acc: 0.95, recall: 0.95, precision: 0.9545454545454546, f_beta: 0.949874686716792
train: step: 2470, loss: 0.056725695729255676, acc: 0.94, recall: 0.94, precision: 0.9428341384863124, f_beta: 0.9399038461538463
train: step: 2471, loss: 0.03519532084465027, acc: 0.95, recall: 0.95, precision: 0.9516258530710557, f_beta: 0.949954959463517
train: step: 2472, loss: 0.0266440249979496, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 2473, loss: 0.016156045719981194, acc: 0.98, recall: 0.98, precision: 0.9807692307692308, f_beta: 0.9799919967987194
train: step: 2474, loss: 0.08827447146177292, acc: 0.89, recall: 0.89, precision: 0.89015606242497, f_beta: 0.88998899889989
train: step: 2475, loss: 0.05888238549232483, acc: 0.94, recall: 0.94, precision: 0.9428341384863124, f_beta: 0.9399038461538463
train: step: 2476, loss: 0.05045071616768837, acc: 0.93, recall: 0.9299999999999999, precision: 0.9315535929345644, f_beta: 0.9299369432489241
train: step: 2477, loss: 0.014720721170306206, acc: 0.99, recall: 0.99, precision: 0.9901960784313726, f_beta: 0.98999899989999
train: step: 2478, loss: 0.04997580498456955, acc: 0.93, recall: 0.9299999999999999, precision: 0.9315535929345644, f_beta: 0.9299369432489241
train: step: 2479, loss: 0.03344301879405975, acc: 0.95, recall: 0.95, precision: 0.9516258530710557, f_beta: 0.949954959463517
train: step: 2480, loss: 0.06942030787467957, acc: 0.9, recall: 0.9, precision: 0.9, f_beta: 0.9
train: step: 2481, loss: 0.038530752062797546, acc: 0.93, recall: 0.9299999999999999, precision: 0.930172068827531, f_beta: 0.9299929992999301
train: step: 2482, loss: 0.05433250591158867, acc: 0.92, recall: 0.9199999999999999, precision: 0.9206730769230769, f_beta: 0.919967987194878
train: step: 2483, loss: 0.06306680291891098, acc: 0.93, recall: 0.9299999999999999, precision: 0.930172068827531, f_beta: 0.9299929992999301
train: step: 2484, loss: 0.058993469923734665, acc: 0.9, recall: 0.9, precision: 0.9006410256410255, f_beta: 0.8999599839935974
train: step: 2485, loss: 0.02790459245443344, acc: 0.98, recall: 0.98, precision: 0.9807692307692308, f_beta: 0.9799919967987194
train: step: 2486, loss: 0.08261732012033463, acc: 0.9, recall: 0.9, precision: 0.9, f_beta: 0.9
train: step: 2487, loss: 0.022815175354480743, acc: 0.97, recall: 0.97, precision: 0.9716981132075472, f_beta: 0.9699729756781104
train: step: 2488, loss: 0.029351938515901566, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 2489, loss: 0.046055447310209274, acc: 0.95, recall: 0.95, precision: 0.9516258530710557, f_beta: 0.949954959463517
train: step: 2490, loss: 0.038973018527030945, acc: 0.96, recall: 0.96, precision: 0.9607371794871795, f_beta: 0.959983993597439
train: step: 2491, loss: 0.041496697813272476, acc: 0.96, recall: 0.96, precision: 0.9607371794871795, f_beta: 0.959983993597439
train: step: 2492, loss: 0.046085651963949203, acc: 0.95, recall: 0.95, precision: 0.9501800720288115, f_beta: 0.9499949994999499
train: step: 2493, loss: 0.0488726906478405, acc: 0.94, recall: 0.94, precision: 0.94, f_beta: 0.94
train: step: 2494, loss: 0.014832849614322186, acc: 0.98, recall: 0.98, precision: 0.98, f_beta: 0.98
train: step: 2495, loss: 0.031311627477407455, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 2496, loss: 0.005352414213120937, acc: 0.99, recall: 0.99, precision: 0.9901960784313726, f_beta: 0.98999899989999
train: step: 2497, loss: 0.027544615790247917, acc: 0.95, recall: 0.95, precision: 0.9501800720288115, f_beta: 0.9499949994999499
train: step: 2498, loss: 0.01636318489909172, acc: 0.99, recall: 0.99, precision: 0.9901960784313726, f_beta: 0.98999899989999
train: step: 2499, loss: 0.06778113543987274, acc: 0.9, recall: 0.8999999999999999, precision: 0.9025764895330113, f_beta: 0.8998397435897436
train: step: 2500, loss: 0.045851968228816986, acc: 0.95, recall: 0.95, precision: 0.9501800720288115, f_beta: 0.9499949994999499
train: step: 2501, loss: 0.02097807638347149, acc: 0.99, recall: 0.99, precision: 0.9901960784313726, f_beta: 0.98999899989999
train: step: 2502, loss: 0.020605137571692467, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 2503, loss: 0.030322188511490822, acc: 0.96, recall: 0.96, precision: 0.9607371794871795, f_beta: 0.959983993597439
train: step: 2504, loss: 0.05292626470327377, acc: 0.94, recall: 0.94, precision: 0.9428341384863124, f_beta: 0.9399038461538463
train: step: 2505, loss: 0.043554916977882385, acc: 0.95, recall: 0.95, precision: 0.9501800720288115, f_beta: 0.9499949994999499
train: step: 2506, loss: 0.018552584573626518, acc: 0.98, recall: 0.98, precision: 0.9807692307692308, f_beta: 0.9799919967987194
train: step: 2507, loss: 0.02148251049220562, acc: 0.98, recall: 0.98, precision: 0.98, f_beta: 0.98
train: step: 2508, loss: 0.036991018801927567, acc: 0.95, recall: 0.95, precision: 0.9516258530710557, f_beta: 0.949954959463517
train: step: 2509, loss: 0.03919588029384613, acc: 0.95, recall: 0.95, precision: 0.9501800720288115, f_beta: 0.9499949994999499
train: step: 2510, loss: 0.043913137167692184, acc: 0.96, recall: 0.96, precision: 0.9607371794871795, f_beta: 0.959983993597439
train: step: 2511, loss: 0.05008525773882866, acc: 0.93, recall: 0.9299999999999999, precision: 0.9315535929345644, f_beta: 0.9299369432489241
train: step: 2512, loss: 0.05111946165561676, acc: 0.95, recall: 0.95, precision: 0.9516258530710557, f_beta: 0.949954959463517
train: step: 2513, loss: 0.07612407207489014, acc: 0.89, recall: 0.89, precision: 0.89015606242497, f_beta: 0.88998899889989
train: step: 2514, loss: 0.05931250751018524, acc: 0.91, recall: 0.91, precision: 0.9101640656262505, f_beta: 0.90999099909991
train: step: 2515, loss: 0.0506654754281044, acc: 0.93, recall: 0.9299999999999999, precision: 0.9343434343434343, f_beta: 0.9298245614035088
train: step: 2516, loss: 0.021695949137210846, acc: 0.98, recall: 0.98, precision: 0.9807692307692308, f_beta: 0.9799919967987194
train: step: 2517, loss: 0.02219378761947155, acc: 0.98, recall: 0.98, precision: 0.9807692307692308, f_beta: 0.9799919967987194
train: step: 2518, loss: 0.004547362215816975, acc: 1.0, recall: 1.0, precision: 1.0, f_beta: 1.0
train: step: 2519, loss: 0.03379794955253601, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 2520, loss: 0.017103232443332672, acc: 0.98, recall: 0.98, precision: 0.9807692307692308, f_beta: 0.9799919967987194
train: step: 2521, loss: 0.03241480514407158, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 2522, loss: 0.013602460734546185, acc: 0.99, recall: 0.99, precision: 0.9901960784313726, f_beta: 0.98999899989999
train: step: 2523, loss: 0.04177336394786835, acc: 0.96, recall: 0.96, precision: 0.962962962962963, f_beta: 0.9599358974358974
train: step: 2524, loss: 0.02916800044476986, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 2525, loss: 0.04944104328751564, acc: 0.93, recall: 0.9299999999999999, precision: 0.9315535929345644, f_beta: 0.9299369432489241
train: step: 2526, loss: 0.040230512619018555, acc: 0.95, recall: 0.95, precision: 0.9545454545454546, f_beta: 0.949874686716792
train: step: 2527, loss: 0.06161302700638771, acc: 0.91, recall: 0.91, precision: 0.9101640656262505, f_beta: 0.90999099909991
train: step: 2528, loss: 0.0405329093337059, acc: 0.94, recall: 0.94, precision: 0.94, f_beta: 0.94
train: step: 2529, loss: 0.030418284237384796, acc: 0.96, recall: 0.96, precision: 0.9607371794871795, f_beta: 0.959983993597439
train: step: 2530, loss: 0.047458142042160034, acc: 0.92, recall: 0.9199999999999999, precision: 0.9206730769230769, f_beta: 0.919967987194878
train: step: 2531, loss: 0.07444144785404205, acc: 0.9, recall: 0.9, precision: 0.9006410256410255, f_beta: 0.8999599839935974
train: step: 2532, loss: 0.04317658394575119, acc: 0.95, recall: 0.95, precision: 0.9501800720288115, f_beta: 0.9499949994999499
train: step: 2533, loss: 0.04670342057943344, acc: 0.95, recall: 0.95, precision: 0.9501800720288115, f_beta: 0.9499949994999499
train: step: 2534, loss: 0.04857749491930008, acc: 0.95, recall: 0.95, precision: 0.9501800720288115, f_beta: 0.9499949994999499
train: step: 2535, loss: 0.01915697194635868, acc: 0.98, recall: 0.98, precision: 0.98, f_beta: 0.98
train: step: 2536, loss: 0.029062118381261826, acc: 0.96, recall: 0.96, precision: 0.9607371794871795, f_beta: 0.959983993597439
train: step: 2537, loss: 0.0469670407474041, acc: 0.95, recall: 0.95, precision: 0.9516258530710557, f_beta: 0.949954959463517
train: step: 2538, loss: 0.06547785550355911, acc: 0.92, recall: 0.9199999999999999, precision: 0.9227053140096618, f_beta: 0.9198717948717948
train: step: 2539, loss: 0.03194400295615196, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 2540, loss: 0.029885130003094673, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 2541, loss: 0.016810469329357147, acc: 0.99, recall: 0.99, precision: 0.9901960784313726, f_beta: 0.98999899989999
train: step: 2542, loss: 0.04418385401368141, acc: 0.94, recall: 0.94, precision: 0.94, f_beta: 0.94
train: step: 2543, loss: 0.08519205451011658, acc: 0.87, recall: 0.87, precision: 0.8701480592236894, f_beta: 0.8699869986998701
train: step: 2544, loss: 0.028359508141875267, acc: 0.95, recall: 0.95, precision: 0.9501800720288115, f_beta: 0.9499949994999499
train: step: 2545, loss: 0.05479126796126366, acc: 0.93, recall: 0.9299999999999999, precision: 0.930172068827531, f_beta: 0.9299929992999301
train: step: 2546, loss: 0.0008443795377388597, acc: 1.0, recall: 1.0, precision: 1.0, f_beta: 1.0
train: step: 2547, loss: 0.040248528122901917, acc: 0.93, recall: 0.9299999999999999, precision: 0.9315535929345644, f_beta: 0.9299369432489241
train: step: 2548, loss: 0.057788487523794174, acc: 0.92, recall: 0.92, precision: 0.92, f_beta: 0.92
train: step: 2549, loss: 0.04234521463513374, acc: 0.95, recall: 0.95, precision: 0.9501800720288115, f_beta: 0.9499949994999499
train: step: 2550, loss: 0.044099025428295135, acc: 0.95, recall: 0.95, precision: 0.9516258530710557, f_beta: 0.949954959463517
train: step: 2551, loss: 0.06695309281349182, acc: 0.93, recall: 0.9299999999999999, precision: 0.930172068827531, f_beta: 0.9299929992999301
train: step: 2552, loss: 0.0377487987279892, acc: 0.95, recall: 0.95, precision: 0.9501800720288115, f_beta: 0.9499949994999499
train: step: 2553, loss: 0.03661765530705452, acc: 0.95, recall: 0.95, precision: 0.9501800720288115, f_beta: 0.9499949994999499
train: step: 2554, loss: 0.0378020703792572, acc: 0.94, recall: 0.94, precision: 0.94, f_beta: 0.94
train: step: 2555, loss: 0.028083568438887596, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 2556, loss: 0.07095909118652344, acc: 0.89, recall: 0.89, precision: 0.8914090726615818, f_beta: 0.8899009108197378
train: step: 2557, loss: 0.042849101126194, acc: 0.93, recall: 0.9299999999999999, precision: 0.9343434343434343, f_beta: 0.9298245614035088
train: step: 2558, loss: 0.0469769686460495, acc: 0.95, recall: 0.95, precision: 0.9501800720288115, f_beta: 0.9499949994999499
train: step: 2559, loss: 0.03768007829785347, acc: 0.94, recall: 0.94, precision: 0.94, f_beta: 0.94
train: step: 2560, loss: 0.04881410300731659, acc: 0.95, recall: 0.95, precision: 0.9545454545454546, f_beta: 0.949874686716792
train: step: 2561, loss: 0.04237090423703194, acc: 0.95, recall: 0.95, precision: 0.9545454545454546, f_beta: 0.949874686716792
train: step: 2562, loss: 0.006164060905575752, acc: 0.99, recall: 0.99, precision: 0.9901960784313726, f_beta: 0.98999899989999
train: step: 2563, loss: 0.015739403665065765, acc: 0.98, recall: 0.98, precision: 0.9807692307692308, f_beta: 0.9799919967987194
train: step: 2564, loss: 0.017193090170621872, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 2565, loss: 0.037919312715530396, acc: 0.96, recall: 0.96, precision: 0.9607371794871795, f_beta: 0.959983993597439
train: step: 2566, loss: 0.046173423528671265, acc: 0.94, recall: 0.94, precision: 0.94, f_beta: 0.94
train: step: 2567, loss: 0.038413144648075104, acc: 0.95, recall: 0.95, precision: 0.9501800720288115, f_beta: 0.9499949994999499
train: step: 2568, loss: 0.055206023156642914, acc: 0.93, recall: 0.9299999999999999, precision: 0.9343434343434343, f_beta: 0.9298245614035088
train: step: 2569, loss: 0.06314127147197723, acc: 0.93, recall: 0.9299999999999999, precision: 0.9315535929345644, f_beta: 0.9299369432489241
train: step: 2570, loss: 0.009944275952875614, acc: 0.98, recall: 0.98, precision: 0.98, f_beta: 0.98
train: step: 2571, loss: 0.04725569859147072, acc: 0.94, recall: 0.94, precision: 0.9428341384863124, f_beta: 0.9399038461538463
train: step: 2572, loss: 0.027522265911102295, acc: 0.96, recall: 0.96, precision: 0.9607371794871795, f_beta: 0.959983993597439
train: step: 2573, loss: 0.026607418432831764, acc: 0.97, recall: 0.97, precision: 0.9716981132075472, f_beta: 0.9699729756781104
train: step: 2574, loss: 0.035351142287254333, acc: 0.96, recall: 0.96, precision: 0.9607371794871795, f_beta: 0.959983993597439
train: step: 2575, loss: 0.05221225321292877, acc: 0.95, recall: 0.95, precision: 0.9545454545454546, f_beta: 0.949874686716792
train: step: 2576, loss: 0.018606897443532944, acc: 0.98, recall: 0.98, precision: 0.9807692307692308, f_beta: 0.9799919967987194
train: step: 2577, loss: 0.057028528302907944, acc: 0.93, recall: 0.9299999999999999, precision: 0.9315535929345644, f_beta: 0.9299369432489241
train: step: 2578, loss: 0.02549990639090538, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 2579, loss: 0.05857113003730774, acc: 0.91, recall: 0.9099999999999999, precision: 0.9181966544267646, f_beta: 0.9095568284594513
train: step: 2580, loss: 0.0602571927011013, acc: 0.92, recall: 0.9199999999999999, precision: 0.9206730769230769, f_beta: 0.919967987194878
train: step: 2581, loss: 0.04641725867986679, acc: 0.95, recall: 0.95, precision: 0.9516258530710557, f_beta: 0.949954959463517
train: step: 2582, loss: 0.026887334883213043, acc: 0.98, recall: 0.98, precision: 0.9807692307692308, f_beta: 0.9799919967987194
train: step: 2583, loss: 0.030038155615329742, acc: 0.94, recall: 0.94, precision: 0.94, f_beta: 0.94
train: step: 2584, loss: 0.0662476122379303, acc: 0.9, recall: 0.8999999999999999, precision: 0.9058441558441559, f_beta: 0.8996386993175431
train: step: 2585, loss: 0.0271154697984457, acc: 0.97, recall: 0.97, precision: 0.9716981132075472, f_beta: 0.9699729756781104
train: step: 2586, loss: 0.031494103372097015, acc: 0.96, recall: 0.96, precision: 0.96, f_beta: 0.96
train: step: 2587, loss: 0.028985245153307915, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 2588, loss: 0.07908128947019577, acc: 0.91, recall: 0.91, precision: 0.9101640656262505, f_beta: 0.90999099909991
train: step: 2589, loss: 0.05118279531598091, acc: 0.91, recall: 0.9099999999999999, precision: 0.9141414141414141, f_beta: 0.9097744360902256
train: step: 2590, loss: 0.04350994527339935, acc: 0.94, recall: 0.94, precision: 0.94, f_beta: 0.94
train: step: 2591, loss: 0.06886789202690125, acc: 0.91, recall: 0.9099999999999999, precision: 0.9114813327980731, f_beta: 0.9099189270343309
train: step: 2592, loss: 0.022919585928320885, acc: 0.97, recall: 0.97, precision: 0.9716981132075472, f_beta: 0.9699729756781104
train: step: 2593, loss: 0.027036480605602264, acc: 0.97, recall: 0.97, precision: 0.9716981132075472, f_beta: 0.9699729756781104
train: step: 2594, loss: 0.049864500761032104, acc: 0.95, recall: 0.95, precision: 0.9501800720288115, f_beta: 0.9499949994999499
train: step: 2595, loss: 0.03536025434732437, acc: 0.95, recall: 0.95, precision: 0.9516258530710557, f_beta: 0.949954959463517
train: step: 2596, loss: 0.028993485495448112, acc: 0.95, recall: 0.95, precision: 0.9545454545454546, f_beta: 0.949874686716792
train: step: 2597, loss: 0.05952639877796173, acc: 0.93, recall: 0.9299999999999999, precision: 0.9343434343434343, f_beta: 0.9298245614035088
train: step: 2598, loss: 0.017706193029880524, acc: 0.98, recall: 0.98, precision: 0.9807692307692308, f_beta: 0.9799919967987194
train: step: 2599, loss: 0.031154394149780273, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 2600, loss: 0.02148713544011116, acc: 0.96, recall: 0.96, precision: 0.96, f_beta: 0.96
train: step: 2601, loss: 0.10720491409301758, acc: 0.88, recall: 0.88, precision: 0.8806089743589745, f_beta: 0.879951980792317
train: step: 2602, loss: 0.021594619378447533, acc: 0.98, recall: 0.98, precision: 0.9807692307692308, f_beta: 0.9799919967987194
train: step: 2603, loss: 0.04856235906481743, acc: 0.94, recall: 0.94, precision: 0.94, f_beta: 0.94
train: step: 2604, loss: 0.08007108420133591, acc: 0.91, recall: 0.91, precision: 0.9101640656262505, f_beta: 0.90999099909991
train: step: 2605, loss: 0.05589127540588379, acc: 0.94, recall: 0.94, precision: 0.9407051282051282, f_beta: 0.9399759903961584
train: step: 2606, loss: 0.03908282145857811, acc: 0.96, recall: 0.96, precision: 0.9607371794871795, f_beta: 0.959983993597439
train: step: 2607, loss: 0.0371067076921463, acc: 0.95, recall: 0.95, precision: 0.9545454545454546, f_beta: 0.949874686716792
train: step: 2608, loss: 0.06261304020881653, acc: 0.92, recall: 0.9199999999999999, precision: 0.9261363636363636, f_beta: 0.9197109594540345
train: step: 2609, loss: 0.055426836013793945, acc: 0.94, recall: 0.94, precision: 0.9428341384863124, f_beta: 0.9399038461538463
train: step: 2610, loss: 0.02386426366865635, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 2611, loss: 0.024864742532372475, acc: 0.97, recall: 0.97, precision: 0.9716981132075472, f_beta: 0.9699729756781104
train: step: 2612, loss: 0.05398916080594063, acc: 0.93, recall: 0.9299999999999999, precision: 0.930172068827531, f_beta: 0.9299929992999301
train: step: 2613, loss: 0.018875880166888237, acc: 0.98, recall: 0.98, precision: 0.9807692307692308, f_beta: 0.9799919967987194
train: step: 2614, loss: 0.0496458038687706, acc: 0.95, recall: 0.95, precision: 0.9501800720288115, f_beta: 0.9499949994999499
train: step: 2615, loss: 0.015157748013734818, acc: 0.99, recall: 0.99, precision: 0.9901960784313726, f_beta: 0.98999899989999
train: step: 2616, loss: 0.026228656992316246, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 2617, loss: 0.04967300966382027, acc: 0.93, recall: 0.9299999999999999, precision: 0.9315535929345644, f_beta: 0.9299369432489241
train: step: 2618, loss: 0.023933453485369682, acc: 0.97, recall: 0.97, precision: 0.9716981132075472, f_beta: 0.9699729756781104
train: step: 2619, loss: 0.03682928904891014, acc: 0.96, recall: 0.96, precision: 0.96, f_beta: 0.96
train: step: 2620, loss: 0.04314355552196503, acc: 0.95, recall: 0.95, precision: 0.9501800720288115, f_beta: 0.9499949994999499
train: step: 2621, loss: 0.03461623564362526, acc: 0.95, recall: 0.95, precision: 0.9501800720288115, f_beta: 0.9499949994999499
train: step: 2622, loss: 0.027864355593919754, acc: 0.96, recall: 0.96, precision: 0.962962962962963, f_beta: 0.9599358974358974
train: step: 2623, loss: 0.07096948474645615, acc: 0.91, recall: 0.9099999999999999, precision: 0.9141414141414141, f_beta: 0.9097744360902256
train: step: 2624, loss: 0.03510669991374016, acc: 0.96, recall: 0.96, precision: 0.96, f_beta: 0.96
train: step: 2625, loss: 0.061740025877952576, acc: 0.94, recall: 0.94, precision: 0.9407051282051282, f_beta: 0.9399759903961584
train: step: 2626, loss: 0.03989722207188606, acc: 0.95, recall: 0.95, precision: 0.9501800720288115, f_beta: 0.9499949994999499
train: step: 2627, loss: 0.08479718863964081, acc: 0.88, recall: 0.88, precision: 0.8824476650563606, f_beta: 0.8798076923076923
train: step: 2628, loss: 0.022470956668257713, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 2629, loss: 0.07367385178804398, acc: 0.9, recall: 0.9, precision: 0.9006410256410255, f_beta: 0.8999599839935974
train: step: 2630, loss: 0.07176964730024338, acc: 0.91, recall: 0.91, precision: 0.9101640656262505, f_beta: 0.90999099909991
train: step: 2631, loss: 0.04264620691537857, acc: 0.95, recall: 0.95, precision: 0.9501800720288115, f_beta: 0.9499949994999499
train: step: 2632, loss: 0.07185158133506775, acc: 0.91, recall: 0.91, precision: 0.9101640656262505, f_beta: 0.90999099909991
train: step: 2633, loss: 0.052955206483602524, acc: 0.93, recall: 0.9299999999999999, precision: 0.9343434343434343, f_beta: 0.9298245614035088
train: step: 2634, loss: 0.050525084137916565, acc: 0.93, recall: 0.9299999999999999, precision: 0.9315535929345644, f_beta: 0.9299369432489241
train: step: 2635, loss: 0.02873694896697998, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 2636, loss: 0.035609595477581024, acc: 0.96, recall: 0.96, precision: 0.962962962962963, f_beta: 0.9599358974358974
train: step: 2637, loss: 0.0315302237868309, acc: 0.97, recall: 0.97, precision: 0.9716981132075472, f_beta: 0.9699729756781104
train: step: 2638, loss: 0.047074392437934875, acc: 0.95, recall: 0.95, precision: 0.9501800720288115, f_beta: 0.9499949994999499
train: step: 2639, loss: 0.02925441786646843, acc: 0.96, recall: 0.96, precision: 0.96, f_beta: 0.96
train: step: 2640, loss: 0.024052781984210014, acc: 0.97, recall: 0.97, precision: 0.9716981132075472, f_beta: 0.9699729756781104
train: step: 2641, loss: 0.025672316551208496, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 2642, loss: 0.03508235514163971, acc: 0.96, recall: 0.96, precision: 0.9607371794871795, f_beta: 0.959983993597439
train: step: 2643, loss: 0.030787533149123192, acc: 0.96, recall: 0.96, precision: 0.962962962962963, f_beta: 0.9599358974358974
train: step: 2644, loss: 0.02820119820535183, acc: 0.96, recall: 0.96, precision: 0.962962962962963, f_beta: 0.9599358974358974
train: step: 2645, loss: 0.02107684686779976, acc: 0.99, recall: 0.99, precision: 0.9901960784313726, f_beta: 0.98999899989999
train: step: 2646, loss: 0.03246767818927765, acc: 0.95, recall: 0.95, precision: 0.9516258530710557, f_beta: 0.949954959463517
train: step: 2647, loss: 0.03790555149316788, acc: 0.95, recall: 0.95, precision: 0.9501800720288115, f_beta: 0.9499949994999499
train: step: 2648, loss: 0.0413019135594368, acc: 0.95, recall: 0.95, precision: 0.9545454545454546, f_beta: 0.949874686716792
train: step: 2649, loss: 0.04952425882220268, acc: 0.95, recall: 0.95, precision: 0.9501800720288115, f_beta: 0.9499949994999499
train: step: 2650, loss: 0.04776587337255478, acc: 0.94, recall: 0.94, precision: 0.94, f_beta: 0.94
train: step: 2651, loss: 0.08975162357091904, acc: 0.89, recall: 0.89, precision: 0.89015606242497, f_beta: 0.88998899889989
train: step: 2652, loss: 0.03863392770290375, acc: 0.96, recall: 0.96, precision: 0.9607371794871795, f_beta: 0.959983993597439
train: step: 2653, loss: 0.04416971281170845, acc: 0.94, recall: 0.94, precision: 0.9464285714285714, f_beta: 0.939783219590526
train: step: 2654, loss: 0.0793394148349762, acc: 0.89, recall: 0.89, precision: 0.8914090726615818, f_beta: 0.8899009108197378
train: step: 2655, loss: 0.058107320219278336, acc: 0.92, recall: 0.9199999999999999, precision: 0.9261363636363636, f_beta: 0.9197109594540345
train: step: 2656, loss: 0.025110650807619095, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 2657, loss: 0.03607754781842232, acc: 0.96, recall: 0.96, precision: 0.9607371794871795, f_beta: 0.959983993597439
train: step: 2658, loss: 0.052576400339603424, acc: 0.94, recall: 0.94, precision: 0.94, f_beta: 0.94
train: step: 2659, loss: 0.06038953736424446, acc: 0.94, recall: 0.94, precision: 0.9407051282051282, f_beta: 0.9399759903961584
train: step: 2660, loss: 0.06597914546728134, acc: 0.92, recall: 0.9199999999999999, precision: 0.9206730769230769, f_beta: 0.919967987194878
train: step: 2661, loss: 0.06932535022497177, acc: 0.93, recall: 0.9299999999999999, precision: 0.930172068827531, f_beta: 0.9299929992999301
train: step: 2662, loss: 0.05431719124317169, acc: 0.94, recall: 0.94, precision: 0.9428341384863124, f_beta: 0.9399038461538463
train: step: 2663, loss: 0.013311678543686867, acc: 0.98, recall: 0.98, precision: 0.9807692307692308, f_beta: 0.9799919967987194
train: step: 2664, loss: 0.021235762163996696, acc: 0.96, recall: 0.96, precision: 0.9607371794871795, f_beta: 0.959983993597439
train: step: 2665, loss: 0.060674477368593216, acc: 0.92, recall: 0.9199999999999999, precision: 0.9310344827586207, f_beta: 0.9194847020933978
train: step: 2666, loss: 0.04157523810863495, acc: 0.96, recall: 0.96, precision: 0.9607371794871795, f_beta: 0.959983993597439
train: step: 2667, loss: 0.03422846645116806, acc: 0.97, recall: 0.97, precision: 0.9716981132075472, f_beta: 0.9699729756781104
train: step: 2668, loss: 0.017959415912628174, acc: 0.98, recall: 0.98, precision: 0.9807692307692308, f_beta: 0.9799919967987194
train: step: 2669, loss: 0.016369987279176712, acc: 0.99, recall: 0.99, precision: 0.9901960784313726, f_beta: 0.98999899989999
train: step: 2670, loss: 0.04566822946071625, acc: 0.94, recall: 0.94, precision: 0.9428341384863124, f_beta: 0.9399038461538463
train: step: 2671, loss: 0.016550863161683083, acc: 0.98, recall: 0.98, precision: 0.9807692307692308, f_beta: 0.9799919967987194
train: step: 2672, loss: 0.056625571101903915, acc: 0.93, recall: 0.9299999999999999, precision: 0.9315535929345644, f_beta: 0.9299369432489241
train: step: 2673, loss: 0.051857348531484604, acc: 0.95, recall: 0.95, precision: 0.9516258530710557, f_beta: 0.949954959463517
train: step: 2674, loss: 0.037993744015693665, acc: 0.95, recall: 0.95, precision: 0.9545454545454546, f_beta: 0.949874686716792
train: step: 2675, loss: 0.04779544472694397, acc: 0.96, recall: 0.96, precision: 0.9607371794871795, f_beta: 0.959983993597439
train: step: 2676, loss: 0.0020896971691399813, acc: 1.0, recall: 1.0, precision: 1.0, f_beta: 1.0
train: step: 2677, loss: 0.02941703237593174, acc: 0.95, recall: 0.95, precision: 0.9501800720288115, f_beta: 0.9499949994999499
train: step: 2678, loss: 0.022730665281414986, acc: 0.96, recall: 0.96, precision: 0.9607371794871795, f_beta: 0.959983993597439
train: step: 2679, loss: 0.045618437230587006, acc: 0.95, recall: 0.95, precision: 0.9501800720288115, f_beta: 0.9499949994999499
train: step: 2680, loss: 0.02257147617638111, acc: 0.98, recall: 0.98, precision: 0.9807692307692308, f_beta: 0.9799919967987194
train: step: 2681, loss: 0.03832331672310829, acc: 0.95, recall: 0.95, precision: 0.9501800720288115, f_beta: 0.9499949994999499
train: step: 2682, loss: 0.011367754079401493, acc: 0.98, recall: 0.98, precision: 0.98, f_beta: 0.98
train: step: 2683, loss: 0.011464571580290794, acc: 1.0, recall: 1.0, precision: 1.0, f_beta: 1.0
train: step: 2684, loss: 0.08368394523859024, acc: 0.89, recall: 0.89, precision: 0.89015606242497, f_beta: 0.88998899889989
train: step: 2685, loss: 0.05888080596923828, acc: 0.94, recall: 0.94, precision: 0.9464285714285714, f_beta: 0.939783219590526
train: step: 2686, loss: 0.03457065671682358, acc: 0.95, recall: 0.95, precision: 0.9501800720288115, f_beta: 0.9499949994999499
train: step: 2687, loss: 0.03206561878323555, acc: 0.96, recall: 0.96, precision: 0.962962962962963, f_beta: 0.9599358974358974
train: step: 2688, loss: 0.04962226003408432, acc: 0.93, recall: 0.9299999999999999, precision: 0.9343434343434343, f_beta: 0.9298245614035088
train: step: 2689, loss: 0.061573244631290436, acc: 0.93, recall: 0.9299999999999999, precision: 0.9315535929345644, f_beta: 0.9299369432489241
train: step: 2690, loss: 0.029117917641997337, acc: 0.97, recall: 0.97, precision: 0.9716981132075472, f_beta: 0.9699729756781104
train: step: 2691, loss: 0.058197423815727234, acc: 0.94, recall: 0.94, precision: 0.9407051282051282, f_beta: 0.9399759903961584
train: step: 2692, loss: 0.03150540217757225, acc: 0.96, recall: 0.96, precision: 0.962962962962963, f_beta: 0.9599358974358974
train: step: 2693, loss: 0.06010935828089714, acc: 0.93, recall: 0.9299999999999999, precision: 0.9315535929345644, f_beta: 0.9299369432489241
train: step: 2694, loss: 0.03630795702338219, acc: 0.96, recall: 0.96, precision: 0.96, f_beta: 0.96
train: step: 2695, loss: 0.05028349906206131, acc: 0.94, recall: 0.94, precision: 0.94, f_beta: 0.94
train: step: 2696, loss: 0.042759474366903305, acc: 0.95, recall: 0.95, precision: 0.9501800720288115, f_beta: 0.9499949994999499
train: step: 2697, loss: 0.08996303379535675, acc: 0.89, recall: 0.89, precision: 0.89015606242497, f_beta: 0.88998899889989
train: step: 2698, loss: 0.025598278269171715, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 2699, loss: 0.030359534546732903, acc: 0.96, recall: 0.96, precision: 0.962962962962963, f_beta: 0.9599358974358974
train: step: 2700, loss: 0.023643316701054573, acc: 0.96, recall: 0.96, precision: 0.9607371794871795, f_beta: 0.959983993597439
train: step: 2701, loss: 0.02150682918727398, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 2702, loss: 0.06463679671287537, acc: 0.9, recall: 0.9, precision: 0.9166666666666667, f_beta: 0.898989898989899
train: step: 2703, loss: 0.00755170825868845, acc: 0.99, recall: 0.99, precision: 0.9901960784313726, f_beta: 0.98999899989999
train: step: 2704, loss: 0.04099468141794205, acc: 0.96, recall: 0.96, precision: 0.9607371794871795, f_beta: 0.959983993597439
train: step: 2705, loss: 0.038355108350515366, acc: 0.95, recall: 0.95, precision: 0.9501800720288115, f_beta: 0.9499949994999499
train: step: 2706, loss: 0.02381495013833046, acc: 0.98, recall: 0.98, precision: 0.98, f_beta: 0.98
train: step: 2707, loss: 0.06986955553293228, acc: 0.91, recall: 0.91, precision: 0.9101640656262505, f_beta: 0.90999099909991
train: step: 2708, loss: 0.014536036178469658, acc: 0.98, recall: 0.98, precision: 0.9807692307692308, f_beta: 0.9799919967987194
train: step: 2709, loss: 0.011556671001017094, acc: 0.99, recall: 0.99, precision: 0.9901960784313726, f_beta: 0.98999899989999
train: step: 2710, loss: 0.06450708210468292, acc: 0.92, recall: 0.92, precision: 0.92, f_beta: 0.92
train: step: 2711, loss: 0.012613155879080296, acc: 0.98, recall: 0.98, precision: 0.98, f_beta: 0.98
train: step: 2712, loss: 0.059269979596138, acc: 0.95, recall: 0.95, precision: 0.9516258530710557, f_beta: 0.949954959463517
train: step: 2713, loss: 0.05654573068022728, acc: 0.95, recall: 0.95, precision: 0.9545454545454546, f_beta: 0.949874686716792
train: step: 2714, loss: 0.017448371276259422, acc: 0.97, recall: 0.97, precision: 0.9716981132075472, f_beta: 0.9699729756781104
train: step: 2715, loss: 0.022407030686736107, acc: 0.98, recall: 0.98, precision: 0.9807692307692308, f_beta: 0.9799919967987194
train: step: 2716, loss: 0.003830135799944401, acc: 0.99, recall: 0.99, precision: 0.9901960784313726, f_beta: 0.98999899989999
train: step: 2717, loss: 0.029584789648652077, acc: 0.96, recall: 0.96, precision: 0.9607371794871795, f_beta: 0.959983993597439
train: step: 2718, loss: 0.0024500854779034853, acc: 1.0, recall: 1.0, precision: 1.0, f_beta: 1.0
train: step: 2719, loss: 0.03293086588382721, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 2720, loss: 0.06408247351646423, acc: 0.93, recall: 0.9299999999999999, precision: 0.930172068827531, f_beta: 0.9299929992999301
train: step: 2721, loss: 0.054897941648960114, acc: 0.93, recall: 0.9299999999999999, precision: 0.930172068827531, f_beta: 0.9299929992999301
train: step: 2722, loss: 0.002793746069073677, acc: 1.0, recall: 1.0, precision: 1.0, f_beta: 1.0
train: step: 2723, loss: 0.04081990197300911, acc: 0.95, recall: 0.95, precision: 0.9516258530710557, f_beta: 0.949954959463517
train: step: 2724, loss: 0.030212102457880974, acc: 0.96, recall: 0.96, precision: 0.9607371794871795, f_beta: 0.959983993597439
train: step: 2725, loss: 0.03714780509471893, acc: 0.96, recall: 0.96, precision: 0.962962962962963, f_beta: 0.9599358974358974
train: step: 2726, loss: 0.01008161623030901, acc: 0.99, recall: 0.99, precision: 0.9901960784313726, f_beta: 0.98999899989999
train: step: 2727, loss: 0.06923705339431763, acc: 0.91, recall: 0.9099999999999999, precision: 0.9141414141414141, f_beta: 0.9097744360902256
train: step: 2728, loss: 0.029987622052431107, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 2729, loss: 0.05397563800215721, acc: 0.93, recall: 0.9299999999999999, precision: 0.9315535929345644, f_beta: 0.9299369432489241
train: step: 2730, loss: 0.048568934202194214, acc: 0.95, recall: 0.95, precision: 0.9501800720288115, f_beta: 0.9499949994999499
train: step: 2731, loss: 0.0418715663254261, acc: 0.94, recall: 0.94, precision: 0.94, f_beta: 0.94
train: step: 2732, loss: 0.06168350577354431, acc: 0.91, recall: 0.9099999999999999, precision: 0.9181966544267646, f_beta: 0.9095568284594513
train: step: 2733, loss: 0.029852919280529022, acc: 0.96, recall: 0.96, precision: 0.9607371794871795, f_beta: 0.959983993597439
train: step: 2734, loss: 0.028160495683550835, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 2735, loss: 0.05714605003595352, acc: 0.94, recall: 0.94, precision: 0.9407051282051282, f_beta: 0.9399759903961584
train: step: 2736, loss: 0.04626510664820671, acc: 0.93, recall: 0.9299999999999999, precision: 0.9315535929345644, f_beta: 0.9299369432489241
train: step: 2737, loss: 0.03897031396627426, acc: 0.96, recall: 0.96, precision: 0.96, f_beta: 0.96
train: step: 2738, loss: 0.042504001408815384, acc: 0.96, recall: 0.96, precision: 0.96, f_beta: 0.96
train: step: 2739, loss: 0.0816102996468544, acc: 0.89, recall: 0.89, precision: 0.8914090726615818, f_beta: 0.8899009108197378
train: step: 2740, loss: 0.021379608660936356, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 2741, loss: 0.052991580218076706, acc: 0.93, recall: 0.9299999999999999, precision: 0.930172068827531, f_beta: 0.9299929992999301
train: step: 2742, loss: 0.01899247244000435, acc: 0.97, recall: 0.97, precision: 0.9716981132075472, f_beta: 0.9699729756781104
train: step: 2743, loss: 0.030883340165019035, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 2744, loss: 0.0584607757627964, acc: 0.93, recall: 0.9299999999999999, precision: 0.930172068827531, f_beta: 0.9299929992999301
train: step: 2745, loss: 0.03837783634662628, acc: 0.95, recall: 0.95, precision: 0.9501800720288115, f_beta: 0.9499949994999499
train: step: 2746, loss: 0.0306109469383955, acc: 0.97, recall: 0.97, precision: 0.9716981132075472, f_beta: 0.9699729756781104
train: step: 2747, loss: 0.033076100051403046, acc: 0.96, recall: 0.96, precision: 0.9607371794871795, f_beta: 0.959983993597439
train: step: 2748, loss: 0.07119952142238617, acc: 0.9, recall: 0.9, precision: 0.9, f_beta: 0.9
train: step: 2749, loss: 0.08150889724493027, acc: 0.89, recall: 0.8899999999999999, precision: 0.8939393939393939, f_beta: 0.8897243107769424
train: step: 2750, loss: 0.03895602747797966, acc: 0.96, recall: 0.96, precision: 0.9607371794871795, f_beta: 0.959983993597439
train: step: 2751, loss: 0.06283661723136902, acc: 0.92, recall: 0.9199999999999999, precision: 0.9206730769230769, f_beta: 0.919967987194878
train: step: 2752, loss: 0.04341563582420349, acc: 0.95, recall: 0.95, precision: 0.9516258530710557, f_beta: 0.949954959463517
train: step: 2753, loss: 0.039754319936037064, acc: 0.95, recall: 0.95, precision: 0.9501800720288115, f_beta: 0.9499949994999499
train: step: 2754, loss: 0.048640936613082886, acc: 0.94, recall: 0.94, precision: 0.94, f_beta: 0.94
train: step: 2755, loss: 0.07739154994487762, acc: 0.91, recall: 0.9099999999999999, precision: 0.9114813327980731, f_beta: 0.9099189270343309
train: step: 2756, loss: 0.034189410507678986, acc: 0.95, recall: 0.95, precision: 0.9501800720288115, f_beta: 0.9499949994999499
train: step: 2757, loss: 0.035531122237443924, acc: 0.94, recall: 0.94, precision: 0.94, f_beta: 0.94
train: step: 2758, loss: 0.03626927733421326, acc: 0.96, recall: 0.96, precision: 0.96, f_beta: 0.96
train: step: 2759, loss: 0.04953740909695625, acc: 0.94, recall: 0.94, precision: 0.9407051282051282, f_beta: 0.9399759903961584
train: step: 2760, loss: 0.08372276276350021, acc: 0.89, recall: 0.89, precision: 0.8914090726615818, f_beta: 0.8899009108197378
train: step: 2761, loss: 0.012025334872305393, acc: 0.98, recall: 0.98, precision: 0.9807692307692308, f_beta: 0.9799919967987194
train: step: 2762, loss: 0.023671207949519157, acc: 0.98, recall: 0.98, precision: 0.98, f_beta: 0.98
train: step: 2763, loss: 0.046753011643886566, acc: 0.95, recall: 0.95, precision: 0.9501800720288115, f_beta: 0.9499949994999499
train: step: 2764, loss: 0.08501768857240677, acc: 0.89, recall: 0.89, precision: 0.8914090726615818, f_beta: 0.8899009108197378
train: step: 2765, loss: 0.030147738754749298, acc: 0.97, recall: 0.97, precision: 0.9716981132075472, f_beta: 0.9699729756781104
train: step: 2766, loss: 0.03307634964585304, acc: 0.96, recall: 0.96, precision: 0.96, f_beta: 0.96
train: step: 2767, loss: 0.03969821706414223, acc: 0.95, recall: 0.95, precision: 0.9501800720288115, f_beta: 0.9499949994999499
train: step: 2768, loss: 0.04665088653564453, acc: 0.96, recall: 0.96, precision: 0.9607371794871795, f_beta: 0.959983993597439
train: step: 2769, loss: 0.028669167309999466, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 2770, loss: 0.05844225361943245, acc: 0.94, recall: 0.94, precision: 0.9428341384863124, f_beta: 0.9399038461538463
train: step: 2771, loss: 0.014515352435410023, acc: 0.98, recall: 0.98, precision: 0.9807692307692308, f_beta: 0.9799919967987194
train: step: 2772, loss: 0.03566180542111397, acc: 0.96, recall: 0.96, precision: 0.962962962962963, f_beta: 0.9599358974358974
train: step: 2773, loss: 0.04193925857543945, acc: 0.95, recall: 0.95, precision: 0.9516258530710557, f_beta: 0.949954959463517
train: step: 2774, loss: 0.04142185300588608, acc: 0.95, recall: 0.95, precision: 0.9545454545454546, f_beta: 0.949874686716792
train: step: 2775, loss: 0.012789645232260227, acc: 0.99, recall: 0.99, precision: 0.9901960784313726, f_beta: 0.98999899989999
train: step: 2776, loss: 0.013785562478005886, acc: 0.99, recall: 0.99, precision: 0.9901960784313726, f_beta: 0.98999899989999
train: step: 2777, loss: 0.02091282606124878, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 2778, loss: 0.03587519749999046, acc: 0.96, recall: 0.96, precision: 0.962962962962963, f_beta: 0.9599358974358974
train: step: 2779, loss: 0.052343253046274185, acc: 0.93, recall: 0.9299999999999999, precision: 0.9343434343434343, f_beta: 0.9298245614035088
train: step: 2780, loss: 0.021105023100972176, acc: 0.98, recall: 0.98, precision: 0.9807692307692308, f_beta: 0.9799919967987194
train: step: 2781, loss: 0.06903314590454102, acc: 0.91, recall: 0.9099999999999999, precision: 0.9114813327980731, f_beta: 0.9099189270343309
train: step: 2782, loss: 0.04780842363834381, acc: 0.94, recall: 0.94, precision: 0.94, f_beta: 0.94
train: step: 2783, loss: 0.0385490357875824, acc: 0.96, recall: 0.96, precision: 0.9607371794871795, f_beta: 0.959983993597439
train: step: 2784, loss: 0.03037259355187416, acc: 0.95, recall: 0.95, precision: 0.9516258530710557, f_beta: 0.949954959463517
train: step: 2785, loss: 0.06815311312675476, acc: 0.9, recall: 0.8999999999999999, precision: 0.9058441558441559, f_beta: 0.8996386993175431
train: step: 2786, loss: 0.06402707099914551, acc: 0.92, recall: 0.9199999999999999, precision: 0.9206730769230769, f_beta: 0.919967987194878
train: step: 2787, loss: 0.049513645470142365, acc: 0.92, recall: 0.9199999999999999, precision: 0.9261363636363636, f_beta: 0.9197109594540345
train: step: 2788, loss: 0.07437776774168015, acc: 0.89, recall: 0.8899999999999999, precision: 0.8977968176254589, f_beta: 0.8894583458948848
train: step: 2789, loss: 0.029661210253834724, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 2790, loss: 0.03617174178361893, acc: 0.95, recall: 0.95, precision: 0.9501800720288115, f_beta: 0.9499949994999499
train: step: 2791, loss: 0.04722588509321213, acc: 0.95, recall: 0.95, precision: 0.9501800720288115, f_beta: 0.9499949994999499
train: step: 2792, loss: 0.04297840967774391, acc: 0.93, recall: 0.9299999999999999, precision: 0.930172068827531, f_beta: 0.9299929992999301
train: step: 2793, loss: 0.026914160698652267, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 2794, loss: 0.08589021861553192, acc: 0.9, recall: 0.8999999999999999, precision: 0.9058441558441559, f_beta: 0.8996386993175431
train: step: 2795, loss: 0.03650183230638504, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 2796, loss: 0.017710698768496513, acc: 0.98, recall: 0.98, precision: 0.9807692307692308, f_beta: 0.9799919967987194
train: step: 2797, loss: 0.032329339534044266, acc: 0.95, recall: 0.95, precision: 0.9516258530710557, f_beta: 0.949954959463517
train: step: 2798, loss: 0.04790539667010307, acc: 0.93, recall: 0.9299999999999999, precision: 0.9315535929345644, f_beta: 0.9299369432489241
train: step: 2799, loss: 0.02966155856847763, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 2800, loss: 0.06286954879760742, acc: 0.92, recall: 0.9199999999999999, precision: 0.9227053140096618, f_beta: 0.9198717948717948
train: step: 2801, loss: 0.015005607157945633, acc: 0.98, recall: 0.98, precision: 0.9807692307692308, f_beta: 0.9799919967987194
train: step: 2802, loss: 0.03133673965930939, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 2803, loss: 0.06062274053692818, acc: 0.93, recall: 0.9299999999999999, precision: 0.930172068827531, f_beta: 0.9299929992999301
train: step: 2804, loss: 0.049140509217977524, acc: 0.93, recall: 0.9299999999999999, precision: 0.930172068827531, f_beta: 0.9299929992999301
train: step: 2805, loss: 0.03591269627213478, acc: 0.96, recall: 0.96, precision: 0.9607371794871795, f_beta: 0.959983993597439
train: step: 2806, loss: 0.043665140867233276, acc: 0.95, recall: 0.95, precision: 0.9501800720288115, f_beta: 0.9499949994999499
train: step: 2807, loss: 0.039031852036714554, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 2808, loss: 0.06768877804279327, acc: 0.92, recall: 0.9199999999999999, precision: 0.9206730769230769, f_beta: 0.919967987194878
train: step: 2809, loss: 0.038476087152957916, acc: 0.96, recall: 0.96, precision: 0.96, f_beta: 0.96
train: step: 2810, loss: 0.027221960946917534, acc: 0.98, recall: 0.98, precision: 0.9807692307692308, f_beta: 0.9799919967987194
train: step: 2811, loss: 0.047132883220911026, acc: 0.94, recall: 0.94, precision: 0.94, f_beta: 0.94
train: step: 2812, loss: 0.02673756331205368, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 2813, loss: 0.05012119188904762, acc: 0.93, recall: 0.9299999999999999, precision: 0.930172068827531, f_beta: 0.9299929992999301
train: step: 2814, loss: 0.04541163519024849, acc: 0.95, recall: 0.95, precision: 0.9501800720288115, f_beta: 0.9499949994999499
train: step: 2815, loss: 0.033769093453884125, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 2816, loss: 0.039050374180078506, acc: 0.95, recall: 0.95, precision: 0.9516258530710557, f_beta: 0.949954959463517
train: step: 2817, loss: 0.02549024671316147, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 2818, loss: 0.03184377774596214, acc: 0.97, recall: 0.97, precision: 0.9716981132075472, f_beta: 0.9699729756781104
train: step: 2819, loss: 0.007918004877865314, acc: 0.99, recall: 0.99, precision: 0.9901960784313726, f_beta: 0.98999899989999
train: step: 2820, loss: 0.02303856797516346, acc: 0.98, recall: 0.98, precision: 0.9807692307692308, f_beta: 0.9799919967987194
train: step: 2821, loss: 0.03777080401778221, acc: 0.94, recall: 0.94, precision: 0.9407051282051282, f_beta: 0.9399759903961584
train: step: 2822, loss: 0.02946469746530056, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 2823, loss: 0.06039774417877197, acc: 0.93, recall: 0.9299999999999999, precision: 0.930172068827531, f_beta: 0.9299929992999301
train: step: 2824, loss: 0.03908318653702736, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 2825, loss: 0.044569503515958786, acc: 0.94, recall: 0.94, precision: 0.9428341384863124, f_beta: 0.9399038461538463
train: step: 2826, loss: 0.013833745382726192, acc: 0.99, recall: 0.99, precision: 0.9901960784313726, f_beta: 0.98999899989999
train: step: 2827, loss: 0.03714221343398094, acc: 0.96, recall: 0.96, precision: 0.9607371794871795, f_beta: 0.959983993597439
train: step: 2828, loss: 0.03261203691363335, acc: 0.95, recall: 0.95, precision: 0.9516258530710557, f_beta: 0.949954959463517
train: step: 2829, loss: 0.07911434024572372, acc: 0.91, recall: 0.9099999999999999, precision: 0.9141414141414141, f_beta: 0.9097744360902256
train: step: 2830, loss: 0.012715645134449005, acc: 0.99, recall: 0.99, precision: 0.9901960784313726, f_beta: 0.98999899989999
train: step: 2831, loss: 0.011498646810650826, acc: 1.0, recall: 1.0, precision: 1.0, f_beta: 1.0
train: step: 2832, loss: 0.06569493561983109, acc: 0.92, recall: 0.92, precision: 0.92, f_beta: 0.92
train: step: 2833, loss: 0.029819488525390625, acc: 0.97, recall: 0.97, precision: 0.9716981132075472, f_beta: 0.9699729756781104
train: step: 2834, loss: 0.027591180056333542, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 2835, loss: 0.02294393628835678, acc: 0.98, recall: 0.98, precision: 0.98, f_beta: 0.98
train: step: 2836, loss: 0.0382222905755043, acc: 0.96, recall: 0.96, precision: 0.96, f_beta: 0.96
train: step: 2837, loss: 0.03367076814174652, acc: 0.96, recall: 0.96, precision: 0.9607371794871795, f_beta: 0.959983993597439
train: step: 2838, loss: 0.03924364596605301, acc: 0.95, recall: 0.95, precision: 0.9501800720288115, f_beta: 0.9499949994999499
train: step: 2839, loss: 0.0320160947740078, acc: 0.96, recall: 0.96, precision: 0.96, f_beta: 0.96
train: step: 2840, loss: 0.04137197509407997, acc: 0.96, recall: 0.96, precision: 0.96, f_beta: 0.96
train: step: 2841, loss: 0.07091312110424042, acc: 0.92, recall: 0.9199999999999999, precision: 0.9206730769230769, f_beta: 0.919967987194878
train: step: 2842, loss: 0.014008953236043453, acc: 0.99, recall: 0.99, precision: 0.9901960784313726, f_beta: 0.98999899989999
train: step: 2843, loss: 0.03770400211215019, acc: 0.94, recall: 0.94, precision: 0.9407051282051282, f_beta: 0.9399759903961584
train: step: 2844, loss: 0.0367046482861042, acc: 0.95, recall: 0.95, precision: 0.9516258530710557, f_beta: 0.949954959463517
train: step: 2845, loss: 0.03033043071627617, acc: 0.96, recall: 0.96, precision: 0.96, f_beta: 0.96
train: step: 2846, loss: 0.01903354376554489, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 2847, loss: 0.0365774929523468, acc: 0.96, recall: 0.96, precision: 0.9607371794871795, f_beta: 0.959983993597439
train: step: 2848, loss: 0.03364570438861847, acc: 0.96, recall: 0.96, precision: 0.9607371794871795, f_beta: 0.959983993597439
train: step: 2849, loss: 0.04637148231267929, acc: 0.92, recall: 0.9199999999999999, precision: 0.9227053140096618, f_beta: 0.9198717948717948
train: step: 2850, loss: 0.04400455951690674, acc: 0.95, recall: 0.95, precision: 0.9516258530710557, f_beta: 0.949954959463517
train: step: 2851, loss: 0.04897761717438698, acc: 0.95, recall: 0.95, precision: 0.9516258530710557, f_beta: 0.949954959463517
train: step: 2852, loss: 0.10828588157892227, acc: 0.86, recall: 0.86, precision: 0.8623188405797102, f_beta: 0.859775641025641
train: step: 2853, loss: 0.031839627772569656, acc: 0.96, recall: 0.96, precision: 0.96, f_beta: 0.96
train: step: 2854, loss: 0.018477454781532288, acc: 0.98, recall: 0.98, precision: 0.9807692307692308, f_beta: 0.9799919967987194
train: step: 2855, loss: 0.05060413479804993, acc: 0.95, recall: 0.95, precision: 0.9501800720288115, f_beta: 0.9499949994999499
train: step: 2856, loss: 0.014559421688318253, acc: 0.98, recall: 0.98, precision: 0.9807692307692308, f_beta: 0.9799919967987194
train: step: 2857, loss: 0.0482768639922142, acc: 0.94, recall: 0.94, precision: 0.9428341384863124, f_beta: 0.9399038461538463
train: step: 2858, loss: 0.06356313079595566, acc: 0.93, recall: 0.9299999999999999, precision: 0.9343434343434343, f_beta: 0.9298245614035088
train: step: 2859, loss: 0.012176704593002796, acc: 0.99, recall: 0.99, precision: 0.9901960784313726, f_beta: 0.98999899989999
train: step: 2860, loss: 0.003992760553956032, acc: 0.99, recall: 0.99, precision: 0.9901960784313726, f_beta: 0.98999899989999
train: step: 2861, loss: 0.05829794704914093, acc: 0.94, recall: 0.94, precision: 0.9407051282051282, f_beta: 0.9399759903961584
train: step: 2862, loss: 0.036691464483737946, acc: 0.95, recall: 0.95, precision: 0.9501800720288115, f_beta: 0.9499949994999499
train: step: 2863, loss: 0.0288153737783432, acc: 0.96, recall: 0.96, precision: 0.9607371794871795, f_beta: 0.959983993597439
train: step: 2864, loss: 0.06276341527700424, acc: 0.91, recall: 0.91, precision: 0.9101640656262505, f_beta: 0.90999099909991
train: step: 2865, loss: 0.08726178854703903, acc: 0.9, recall: 0.8999999999999999, precision: 0.9025764895330113, f_beta: 0.8998397435897436
train: step: 2866, loss: 0.02842719852924347, acc: 0.98, recall: 0.98, precision: 0.9807692307692308, f_beta: 0.9799919967987194
train: step: 2867, loss: 0.04604516178369522, acc: 0.94, recall: 0.94, precision: 0.9407051282051282, f_beta: 0.9399759903961584
train: step: 2868, loss: 0.013150755316019058, acc: 0.98, recall: 0.98, precision: 0.98, f_beta: 0.98
train: step: 2869, loss: 0.026974182575941086, acc: 0.96, recall: 0.96, precision: 0.962962962962963, f_beta: 0.9599358974358974
train: step: 2870, loss: 0.020670127123594284, acc: 0.98, recall: 0.98, precision: 0.9807692307692308, f_beta: 0.9799919967987194
train: step: 2871, loss: 0.03583409637212753, acc: 0.95, recall: 0.95, precision: 0.9501800720288115, f_beta: 0.9499949994999499
train: step: 2872, loss: 0.0496525913476944, acc: 0.94, recall: 0.94, precision: 0.94, f_beta: 0.94
train: step: 2873, loss: 0.041507821530103683, acc: 0.95, recall: 0.95, precision: 0.9501800720288115, f_beta: 0.9499949994999499
train: step: 2874, loss: 0.03275778144598007, acc: 0.95, recall: 0.95, precision: 0.9516258530710557, f_beta: 0.949954959463517
train: step: 2875, loss: 0.06997951120138168, acc: 0.91, recall: 0.9099999999999999, precision: 0.9114813327980731, f_beta: 0.9099189270343309
train: step: 2876, loss: 0.029660040512681007, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 2877, loss: 0.03335709497332573, acc: 0.97, recall: 0.97, precision: 0.9716981132075472, f_beta: 0.9699729756781104
train: step: 2878, loss: 0.01835966669023037, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 2879, loss: 0.02675454132258892, acc: 0.97, recall: 0.97, precision: 0.9716981132075472, f_beta: 0.9699729756781104
train: step: 2880, loss: 0.0333721749484539, acc: 0.96, recall: 0.96, precision: 0.9607371794871795, f_beta: 0.959983993597439
train: step: 2881, loss: 0.04413241520524025, acc: 0.96, recall: 0.96, precision: 0.96, f_beta: 0.96
train: step: 2882, loss: 0.04965326189994812, acc: 0.95, recall: 0.95, precision: 0.9516258530710557, f_beta: 0.949954959463517
train: step: 2883, loss: 0.036183129996061325, acc: 0.95, recall: 0.95, precision: 0.9516258530710557, f_beta: 0.949954959463517
train: step: 2884, loss: 0.07709050923585892, acc: 0.92, recall: 0.9199999999999999, precision: 0.9206730769230769, f_beta: 0.919967987194878
train: step: 2885, loss: 0.04487631842494011, acc: 0.95, recall: 0.95, precision: 0.9516258530710557, f_beta: 0.949954959463517
train: step: 2886, loss: 0.034613799303770065, acc: 0.96, recall: 0.96, precision: 0.96, f_beta: 0.96
train: step: 2887, loss: 0.02415577881038189, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 2888, loss: 0.012723786756396294, acc: 0.99, recall: 0.99, precision: 0.9901960784313726, f_beta: 0.98999899989999
train: step: 2889, loss: 0.00899165216833353, acc: 0.99, recall: 0.99, precision: 0.9901960784313726, f_beta: 0.98999899989999
train: step: 2890, loss: 0.04047798737883568, acc: 0.94, recall: 0.94, precision: 0.94, f_beta: 0.94
train: step: 2891, loss: 0.07017777115106583, acc: 0.9, recall: 0.9, precision: 0.9, f_beta: 0.9
train: step: 2892, loss: 0.024294063448905945, acc: 0.96, recall: 0.96, precision: 0.96, f_beta: 0.96
train: step: 2893, loss: 0.07453551888465881, acc: 0.9, recall: 0.9, precision: 0.9006410256410255, f_beta: 0.8999599839935974
train: step: 2894, loss: 0.03267691284418106, acc: 0.96, recall: 0.96, precision: 0.9607371794871795, f_beta: 0.959983993597439
train: step: 2895, loss: 0.06513431668281555, acc: 0.93, recall: 0.9299999999999999, precision: 0.9315535929345644, f_beta: 0.9299369432489241
train: step: 2896, loss: 0.06098604574799538, acc: 0.92, recall: 0.9199999999999999, precision: 0.9227053140096618, f_beta: 0.9198717948717948
train: step: 2897, loss: 0.033103689551353455, acc: 0.96, recall: 0.96, precision: 0.96, f_beta: 0.96
train: step: 2898, loss: 0.03600943088531494, acc: 0.95, recall: 0.95, precision: 0.9545454545454546, f_beta: 0.949874686716792
train: step: 2899, loss: 0.030877556651830673, acc: 0.94, recall: 0.94, precision: 0.9407051282051282, f_beta: 0.9399759903961584
train: step: 2900, loss: 0.04268636554479599, acc: 0.94, recall: 0.94, precision: 0.9407051282051282, f_beta: 0.9399759903961584
train: step: 2901, loss: 0.030075807124376297, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 2902, loss: 0.06174000725150108, acc: 0.92, recall: 0.92, precision: 0.92, f_beta: 0.92
train: step: 2903, loss: 0.035026878118515015, acc: 0.96, recall: 0.96, precision: 0.96, f_beta: 0.96
train: step: 2904, loss: 0.04121841862797737, acc: 0.93, recall: 0.9299999999999999, precision: 0.9343434343434343, f_beta: 0.9298245614035088
train: step: 2905, loss: 0.03618717938661575, acc: 0.96, recall: 0.96, precision: 0.962962962962963, f_beta: 0.9599358974358974
train: step: 2906, loss: 0.027293091639876366, acc: 0.96, recall: 0.96, precision: 0.962962962962963, f_beta: 0.9599358974358974
train: step: 2907, loss: 0.043998535722494125, acc: 0.94, recall: 0.94, precision: 0.9407051282051282, f_beta: 0.9399759903961584
train: step: 2908, loss: 0.03651511296629906, acc: 0.96, recall: 0.96, precision: 0.9607371794871795, f_beta: 0.959983993597439
train: step: 2909, loss: 0.02886665053665638, acc: 0.96, recall: 0.96, precision: 0.9607371794871795, f_beta: 0.959983993597439
train: step: 2910, loss: 0.05796797201037407, acc: 0.94, recall: 0.94, precision: 0.9464285714285714, f_beta: 0.939783219590526
train: step: 2911, loss: 0.04775337874889374, acc: 0.95, recall: 0.95, precision: 0.9501800720288115, f_beta: 0.9499949994999499
train: step: 2912, loss: 0.03603978082537651, acc: 0.96, recall: 0.96, precision: 0.9607371794871795, f_beta: 0.959983993597439
train: step: 2913, loss: 0.022412490099668503, acc: 0.97, recall: 0.97, precision: 0.9716981132075472, f_beta: 0.9699729756781104
train: step: 2914, loss: 0.047257013618946075, acc: 0.94, recall: 0.94, precision: 0.9428341384863124, f_beta: 0.9399038461538463
train: step: 2915, loss: 0.050072766840457916, acc: 0.93, recall: 0.9299999999999999, precision: 0.9315535929345644, f_beta: 0.9299369432489241
train: step: 2916, loss: 0.04772809147834778, acc: 0.96, recall: 0.96, precision: 0.9607371794871795, f_beta: 0.959983993597439
train: step: 2917, loss: 0.020104827359318733, acc: 0.98, recall: 0.98, precision: 0.98, f_beta: 0.98
train: step: 2918, loss: 0.022387148812413216, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 2919, loss: 0.06323850154876709, acc: 0.92, recall: 0.9199999999999999, precision: 0.9227053140096618, f_beta: 0.9198717948717948
train: step: 2920, loss: 0.04292818158864975, acc: 0.95, recall: 0.95, precision: 0.9501800720288115, f_beta: 0.9499949994999499
train: step: 2921, loss: 0.039969302713871, acc: 0.94, recall: 0.94, precision: 0.9428341384863124, f_beta: 0.9399038461538463
train: step: 2922, loss: 0.037926048040390015, acc: 0.96, recall: 0.96, precision: 0.9607371794871795, f_beta: 0.959983993597439
train: step: 2923, loss: 0.047365136444568634, acc: 0.95, recall: 0.95, precision: 0.9501800720288115, f_beta: 0.9499949994999499
train: step: 2924, loss: 0.03591414541006088, acc: 0.96, recall: 0.96, precision: 0.9607371794871795, f_beta: 0.959983993597439
train: step: 2925, loss: 0.022876529023051262, acc: 0.98, recall: 0.98, precision: 0.98, f_beta: 0.98
train: step: 2926, loss: 0.05500118434429169, acc: 0.93, recall: 0.9299999999999999, precision: 0.9343434343434343, f_beta: 0.9298245614035088
train: step: 2927, loss: 0.041233643889427185, acc: 0.96, recall: 0.96, precision: 0.96, f_beta: 0.96
train: step: 2928, loss: 0.03709451109170914, acc: 0.95, recall: 0.95, precision: 0.9501800720288115, f_beta: 0.9499949994999499
train: step: 2929, loss: 0.016540048643946648, acc: 0.98, recall: 0.98, precision: 0.98, f_beta: 0.98
train: step: 2930, loss: 0.03340211883187294, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 2931, loss: 0.061526108533144, acc: 0.92, recall: 0.9199999999999999, precision: 0.9261363636363636, f_beta: 0.9197109594540345
train: step: 2932, loss: 0.04977061599493027, acc: 0.94, recall: 0.94, precision: 0.94, f_beta: 0.94
train: step: 2933, loss: 0.025511741638183594, acc: 0.97, recall: 0.97, precision: 0.9716981132075472, f_beta: 0.9699729756781104
train: step: 2934, loss: 0.04767666012048721, acc: 0.94, recall: 0.94, precision: 0.9428341384863124, f_beta: 0.9399038461538463
train: step: 2935, loss: 0.04070796072483063, acc: 0.95, recall: 0.95, precision: 0.9516258530710557, f_beta: 0.949954959463517
train: step: 2936, loss: 0.027727827429771423, acc: 0.96, recall: 0.96, precision: 0.96, f_beta: 0.96
train: step: 2937, loss: 0.03516355901956558, acc: 0.96, recall: 0.96, precision: 0.962962962962963, f_beta: 0.9599358974358974
train: step: 2938, loss: 0.04886942729353905, acc: 0.94, recall: 0.94, precision: 0.9428341384863124, f_beta: 0.9399038461538463
train: step: 2939, loss: 0.022503381595015526, acc: 0.98, recall: 0.98, precision: 0.98, f_beta: 0.98
train: step: 2940, loss: 0.01781340315937996, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 2941, loss: 0.05064209923148155, acc: 0.93, recall: 0.9299999999999999, precision: 0.9315535929345644, f_beta: 0.9299369432489241
train: step: 2942, loss: 0.01927911676466465, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 2943, loss: 0.03937249258160591, acc: 0.95, recall: 0.95, precision: 0.9516258530710557, f_beta: 0.949954959463517
train: step: 2944, loss: 0.02660461887717247, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 2945, loss: 0.029110513627529144, acc: 0.96, recall: 0.96, precision: 0.9607371794871795, f_beta: 0.959983993597439
train: step: 2946, loss: 0.03298408165574074, acc: 0.95, recall: 0.95, precision: 0.9516258530710557, f_beta: 0.949954959463517
train: step: 2947, loss: 0.030177569016814232, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 2948, loss: 0.03267226740717888, acc: 0.96, recall: 0.96, precision: 0.9607371794871795, f_beta: 0.959983993597439
train: step: 2949, loss: 0.024457406252622604, acc: 0.96, recall: 0.96, precision: 0.96, f_beta: 0.96
train: step: 2950, loss: 0.03293275833129883, acc: 0.96, recall: 0.96, precision: 0.962962962962963, f_beta: 0.9599358974358974
train: step: 2951, loss: 0.06658586114645004, acc: 0.92, recall: 0.9199999999999999, precision: 0.9206730769230769, f_beta: 0.919967987194878
train: step: 2952, loss: 0.03496430814266205, acc: 0.95, recall: 0.95, precision: 0.9516258530710557, f_beta: 0.949954959463517
train: step: 2953, loss: 0.023285480216145515, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 2954, loss: 0.052897654473781586, acc: 0.93, recall: 0.9299999999999999, precision: 0.930172068827531, f_beta: 0.9299929992999301
train: step: 2955, loss: 0.016321344301104546, acc: 0.98, recall: 0.98, precision: 0.9807692307692308, f_beta: 0.9799919967987194
train: step: 2956, loss: 0.07250858098268509, acc: 0.9, recall: 0.9, precision: 0.9006410256410255, f_beta: 0.8999599839935974
train: step: 2957, loss: 0.03281848505139351, acc: 0.95, recall: 0.95, precision: 0.9516258530710557, f_beta: 0.949954959463517
train: step: 2958, loss: 0.03678293898701668, acc: 0.95, recall: 0.95, precision: 0.9545454545454546, f_beta: 0.949874686716792
train: step: 2959, loss: 0.03890686854720116, acc: 0.95, recall: 0.95, precision: 0.9516258530710557, f_beta: 0.949954959463517
train: step: 2960, loss: 0.023979611694812775, acc: 0.98, recall: 0.98, precision: 0.98, f_beta: 0.98
train: step: 2961, loss: 0.031108703464269638, acc: 0.96, recall: 0.96, precision: 0.9607371794871795, f_beta: 0.959983993597439
train: step: 2962, loss: 0.0528554767370224, acc: 0.94, recall: 0.94, precision: 0.9428341384863124, f_beta: 0.9399038461538463
train: step: 2963, loss: 0.05297607555985451, acc: 0.93, recall: 0.9299999999999999, precision: 0.9315535929345644, f_beta: 0.9299369432489241
train: step: 2964, loss: 0.04861120134592056, acc: 0.93, recall: 0.9299999999999999, precision: 0.930172068827531, f_beta: 0.9299929992999301
train: step: 2965, loss: 0.07818274199962616, acc: 0.89, recall: 0.89, precision: 0.8914090726615818, f_beta: 0.8899009108197378
train: step: 2966, loss: 0.054136790335178375, acc: 0.91, recall: 0.91, precision: 0.9101640656262505, f_beta: 0.90999099909991
train: step: 2967, loss: 0.01864311471581459, acc: 0.98, recall: 0.98, precision: 0.98, f_beta: 0.98
train: step: 2968, loss: 0.05456387624144554, acc: 0.93, recall: 0.9299999999999999, precision: 0.930172068827531, f_beta: 0.9299929992999301
train: step: 2969, loss: 0.027130722999572754, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 2970, loss: 0.03867482766509056, acc: 0.94, recall: 0.94, precision: 0.9407051282051282, f_beta: 0.9399759903961584
train: step: 2971, loss: 0.023067224770784378, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 2972, loss: 0.04745030403137207, acc: 0.95, recall: 0.95, precision: 0.9501800720288115, f_beta: 0.9499949994999499
train: step: 2973, loss: 0.043219584971666336, acc: 0.94, recall: 0.94, precision: 0.9428341384863124, f_beta: 0.9399038461538463
train: step: 2974, loss: 0.03075697459280491, acc: 0.96, recall: 0.96, precision: 0.9607371794871795, f_beta: 0.959983993597439
train: step: 2975, loss: 0.03663201257586479, acc: 0.95, recall: 0.95, precision: 0.9501800720288115, f_beta: 0.9499949994999499
train: step: 2976, loss: 0.025363683700561523, acc: 0.98, recall: 0.98, precision: 0.98, f_beta: 0.98
train: step: 2977, loss: 0.0057612149976193905, acc: 1.0, recall: 1.0, precision: 1.0, f_beta: 1.0
train: step: 2978, loss: 0.046869486570358276, acc: 0.93, recall: 0.9299999999999999, precision: 0.9315535929345644, f_beta: 0.9299369432489241
train: step: 2979, loss: 0.023396536707878113, acc: 0.96, recall: 0.96, precision: 0.9607371794871795, f_beta: 0.959983993597439
train: step: 2980, loss: 0.03928627073764801, acc: 0.94, recall: 0.94, precision: 0.9407051282051282, f_beta: 0.9399759903961584
train: step: 2981, loss: 0.010573265142738819, acc: 0.98, recall: 0.98, precision: 0.9807692307692308, f_beta: 0.9799919967987194
train: step: 2982, loss: 0.023589856922626495, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 2983, loss: 0.0326191671192646, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 2984, loss: 0.04065992310643196, acc: 0.96, recall: 0.96, precision: 0.9607371794871795, f_beta: 0.959983993597439
train: step: 2985, loss: 0.047320302575826645, acc: 0.94, recall: 0.94, precision: 0.9407051282051282, f_beta: 0.9399759903961584
train: step: 2986, loss: 0.05141963064670563, acc: 0.94, recall: 0.94, precision: 0.9428341384863124, f_beta: 0.9399038461538463
train: step: 2987, loss: 0.023392483592033386, acc: 0.96, recall: 0.96, precision: 0.96, f_beta: 0.96
train: step: 2988, loss: 0.0398673452436924, acc: 0.94, recall: 0.94, precision: 0.9428341384863124, f_beta: 0.9399038461538463
train: step: 2989, loss: 0.07903940975666046, acc: 0.91, recall: 0.91, precision: 0.9101640656262505, f_beta: 0.90999099909991
train: step: 2990, loss: 0.0682247132062912, acc: 0.92, recall: 0.9199999999999999, precision: 0.9206730769230769, f_beta: 0.919967987194878
train: step: 2991, loss: 0.031825970858335495, acc: 0.96, recall: 0.96, precision: 0.96, f_beta: 0.96
train: step: 2992, loss: 0.009685097262263298, acc: 0.98, recall: 0.98, precision: 0.9807692307692308, f_beta: 0.9799919967987194
train: step: 2993, loss: 0.011479527689516544, acc: 0.98, recall: 0.98, precision: 0.9807692307692308, f_beta: 0.9799919967987194
train: step: 2994, loss: 0.051038339734077454, acc: 0.93, recall: 0.9299999999999999, precision: 0.930172068827531, f_beta: 0.9299929992999301
train: step: 2995, loss: 0.057107362896203995, acc: 0.93, recall: 0.9299999999999999, precision: 0.930172068827531, f_beta: 0.9299929992999301
train: step: 2996, loss: 0.05631507933139801, acc: 0.93, recall: 0.9299999999999999, precision: 0.9343434343434343, f_beta: 0.9298245614035088
train: step: 2997, loss: 0.015388276427984238, acc: 0.98, recall: 0.98, precision: 0.98, f_beta: 0.98
train: step: 2998, loss: 0.018020179122686386, acc: 0.98, recall: 0.98, precision: 0.98, f_beta: 0.98
train: step: 2999, loss: 0.019842181354761124, acc: 0.98, recall: 0.98, precision: 0.9807692307692308, f_beta: 0.9799919967987194
train: step: 3000, loss: 0.024572281166911125, acc: 0.98, recall: 0.98, precision: 0.9807692307692308, f_beta: 0.9799919967987194
train: step: 3001, loss: 0.03604121133685112, acc: 0.96, recall: 0.96, precision: 0.96, f_beta: 0.96
train: step: 3002, loss: 0.02170957624912262, acc: 0.98, recall: 0.98, precision: 0.9807692307692308, f_beta: 0.9799919967987194
train: step: 3003, loss: 0.07868499308824539, acc: 0.89, recall: 0.8899999999999999, precision: 0.8939393939393939, f_beta: 0.8897243107769424
train: step: 3004, loss: 0.05817046016454697, acc: 0.93, recall: 0.9299999999999999, precision: 0.930172068827531, f_beta: 0.9299929992999301
train: step: 3005, loss: 0.019876711070537567, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 3006, loss: 0.007583437487483025, acc: 0.99, recall: 0.99, precision: 0.9901960784313726, f_beta: 0.98999899989999
train: step: 3007, loss: 0.04912211000919342, acc: 0.94, recall: 0.94, precision: 0.94, f_beta: 0.94
train: step: 3008, loss: 0.035572923719882965, acc: 0.96, recall: 0.96, precision: 0.9607371794871795, f_beta: 0.959983993597439
train: step: 3009, loss: 0.03540858253836632, acc: 0.96, recall: 0.96, precision: 0.962962962962963, f_beta: 0.9599358974358974
train: step: 3010, loss: 0.014925654046237469, acc: 0.98, recall: 0.98, precision: 0.9807692307692308, f_beta: 0.9799919967987194
train: step: 3011, loss: 0.0333288200199604, acc: 0.97, recall: 0.97, precision: 0.9716981132075472, f_beta: 0.9699729756781104
train: step: 3012, loss: 0.00923951156437397, acc: 0.98, recall: 0.98, precision: 0.98, f_beta: 0.98
train: step: 3013, loss: 0.024361008778214455, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 3014, loss: 0.020434539765119553, acc: 0.98, recall: 0.98, precision: 0.9807692307692308, f_beta: 0.9799919967987194
train: step: 3015, loss: 0.049298547208309174, acc: 0.93, recall: 0.9299999999999999, precision: 0.9315535929345644, f_beta: 0.9299369432489241
train: step: 3016, loss: 0.0550263486802578, acc: 0.92, recall: 0.9199999999999999, precision: 0.9261363636363636, f_beta: 0.9197109594540345
train: step: 3017, loss: 0.04268346726894379, acc: 0.96, recall: 0.96, precision: 0.9607371794871795, f_beta: 0.959983993597439
train: step: 3018, loss: 0.017394891008734703, acc: 0.99, recall: 0.99, precision: 0.9901960784313726, f_beta: 0.98999899989999
train: step: 3019, loss: 0.0352603979408741, acc: 0.94, recall: 0.94, precision: 0.94, f_beta: 0.94
train: step: 3020, loss: 0.02715284749865532, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 3021, loss: 0.024685358628630638, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 3022, loss: 0.04769464582204819, acc: 0.94, recall: 0.94, precision: 0.9407051282051282, f_beta: 0.9399759903961584
train: step: 3023, loss: 0.0006471119704656303, acc: 1.0, recall: 1.0, precision: 1.0, f_beta: 1.0
train: step: 3024, loss: 0.024700775742530823, acc: 0.93, recall: 0.9299999999999999, precision: 0.9343434343434343, f_beta: 0.9298245614035088
train: step: 3025, loss: 0.033875200897455215, acc: 0.96, recall: 0.96, precision: 0.9607371794871795, f_beta: 0.959983993597439
train: step: 3026, loss: 0.01750360056757927, acc: 0.99, recall: 0.99, precision: 0.9901960784313726, f_beta: 0.98999899989999
train: step: 3027, loss: 0.008715126663446426, acc: 0.99, recall: 0.99, precision: 0.9901960784313726, f_beta: 0.98999899989999
train: step: 3028, loss: 0.038235682994127274, acc: 0.96, recall: 0.96, precision: 0.9607371794871795, f_beta: 0.959983993597439
train: step: 3029, loss: 0.04731278121471405, acc: 0.93, recall: 0.9299999999999999, precision: 0.930172068827531, f_beta: 0.9299929992999301
train: step: 3030, loss: 0.03046141192317009, acc: 0.96, recall: 0.96, precision: 0.9607371794871795, f_beta: 0.959983993597439
train: step: 3031, loss: 0.025765223428606987, acc: 0.98, recall: 0.98, precision: 0.9807692307692308, f_beta: 0.9799919967987194
train: step: 3032, loss: 0.04182426631450653, acc: 0.95, recall: 0.95, precision: 0.9516258530710557, f_beta: 0.949954959463517
train: step: 3033, loss: 0.030223295092582703, acc: 0.98, recall: 0.98, precision: 0.98, f_beta: 0.98
train: step: 3034, loss: 0.07974819093942642, acc: 0.91, recall: 0.9099999999999999, precision: 0.9141414141414141, f_beta: 0.9097744360902256
train: step: 3035, loss: 0.06709448993206024, acc: 0.94, recall: 0.94, precision: 0.9428341384863124, f_beta: 0.9399038461538463
train: step: 3036, loss: 0.016693061217665672, acc: 0.98, recall: 0.98, precision: 0.9807692307692308, f_beta: 0.9799919967987194
train: step: 3037, loss: 0.03894426301121712, acc: 0.95, recall: 0.95, precision: 0.9516258530710557, f_beta: 0.949954959463517
train: step: 3038, loss: 0.04571155458688736, acc: 0.95, recall: 0.95, precision: 0.9545454545454546, f_beta: 0.949874686716792
train: step: 3039, loss: 0.07395344227552414, acc: 0.91, recall: 0.91, precision: 0.9101640656262505, f_beta: 0.90999099909991
train: step: 3040, loss: 0.031884144991636276, acc: 0.96, recall: 0.96, precision: 0.9607371794871795, f_beta: 0.959983993597439
train: step: 3041, loss: 0.03455589339137077, acc: 0.96, recall: 0.96, precision: 0.962962962962963, f_beta: 0.9599358974358974
train: step: 3042, loss: 0.07341615110635757, acc: 0.89, recall: 0.89, precision: 0.8914090726615818, f_beta: 0.8899009108197378
train: step: 3043, loss: 0.040544234216213226, acc: 0.94, recall: 0.94, precision: 0.9428341384863124, f_beta: 0.9399038461538463
train: step: 3044, loss: 0.01902643032371998, acc: 0.98, recall: 0.98, precision: 0.9807692307692308, f_beta: 0.9799919967987194
train: step: 3045, loss: 0.05591262876987457, acc: 0.94, recall: 0.94, precision: 0.9464285714285714, f_beta: 0.939783219590526
train: step: 3046, loss: 0.02906852215528488, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 3047, loss: 0.07922857999801636, acc: 0.89, recall: 0.89, precision: 0.8914090726615818, f_beta: 0.8899009108197378
train: step: 3048, loss: 0.018274221569299698, acc: 0.98, recall: 0.98, precision: 0.9807692307692308, f_beta: 0.9799919967987194
train: step: 3049, loss: 0.03354479372501373, acc: 0.96, recall: 0.96, precision: 0.9607371794871795, f_beta: 0.959983993597439
train: step: 3050, loss: 0.03786768019199371, acc: 0.94, recall: 0.94, precision: 0.9428341384863124, f_beta: 0.9399038461538463
train: step: 3051, loss: 0.04703314229846001, acc: 0.94, recall: 0.94, precision: 0.94, f_beta: 0.94
train: step: 3052, loss: 0.02739212103188038, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 3053, loss: 0.04141666367650032, acc: 0.96, recall: 0.96, precision: 0.96, f_beta: 0.96
train: step: 3054, loss: 0.02436154894530773, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 3055, loss: 0.01911800727248192, acc: 0.99, recall: 0.99, precision: 0.9901960784313726, f_beta: 0.98999899989999
train: step: 3056, loss: 0.010263213887810707, acc: 0.99, recall: 0.99, precision: 0.9901960784313726, f_beta: 0.98999899989999
train: step: 3057, loss: 0.030277764424681664, acc: 0.94, recall: 0.94, precision: 0.9464285714285714, f_beta: 0.939783219590526
train: step: 3058, loss: 0.0639149397611618, acc: 0.92, recall: 0.9199999999999999, precision: 0.9206730769230769, f_beta: 0.919967987194878
train: step: 3059, loss: 0.025166722014546394, acc: 0.97, recall: 0.97, precision: 0.9716981132075472, f_beta: 0.9699729756781104
train: step: 3060, loss: 0.04007953777909279, acc: 0.95, recall: 0.95, precision: 0.9516258530710557, f_beta: 0.949954959463517
train: step: 3061, loss: 0.019041921943426132, acc: 0.98, recall: 0.98, precision: 0.98, f_beta: 0.98
train: step: 3062, loss: 0.03783322498202324, acc: 0.96, recall: 0.96, precision: 0.9607371794871795, f_beta: 0.959983993597439
train: step: 3063, loss: 0.028060035780072212, acc: 0.95, recall: 0.95, precision: 0.9501800720288115, f_beta: 0.9499949994999499
train: step: 3064, loss: 0.032166022807359695, acc: 0.97, recall: 0.97, precision: 0.9716981132075472, f_beta: 0.9699729756781104
train: step: 3065, loss: 0.025329751893877983, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 3066, loss: 0.014754516072571278, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 3067, loss: 0.02542419731616974, acc: 0.98, recall: 0.98, precision: 0.98, f_beta: 0.98
train: step: 3068, loss: 0.018752053380012512, acc: 0.98, recall: 0.98, precision: 0.98, f_beta: 0.98
train: step: 3069, loss: 0.028018584474921227, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 3070, loss: 0.03205499425530434, acc: 0.96, recall: 0.96, precision: 0.9607371794871795, f_beta: 0.959983993597439
train: step: 3071, loss: 0.03765782713890076, acc: 0.96, recall: 0.96, precision: 0.962962962962963, f_beta: 0.9599358974358974
train: step: 3072, loss: 0.03727148100733757, acc: 0.95, recall: 0.95, precision: 0.9501800720288115, f_beta: 0.9499949994999499
train: step: 3073, loss: 0.043427370488643646, acc: 0.95, recall: 0.95, precision: 0.9516258530710557, f_beta: 0.949954959463517
train: step: 3074, loss: 0.04122236371040344, acc: 0.96, recall: 0.96, precision: 0.9607371794871795, f_beta: 0.959983993597439
train: step: 3075, loss: 0.028191978111863136, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 3076, loss: 0.05892566218972206, acc: 0.93, recall: 0.9299999999999999, precision: 0.9343434343434343, f_beta: 0.9298245614035088
train: step: 3077, loss: 0.020430469885468483, acc: 0.98, recall: 0.98, precision: 0.9807692307692308, f_beta: 0.9799919967987194
train: step: 3078, loss: 0.03943904489278793, acc: 0.95, recall: 0.95, precision: 0.9501800720288115, f_beta: 0.9499949994999499
train: step: 3079, loss: 0.040164947509765625, acc: 0.94, recall: 0.94, precision: 0.94, f_beta: 0.94
train: step: 3080, loss: 0.04117007181048393, acc: 0.94, recall: 0.94, precision: 0.9407051282051282, f_beta: 0.9399759903961584
train: step: 3081, loss: 0.03472026810050011, acc: 0.96, recall: 0.96, precision: 0.96, f_beta: 0.96
train: step: 3082, loss: 0.061925336718559265, acc: 0.94, recall: 0.94, precision: 0.9428341384863124, f_beta: 0.9399038461538463
train: step: 3083, loss: 0.04269365221261978, acc: 0.95, recall: 0.95, precision: 0.9516258530710557, f_beta: 0.949954959463517
train: step: 3084, loss: 0.03129037097096443, acc: 0.95, recall: 0.95, precision: 0.9516258530710557, f_beta: 0.949954959463517
train: step: 3085, loss: 0.03193191438913345, acc: 0.94, recall: 0.94, precision: 0.9428341384863124, f_beta: 0.9399038461538463
train: step: 3086, loss: 0.04294298589229584, acc: 0.95, recall: 0.95, precision: 0.9501800720288115, f_beta: 0.9499949994999499
train: step: 3087, loss: 0.025412803515791893, acc: 0.98, recall: 0.98, precision: 0.9807692307692308, f_beta: 0.9799919967987194
train: step: 3088, loss: 0.02708054706454277, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 3089, loss: 0.01757456175982952, acc: 0.98, recall: 0.98, precision: 0.9807692307692308, f_beta: 0.9799919967987194
train: step: 3090, loss: 0.012408928945660591, acc: 0.99, recall: 0.99, precision: 0.9901960784313726, f_beta: 0.98999899989999
train: step: 3091, loss: 0.05693470314145088, acc: 0.92, recall: 0.9199999999999999, precision: 0.9310344827586207, f_beta: 0.9194847020933978
train: step: 3092, loss: 0.06268052011728287, acc: 0.92, recall: 0.9199999999999999, precision: 0.9206730769230769, f_beta: 0.919967987194878
train: step: 3093, loss: 0.059798240661621094, acc: 0.93, recall: 0.9299999999999999, precision: 0.930172068827531, f_beta: 0.9299929992999301
train: step: 3094, loss: 0.02417762018740177, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 3095, loss: 0.030861899256706238, acc: 0.97, recall: 0.97, precision: 0.9716981132075472, f_beta: 0.9699729756781104
train: step: 3096, loss: 0.018825171515345573, acc: 0.99, recall: 0.99, precision: 0.9901960784313726, f_beta: 0.98999899989999
train: step: 3097, loss: 0.027800969779491425, acc: 0.98, recall: 0.98, precision: 0.9807692307692308, f_beta: 0.9799919967987194
train: step: 3098, loss: 0.0275984238833189, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 3099, loss: 0.03852725401520729, acc: 0.96, recall: 0.96, precision: 0.962962962962963, f_beta: 0.9599358974358974
train: step: 3100, loss: 0.08018521964550018, acc: 0.9, recall: 0.9, precision: 0.9006410256410255, f_beta: 0.8999599839935974
train: step: 3101, loss: 0.0315188467502594, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 3102, loss: 0.05154284089803696, acc: 0.95, recall: 0.95, precision: 0.9516258530710557, f_beta: 0.949954959463517
train: step: 3103, loss: 0.05175406113266945, acc: 0.92, recall: 0.9199999999999999, precision: 0.9206730769230769, f_beta: 0.919967987194878
train: step: 3104, loss: 0.03307915851473808, acc: 0.95, recall: 0.95, precision: 0.9501800720288115, f_beta: 0.9499949994999499
train: step: 3105, loss: 0.011646684259176254, acc: 0.99, recall: 0.99, precision: 0.9901960784313726, f_beta: 0.98999899989999
train: step: 3106, loss: 0.050197288393974304, acc: 0.95, recall: 0.95, precision: 0.9501800720288115, f_beta: 0.9499949994999499
train: step: 3107, loss: 0.04577670618891716, acc: 0.95, recall: 0.95, precision: 0.9545454545454546, f_beta: 0.949874686716792
train: step: 3108, loss: 0.012910720892250538, acc: 0.99, recall: 0.99, precision: 0.9901960784313726, f_beta: 0.98999899989999
train: step: 3109, loss: 0.048264142125844955, acc: 0.95, recall: 0.95, precision: 0.9501800720288115, f_beta: 0.9499949994999499
train: step: 3110, loss: 0.04455149546265602, acc: 0.95, recall: 0.95, precision: 0.9501800720288115, f_beta: 0.9499949994999499
train: step: 3111, loss: 0.010671574622392654, acc: 0.98, recall: 0.98, precision: 0.9807692307692308, f_beta: 0.9799919967987194
train: step: 3112, loss: 0.011741932481527328, acc: 0.99, recall: 0.99, precision: 0.9901960784313726, f_beta: 0.98999899989999
train: step: 3113, loss: 0.031069502234458923, acc: 0.96, recall: 0.96, precision: 0.962962962962963, f_beta: 0.9599358974358974
train: step: 3114, loss: 0.09726735949516296, acc: 0.86, recall: 0.86, precision: 0.8605769230769231, f_beta: 0.8599439775910365
train: step: 3115, loss: 0.04345237836241722, acc: 0.96, recall: 0.96, precision: 0.96, f_beta: 0.96
train: step: 3116, loss: 0.027789516374468803, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 3117, loss: 0.052487391978502274, acc: 0.93, recall: 0.9299999999999999, precision: 0.9343434343434343, f_beta: 0.9298245614035088
train: step: 3118, loss: 0.020400645211338997, acc: 0.97, recall: 0.97, precision: 0.9716981132075472, f_beta: 0.9699729756781104
train: step: 3119, loss: 0.03041096031665802, acc: 0.96, recall: 0.96, precision: 0.96, f_beta: 0.96
train: step: 3120, loss: 0.036033015698194504, acc: 0.96, recall: 0.96, precision: 0.96, f_beta: 0.96
train: step: 3121, loss: 0.03015710413455963, acc: 0.96, recall: 0.96, precision: 0.9607371794871795, f_beta: 0.959983993597439
train: step: 3122, loss: 0.01708108000457287, acc: 0.99, recall: 0.99, precision: 0.9901960784313726, f_beta: 0.98999899989999
train: step: 3123, loss: 0.06950272619724274, acc: 0.91, recall: 0.91, precision: 0.9101640656262505, f_beta: 0.90999099909991
train: step: 3124, loss: 0.03271585330367088, acc: 0.96, recall: 0.96, precision: 0.9607371794871795, f_beta: 0.959983993597439
train: step: 3125, loss: 0.04017828404903412, acc: 0.96, recall: 0.96, precision: 0.96, f_beta: 0.96
train: step: 3126, loss: 0.04186245799064636, acc: 0.96, recall: 0.96, precision: 0.96, f_beta: 0.96
train: step: 3127, loss: 0.0389791764318943, acc: 0.95, recall: 0.95, precision: 0.9516258530710557, f_beta: 0.949954959463517
train: step: 3128, loss: 0.01735566183924675, acc: 0.97, recall: 0.97, precision: 0.9716981132075472, f_beta: 0.9699729756781104
train: step: 3129, loss: 0.04764655977487564, acc: 0.94, recall: 0.94, precision: 0.94, f_beta: 0.94
train: step: 3130, loss: 0.058065108954906464, acc: 0.93, recall: 0.9299999999999999, precision: 0.9315535929345644, f_beta: 0.9299369432489241
train: step: 3131, loss: 0.014136988669633865, acc: 0.98, recall: 0.98, precision: 0.98, f_beta: 0.98
train: step: 3132, loss: 0.023101791739463806, acc: 0.98, recall: 0.98, precision: 0.9807692307692308, f_beta: 0.9799919967987194
train: step: 3133, loss: 0.03527691960334778, acc: 0.96, recall: 0.96, precision: 0.962962962962963, f_beta: 0.9599358974358974
train: step: 3134, loss: 0.05413954704999924, acc: 0.94, recall: 0.94, precision: 0.9407051282051282, f_beta: 0.9399759903961584
train: step: 3135, loss: 0.044492945075035095, acc: 0.95, recall: 0.95, precision: 0.9516258530710557, f_beta: 0.949954959463517
train: step: 3136, loss: 0.006624582223594189, acc: 0.99, recall: 0.99, precision: 0.9901960784313726, f_beta: 0.98999899989999
train: step: 3137, loss: 0.008566337637603283, acc: 0.99, recall: 0.99, precision: 0.9901960784313726, f_beta: 0.98999899989999
train: step: 3138, loss: 0.02936691790819168, acc: 0.96, recall: 0.96, precision: 0.9607371794871795, f_beta: 0.959983993597439
train: step: 3139, loss: 0.020722318440675735, acc: 0.98, recall: 0.98, precision: 0.9807692307692308, f_beta: 0.9799919967987194
train: step: 3140, loss: 0.04382137209177017, acc: 0.95, recall: 0.95, precision: 0.9501800720288115, f_beta: 0.9499949994999499
train: step: 3141, loss: 0.047053128480911255, acc: 0.94, recall: 0.94, precision: 0.94, f_beta: 0.94
train: step: 3142, loss: 0.027577484026551247, acc: 0.96, recall: 0.96, precision: 0.96, f_beta: 0.96
train: step: 3143, loss: 0.02692541666328907, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 3144, loss: 0.04988769441843033, acc: 0.93, recall: 0.9299999999999999, precision: 0.9315535929345644, f_beta: 0.9299369432489241
train: step: 3145, loss: 0.034559085965156555, acc: 0.96, recall: 0.96, precision: 0.96, f_beta: 0.96
train: step: 3146, loss: 0.029096942394971848, acc: 0.95, recall: 0.95, precision: 0.9545454545454546, f_beta: 0.949874686716792
train: step: 3147, loss: 0.012931480072438717, acc: 0.99, recall: 0.99, precision: 0.9901960784313726, f_beta: 0.98999899989999
train: step: 3148, loss: 0.021857943385839462, acc: 0.98, recall: 0.98, precision: 0.98, f_beta: 0.98
train: step: 3149, loss: 0.02797776274383068, acc: 0.96, recall: 0.96, precision: 0.9607371794871795, f_beta: 0.959983993597439
train: step: 3150, loss: 0.018166951835155487, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 3151, loss: 0.02867698110640049, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 3152, loss: 0.024966411292552948, acc: 0.97, recall: 0.97, precision: 0.9716981132075472, f_beta: 0.9699729756781104
train: step: 3153, loss: 0.010126734152436256, acc: 0.99, recall: 0.99, precision: 0.9901960784313726, f_beta: 0.98999899989999
train: step: 3154, loss: 0.043786171823740005, acc: 0.94, recall: 0.94, precision: 0.94, f_beta: 0.94
train: step: 3155, loss: 0.07172364741563797, acc: 0.9, recall: 0.8999999999999999, precision: 0.9025764895330113, f_beta: 0.8998397435897436
train: step: 3156, loss: 0.05399122089147568, acc: 0.93, recall: 0.9299999999999999, precision: 0.9315535929345644, f_beta: 0.9299369432489241
train: step: 3157, loss: 0.030197106301784515, acc: 0.96, recall: 0.96, precision: 0.9607371794871795, f_beta: 0.959983993597439
train: step: 3158, loss: 0.029594413936138153, acc: 0.96, recall: 0.96, precision: 0.9607371794871795, f_beta: 0.959983993597439
train: step: 3159, loss: 0.014615454711019993, acc: 0.98, recall: 0.98, precision: 0.98, f_beta: 0.98
train: step: 3160, loss: 0.02075905352830887, acc: 0.96, recall: 0.96, precision: 0.962962962962963, f_beta: 0.9599358974358974
train: step: 3161, loss: 0.047338321805000305, acc: 0.95, recall: 0.95, precision: 0.9501800720288115, f_beta: 0.9499949994999499
train: step: 3162, loss: 0.04994919151067734, acc: 0.94, recall: 0.94, precision: 0.94, f_beta: 0.94
train: step: 3163, loss: 0.034672196954488754, acc: 0.96, recall: 0.96, precision: 0.9607371794871795, f_beta: 0.959983993597439
train: step: 3164, loss: 0.07681699097156525, acc: 0.91, recall: 0.9099999999999999, precision: 0.9141414141414141, f_beta: 0.9097744360902256
train: step: 3165, loss: 0.04654325544834137, acc: 0.95, recall: 0.95, precision: 0.9516258530710557, f_beta: 0.949954959463517
train: step: 3166, loss: 0.030673939734697342, acc: 0.95, recall: 0.95, precision: 0.9516258530710557, f_beta: 0.949954959463517
train: step: 3167, loss: 0.05115249752998352, acc: 0.93, recall: 0.9299999999999999, precision: 0.9315535929345644, f_beta: 0.9299369432489241
train: step: 3168, loss: 0.07134106755256653, acc: 0.92, recall: 0.9199999999999999, precision: 0.9206730769230769, f_beta: 0.919967987194878
train: step: 3169, loss: 0.04932723939418793, acc: 0.92, recall: 0.9199999999999999, precision: 0.9206730769230769, f_beta: 0.919967987194878
train: step: 3170, loss: 0.034407779574394226, acc: 0.96, recall: 0.96, precision: 0.9607371794871795, f_beta: 0.959983993597439
train: step: 3171, loss: 0.024074144661426544, acc: 0.96, recall: 0.96, precision: 0.9607371794871795, f_beta: 0.959983993597439
train: step: 3172, loss: 0.030928468331694603, acc: 0.96, recall: 0.96, precision: 0.9607371794871795, f_beta: 0.959983993597439
train: step: 3173, loss: 0.03622252866625786, acc: 0.96, recall: 0.96, precision: 0.96, f_beta: 0.96
train: step: 3174, loss: 0.04255460575222969, acc: 0.94, recall: 0.94, precision: 0.9428341384863124, f_beta: 0.9399038461538463
train: step: 3175, loss: 0.018081331625580788, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 3176, loss: 0.07105736434459686, acc: 0.91, recall: 0.9099999999999999, precision: 0.9114813327980731, f_beta: 0.9099189270343309
train: step: 3177, loss: 0.04655143618583679, acc: 0.93, recall: 0.9299999999999999, precision: 0.930172068827531, f_beta: 0.9299929992999301
train: step: 3178, loss: 0.0649230033159256, acc: 0.93, recall: 0.9299999999999999, precision: 0.9343434343434343, f_beta: 0.9298245614035088
train: step: 3179, loss: 0.05585401505231857, acc: 0.93, recall: 0.9299999999999999, precision: 0.9315535929345644, f_beta: 0.9299369432489241
train: step: 3180, loss: 0.041903313249349594, acc: 0.93, recall: 0.9299999999999999, precision: 0.9315535929345644, f_beta: 0.9299369432489241
train: step: 3181, loss: 0.037232495844364166, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 3182, loss: 0.0336872935295105, acc: 0.96, recall: 0.96, precision: 0.96, f_beta: 0.96
train: step: 3183, loss: 0.06663195788860321, acc: 0.89, recall: 0.8899999999999999, precision: 0.8939393939393939, f_beta: 0.8897243107769424
train: step: 3184, loss: 0.02403780072927475, acc: 0.98, recall: 0.98, precision: 0.9807692307692308, f_beta: 0.9799919967987194
train: step: 3185, loss: 0.05586104467511177, acc: 0.91, recall: 0.9099999999999999, precision: 0.9181966544267646, f_beta: 0.9095568284594513
train: step: 3186, loss: 0.03633883595466614, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 3187, loss: 0.04113726690411568, acc: 0.96, recall: 0.96, precision: 0.96, f_beta: 0.96
train: step: 3188, loss: 0.05690949782729149, acc: 0.94, recall: 0.94, precision: 0.9407051282051282, f_beta: 0.9399759903961584
train: step: 3189, loss: 0.031130172312259674, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 3190, loss: 0.059474505484104156, acc: 0.91, recall: 0.9099999999999999, precision: 0.9181966544267646, f_beta: 0.9095568284594513
train: step: 3191, loss: 0.04716436564922333, acc: 0.95, recall: 0.95, precision: 0.9501800720288115, f_beta: 0.9499949994999499
train: step: 3192, loss: 0.0358797125518322, acc: 0.96, recall: 0.96, precision: 0.9607371794871795, f_beta: 0.959983993597439
train: step: 3193, loss: 0.05310428515076637, acc: 0.92, recall: 0.9199999999999999, precision: 0.9261363636363636, f_beta: 0.9197109594540345
train: step: 3194, loss: 0.054875001311302185, acc: 0.91, recall: 0.91, precision: 0.9101640656262505, f_beta: 0.90999099909991
train: step: 3195, loss: 0.030441690236330032, acc: 0.96, recall: 0.96, precision: 0.9607371794871795, f_beta: 0.959983993597439
train: step: 3196, loss: 0.005134442821145058, acc: 1.0, recall: 1.0, precision: 1.0, f_beta: 1.0
train: step: 3197, loss: 0.03312917798757553, acc: 0.96, recall: 0.96, precision: 0.9607371794871795, f_beta: 0.959983993597439
train: step: 3198, loss: 0.022474879398941994, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 3199, loss: 0.0625266432762146, acc: 0.94, recall: 0.94, precision: 0.9407051282051282, f_beta: 0.9399759903961584
train: step: 3200, loss: 0.010943233966827393, acc: 0.99, recall: 0.99, precision: 0.9901960784313726, f_beta: 0.98999899989999
train: step: 3201, loss: 0.02256881631910801, acc: 0.98, recall: 0.98, precision: 0.98, f_beta: 0.98
train: step: 3202, loss: 0.05432431772351265, acc: 0.93, recall: 0.9299999999999999, precision: 0.9343434343434343, f_beta: 0.9298245614035088
train: step: 3203, loss: 0.022378914058208466, acc: 0.98, recall: 0.98, precision: 0.98, f_beta: 0.98
train: step: 3204, loss: 0.05523402616381645, acc: 0.92, recall: 0.9199999999999999, precision: 0.9206730769230769, f_beta: 0.919967987194878
train: step: 3205, loss: 0.03726451098918915, acc: 0.96, recall: 0.96, precision: 0.96, f_beta: 0.96
train: step: 3206, loss: 0.09186655282974243, acc: 0.89, recall: 0.89, precision: 0.89015606242497, f_beta: 0.88998899889989
train: step: 3207, loss: 0.05299985036253929, acc: 0.94, recall: 0.94, precision: 0.9428341384863124, f_beta: 0.9399038461538463
train: step: 3208, loss: 0.018815668299794197, acc: 0.98, recall: 0.98, precision: 0.9807692307692308, f_beta: 0.9799919967987194
train: step: 3209, loss: 0.020473675802350044, acc: 0.98, recall: 0.98, precision: 0.9807692307692308, f_beta: 0.9799919967987194
train: step: 3210, loss: 0.016294360160827637, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 3211, loss: 0.03410515561699867, acc: 0.94, recall: 0.94, precision: 0.9464285714285714, f_beta: 0.939783219590526
train: step: 3212, loss: 0.03821457922458649, acc: 0.95, recall: 0.95, precision: 0.9501800720288115, f_beta: 0.9499949994999499
train: step: 3213, loss: 0.02126561850309372, acc: 0.98, recall: 0.98, precision: 0.9807692307692308, f_beta: 0.9799919967987194
train: step: 3214, loss: 0.03614122048020363, acc: 0.96, recall: 0.96, precision: 0.962962962962963, f_beta: 0.9599358974358974
train: step: 3215, loss: 0.03807676210999489, acc: 0.96, recall: 0.96, precision: 0.962962962962963, f_beta: 0.9599358974358974
train: step: 3216, loss: 0.029449889436364174, acc: 0.95, recall: 0.95, precision: 0.9516258530710557, f_beta: 0.949954959463517
train: step: 3217, loss: 0.07407505810260773, acc: 0.9, recall: 0.8999999999999999, precision: 0.9025764895330113, f_beta: 0.8998397435897436
train: step: 3218, loss: 0.057236891239881516, acc: 0.93, recall: 0.9299999999999999, precision: 0.930172068827531, f_beta: 0.9299929992999301
train: step: 3219, loss: 0.03759938105940819, acc: 0.96, recall: 0.96, precision: 0.96, f_beta: 0.96
train: step: 3220, loss: 0.02370535396039486, acc: 0.97, recall: 0.97, precision: 0.9716981132075472, f_beta: 0.9699729756781104
train: step: 3221, loss: 0.04235183820128441, acc: 0.95, recall: 0.95, precision: 0.9501800720288115, f_beta: 0.9499949994999499
train: step: 3222, loss: 0.03895694389939308, acc: 0.96, recall: 0.96, precision: 0.9607371794871795, f_beta: 0.959983993597439
train: step: 3223, loss: 0.05885476991534233, acc: 0.92, recall: 0.9199999999999999, precision: 0.9206730769230769, f_beta: 0.919967987194878
train: step: 3224, loss: 0.023679357022047043, acc: 0.97, recall: 0.97, precision: 0.9716981132075472, f_beta: 0.9699729756781104
train: step: 3225, loss: 0.0314234159886837, acc: 0.96, recall: 0.96, precision: 0.9607371794871795, f_beta: 0.959983993597439
train: step: 3226, loss: 0.02806658111512661, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 3227, loss: 0.04335309937596321, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 3228, loss: 0.04613263159990311, acc: 0.93, recall: 0.9299999999999999, precision: 0.9343434343434343, f_beta: 0.9298245614035088
train: step: 3229, loss: 0.016271812841296196, acc: 0.97, recall: 0.97, precision: 0.9716981132075472, f_beta: 0.9699729756781104
train: step: 3230, loss: 0.02430422231554985, acc: 0.98, recall: 0.98, precision: 0.9807692307692308, f_beta: 0.9799919967987194
train: step: 3231, loss: 0.06545053422451019, acc: 0.93, recall: 0.9299999999999999, precision: 0.930172068827531, f_beta: 0.9299929992999301
train: step: 3232, loss: 0.030153846368193626, acc: 0.97, recall: 0.97, precision: 0.9716981132075472, f_beta: 0.9699729756781104
train: step: 3233, loss: 0.05864913761615753, acc: 0.93, recall: 0.9299999999999999, precision: 0.9343434343434343, f_beta: 0.9298245614035088
train: step: 3234, loss: 0.04842088744044304, acc: 0.94, recall: 0.94, precision: 0.9407051282051282, f_beta: 0.9399759903961584
train: step: 3235, loss: 0.038847941905260086, acc: 0.96, recall: 0.96, precision: 0.9607371794871795, f_beta: 0.959983993597439
train: step: 3236, loss: 0.017392046749591827, acc: 0.98, recall: 0.98, precision: 0.9807692307692308, f_beta: 0.9799919967987194
train: step: 3237, loss: 0.02649848535656929, acc: 0.96, recall: 0.96, precision: 0.9607371794871795, f_beta: 0.959983993597439
train: step: 3238, loss: 0.02780756726861, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 3239, loss: 0.05127602443099022, acc: 0.94, recall: 0.94, precision: 0.94, f_beta: 0.94
train: step: 3240, loss: 0.019056040793657303, acc: 0.98, recall: 0.98, precision: 0.9807692307692308, f_beta: 0.9799919967987194
train: step: 3241, loss: 0.012983748689293861, acc: 0.99, recall: 0.99, precision: 0.9901960784313726, f_beta: 0.98999899989999
train: step: 3242, loss: 0.03754454478621483, acc: 0.96, recall: 0.96, precision: 0.9607371794871795, f_beta: 0.959983993597439
train: step: 3243, loss: 0.014701093547046185, acc: 0.99, recall: 0.99, precision: 0.9901960784313726, f_beta: 0.98999899989999
train: step: 3244, loss: 0.02352820336818695, acc: 0.98, recall: 0.98, precision: 0.98, f_beta: 0.98
train: step: 3245, loss: 0.03842323273420334, acc: 0.95, recall: 0.95, precision: 0.9501800720288115, f_beta: 0.9499949994999499
train: step: 3246, loss: 0.04515602067112923, acc: 0.93, recall: 0.9299999999999999, precision: 0.930172068827531, f_beta: 0.9299929992999301
train: step: 3247, loss: 0.03864250332117081, acc: 0.94, recall: 0.94, precision: 0.9407051282051282, f_beta: 0.9399759903961584
train: step: 3248, loss: 0.04277368634939194, acc: 0.95, recall: 0.95, precision: 0.9501800720288115, f_beta: 0.9499949994999499
train: step: 3249, loss: 0.02259748987853527, acc: 0.98, recall: 0.98, precision: 0.98, f_beta: 0.98
train: step: 3250, loss: 0.03691998869180679, acc: 0.96, recall: 0.96, precision: 0.9607371794871795, f_beta: 0.959983993597439
train: step: 3251, loss: 0.007900793105363846, acc: 0.99, recall: 0.99, precision: 0.9901960784313726, f_beta: 0.98999899989999
train: step: 3252, loss: 0.02752433344721794, acc: 0.96, recall: 0.96, precision: 0.962962962962963, f_beta: 0.9599358974358974
train: step: 3253, loss: 0.023223653435707092, acc: 0.96, recall: 0.96, precision: 0.96, f_beta: 0.96
train: step: 3254, loss: 0.007394384127110243, acc: 0.99, recall: 0.99, precision: 0.9901960784313726, f_beta: 0.98999899989999
train: step: 3255, loss: 0.03210485354065895, acc: 0.95, recall: 0.95, precision: 0.9516258530710557, f_beta: 0.949954959463517
train: step: 3256, loss: 0.024825192987918854, acc: 0.98, recall: 0.98, precision: 0.9807692307692308, f_beta: 0.9799919967987194
train: step: 3257, loss: 0.06755445152521133, acc: 0.91, recall: 0.9099999999999999, precision: 0.9114813327980731, f_beta: 0.9099189270343309
train: step: 3258, loss: 0.005955313332378864, acc: 0.99, recall: 0.99, precision: 0.9901960784313726, f_beta: 0.98999899989999
train: step: 3259, loss: 0.08707195520401001, acc: 0.88, recall: 0.88, precision: 0.8824476650563606, f_beta: 0.8798076923076923
train: step: 3260, loss: 0.07036961615085602, acc: 0.9, recall: 0.9, precision: 0.9006410256410255, f_beta: 0.8999599839935974
train: step: 3261, loss: 0.04112711921334267, acc: 0.95, recall: 0.95, precision: 0.9501800720288115, f_beta: 0.9499949994999499
train: step: 3262, loss: 0.016964416950941086, acc: 0.98, recall: 0.98, precision: 0.98, f_beta: 0.98
train: step: 3263, loss: 0.04064517095685005, acc: 0.96, recall: 0.96, precision: 0.962962962962963, f_beta: 0.9599358974358974
train: step: 3264, loss: 0.008733508177101612, acc: 0.99, recall: 0.99, precision: 0.9901960784313726, f_beta: 0.98999899989999
train: step: 3265, loss: 0.055501021444797516, acc: 0.94, recall: 0.94, precision: 0.94, f_beta: 0.94
train: step: 3266, loss: 0.004741034470498562, acc: 1.0, recall: 1.0, precision: 1.0, f_beta: 1.0
train: step: 3267, loss: 0.023313239216804504, acc: 0.98, recall: 0.98, precision: 0.9807692307692308, f_beta: 0.9799919967987194
train: step: 3268, loss: 0.029841575771570206, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 3269, loss: 0.06116829067468643, acc: 0.92, recall: 0.9199999999999999, precision: 0.9227053140096618, f_beta: 0.9198717948717948
train: step: 3270, loss: 0.042560625821352005, acc: 0.95, recall: 0.95, precision: 0.9501800720288115, f_beta: 0.9499949994999499
train: step: 3271, loss: 0.0277567096054554, acc: 0.96, recall: 0.96, precision: 0.96, f_beta: 0.96
train: step: 3272, loss: 0.023682652041316032, acc: 0.98, recall: 0.98, precision: 0.98, f_beta: 0.98
train: step: 3273, loss: 0.027017412707209587, acc: 0.96, recall: 0.96, precision: 0.96, f_beta: 0.96
train: step: 3274, loss: 0.048932019621133804, acc: 0.94, recall: 0.94, precision: 0.94, f_beta: 0.94
train: step: 3275, loss: 0.022320080548524857, acc: 0.98, recall: 0.98, precision: 0.9807692307692308, f_beta: 0.9799919967987194
train: step: 3276, loss: 0.03378194943070412, acc: 0.96, recall: 0.96, precision: 0.9607371794871795, f_beta: 0.959983993597439
train: step: 3277, loss: 0.03789183497428894, acc: 0.95, recall: 0.95, precision: 0.9516258530710557, f_beta: 0.949954959463517
train: step: 3278, loss: 0.02392851561307907, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 3279, loss: 0.03655312955379486, acc: 0.96, recall: 0.96, precision: 0.96, f_beta: 0.96
train: step: 3280, loss: 0.04037337750196457, acc: 0.95, recall: 0.95, precision: 0.9501800720288115, f_beta: 0.9499949994999499
train: step: 3281, loss: 0.04179801791906357, acc: 0.94, recall: 0.94, precision: 0.9428341384863124, f_beta: 0.9399038461538463
train: step: 3282, loss: 0.03575599566102028, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 3283, loss: 0.060286130756139755, acc: 0.91, recall: 0.9099999999999999, precision: 0.9114813327980731, f_beta: 0.9099189270343309
train: step: 3284, loss: 0.01362286601215601, acc: 0.98, recall: 0.98, precision: 0.9807692307692308, f_beta: 0.9799919967987194
train: step: 3285, loss: 0.04675830900669098, acc: 0.94, recall: 0.94, precision: 0.9407051282051282, f_beta: 0.9399759903961584
train: step: 3286, loss: 0.016194920986890793, acc: 0.98, recall: 0.98, precision: 0.9807692307692308, f_beta: 0.9799919967987194
train: step: 3287, loss: 0.0270758718252182, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 3288, loss: 0.051779214292764664, acc: 0.93, recall: 0.9299999999999999, precision: 0.930172068827531, f_beta: 0.9299929992999301
train: step: 3289, loss: 0.01782863587141037, acc: 0.98, recall: 0.98, precision: 0.9807692307692308, f_beta: 0.9799919967987194
train: step: 3290, loss: 0.083287812769413, acc: 0.9, recall: 0.9, precision: 0.9006410256410255, f_beta: 0.8999599839935974
train: step: 3291, loss: 0.022200321778655052, acc: 0.98, recall: 0.98, precision: 0.9807692307692308, f_beta: 0.9799919967987194
train: step: 3292, loss: 0.03488586097955704, acc: 0.95, recall: 0.95, precision: 0.9516258530710557, f_beta: 0.949954959463517
train: step: 3293, loss: 0.03118760511279106, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 3294, loss: 0.029082683846354485, acc: 0.96, recall: 0.96, precision: 0.96, f_beta: 0.96
train: step: 3295, loss: 0.026930423453450203, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 3296, loss: 0.018878351897001266, acc: 0.98, recall: 0.98, precision: 0.98, f_beta: 0.98
train: step: 3297, loss: 0.022203831002116203, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 3298, loss: 0.014000742696225643, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 3299, loss: 0.03502616286277771, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 3300, loss: 0.05323885753750801, acc: 0.91, recall: 0.9099999999999999, precision: 0.9141414141414141, f_beta: 0.9097744360902256
train: step: 3301, loss: 0.028194375336170197, acc: 0.96, recall: 0.96, precision: 0.962962962962963, f_beta: 0.9599358974358974
train: step: 3302, loss: 0.012111663818359375, acc: 0.99, recall: 0.99, precision: 0.9901960784313726, f_beta: 0.98999899989999
train: step: 3303, loss: 0.036744263023138046, acc: 0.95, recall: 0.95, precision: 0.9516258530710557, f_beta: 0.949954959463517
train: step: 3304, loss: 0.01998167298734188, acc: 0.98, recall: 0.98, precision: 0.98, f_beta: 0.98
train: step: 3305, loss: 0.08758929371833801, acc: 0.89, recall: 0.8899999999999999, precision: 0.8939393939393939, f_beta: 0.8897243107769424
train: step: 3306, loss: 0.030643949285149574, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 3307, loss: 0.020576857030391693, acc: 0.98, recall: 0.98, precision: 0.98, f_beta: 0.98
train: step: 3308, loss: 0.027827970683574677, acc: 0.96, recall: 0.96, precision: 0.96, f_beta: 0.96
train: step: 3309, loss: 0.0868372917175293, acc: 0.9, recall: 0.8999999999999999, precision: 0.9025764895330113, f_beta: 0.8998397435897436
train: step: 3310, loss: 0.041237521916627884, acc: 0.96, recall: 0.96, precision: 0.962962962962963, f_beta: 0.9599358974358974
train: step: 3311, loss: 0.040474552661180496, acc: 0.95, recall: 0.95, precision: 0.9501800720288115, f_beta: 0.9499949994999499
train: step: 3312, loss: 0.04111303389072418, acc: 0.95, recall: 0.95, precision: 0.9501800720288115, f_beta: 0.9499949994999499
train: step: 3313, loss: 0.042900074273347855, acc: 0.94, recall: 0.94, precision: 0.94, f_beta: 0.94
train: step: 3314, loss: 0.016870252788066864, acc: 0.98, recall: 0.98, precision: 0.98, f_beta: 0.98
train: step: 3315, loss: 0.008683465421199799, acc: 0.98, recall: 0.98, precision: 0.9807692307692308, f_beta: 0.9799919967987194
train: step: 3316, loss: 0.021749012172222137, acc: 0.98, recall: 0.98, precision: 0.9807692307692308, f_beta: 0.9799919967987194
train: step: 3317, loss: 0.044848691672086716, acc: 0.95, recall: 0.95, precision: 0.9516258530710557, f_beta: 0.949954959463517
train: step: 3318, loss: 0.041301194578409195, acc: 0.96, recall: 0.96, precision: 0.9607371794871795, f_beta: 0.959983993597439
train: step: 3319, loss: 0.05942966416478157, acc: 0.92, recall: 0.92, precision: 0.92, f_beta: 0.92
train: step: 3320, loss: 0.04013131931424141, acc: 0.95, recall: 0.95, precision: 0.9545454545454546, f_beta: 0.949874686716792
train: step: 3321, loss: 0.021074319258332253, acc: 0.98, recall: 0.98, precision: 0.98, f_beta: 0.98
train: step: 3322, loss: 0.032471295446157455, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 3323, loss: 0.020814532414078712, acc: 0.98, recall: 0.98, precision: 0.9807692307692308, f_beta: 0.9799919967987194
train: step: 3324, loss: 0.0539696104824543, acc: 0.94, recall: 0.94, precision: 0.9407051282051282, f_beta: 0.9399759903961584
train: step: 3325, loss: 0.03415388613939285, acc: 0.95, recall: 0.95, precision: 0.9501800720288115, f_beta: 0.9499949994999499
train: step: 3326, loss: 0.01857496239244938, acc: 0.98, recall: 0.98, precision: 0.9807692307692308, f_beta: 0.9799919967987194
train: step: 3327, loss: 0.015606223605573177, acc: 0.99, recall: 0.99, precision: 0.9901960784313726, f_beta: 0.98999899989999
train: step: 3328, loss: 0.018771423026919365, acc: 0.97, recall: 0.97, precision: 0.9716981132075472, f_beta: 0.9699729756781104
train: step: 3329, loss: 0.07795217633247375, acc: 0.91, recall: 0.9099999999999999, precision: 0.9114813327980731, f_beta: 0.9099189270343309
train: step: 3330, loss: 0.05161804333329201, acc: 0.94, recall: 0.94, precision: 0.9407051282051282, f_beta: 0.9399759903961584
train: step: 3331, loss: 0.0026382217183709145, acc: 1.0, recall: 1.0, precision: 1.0, f_beta: 1.0
train: step: 3332, loss: 0.03829580917954445, acc: 0.95, recall: 0.95, precision: 0.9516258530710557, f_beta: 0.949954959463517
train: step: 3333, loss: 0.048051461577415466, acc: 0.94, recall: 0.94, precision: 0.9407051282051282, f_beta: 0.9399759903961584
train: step: 3334, loss: 0.04159163311123848, acc: 0.95, recall: 0.95, precision: 0.9501800720288115, f_beta: 0.9499949994999499
train: step: 3335, loss: 0.04323862865567207, acc: 0.96, recall: 0.96, precision: 0.96, f_beta: 0.96
train: step: 3336, loss: 0.020148858428001404, acc: 0.96, recall: 0.96, precision: 0.96, f_beta: 0.96
train: step: 3337, loss: 0.04062364995479584, acc: 0.94, recall: 0.94, precision: 0.94, f_beta: 0.94
train: step: 3338, loss: 0.01601386070251465, acc: 0.98, recall: 0.98, precision: 0.9807692307692308, f_beta: 0.9799919967987194
train: step: 3339, loss: 0.03417413681745529, acc: 0.95, recall: 0.95, precision: 0.9516258530710557, f_beta: 0.949954959463517
train: step: 3340, loss: 0.008424472063779831, acc: 1.0, recall: 1.0, precision: 1.0, f_beta: 1.0
train: step: 3341, loss: 0.04022840037941933, acc: 0.95, recall: 0.95, precision: 0.9516258530710557, f_beta: 0.949954959463517
train: step: 3342, loss: 0.045770592987537384, acc: 0.94, recall: 0.94, precision: 0.9407051282051282, f_beta: 0.9399759903961584
train: step: 3343, loss: 0.04526117444038391, acc: 0.96, recall: 0.96, precision: 0.9607371794871795, f_beta: 0.959983993597439
train: step: 3344, loss: 0.06490426510572433, acc: 0.91, recall: 0.91, precision: 0.9101640656262505, f_beta: 0.90999099909991
train: step: 3345, loss: 0.03558722510933876, acc: 0.96, recall: 0.96, precision: 0.9607371794871795, f_beta: 0.959983993597439
train: step: 3346, loss: 0.05491068959236145, acc: 0.93, recall: 0.9299999999999999, precision: 0.930172068827531, f_beta: 0.9299929992999301
train: step: 3347, loss: 0.005045692436397076, acc: 1.0, recall: 1.0, precision: 1.0, f_beta: 1.0
train: step: 3348, loss: 0.0084774699062109, acc: 0.99, recall: 0.99, precision: 0.9901960784313726, f_beta: 0.98999899989999
train: step: 3349, loss: 0.007207901682704687, acc: 0.99, recall: 0.99, precision: 0.9901960784313726, f_beta: 0.98999899989999
train: step: 3350, loss: 0.02514219842851162, acc: 0.98, recall: 0.98, precision: 0.98, f_beta: 0.98
train: step: 3351, loss: 0.0327582024037838, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 3352, loss: 0.012723355554044247, acc: 0.99, recall: 0.99, precision: 0.9901960784313726, f_beta: 0.98999899989999
train: step: 3353, loss: 0.014411606825888157, acc: 0.97, recall: 0.97, precision: 0.9716981132075472, f_beta: 0.9699729756781104
train: step: 3354, loss: 0.07369888573884964, acc: 0.9, recall: 0.8999999999999999, precision: 0.9025764895330113, f_beta: 0.8998397435897436
train: step: 3355, loss: 0.015102719888091087, acc: 0.98, recall: 0.98, precision: 0.9807692307692308, f_beta: 0.9799919967987194
train: step: 3356, loss: 0.017015429213643074, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 3357, loss: 0.0211119893938303, acc: 0.98, recall: 0.98, precision: 0.9807692307692308, f_beta: 0.9799919967987194
train: step: 3358, loss: 0.04515718296170235, acc: 0.93, recall: 0.9299999999999999, precision: 0.930172068827531, f_beta: 0.9299929992999301
train: step: 3359, loss: 0.05341505631804466, acc: 0.93, recall: 0.9299999999999999, precision: 0.9315535929345644, f_beta: 0.9299369432489241
train: step: 3360, loss: 0.007993918843567371, acc: 0.99, recall: 0.99, precision: 0.9901960784313726, f_beta: 0.98999899989999
train: step: 3361, loss: 0.03778497129678726, acc: 0.94, recall: 0.94, precision: 0.9464285714285714, f_beta: 0.939783219590526
train: step: 3362, loss: 0.007955255918204784, acc: 1.0, recall: 1.0, precision: 1.0, f_beta: 1.0
train: step: 3363, loss: 0.03905653581023216, acc: 0.95, recall: 0.95, precision: 0.9516258530710557, f_beta: 0.949954959463517
train: step: 3364, loss: 0.033748190850019455, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 3365, loss: 0.009598583914339542, acc: 0.99, recall: 0.99, precision: 0.9901960784313726, f_beta: 0.98999899989999
train: step: 3366, loss: 0.03419588878750801, acc: 0.96, recall: 0.96, precision: 0.962962962962963, f_beta: 0.9599358974358974
train: step: 3367, loss: 0.04753648117184639, acc: 0.94, recall: 0.94, precision: 0.9407051282051282, f_beta: 0.9399759903961584
train: step: 3368, loss: 0.02508879266679287, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 3369, loss: 0.021891199052333832, acc: 0.98, recall: 0.98, precision: 0.9807692307692308, f_beta: 0.9799919967987194
train: step: 3370, loss: 0.003180083818733692, acc: 1.0, recall: 1.0, precision: 1.0, f_beta: 1.0
train: step: 3371, loss: 0.06012607738375664, acc: 0.91, recall: 0.91, precision: 0.9101640656262505, f_beta: 0.90999099909991
train: step: 3372, loss: 0.023353777825832367, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 3373, loss: 0.02451288513839245, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 3374, loss: 0.004245298448950052, acc: 1.0, recall: 1.0, precision: 1.0, f_beta: 1.0
train: step: 3375, loss: 0.03247959911823273, acc: 0.96, recall: 0.96, precision: 0.962962962962963, f_beta: 0.9599358974358974
train: step: 3376, loss: 0.03377057611942291, acc: 0.95, recall: 0.95, precision: 0.9501800720288115, f_beta: 0.9499949994999499
train: step: 3377, loss: 0.028359994292259216, acc: 0.98, recall: 0.98, precision: 0.9807692307692308, f_beta: 0.9799919967987194
train: step: 3378, loss: 0.03622664138674736, acc: 0.96, recall: 0.96, precision: 0.9607371794871795, f_beta: 0.959983993597439
train: step: 3379, loss: 0.024294914677739143, acc: 0.98, recall: 0.98, precision: 0.98, f_beta: 0.98
train: step: 3380, loss: 0.04264785721898079, acc: 0.94, recall: 0.94, precision: 0.9407051282051282, f_beta: 0.9399759903961584
train: step: 3381, loss: 0.05899335816502571, acc: 0.93, recall: 0.9299999999999999, precision: 0.930172068827531, f_beta: 0.9299929992999301
train: step: 3382, loss: 0.027268821373581886, acc: 0.96, recall: 0.96, precision: 0.96, f_beta: 0.96
train: step: 3383, loss: 0.01433985959738493, acc: 0.99, recall: 0.99, precision: 0.9901960784313726, f_beta: 0.98999899989999
train: step: 3384, loss: 0.030884813517332077, acc: 0.96, recall: 0.96, precision: 0.96, f_beta: 0.96
train: step: 3385, loss: 0.02393246442079544, acc: 0.97, recall: 0.97, precision: 0.9716981132075472, f_beta: 0.9699729756781104
train: step: 3386, loss: 0.013652276247739792, acc: 0.99, recall: 0.99, precision: 0.9901960784313726, f_beta: 0.98999899989999
train: step: 3387, loss: 0.04198909178376198, acc: 0.94, recall: 0.94, precision: 0.9428341384863124, f_beta: 0.9399038461538463
train: step: 3388, loss: 0.024646170437335968, acc: 0.98, recall: 0.98, precision: 0.9807692307692308, f_beta: 0.9799919967987194
train: step: 3389, loss: 0.03579282388091087, acc: 0.95, recall: 0.95, precision: 0.9501800720288115, f_beta: 0.9499949994999499
train: step: 3390, loss: 0.020606456324458122, acc: 0.98, recall: 0.98, precision: 0.98, f_beta: 0.98
train: step: 3391, loss: 0.04554188251495361, acc: 0.95, recall: 0.95, precision: 0.9501800720288115, f_beta: 0.9499949994999499
train: step: 3392, loss: 0.012102275155484676, acc: 0.99, recall: 0.99, precision: 0.9901960784313726, f_beta: 0.98999899989999
train: step: 3393, loss: 0.01940874755382538, acc: 0.98, recall: 0.98, precision: 0.9807692307692308, f_beta: 0.9799919967987194
train: step: 3394, loss: 0.02228582836687565, acc: 0.98, recall: 0.98, precision: 0.9807692307692308, f_beta: 0.9799919967987194
train: step: 3395, loss: 0.07369619607925415, acc: 0.91, recall: 0.9099999999999999, precision: 0.9114813327980731, f_beta: 0.9099189270343309
train: step: 3396, loss: 0.05577007308602333, acc: 0.94, recall: 0.94, precision: 0.9407051282051282, f_beta: 0.9399759903961584
train: step: 3397, loss: 0.045584842562675476, acc: 0.94, recall: 0.94, precision: 0.9428341384863124, f_beta: 0.9399038461538463
train: step: 3398, loss: 0.04463270679116249, acc: 0.94, recall: 0.94, precision: 0.94, f_beta: 0.94
train: step: 3399, loss: 0.029480881989002228, acc: 0.96, recall: 0.96, precision: 0.9607371794871795, f_beta: 0.959983993597439
train: step: 3400, loss: 0.013537326827645302, acc: 0.98, recall: 0.98, precision: 0.98, f_beta: 0.98
train: step: 3401, loss: 0.024757813662290573, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 3402, loss: 0.06106477603316307, acc: 0.94, recall: 0.94, precision: 0.9428341384863124, f_beta: 0.9399038461538463
train: step: 3403, loss: 0.06521237641572952, acc: 0.92, recall: 0.9199999999999999, precision: 0.9227053140096618, f_beta: 0.9198717948717948
train: step: 3404, loss: 0.0055827368050813675, acc: 0.99, recall: 0.99, precision: 0.9901960784313726, f_beta: 0.98999899989999
train: step: 3405, loss: 0.03542877361178398, acc: 0.95, recall: 0.95, precision: 0.9501800720288115, f_beta: 0.9499949994999499
train: step: 3406, loss: 0.0403207503259182, acc: 0.95, recall: 0.95, precision: 0.9501800720288115, f_beta: 0.9499949994999499
train: step: 3407, loss: 0.021308308467268944, acc: 0.98, recall: 0.98, precision: 0.9807692307692308, f_beta: 0.9799919967987194
train: step: 3408, loss: 0.04037073999643326, acc: 0.95, recall: 0.95, precision: 0.9501800720288115, f_beta: 0.9499949994999499
train: step: 3409, loss: 0.05570552870631218, acc: 0.94, recall: 0.94, precision: 0.94, f_beta: 0.94
train: step: 3410, loss: 0.0615883432328701, acc: 0.93, recall: 0.9299999999999999, precision: 0.930172068827531, f_beta: 0.9299929992999301
train: step: 3411, loss: 0.047968540340662, acc: 0.94, recall: 0.94, precision: 0.94, f_beta: 0.94
train: step: 3412, loss: 0.02663443051278591, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 3413, loss: 0.02123154141008854, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 3414, loss: 0.039207156747579575, acc: 0.96, recall: 0.96, precision: 0.9607371794871795, f_beta: 0.959983993597439
train: step: 3415, loss: 0.04361126944422722, acc: 0.95, recall: 0.95, precision: 0.9545454545454546, f_beta: 0.949874686716792
train: step: 3416, loss: 0.024145247414708138, acc: 0.95, recall: 0.95, precision: 0.9545454545454546, f_beta: 0.949874686716792
train: step: 3417, loss: 0.019755247980356216, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 3418, loss: 0.02685687132179737, acc: 0.96, recall: 0.96, precision: 0.9607371794871795, f_beta: 0.959983993597439
train: step: 3419, loss: 0.06204036623239517, acc: 0.94, recall: 0.94, precision: 0.9407051282051282, f_beta: 0.9399759903961584
train: step: 3420, loss: 0.028977086767554283, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 3421, loss: 0.031402889639139175, acc: 0.96, recall: 0.96, precision: 0.96, f_beta: 0.96
train: step: 3422, loss: 0.011165418662130833, acc: 0.98, recall: 0.98, precision: 0.98, f_beta: 0.98
train: step: 3423, loss: 0.05198521539568901, acc: 0.93, recall: 0.9299999999999999, precision: 0.9343434343434343, f_beta: 0.9298245614035088
train: step: 3424, loss: 0.05063159018754959, acc: 0.94, recall: 0.94, precision: 0.94, f_beta: 0.94
train: step: 3425, loss: 0.038900092244148254, acc: 0.96, recall: 0.96, precision: 0.96, f_beta: 0.96
train: step: 3426, loss: 0.04203379154205322, acc: 0.95, recall: 0.95, precision: 0.9501800720288115, f_beta: 0.9499949994999499
train: step: 3427, loss: 0.034479741007089615, acc: 0.95, recall: 0.95, precision: 0.9501800720288115, f_beta: 0.9499949994999499
train: step: 3428, loss: 0.018900061026215553, acc: 0.97, recall: 0.97, precision: 0.9716981132075472, f_beta: 0.9699729756781104
train: step: 3429, loss: 0.03491813689470291, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 3430, loss: 0.03594591096043587, acc: 0.96, recall: 0.96, precision: 0.9607371794871795, f_beta: 0.959983993597439
train: step: 3431, loss: 0.0636308491230011, acc: 0.92, recall: 0.9199999999999999, precision: 0.9227053140096618, f_beta: 0.9198717948717948
train: step: 3432, loss: 0.07454406470060349, acc: 0.91, recall: 0.9099999999999999, precision: 0.9141414141414141, f_beta: 0.9097744360902256
train: step: 3433, loss: 0.05623501166701317, acc: 0.94, recall: 0.94, precision: 0.9407051282051282, f_beta: 0.9399759903961584
train: step: 3434, loss: 0.04347364604473114, acc: 0.96, recall: 0.96, precision: 0.9607371794871795, f_beta: 0.959983993597439
train: step: 3435, loss: 0.018035616725683212, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 3436, loss: 0.02565591409802437, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 3437, loss: 0.012591852806508541, acc: 0.99, recall: 0.99, precision: 0.9901960784313726, f_beta: 0.98999899989999
train: step: 3438, loss: 0.021162621676921844, acc: 0.99, recall: 0.99, precision: 0.9901960784313726, f_beta: 0.98999899989999
train: step: 3439, loss: 0.030828289687633514, acc: 0.96, recall: 0.96, precision: 0.96, f_beta: 0.96
train: step: 3440, loss: 0.05099337548017502, acc: 0.94, recall: 0.94, precision: 0.9407051282051282, f_beta: 0.9399759903961584
train: step: 3441, loss: 0.0068183173425495625, acc: 0.99, recall: 0.99, precision: 0.9901960784313726, f_beta: 0.98999899989999
train: step: 3442, loss: 0.02196682058274746, acc: 0.98, recall: 0.98, precision: 0.9807692307692308, f_beta: 0.9799919967987194
train: step: 3443, loss: 0.042928095906972885, acc: 0.94, recall: 0.94, precision: 0.9428341384863124, f_beta: 0.9399038461538463
train: step: 3444, loss: 0.045208368450403214, acc: 0.93, recall: 0.9299999999999999, precision: 0.9315535929345644, f_beta: 0.9299369432489241
train: step: 3445, loss: 0.0537853017449379, acc: 0.94, recall: 0.94, precision: 0.9407051282051282, f_beta: 0.9399759903961584
train: step: 3446, loss: 0.017477361485362053, acc: 0.98, recall: 0.98, precision: 0.9807692307692308, f_beta: 0.9799919967987194
train: step: 3447, loss: 0.08265388756990433, acc: 0.9, recall: 0.8999999999999999, precision: 0.9105090311986863, f_beta: 0.8993558776167472
train: step: 3448, loss: 0.03683680295944214, acc: 0.96, recall: 0.96, precision: 0.9607371794871795, f_beta: 0.959983993597439
train: step: 3449, loss: 0.05204428359866142, acc: 0.93, recall: 0.9299999999999999, precision: 0.930172068827531, f_beta: 0.9299929992999301
train: step: 3450, loss: 0.03539767116308212, acc: 0.96, recall: 0.96, precision: 0.96, f_beta: 0.96
train: step: 3451, loss: 0.054682523012161255, acc: 0.93, recall: 0.9299999999999999, precision: 0.930172068827531, f_beta: 0.9299929992999301
train: step: 3452, loss: 0.04306379333138466, acc: 0.95, recall: 0.95, precision: 0.9516258530710557, f_beta: 0.949954959463517
train: step: 3453, loss: 0.004708460066467524, acc: 1.0, recall: 1.0, precision: 1.0, f_beta: 1.0
train: step: 3454, loss: 0.04549297317862511, acc: 0.94, recall: 0.94, precision: 0.9407051282051282, f_beta: 0.9399759903961584
train: step: 3455, loss: 0.01569916307926178, acc: 0.98, recall: 0.98, precision: 0.9807692307692308, f_beta: 0.9799919967987194
train: step: 3456, loss: 0.06940729916095734, acc: 0.92, recall: 0.9199999999999999, precision: 0.9261363636363636, f_beta: 0.9197109594540345
train: step: 3457, loss: 0.013652251102030277, acc: 0.99, recall: 0.99, precision: 0.9901960784313726, f_beta: 0.98999899989999
train: step: 3458, loss: 0.01711259037256241, acc: 0.99, recall: 0.99, precision: 0.9901960784313726, f_beta: 0.98999899989999
train: step: 3459, loss: 0.025783931836485863, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 3460, loss: 0.018931591883301735, acc: 0.97, recall: 0.97, precision: 0.9716981132075472, f_beta: 0.9699729756781104
train: step: 3461, loss: 0.03324645012617111, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 3462, loss: 0.02356521412730217, acc: 0.98, recall: 0.98, precision: 0.98, f_beta: 0.98
train: step: 3463, loss: 0.03106049820780754, acc: 0.96, recall: 0.96, precision: 0.96, f_beta: 0.96
train: step: 3464, loss: 0.05042378976941109, acc: 0.94, recall: 0.94, precision: 0.94, f_beta: 0.94
train: step: 3465, loss: 0.025596342980861664, acc: 0.97, recall: 0.97, precision: 0.9716981132075472, f_beta: 0.9699729756781104
train: step: 3466, loss: 0.02412356808781624, acc: 0.96, recall: 0.96, precision: 0.9607371794871795, f_beta: 0.959983993597439
train: step: 3467, loss: 0.015139362774789333, acc: 0.98, recall: 0.98, precision: 0.9807692307692308, f_beta: 0.9799919967987194
train: step: 3468, loss: 0.04072059318423271, acc: 0.94, recall: 0.94, precision: 0.94, f_beta: 0.94
train: step: 3469, loss: 0.052210740745067596, acc: 0.95, recall: 0.95, precision: 0.9501800720288115, f_beta: 0.9499949994999499
train: step: 3470, loss: 0.024642203003168106, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 3471, loss: 0.055569447576999664, acc: 0.94, recall: 0.94, precision: 0.9407051282051282, f_beta: 0.9399759903961584
train: step: 3472, loss: 0.026319369673728943, acc: 0.96, recall: 0.96, precision: 0.9607371794871795, f_beta: 0.959983993597439
train: step: 3473, loss: 0.045216888189315796, acc: 0.96, recall: 0.96, precision: 0.96, f_beta: 0.96
train: step: 3474, loss: 0.03130928426980972, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 3475, loss: 0.04260009154677391, acc: 0.94, recall: 0.94, precision: 0.9407051282051282, f_beta: 0.9399759903961584
train: step: 3476, loss: 0.04578952491283417, acc: 0.94, recall: 0.94, precision: 0.9407051282051282, f_beta: 0.9399759903961584
train: step: 3477, loss: 0.041680995374917984, acc: 0.95, recall: 0.95, precision: 0.9516258530710557, f_beta: 0.949954959463517
train: step: 3478, loss: 0.0316426083445549, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 3479, loss: 0.024628223851323128, acc: 0.98, recall: 0.98, precision: 0.98, f_beta: 0.98
train: step: 3480, loss: 0.02985621616244316, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 3481, loss: 0.03285881131887436, acc: 0.96, recall: 0.96, precision: 0.962962962962963, f_beta: 0.9599358974358974
train: step: 3482, loss: 0.03773321583867073, acc: 0.95, recall: 0.95, precision: 0.9501800720288115, f_beta: 0.9499949994999499
train: step: 3483, loss: 0.013213226571679115, acc: 0.99, recall: 0.99, precision: 0.9901960784313726, f_beta: 0.98999899989999
train: step: 3484, loss: 0.04951183870434761, acc: 0.95, recall: 0.95, precision: 0.9501800720288115, f_beta: 0.9499949994999499
train: step: 3485, loss: 0.04454270005226135, acc: 0.94, recall: 0.94, precision: 0.9428341384863124, f_beta: 0.9399038461538463
train: step: 3486, loss: 0.010936527512967587, acc: 0.99, recall: 0.99, precision: 0.9901960784313726, f_beta: 0.98999899989999
train: step: 3487, loss: 0.0062065208330750465, acc: 0.99, recall: 0.99, precision: 0.9901960784313726, f_beta: 0.98999899989999
train: step: 3488, loss: 0.03342224285006523, acc: 0.96, recall: 0.96, precision: 0.9607371794871795, f_beta: 0.959983993597439
train: step: 3489, loss: 0.035751160234212875, acc: 0.95, recall: 0.95, precision: 0.9516258530710557, f_beta: 0.949954959463517
train: step: 3490, loss: 0.012068264186382294, acc: 0.99, recall: 0.99, precision: 0.9901960784313726, f_beta: 0.98999899989999
train: step: 3491, loss: 0.031873419880867004, acc: 0.97, recall: 0.97, precision: 0.9716981132075472, f_beta: 0.9699729756781104
train: step: 3492, loss: 0.03260832652449608, acc: 0.96, recall: 0.96, precision: 0.9607371794871795, f_beta: 0.959983993597439
train: step: 3493, loss: 0.040646061301231384, acc: 0.93, recall: 0.9299999999999999, precision: 0.9315535929345644, f_beta: 0.9299369432489241
train: step: 3494, loss: 0.07178319990634918, acc: 0.92, recall: 0.9199999999999999, precision: 0.9206730769230769, f_beta: 0.919967987194878
train: step: 3495, loss: 0.03879444673657417, acc: 0.95, recall: 0.95, precision: 0.9501800720288115, f_beta: 0.9499949994999499
train: step: 3496, loss: 0.02280845120549202, acc: 0.96, recall: 0.96, precision: 0.96, f_beta: 0.96
train: step: 3497, loss: 0.0178932324051857, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 3498, loss: 0.025477314367890358, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 3499, loss: 0.028716959059238434, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 3500, loss: 0.006991627160459757, acc: 1.0, recall: 1.0, precision: 1.0, f_beta: 1.0
train: step: 3501, loss: 0.0362824909389019, acc: 0.96, recall: 0.96, precision: 0.9607371794871795, f_beta: 0.959983993597439
train: step: 3502, loss: 0.006800036411732435, acc: 0.99, recall: 0.99, precision: 0.9901960784313726, f_beta: 0.98999899989999
train: step: 3503, loss: 0.06626734137535095, acc: 0.91, recall: 0.9099999999999999, precision: 0.9141414141414141, f_beta: 0.9097744360902256
train: step: 3504, loss: 0.021030759438872337, acc: 0.96, recall: 0.96, precision: 0.962962962962963, f_beta: 0.9599358974358974
train: step: 3505, loss: 0.020944124087691307, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 3506, loss: 0.05466332286596298, acc: 0.94, recall: 0.94, precision: 0.9407051282051282, f_beta: 0.9399759903961584
train: step: 3507, loss: 0.020451340824365616, acc: 0.98, recall: 0.98, precision: 0.98, f_beta: 0.98
train: step: 3508, loss: 0.0350915864109993, acc: 0.96, recall: 0.96, precision: 0.96, f_beta: 0.96
train: step: 3509, loss: 0.022705672308802605, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 3510, loss: 0.031125904992222786, acc: 0.97, recall: 0.97, precision: 0.9716981132075472, f_beta: 0.9699729756781104
train: step: 3511, loss: 0.034125689417123795, acc: 0.96, recall: 0.96, precision: 0.962962962962963, f_beta: 0.9599358974358974
train: step: 3512, loss: 0.013076143339276314, acc: 0.99, recall: 0.99, precision: 0.9901960784313726, f_beta: 0.98999899989999
train: step: 3513, loss: 0.0319787934422493, acc: 0.96, recall: 0.96, precision: 0.96, f_beta: 0.96
train: step: 3514, loss: 0.08392999321222305, acc: 0.92, recall: 0.9199999999999999, precision: 0.9261363636363636, f_beta: 0.9197109594540345
train: step: 3515, loss: 0.05682181194424629, acc: 0.93, recall: 0.9299999999999999, precision: 0.930172068827531, f_beta: 0.9299929992999301
train: step: 3516, loss: 0.04435913264751434, acc: 0.94, recall: 0.94, precision: 0.94, f_beta: 0.94
train: step: 3517, loss: 0.03175819292664528, acc: 0.95, recall: 0.95, precision: 0.9501800720288115, f_beta: 0.9499949994999499
train: step: 3518, loss: 0.02240675501525402, acc: 0.97, recall: 0.97, precision: 0.9716981132075472, f_beta: 0.9699729756781104
train: step: 3519, loss: 0.04134083166718483, acc: 0.96, recall: 0.96, precision: 0.96, f_beta: 0.96
train: step: 3520, loss: 0.03887348994612694, acc: 0.95, recall: 0.95, precision: 0.9501800720288115, f_beta: 0.9499949994999499
train: step: 3521, loss: 0.02969965524971485, acc: 0.96, recall: 0.96, precision: 0.962962962962963, f_beta: 0.9599358974358974
train: step: 3522, loss: 0.051833949983119965, acc: 0.94, recall: 0.94, precision: 0.94, f_beta: 0.94
train: step: 3523, loss: 0.028005313128232956, acc: 0.96, recall: 0.96, precision: 0.9607371794871795, f_beta: 0.959983993597439
train: step: 3524, loss: 0.012076006270945072, acc: 0.98, recall: 0.98, precision: 0.98, f_beta: 0.98
train: step: 3525, loss: 0.06346798688173294, acc: 0.93, recall: 0.9299999999999999, precision: 0.930172068827531, f_beta: 0.9299929992999301
train: step: 3526, loss: 0.06572075188159943, acc: 0.91, recall: 0.9099999999999999, precision: 0.9141414141414141, f_beta: 0.9097744360902256
train: step: 3527, loss: 0.038037896156311035, acc: 0.94, recall: 0.94, precision: 0.94, f_beta: 0.94
train: step: 3528, loss: 0.027537569403648376, acc: 0.97, recall: 0.97, precision: 0.9716981132075472, f_beta: 0.9699729756781104
train: step: 3529, loss: 0.04155141860246658, acc: 0.94, recall: 0.94, precision: 0.9407051282051282, f_beta: 0.9399759903961584
train: step: 3530, loss: 0.02145792543888092, acc: 0.98, recall: 0.98, precision: 0.98, f_beta: 0.98
train: step: 3531, loss: 0.03480681777000427, acc: 0.95, recall: 0.95, precision: 0.9516258530710557, f_beta: 0.949954959463517
train: step: 3532, loss: 0.03325222432613373, acc: 0.97, recall: 0.97, precision: 0.9716981132075472, f_beta: 0.9699729756781104
train: step: 3533, loss: 0.03637544810771942, acc: 0.95, recall: 0.95, precision: 0.9545454545454546, f_beta: 0.949874686716792
train: step: 3534, loss: 0.009423072449862957, acc: 0.98, recall: 0.98, precision: 0.9807692307692308, f_beta: 0.9799919967987194
train: step: 3535, loss: 0.04171358421444893, acc: 0.96, recall: 0.96, precision: 0.96, f_beta: 0.96
train: step: 3536, loss: 0.03914959356188774, acc: 0.95, recall: 0.95, precision: 0.9516258530710557, f_beta: 0.949954959463517
train: step: 3537, loss: 0.035029634833335876, acc: 0.96, recall: 0.96, precision: 0.9607371794871795, f_beta: 0.959983993597439
train: step: 3538, loss: 0.04463532567024231, acc: 0.95, recall: 0.95, precision: 0.9516258530710557, f_beta: 0.949954959463517
train: step: 3539, loss: 0.05974918231368065, acc: 0.92, recall: 0.9199999999999999, precision: 0.9206730769230769, f_beta: 0.919967987194878
train: step: 3540, loss: 0.008467121049761772, acc: 0.99, recall: 0.99, precision: 0.9901960784313726, f_beta: 0.98999899989999
train: step: 3541, loss: 0.01268372219055891, acc: 0.98, recall: 0.98, precision: 0.9807692307692308, f_beta: 0.9799919967987194
train: step: 3542, loss: 0.037907905876636505, acc: 0.95, recall: 0.95, precision: 0.9501800720288115, f_beta: 0.9499949994999499
train: step: 3543, loss: 0.018282871693372726, acc: 0.98, recall: 0.98, precision: 0.9807692307692308, f_beta: 0.9799919967987194
train: step: 3544, loss: 0.04622505232691765, acc: 0.94, recall: 0.94, precision: 0.94, f_beta: 0.94
train: step: 3545, loss: 0.07123862951993942, acc: 0.89, recall: 0.89, precision: 0.9030591153369161, f_beta: 0.8891017239641092
train: step: 3546, loss: 0.006724493112415075, acc: 0.99, recall: 0.99, precision: 0.9901960784313726, f_beta: 0.98999899989999
train: step: 3547, loss: 0.042227163910865784, acc: 0.96, recall: 0.96, precision: 0.9607371794871795, f_beta: 0.959983993597439
train: step: 3548, loss: 0.0484933964908123, acc: 0.95, recall: 0.95, precision: 0.9501800720288115, f_beta: 0.9499949994999499
train: step: 3549, loss: 0.02648214064538479, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 3550, loss: 0.019495898857712746, acc: 0.98, recall: 0.98, precision: 0.9807692307692308, f_beta: 0.9799919967987194
train: step: 3551, loss: 0.02625681832432747, acc: 0.97, recall: 0.97, precision: 0.9716981132075472, f_beta: 0.9699729756781104
train: step: 3552, loss: 0.013098794966936111, acc: 0.98, recall: 0.98, precision: 0.98, f_beta: 0.98
train: step: 3553, loss: 0.032280851155519485, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 3554, loss: 0.035129107534885406, acc: 0.96, recall: 0.96, precision: 0.962962962962963, f_beta: 0.9599358974358974
train: step: 3555, loss: 0.02845957688987255, acc: 0.98, recall: 0.98, precision: 0.9807692307692308, f_beta: 0.9799919967987194
train: step: 3556, loss: 0.021741097792983055, acc: 0.98, recall: 0.98, precision: 0.98, f_beta: 0.98
train: step: 3557, loss: 0.024851994588971138, acc: 0.98, recall: 0.98, precision: 0.9807692307692308, f_beta: 0.9799919967987194
train: step: 3558, loss: 0.023147540166974068, acc: 0.98, recall: 0.98, precision: 0.9807692307692308, f_beta: 0.9799919967987194
train: step: 3559, loss: 0.031225718557834625, acc: 0.96, recall: 0.96, precision: 0.962962962962963, f_beta: 0.9599358974358974
train: step: 3560, loss: 0.07080254703760147, acc: 0.92, recall: 0.9199999999999999, precision: 0.9206730769230769, f_beta: 0.919967987194878
train: step: 3561, loss: 0.04570251330733299, acc: 0.96, recall: 0.96, precision: 0.9607371794871795, f_beta: 0.959983993597439
train: step: 3562, loss: 0.028969693928956985, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 3563, loss: 0.03968362137675285, acc: 0.95, recall: 0.95, precision: 0.9516258530710557, f_beta: 0.949954959463517
train: step: 3564, loss: 0.022725848481059074, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 3565, loss: 0.014034610241651535, acc: 0.98, recall: 0.98, precision: 0.9807692307692308, f_beta: 0.9799919967987194
train: step: 3566, loss: 0.006396522745490074, acc: 1.0, recall: 1.0, precision: 1.0, f_beta: 1.0
train: step: 3567, loss: 0.02666831947863102, acc: 0.95, recall: 0.95, precision: 0.9501800720288115, f_beta: 0.9499949994999499
train: step: 3568, loss: 0.050813738256692886, acc: 0.94, recall: 0.94, precision: 0.9407051282051282, f_beta: 0.9399759903961584
train: step: 3569, loss: 0.01703990250825882, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 3570, loss: 0.04678194224834442, acc: 0.94, recall: 0.94, precision: 0.9428341384863124, f_beta: 0.9399038461538463
train: step: 3571, loss: 0.033925026655197144, acc: 0.95, recall: 0.95, precision: 0.9501800720288115, f_beta: 0.9499949994999499
train: step: 3572, loss: 0.012411530129611492, acc: 0.98, recall: 0.98, precision: 0.9807692307692308, f_beta: 0.9799919967987194
train: step: 3573, loss: 0.025556018576025963, acc: 0.97, recall: 0.97, precision: 0.9716981132075472, f_beta: 0.9699729756781104
train: step: 3574, loss: 0.03432362154126167, acc: 0.96, recall: 0.96, precision: 0.96, f_beta: 0.96
train: step: 3575, loss: 0.02536846697330475, acc: 0.97, recall: 0.97, precision: 0.9716981132075472, f_beta: 0.9699729756781104
train: step: 3576, loss: 0.07685232907533646, acc: 0.9, recall: 0.9, precision: 0.9006410256410255, f_beta: 0.8999599839935974
train: step: 3577, loss: 0.026354456320405006, acc: 0.96, recall: 0.96, precision: 0.9607371794871795, f_beta: 0.959983993597439
train: step: 3578, loss: 0.050786953419446945, acc: 0.95, recall: 0.95, precision: 0.9501800720288115, f_beta: 0.9499949994999499
train: step: 3579, loss: 0.02123125083744526, acc: 0.98, recall: 0.98, precision: 0.98, f_beta: 0.98
train: step: 3580, loss: 0.06919687986373901, acc: 0.91, recall: 0.9099999999999999, precision: 0.9114813327980731, f_beta: 0.9099189270343309
train: step: 3581, loss: 0.06842634081840515, acc: 0.9, recall: 0.9, precision: 0.9, f_beta: 0.9
train: step: 3582, loss: 0.03017439879477024, acc: 0.96, recall: 0.96, precision: 0.9607371794871795, f_beta: 0.959983993597439
train: step: 3583, loss: 0.030838821083307266, acc: 0.95, recall: 0.95, precision: 0.9516258530710557, f_beta: 0.949954959463517
train: step: 3584, loss: 0.03079535998404026, acc: 0.96, recall: 0.96, precision: 0.96, f_beta: 0.96
train: step: 3585, loss: 0.010170978493988514, acc: 0.99, recall: 0.99, precision: 0.9901960784313726, f_beta: 0.98999899989999
train: step: 3586, loss: 0.023976769298315048, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 3587, loss: 0.0294576995074749, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 3588, loss: 0.034007832407951355, acc: 0.95, recall: 0.95, precision: 0.9516258530710557, f_beta: 0.949954959463517
train: step: 3589, loss: 0.0010441383346915245, acc: 1.0, recall: 1.0, precision: 1.0, f_beta: 1.0
train: step: 3590, loss: 0.029294293373823166, acc: 0.98, recall: 0.98, precision: 0.9807692307692308, f_beta: 0.9799919967987194
train: step: 3591, loss: 0.01843877322971821, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 3592, loss: 0.03724895790219307, acc: 0.95, recall: 0.95, precision: 0.9516258530710557, f_beta: 0.949954959463517
train: step: 3593, loss: 0.012721598148345947, acc: 0.99, recall: 0.99, precision: 0.9901960784313726, f_beta: 0.98999899989999
train: step: 3594, loss: 0.04453383386135101, acc: 0.94, recall: 0.94, precision: 0.9464285714285714, f_beta: 0.939783219590526
train: step: 3595, loss: 0.03414488956332207, acc: 0.94, recall: 0.94, precision: 0.9407051282051282, f_beta: 0.9399759903961584
train: step: 3596, loss: 0.045468416064977646, acc: 0.93, recall: 0.9299999999999999, precision: 0.9315535929345644, f_beta: 0.9299369432489241
train: step: 3597, loss: 0.028035471215844154, acc: 0.96, recall: 0.96, precision: 0.9607371794871795, f_beta: 0.959983993597439
train: step: 3598, loss: 0.08349457383155823, acc: 0.89, recall: 0.89, precision: 0.89015606242497, f_beta: 0.88998899889989
train: step: 3599, loss: 0.04180843383073807, acc: 0.95, recall: 0.95, precision: 0.9501800720288115, f_beta: 0.9499949994999499
train: step: 3600, loss: 0.036111168563365936, acc: 0.96, recall: 0.96, precision: 0.9607371794871795, f_beta: 0.959983993597439
train: step: 3601, loss: 0.02811022289097309, acc: 0.95, recall: 0.95, precision: 0.9501800720288115, f_beta: 0.9499949994999499
train: step: 3602, loss: 0.042754724621772766, acc: 0.95, recall: 0.95, precision: 0.9501800720288115, f_beta: 0.9499949994999499
train: step: 3603, loss: 0.007816601544618607, acc: 0.99, recall: 0.99, precision: 0.9901960784313726, f_beta: 0.98999899989999
train: step: 3604, loss: 0.043015461415052414, acc: 0.95, recall: 0.95, precision: 0.9516258530710557, f_beta: 0.949954959463517
train: step: 3605, loss: 0.014501672238111496, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 3606, loss: 0.04331985488533974, acc: 0.96, recall: 0.96, precision: 0.9607371794871795, f_beta: 0.959983993597439
train: step: 3607, loss: 0.016890456900000572, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 3608, loss: 0.06749261170625687, acc: 0.93, recall: 0.9299999999999999, precision: 0.9343434343434343, f_beta: 0.9298245614035088
train: step: 3609, loss: 0.008662465028464794, acc: 0.99, recall: 0.99, precision: 0.9901960784313726, f_beta: 0.98999899989999
train: step: 3610, loss: 0.04118954762816429, acc: 0.95, recall: 0.95, precision: 0.9545454545454546, f_beta: 0.949874686716792
train: step: 3611, loss: 0.0232720747590065, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 3612, loss: 0.05128994956612587, acc: 0.93, recall: 0.9299999999999999, precision: 0.930172068827531, f_beta: 0.9299929992999301
train: step: 3613, loss: 0.016637487336993217, acc: 0.98, recall: 0.98, precision: 0.9807692307692308, f_beta: 0.9799919967987194
train: step: 3614, loss: 0.02721387892961502, acc: 0.94, recall: 0.94, precision: 0.9407051282051282, f_beta: 0.9399759903961584
train: step: 3615, loss: 0.031610745936632156, acc: 0.95, recall: 0.95, precision: 0.9501800720288115, f_beta: 0.9499949994999499
train: step: 3616, loss: 0.06759081035852432, acc: 0.92, recall: 0.92, precision: 0.92, f_beta: 0.92
train: step: 3617, loss: 0.07042388617992401, acc: 0.91, recall: 0.9099999999999999, precision: 0.9114813327980731, f_beta: 0.9099189270343309
train: step: 3618, loss: 0.049548130482435226, acc: 0.94, recall: 0.94, precision: 0.9428341384863124, f_beta: 0.9399038461538463
train: step: 3619, loss: 0.00928263645619154, acc: 0.98, recall: 0.98, precision: 0.9807692307692308, f_beta: 0.9799919967987194
train: step: 3620, loss: 0.02328789234161377, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 3621, loss: 0.012061779387295246, acc: 0.99, recall: 0.99, precision: 0.9901960784313726, f_beta: 0.98999899989999
train: step: 3622, loss: 0.032316092401742935, acc: 0.95, recall: 0.95, precision: 0.9516258530710557, f_beta: 0.949954959463517
train: step: 3623, loss: 0.024329962208867073, acc: 0.96, recall: 0.96, precision: 0.96, f_beta: 0.96
train: step: 3624, loss: 0.023898109793663025, acc: 0.96, recall: 0.96, precision: 0.96, f_beta: 0.96
train: step: 3625, loss: 0.019444799050688744, acc: 0.97, recall: 0.97, precision: 0.9716981132075472, f_beta: 0.9699729756781104
train: step: 3626, loss: 0.008648225106298923, acc: 0.99, recall: 0.99, precision: 0.9901960784313726, f_beta: 0.98999899989999
train: step: 3627, loss: 0.04614489898085594, acc: 0.94, recall: 0.94, precision: 0.9428341384863124, f_beta: 0.9399038461538463
train: step: 3628, loss: 0.024085579439997673, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 3629, loss: 0.021324044093489647, acc: 0.98, recall: 0.98, precision: 0.9807692307692308, f_beta: 0.9799919967987194
train: step: 3630, loss: 0.03405674919486046, acc: 0.96, recall: 0.96, precision: 0.9607371794871795, f_beta: 0.959983993597439
train: step: 3631, loss: 0.03356427699327469, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 3632, loss: 0.005805195774883032, acc: 1.0, recall: 1.0, precision: 1.0, f_beta: 1.0
train: step: 3633, loss: 0.05340150371193886, acc: 0.9, recall: 0.9, precision: 0.9, f_beta: 0.9
train: step: 3634, loss: 0.01757051795721054, acc: 0.99, recall: 0.99, precision: 0.9901960784313726, f_beta: 0.98999899989999
train: step: 3635, loss: 0.004150195978581905, acc: 1.0, recall: 1.0, precision: 1.0, f_beta: 1.0
train: step: 3636, loss: 0.01798059791326523, acc: 0.98, recall: 0.98, precision: 0.98, f_beta: 0.98
train: step: 3637, loss: 0.01908598281443119, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 3638, loss: 0.012910127639770508, acc: 0.99, recall: 0.99, precision: 0.9901960784313726, f_beta: 0.98999899989999
train: step: 3639, loss: 0.007943416014313698, acc: 0.98, recall: 0.98, precision: 0.9807692307692308, f_beta: 0.9799919967987194
train: step: 3640, loss: 0.05296773090958595, acc: 0.93, recall: 0.9299999999999999, precision: 0.930172068827531, f_beta: 0.9299929992999301
train: step: 3641, loss: 0.014898309484124184, acc: 0.98, recall: 0.98, precision: 0.98, f_beta: 0.98
train: step: 3642, loss: 0.025835411623120308, acc: 0.97, recall: 0.97, precision: 0.9716981132075472, f_beta: 0.9699729756781104
train: step: 3643, loss: 0.004453670233488083, acc: 1.0, recall: 1.0, precision: 1.0, f_beta: 1.0
train: step: 3644, loss: 0.01605428196489811, acc: 0.98, recall: 0.98, precision: 0.98, f_beta: 0.98
train: step: 3645, loss: 0.025771062821149826, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 3646, loss: 0.0472707599401474, acc: 0.94, recall: 0.94, precision: 0.9407051282051282, f_beta: 0.9399759903961584
train: step: 3647, loss: 0.04986591264605522, acc: 0.94, recall: 0.94, precision: 0.9464285714285714, f_beta: 0.939783219590526
train: step: 3648, loss: 0.016872994601726532, acc: 0.98, recall: 0.98, precision: 0.98, f_beta: 0.98
train: step: 3649, loss: 0.031154785305261612, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 3650, loss: 0.0008476445218548179, acc: 1.0, recall: 1.0, precision: 1.0, f_beta: 1.0
train: step: 3651, loss: 0.05011751130223274, acc: 0.93, recall: 0.9299999999999999, precision: 0.9315535929345644, f_beta: 0.9299369432489241
train: step: 3652, loss: 0.031925320625305176, acc: 0.96, recall: 0.96, precision: 0.96, f_beta: 0.96
train: step: 3653, loss: 0.014280669391155243, acc: 0.99, recall: 0.99, precision: 0.9901960784313726, f_beta: 0.98999899989999
train: step: 3654, loss: 0.054651424288749695, acc: 0.93, recall: 0.9299999999999999, precision: 0.9315535929345644, f_beta: 0.9299369432489241
train: step: 3655, loss: 0.022588467225432396, acc: 0.96, recall: 0.96, precision: 0.9607371794871795, f_beta: 0.959983993597439
train: step: 3656, loss: 0.06147265434265137, acc: 0.94, recall: 0.94, precision: 0.9407051282051282, f_beta: 0.9399759903961584
train: step: 3657, loss: 0.02234504371881485, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 3658, loss: 0.02762400545179844, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 3659, loss: 0.04111102968454361, acc: 0.95, recall: 0.95, precision: 0.9501800720288115, f_beta: 0.9499949994999499
train: step: 3660, loss: 0.0522954948246479, acc: 0.94, recall: 0.94, precision: 0.94, f_beta: 0.94
train: step: 3661, loss: 0.040719758719205856, acc: 0.94, recall: 0.94, precision: 0.9464285714285714, f_beta: 0.939783219590526
train: step: 3662, loss: 0.03034554421901703, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 3663, loss: 0.014968869276344776, acc: 0.98, recall: 0.98, precision: 0.9807692307692308, f_beta: 0.9799919967987194
train: step: 3664, loss: 0.01317269541323185, acc: 0.99, recall: 0.99, precision: 0.9901960784313726, f_beta: 0.98999899989999
train: step: 3665, loss: 0.052691224962472916, acc: 0.94, recall: 0.94, precision: 0.9407051282051282, f_beta: 0.9399759903961584
train: step: 3666, loss: 0.04162425920367241, acc: 0.95, recall: 0.95, precision: 0.9501800720288115, f_beta: 0.9499949994999499
train: step: 3667, loss: 0.04333997145295143, acc: 0.95, recall: 0.95, precision: 0.9501800720288115, f_beta: 0.9499949994999499
train: step: 3668, loss: 0.02128671668469906, acc: 0.98, recall: 0.98, precision: 0.9807692307692308, f_beta: 0.9799919967987194
train: step: 3669, loss: 0.009122165851294994, acc: 0.98, recall: 0.98, precision: 0.98, f_beta: 0.98
train: step: 3670, loss: 0.022945258766412735, acc: 0.98, recall: 0.98, precision: 0.9807692307692308, f_beta: 0.9799919967987194
train: step: 3671, loss: 0.03654192388057709, acc: 0.96, recall: 0.96, precision: 0.9607371794871795, f_beta: 0.959983993597439
train: step: 3672, loss: 0.025694645941257477, acc: 0.98, recall: 0.98, precision: 0.98, f_beta: 0.98
train: step: 3673, loss: 0.0030999802984297276, acc: 1.0, recall: 1.0, precision: 1.0, f_beta: 1.0
train: step: 3674, loss: 0.042221516370773315, acc: 0.94, recall: 0.94, precision: 0.94, f_beta: 0.94
train: step: 3675, loss: 0.023613614961504936, acc: 0.96, recall: 0.96, precision: 0.9607371794871795, f_beta: 0.959983993597439
train: step: 3676, loss: 0.043664637953042984, acc: 0.92, recall: 0.9199999999999999, precision: 0.9206730769230769, f_beta: 0.919967987194878
train: step: 3677, loss: 0.056938961148262024, acc: 0.92, recall: 0.9199999999999999, precision: 0.9206730769230769, f_beta: 0.919967987194878
train: step: 3678, loss: 0.019197901710867882, acc: 0.98, recall: 0.98, precision: 0.9807692307692308, f_beta: 0.9799919967987194
train: step: 3679, loss: 0.03297562897205353, acc: 0.97, recall: 0.97, precision: 0.9716981132075472, f_beta: 0.9699729756781104
train: step: 3680, loss: 0.04021716117858887, acc: 0.94, recall: 0.94, precision: 0.9407051282051282, f_beta: 0.9399759903961584
train: step: 3681, loss: 0.024541471153497696, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 3682, loss: 0.04113566502928734, acc: 0.94, recall: 0.94, precision: 0.9407051282051282, f_beta: 0.9399759903961584
train: step: 3683, loss: 0.049839816987514496, acc: 0.92, recall: 0.9199999999999999, precision: 0.9227053140096618, f_beta: 0.9198717948717948
train: step: 3684, loss: 0.04802240803837776, acc: 0.95, recall: 0.95, precision: 0.9516258530710557, f_beta: 0.949954959463517
train: step: 3685, loss: 0.04511260613799095, acc: 0.94, recall: 0.94, precision: 0.9407051282051282, f_beta: 0.9399759903961584
train: step: 3686, loss: 0.01641980931162834, acc: 0.98, recall: 0.98, precision: 0.98, f_beta: 0.98
train: step: 3687, loss: 0.04617590084671974, acc: 0.94, recall: 0.94, precision: 0.9428341384863124, f_beta: 0.9399038461538463
train: step: 3688, loss: 0.03695652633905411, acc: 0.96, recall: 0.96, precision: 0.962962962962963, f_beta: 0.9599358974358974
train: step: 3689, loss: 0.058023080229759216, acc: 0.93, recall: 0.9299999999999999, precision: 0.9385964912280702, f_beta: 0.9296553110240177
train: step: 3690, loss: 0.0542975515127182, acc: 0.94, recall: 0.94, precision: 0.9407051282051282, f_beta: 0.9399759903961584
train: step: 3691, loss: 0.0416640006005764, acc: 0.96, recall: 0.96, precision: 0.9607371794871795, f_beta: 0.959983993597439
train: step: 3692, loss: 0.03196949511766434, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 3693, loss: 0.011442436836659908, acc: 0.99, recall: 0.99, precision: 0.9901960784313726, f_beta: 0.98999899989999
train: step: 3694, loss: 0.01759655959904194, acc: 0.96, recall: 0.96, precision: 0.9607371794871795, f_beta: 0.959983993597439
train: step: 3695, loss: 0.03505590185523033, acc: 0.93, recall: 0.9299999999999999, precision: 0.930172068827531, f_beta: 0.9299929992999301
train: step: 3696, loss: 0.04977385699748993, acc: 0.96, recall: 0.96, precision: 0.96, f_beta: 0.96
train: step: 3697, loss: 0.05233251675963402, acc: 0.91, recall: 0.9099999999999999, precision: 0.9181966544267646, f_beta: 0.9095568284594513
train: step: 3698, loss: 0.029302246868610382, acc: 0.96, recall: 0.96, precision: 0.96, f_beta: 0.96
train: step: 3699, loss: 0.08303860574960709, acc: 0.9, recall: 0.8999999999999999, precision: 0.9025764895330113, f_beta: 0.8998397435897436
train: step: 3700, loss: 0.02675056643784046, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 3701, loss: 0.07279600948095322, acc: 0.92, recall: 0.9199999999999999, precision: 0.9206730769230769, f_beta: 0.919967987194878
train: step: 3702, loss: 0.02130351960659027, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 3703, loss: 0.018392466008663177, acc: 0.98, recall: 0.98, precision: 0.98, f_beta: 0.98
train: step: 3704, loss: 0.02299818955361843, acc: 0.98, recall: 0.98, precision: 0.9807692307692308, f_beta: 0.9799919967987194
train: step: 3705, loss: 0.018641270697116852, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 3706, loss: 0.0538644902408123, acc: 0.94, recall: 0.94, precision: 0.94, f_beta: 0.94
train: step: 3707, loss: 0.050373245030641556, acc: 0.94, recall: 0.94, precision: 0.9428341384863124, f_beta: 0.9399038461538463
train: step: 3708, loss: 0.02237323671579361, acc: 0.98, recall: 0.98, precision: 0.9807692307692308, f_beta: 0.9799919967987194
train: step: 3709, loss: 0.04169054329395294, acc: 0.92, recall: 0.9199999999999999, precision: 0.9261363636363636, f_beta: 0.9197109594540345
train: step: 3710, loss: 0.056151553988456726, acc: 0.94, recall: 0.94, precision: 0.9407051282051282, f_beta: 0.9399759903961584
train: step: 3711, loss: 0.05825446918606758, acc: 0.94, recall: 0.94, precision: 0.9407051282051282, f_beta: 0.9399759903961584
train: step: 3712, loss: 0.05186043679714203, acc: 0.95, recall: 0.95, precision: 0.9501800720288115, f_beta: 0.9499949994999499
train: step: 3713, loss: 0.02648102678358555, acc: 0.96, recall: 0.96, precision: 0.9607371794871795, f_beta: 0.959983993597439
train: step: 3714, loss: 0.030301084741950035, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 3715, loss: 0.017234111204743385, acc: 0.98, recall: 0.98, precision: 0.9807692307692308, f_beta: 0.9799919967987194
train: step: 3716, loss: 0.009128440171480179, acc: 0.99, recall: 0.99, precision: 0.9901960784313726, f_beta: 0.98999899989999
train: step: 3717, loss: 0.027826929464936256, acc: 0.96, recall: 0.96, precision: 0.9607371794871795, f_beta: 0.959983993597439
train: step: 3718, loss: 0.026045560836791992, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 3719, loss: 0.025846635922789574, acc: 0.96, recall: 0.96, precision: 0.96, f_beta: 0.96
train: step: 3720, loss: 0.038520265370607376, acc: 0.95, recall: 0.95, precision: 0.9516258530710557, f_beta: 0.949954959463517
train: step: 3721, loss: 0.03520427271723747, acc: 0.96, recall: 0.96, precision: 0.962962962962963, f_beta: 0.9599358974358974
train: step: 3722, loss: 0.04733511060476303, acc: 0.94, recall: 0.94, precision: 0.9407051282051282, f_beta: 0.9399759903961584
train: step: 3723, loss: 0.04149998724460602, acc: 0.94, recall: 0.94, precision: 0.9428341384863124, f_beta: 0.9399038461538463
train: step: 3724, loss: 0.030595185235142708, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 3725, loss: 0.0021936679258942604, acc: 1.0, recall: 1.0, precision: 1.0, f_beta: 1.0
train: step: 3726, loss: 0.058680709451436996, acc: 0.9, recall: 0.8999999999999999, precision: 0.9105090311986863, f_beta: 0.8993558776167472
train: step: 3727, loss: 0.029587561264634132, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 3728, loss: 0.0232295673340559, acc: 0.98, recall: 0.98, precision: 0.9807692307692308, f_beta: 0.9799919967987194
train: step: 3729, loss: 0.029993517324328423, acc: 0.97, recall: 0.97, precision: 0.9716981132075472, f_beta: 0.9699729756781104
train: step: 3730, loss: 0.05355166271328926, acc: 0.94, recall: 0.94, precision: 0.9464285714285714, f_beta: 0.939783219590526
train: step: 3731, loss: 0.05743889510631561, acc: 0.94, recall: 0.94, precision: 0.9464285714285714, f_beta: 0.939783219590526
train: step: 3732, loss: 0.045316509902477264, acc: 0.94, recall: 0.94, precision: 0.9407051282051282, f_beta: 0.9399759903961584
train: step: 3733, loss: 0.05316603556275368, acc: 0.96, recall: 0.96, precision: 0.9607371794871795, f_beta: 0.959983993597439
train: step: 3734, loss: 0.04520208761096001, acc: 0.95, recall: 0.95, precision: 0.9501800720288115, f_beta: 0.9499949994999499
train: step: 3735, loss: 0.020856456831097603, acc: 0.98, recall: 0.98, precision: 0.98, f_beta: 0.98
train: step: 3736, loss: 0.04041474685072899, acc: 0.96, recall: 0.96, precision: 0.96, f_beta: 0.96
train: step: 3737, loss: 0.04251383617520332, acc: 0.93, recall: 0.9299999999999999, precision: 0.9315535929345644, f_beta: 0.9299369432489241
train: step: 3738, loss: 0.020196883007884026, acc: 0.98, recall: 0.98, precision: 0.98, f_beta: 0.98
train: step: 3739, loss: 0.026721008121967316, acc: 0.96, recall: 0.96, precision: 0.96, f_beta: 0.96
train: step: 3740, loss: 0.019213341176509857, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 3741, loss: 0.005657400470227003, acc: 0.99, recall: 0.99, precision: 0.9901960784313726, f_beta: 0.98999899989999
train: step: 3742, loss: 0.011455381289124489, acc: 0.99, recall: 0.99, precision: 0.9901960784313726, f_beta: 0.98999899989999
train: step: 3743, loss: 0.04512703791260719, acc: 0.95, recall: 0.95, precision: 0.9501800720288115, f_beta: 0.9499949994999499
train: step: 3744, loss: 0.020893411710858345, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 3745, loss: 0.04882718622684479, acc: 0.92, recall: 0.92, precision: 0.92, f_beta: 0.92
train: step: 3746, loss: 0.014336220920085907, acc: 0.99, recall: 0.99, precision: 0.9901960784313726, f_beta: 0.98999899989999
train: step: 3747, loss: 0.03879696503281593, acc: 0.95, recall: 0.95, precision: 0.9516258530710557, f_beta: 0.949954959463517
train: step: 3748, loss: 0.03868446499109268, acc: 0.97, recall: 0.97, precision: 0.9716981132075472, f_beta: 0.9699729756781104
train: step: 3749, loss: 0.009560937993228436, acc: 0.99, recall: 0.99, precision: 0.9901960784313726, f_beta: 0.98999899989999
train: step: 3750, loss: 0.010208087041974068, acc: 0.99, recall: 0.99, precision: 0.9901960784313726, f_beta: 0.98999899989999
train: step: 3751, loss: 0.04142983257770538, acc: 0.96, recall: 0.96, precision: 0.96, f_beta: 0.96
train: step: 3752, loss: 0.01304207555949688, acc: 0.99, recall: 0.99, precision: 0.9901960784313726, f_beta: 0.98999899989999
train: step: 3753, loss: 0.028610287234187126, acc: 0.96, recall: 0.96, precision: 0.9607371794871795, f_beta: 0.959983993597439
train: step: 3754, loss: 0.05001578480005264, acc: 0.95, recall: 0.95, precision: 0.9501800720288115, f_beta: 0.9499949994999499
train: step: 3755, loss: 0.040808118879795074, acc: 0.95, recall: 0.95, precision: 0.9501800720288115, f_beta: 0.9499949994999499
train: step: 3756, loss: 0.03301024064421654, acc: 0.96, recall: 0.96, precision: 0.96, f_beta: 0.96
train: step: 3757, loss: 0.023133493959903717, acc: 0.97, recall: 0.97, precision: 0.9716981132075472, f_beta: 0.9699729756781104
train: step: 3758, loss: 0.030672064051032066, acc: 0.96, recall: 0.96, precision: 0.9607371794871795, f_beta: 0.959983993597439
train: step: 3759, loss: 0.03441132232546806, acc: 0.96, recall: 0.96, precision: 0.9607371794871795, f_beta: 0.959983993597439
train: step: 3760, loss: 0.032812174409627914, acc: 0.97, recall: 0.97, precision: 0.9716981132075472, f_beta: 0.9699729756781104
train: step: 3761, loss: 0.03905817121267319, acc: 0.96, recall: 0.96, precision: 0.96, f_beta: 0.96
train: step: 3762, loss: 0.04492903873324394, acc: 0.95, recall: 0.95, precision: 0.9516258530710557, f_beta: 0.949954959463517
train: step: 3763, loss: 0.012654240243136883, acc: 0.99, recall: 0.99, precision: 0.9901960784313726, f_beta: 0.98999899989999
train: step: 3764, loss: 0.047070447355508804, acc: 0.94, recall: 0.94, precision: 0.94, f_beta: 0.94
train: step: 3765, loss: 0.03227248787879944, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 3766, loss: 0.03155546262860298, acc: 0.97, recall: 0.97, precision: 0.9716981132075472, f_beta: 0.9699729756781104
train: step: 3767, loss: 0.02042704075574875, acc: 0.97, recall: 0.97, precision: 0.9716981132075472, f_beta: 0.9699729756781104
train: step: 3768, loss: 0.017643384635448456, acc: 0.97, recall: 0.97, precision: 0.9716981132075472, f_beta: 0.9699729756781104
train: step: 3769, loss: 0.01767408289015293, acc: 0.98, recall: 0.98, precision: 0.98, f_beta: 0.98
train: step: 3770, loss: 0.01950692944228649, acc: 0.98, recall: 0.98, precision: 0.9807692307692308, f_beta: 0.9799919967987194
train: step: 3771, loss: 0.015446099452674389, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 3772, loss: 0.019555633887648582, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 3773, loss: 0.021250247955322266, acc: 0.97, recall: 0.97, precision: 0.9716981132075472, f_beta: 0.9699729756781104
train: step: 3774, loss: 0.03121226280927658, acc: 0.97, recall: 0.97, precision: 0.9716981132075472, f_beta: 0.9699729756781104
train: step: 3775, loss: 0.050662536174058914, acc: 0.93, recall: 0.9299999999999999, precision: 0.9315535929345644, f_beta: 0.9299369432489241
train: step: 3776, loss: 0.0048113917000591755, acc: 0.99, recall: 0.99, precision: 0.9901960784313726, f_beta: 0.98999899989999
train: step: 3777, loss: 0.04212023690342903, acc: 0.95, recall: 0.95, precision: 0.9516258530710557, f_beta: 0.949954959463517
train: step: 3778, loss: 0.01570238545536995, acc: 0.98, recall: 0.98, precision: 0.9807692307692308, f_beta: 0.9799919967987194
train: step: 3779, loss: 0.04176516458392143, acc: 0.95, recall: 0.95, precision: 0.9545454545454546, f_beta: 0.949874686716792
train: step: 3780, loss: 0.07469861954450607, acc: 0.91, recall: 0.91, precision: 0.9101640656262505, f_beta: 0.90999099909991
train: step: 3781, loss: 0.02359936200082302, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 3782, loss: 0.051454439759254456, acc: 0.91, recall: 0.9099999999999999, precision: 0.9114813327980731, f_beta: 0.9099189270343309
train: step: 3783, loss: 0.05802667513489723, acc: 0.92, recall: 0.92, precision: 0.92, f_beta: 0.92
train: step: 3784, loss: 0.037841591984033585, acc: 0.94, recall: 0.94, precision: 0.9407051282051282, f_beta: 0.9399759903961584
train: step: 3785, loss: 0.07644295692443848, acc: 0.91, recall: 0.9099999999999999, precision: 0.9181966544267646, f_beta: 0.9095568284594513
train: step: 3786, loss: 0.03189430013298988, acc: 0.95, recall: 0.95, precision: 0.9545454545454546, f_beta: 0.949874686716792
train: step: 3787, loss: 0.05503416433930397, acc: 0.92, recall: 0.92, precision: 0.92, f_beta: 0.92
train: step: 3788, loss: 0.04539104551076889, acc: 0.95, recall: 0.95, precision: 0.9501800720288115, f_beta: 0.9499949994999499
train: step: 3789, loss: 0.007052059285342693, acc: 0.99, recall: 0.99, precision: 0.9901960784313726, f_beta: 0.98999899989999
train: step: 3790, loss: 0.014697041362524033, acc: 0.99, recall: 0.99, precision: 0.9901960784313726, f_beta: 0.98999899989999
train: step: 3791, loss: 0.030176792293787003, acc: 0.95, recall: 0.95, precision: 0.9545454545454546, f_beta: 0.949874686716792
train: step: 3792, loss: 0.050902534276247025, acc: 0.92, recall: 0.9199999999999999, precision: 0.9227053140096618, f_beta: 0.9198717948717948
train: step: 3793, loss: 0.024781914427876472, acc: 0.98, recall: 0.98, precision: 0.9807692307692308, f_beta: 0.9799919967987194
train: step: 3794, loss: 0.012929843738675117, acc: 0.99, recall: 0.99, precision: 0.9901960784313726, f_beta: 0.98999899989999
train: step: 3795, loss: 0.08551313728094101, acc: 0.91, recall: 0.9099999999999999, precision: 0.9181966544267646, f_beta: 0.9095568284594513
train: step: 3796, loss: 0.04388807341456413, acc: 0.94, recall: 0.94, precision: 0.9464285714285714, f_beta: 0.939783219590526
train: step: 3797, loss: 0.03882553428411484, acc: 0.95, recall: 0.95, precision: 0.9516258530710557, f_beta: 0.949954959463517
train: step: 3798, loss: 0.02644035778939724, acc: 0.98, recall: 0.98, precision: 0.98, f_beta: 0.98
train: step: 3799, loss: 0.013419060967862606, acc: 0.99, recall: 0.99, precision: 0.9901960784313726, f_beta: 0.98999899989999
train: step: 3800, loss: 0.0783374011516571, acc: 0.91, recall: 0.9099999999999999, precision: 0.9141414141414141, f_beta: 0.9097744360902256
train: step: 3801, loss: 0.015561435371637344, acc: 0.98, recall: 0.98, precision: 0.98, f_beta: 0.98
train: step: 3802, loss: 0.05338151007890701, acc: 0.94, recall: 0.94, precision: 0.94, f_beta: 0.94
train: step: 3803, loss: 0.05542716011404991, acc: 0.92, recall: 0.9199999999999999, precision: 0.9227053140096618, f_beta: 0.9198717948717948
train: step: 3804, loss: 0.036047860980033875, acc: 0.96, recall: 0.96, precision: 0.96, f_beta: 0.96
train: step: 3805, loss: 0.017222175374627113, acc: 0.98, recall: 0.98, precision: 0.98, f_beta: 0.98
train: step: 3806, loss: 0.031997308135032654, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 3807, loss: 0.0406855009496212, acc: 0.93, recall: 0.9299999999999999, precision: 0.9315535929345644, f_beta: 0.9299369432489241
train: step: 3808, loss: 0.0022194392513483763, acc: 1.0, recall: 1.0, precision: 1.0, f_beta: 1.0
train: step: 3809, loss: 0.0365648977458477, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 3810, loss: 0.022543039172887802, acc: 0.98, recall: 0.98, precision: 0.98, f_beta: 0.98
train: step: 3811, loss: 0.018504416570067406, acc: 0.98, recall: 0.98, precision: 0.98, f_beta: 0.98
train: step: 3812, loss: 0.0452151745557785, acc: 0.95, recall: 0.95, precision: 0.9501800720288115, f_beta: 0.9499949994999499
train: step: 3813, loss: 0.018756525591015816, acc: 0.98, recall: 0.98, precision: 0.9807692307692308, f_beta: 0.9799919967987194
train: step: 3814, loss: 0.034445032477378845, acc: 0.94, recall: 0.94, precision: 0.94, f_beta: 0.94
train: step: 3815, loss: 0.018899336457252502, acc: 0.98, recall: 0.98, precision: 0.9807692307692308, f_beta: 0.9799919967987194
train: step: 3816, loss: 0.0417865514755249, acc: 0.94, recall: 0.94, precision: 0.94, f_beta: 0.94
train: step: 3817, loss: 0.04690655693411827, acc: 0.94, recall: 0.94, precision: 0.9428341384863124, f_beta: 0.9399038461538463
train: step: 3818, loss: 0.030235527083277702, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 3819, loss: 0.027907581999897957, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 3820, loss: 0.0396040640771389, acc: 0.95, recall: 0.95, precision: 0.9516258530710557, f_beta: 0.949954959463517
train: step: 3821, loss: 0.04698644578456879, acc: 0.94, recall: 0.94, precision: 0.9407051282051282, f_beta: 0.9399759903961584
train: step: 3822, loss: 0.05151356756687164, acc: 0.94, recall: 0.94, precision: 0.9428341384863124, f_beta: 0.9399038461538463
train: step: 3823, loss: 0.010797509923577309, acc: 0.99, recall: 0.99, precision: 0.9901960784313726, f_beta: 0.98999899989999
train: step: 3824, loss: 0.06457513570785522, acc: 0.93, recall: 0.9299999999999999, precision: 0.9343434343434343, f_beta: 0.9298245614035088
train: step: 3825, loss: 0.05226394161581993, acc: 0.94, recall: 0.94, precision: 0.94, f_beta: 0.94
train: step: 3826, loss: 0.03890377655625343, acc: 0.96, recall: 0.96, precision: 0.9607371794871795, f_beta: 0.959983993597439
train: step: 3827, loss: 0.011826073750853539, acc: 0.99, recall: 0.99, precision: 0.9901960784313726, f_beta: 0.98999899989999
train: step: 3828, loss: 0.0635761246085167, acc: 0.92, recall: 0.9199999999999999, precision: 0.9206730769230769, f_beta: 0.919967987194878
train: step: 3829, loss: 0.037368178367614746, acc: 0.97, recall: 0.97, precision: 0.9716981132075472, f_beta: 0.9699729756781104
train: step: 3830, loss: 0.008260476402938366, acc: 0.99, recall: 0.99, precision: 0.9901960784313726, f_beta: 0.98999899989999
train: step: 3831, loss: 0.024543453007936478, acc: 0.98, recall: 0.98, precision: 0.98, f_beta: 0.98
train: step: 3832, loss: 0.058881692588329315, acc: 0.92, recall: 0.9199999999999999, precision: 0.9206730769230769, f_beta: 0.919967987194878
train: step: 3833, loss: 0.02642887830734253, acc: 0.96, recall: 0.96, precision: 0.962962962962963, f_beta: 0.9599358974358974
train: step: 3834, loss: 0.014817185699939728, acc: 0.99, recall: 0.99, precision: 0.9901960784313726, f_beta: 0.98999899989999
train: step: 3835, loss: 0.03827613964676857, acc: 0.96, recall: 0.96, precision: 0.962962962962963, f_beta: 0.9599358974358974
train: step: 3836, loss: 0.0022381118033081293, acc: 1.0, recall: 1.0, precision: 1.0, f_beta: 1.0
train: step: 3837, loss: 0.01606234535574913, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 3838, loss: 0.046305764466524124, acc: 0.95, recall: 0.95, precision: 0.9501800720288115, f_beta: 0.9499949994999499
train: step: 3839, loss: 0.04669773951172829, acc: 0.94, recall: 0.94, precision: 0.9407051282051282, f_beta: 0.9399759903961584
train: step: 3840, loss: 0.026320695877075195, acc: 0.96, recall: 0.96, precision: 0.9607371794871795, f_beta: 0.959983993597439
train: step: 3841, loss: 0.03142975643277168, acc: 0.96, recall: 0.96, precision: 0.96, f_beta: 0.96
train: step: 3842, loss: 0.024886146187782288, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 3843, loss: 0.013725090771913528, acc: 0.99, recall: 0.99, precision: 0.9901960784313726, f_beta: 0.98999899989999
train: step: 3844, loss: 0.058451708406209946, acc: 0.94, recall: 0.94, precision: 0.94, f_beta: 0.94
train: step: 3845, loss: 0.01669137179851532, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 3846, loss: 0.020658817142248154, acc: 0.98, recall: 0.98, precision: 0.9807692307692308, f_beta: 0.9799919967987194
train: step: 3847, loss: 0.02356301061809063, acc: 0.98, recall: 0.98, precision: 0.9807692307692308, f_beta: 0.9799919967987194
train: step: 3848, loss: 0.04034270718693733, acc: 0.95, recall: 0.95, precision: 0.9501800720288115, f_beta: 0.9499949994999499
train: step: 3849, loss: 0.06746180355548859, acc: 0.92, recall: 0.92, precision: 0.92, f_beta: 0.92
train: step: 3850, loss: 0.02264758199453354, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 3851, loss: 0.03719988837838173, acc: 0.97, recall: 0.97, precision: 0.9716981132075472, f_beta: 0.9699729756781104
train: step: 3852, loss: 0.04020971432328224, acc: 0.95, recall: 0.95, precision: 0.9516258530710557, f_beta: 0.949954959463517
train: step: 3853, loss: 0.03249302878975868, acc: 0.96, recall: 0.96, precision: 0.9607371794871795, f_beta: 0.959983993597439
train: step: 3854, loss: 0.03466513752937317, acc: 0.96, recall: 0.96, precision: 0.962962962962963, f_beta: 0.9599358974358974
train: step: 3855, loss: 0.02989264950156212, acc: 0.96, recall: 0.96, precision: 0.962962962962963, f_beta: 0.9599358974358974
train: step: 3856, loss: 0.02565477415919304, acc: 0.96, recall: 0.96, precision: 0.9607371794871795, f_beta: 0.959983993597439
train: step: 3857, loss: 0.036617476493120193, acc: 0.95, recall: 0.95, precision: 0.9501800720288115, f_beta: 0.9499949994999499
train: step: 3858, loss: 0.02842589095234871, acc: 0.96, recall: 0.96, precision: 0.96, f_beta: 0.96
train: step: 3859, loss: 0.003758535487577319, acc: 0.99, recall: 0.99, precision: 0.9901960784313726, f_beta: 0.98999899989999
train: step: 3860, loss: 0.04054546356201172, acc: 0.95, recall: 0.95, precision: 0.9501800720288115, f_beta: 0.9499949994999499
train: step: 3861, loss: 0.013760051690042019, acc: 0.98, recall: 0.98, precision: 0.9807692307692308, f_beta: 0.9799919967987194
train: step: 3862, loss: 0.022762270644307137, acc: 0.96, recall: 0.96, precision: 0.9607371794871795, f_beta: 0.959983993597439
train: step: 3863, loss: 0.039373721927404404, acc: 0.95, recall: 0.95, precision: 0.9501800720288115, f_beta: 0.9499949994999499
train: step: 3864, loss: 0.013440974056720734, acc: 0.99, recall: 0.99, precision: 0.9901960784313726, f_beta: 0.98999899989999
train: step: 3865, loss: 0.03428472578525543, acc: 0.96, recall: 0.96, precision: 0.9607371794871795, f_beta: 0.959983993597439
train: step: 3866, loss: 0.03396035358309746, acc: 0.96, recall: 0.96, precision: 0.962962962962963, f_beta: 0.9599358974358974
train: step: 3867, loss: 0.02787034958600998, acc: 0.96, recall: 0.96, precision: 0.9607371794871795, f_beta: 0.959983993597439
train: step: 3868, loss: 0.008206188678741455, acc: 0.99, recall: 0.99, precision: 0.9901960784313726, f_beta: 0.98999899989999
train: step: 3869, loss: 0.02053764835000038, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 3870, loss: 0.028458580374717712, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 3871, loss: 0.008178029209375381, acc: 0.99, recall: 0.99, precision: 0.9901960784313726, f_beta: 0.98999899989999
train: step: 3872, loss: 0.019366437569260597, acc: 0.98, recall: 0.98, precision: 0.9807692307692308, f_beta: 0.9799919967987194
train: step: 3873, loss: 0.011564302258193493, acc: 0.99, recall: 0.99, precision: 0.9901960784313726, f_beta: 0.98999899989999
train: step: 3874, loss: 0.015950296074151993, acc: 0.99, recall: 0.99, precision: 0.9901960784313726, f_beta: 0.98999899989999
train: step: 3875, loss: 0.029458235949277878, acc: 0.96, recall: 0.96, precision: 0.9607371794871795, f_beta: 0.959983993597439
train: step: 3876, loss: 0.005008908454328775, acc: 1.0, recall: 1.0, precision: 1.0, f_beta: 1.0
train: step: 3877, loss: 0.04172281175851822, acc: 0.96, recall: 0.96, precision: 0.962962962962963, f_beta: 0.9599358974358974
train: step: 3878, loss: 0.04878200963139534, acc: 0.94, recall: 0.94, precision: 0.94, f_beta: 0.94
train: step: 3879, loss: 0.037723787128925323, acc: 0.96, recall: 0.96, precision: 0.9607371794871795, f_beta: 0.959983993597439
train: step: 3880, loss: 0.02466074377298355, acc: 0.97, recall: 0.97, precision: 0.9716981132075472, f_beta: 0.9699729756781104
train: step: 3881, loss: 0.01975969970226288, acc: 0.97, recall: 0.97, precision: 0.9716981132075472, f_beta: 0.9699729756781104
train: step: 3882, loss: 0.05308758094906807, acc: 0.92, recall: 0.9199999999999999, precision: 0.9227053140096618, f_beta: 0.9198717948717948
train: step: 3883, loss: 0.03212862089276314, acc: 0.96, recall: 0.96, precision: 0.962962962962963, f_beta: 0.9599358974358974
train: step: 3884, loss: 0.051136087626218796, acc: 0.95, recall: 0.95, precision: 0.9516258530710557, f_beta: 0.949954959463517
train: step: 3885, loss: 0.052256401628255844, acc: 0.94, recall: 0.94, precision: 0.9428341384863124, f_beta: 0.9399038461538463
train: step: 3886, loss: 0.0361400730907917, acc: 0.96, recall: 0.96, precision: 0.962962962962963, f_beta: 0.9599358974358974
train: step: 3887, loss: 0.025140799582004547, acc: 0.98, recall: 0.98, precision: 0.9807692307692308, f_beta: 0.9799919967987194
train: step: 3888, loss: 0.014792454428970814, acc: 0.98, recall: 0.98, precision: 0.9807692307692308, f_beta: 0.9799919967987194
train: step: 3889, loss: 0.003006051992997527, acc: 1.0, recall: 1.0, precision: 1.0, f_beta: 1.0
train: step: 3890, loss: 0.028443049639463425, acc: 0.96, recall: 0.96, precision: 0.9607371794871795, f_beta: 0.959983993597439
train: step: 3891, loss: 0.0009434839594177902, acc: 1.0, recall: 1.0, precision: 1.0, f_beta: 1.0
train: step: 3892, loss: 0.02127920649945736, acc: 0.98, recall: 0.98, precision: 0.98, f_beta: 0.98
train: step: 3893, loss: 0.040364619344472885, acc: 0.96, recall: 0.96, precision: 0.96, f_beta: 0.96
train: step: 3894, loss: 0.014471576549112797, acc: 0.99, recall: 0.99, precision: 0.9901960784313726, f_beta: 0.98999899989999
train: step: 3895, loss: 0.02188294008374214, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 3896, loss: 0.036212313920259476, acc: 0.96, recall: 0.96, precision: 0.9607371794871795, f_beta: 0.959983993597439
train: step: 3897, loss: 0.012990791350603104, acc: 0.98, recall: 0.98, precision: 0.9807692307692308, f_beta: 0.9799919967987194
train: step: 3898, loss: 0.024331578984856606, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 3899, loss: 0.03621228411793709, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 3900, loss: 0.040676649659872055, acc: 0.94, recall: 0.94, precision: 0.94, f_beta: 0.94
train: step: 3901, loss: 0.06278157979249954, acc: 0.93, recall: 0.9299999999999999, precision: 0.930172068827531, f_beta: 0.9299929992999301
train: step: 3902, loss: 0.02346314862370491, acc: 0.96, recall: 0.96, precision: 0.96, f_beta: 0.96
train: step: 3903, loss: 0.028493117541074753, acc: 0.96, recall: 0.96, precision: 0.96, f_beta: 0.96
train: step: 3904, loss: 0.047424014657735825, acc: 0.92, recall: 0.92, precision: 0.92, f_beta: 0.92
train: step: 3905, loss: 0.03025447390973568, acc: 0.96, recall: 0.96, precision: 0.9607371794871795, f_beta: 0.959983993597439
train: step: 3906, loss: 0.024658117443323135, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 3907, loss: 0.005620819516479969, acc: 1.0, recall: 1.0, precision: 1.0, f_beta: 1.0
train: step: 3908, loss: 0.010112650692462921, acc: 0.98, recall: 0.98, precision: 0.9807692307692308, f_beta: 0.9799919967987194
train: step: 3909, loss: 0.0716777890920639, acc: 0.91, recall: 0.9099999999999999, precision: 0.9114813327980731, f_beta: 0.9099189270343309
train: step: 3910, loss: 0.024040501564741135, acc: 0.97, recall: 0.97, precision: 0.9716981132075472, f_beta: 0.9699729756781104
train: step: 3911, loss: 0.032731615006923676, acc: 0.94, recall: 0.94, precision: 0.9428341384863124, f_beta: 0.9399038461538463
train: step: 3912, loss: 0.022353043779730797, acc: 0.98, recall: 0.98, precision: 0.9807692307692308, f_beta: 0.9799919967987194
train: step: 3913, loss: 0.03463159129023552, acc: 0.97, recall: 0.97, precision: 0.9716981132075472, f_beta: 0.9699729756781104
train: step: 3914, loss: 0.042704418301582336, acc: 0.96, recall: 0.96, precision: 0.96, f_beta: 0.96
train: step: 3915, loss: 0.05199006199836731, acc: 0.95, recall: 0.95, precision: 0.9501800720288115, f_beta: 0.9499949994999499
train: step: 3916, loss: 0.006643614266067743, acc: 0.99, recall: 0.99, precision: 0.9901960784313726, f_beta: 0.98999899989999
train: step: 3917, loss: 0.023506008088588715, acc: 0.98, recall: 0.98, precision: 0.98, f_beta: 0.98
train: step: 3918, loss: 0.05801112949848175, acc: 0.93, recall: 0.9299999999999999, precision: 0.9343434343434343, f_beta: 0.9298245614035088
train: step: 3919, loss: 0.02938707172870636, acc: 0.96, recall: 0.96, precision: 0.96, f_beta: 0.96
train: step: 3920, loss: 0.05349273979663849, acc: 0.93, recall: 0.9299999999999999, precision: 0.930172068827531, f_beta: 0.9299929992999301
train: step: 3921, loss: 0.07985234260559082, acc: 0.92, recall: 0.92, precision: 0.92, f_beta: 0.92
train: step: 3922, loss: 0.014950540848076344, acc: 0.98, recall: 0.98, precision: 0.98, f_beta: 0.98
train: step: 3923, loss: 0.03284255415201187, acc: 0.96, recall: 0.96, precision: 0.9607371794871795, f_beta: 0.959983993597439
train: step: 3924, loss: 0.03632921352982521, acc: 0.96, recall: 0.96, precision: 0.962962962962963, f_beta: 0.9599358974358974
train: step: 3925, loss: 0.01972348988056183, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 3926, loss: 0.025120830163359642, acc: 0.97, recall: 0.97, precision: 0.9716981132075472, f_beta: 0.9699729756781104
train: step: 3927, loss: 0.04655567184090614, acc: 0.95, recall: 0.95, precision: 0.9516258530710557, f_beta: 0.949954959463517
train: step: 3928, loss: 0.038550376892089844, acc: 0.95, recall: 0.95, precision: 0.9501800720288115, f_beta: 0.9499949994999499
train: step: 3929, loss: 0.02493401989340782, acc: 0.96, recall: 0.96, precision: 0.96, f_beta: 0.96
train: step: 3930, loss: 0.02066233567893505, acc: 0.97, recall: 0.97, precision: 0.9716981132075472, f_beta: 0.9699729756781104
train: step: 3931, loss: 0.02080349437892437, acc: 0.98, recall: 0.98, precision: 0.9807692307692308, f_beta: 0.9799919967987194
train: step: 3932, loss: 0.07537999004125595, acc: 0.91, recall: 0.91, precision: 0.9101640656262505, f_beta: 0.90999099909991
train: step: 3933, loss: 0.04915560781955719, acc: 0.94, recall: 0.94, precision: 0.94, f_beta: 0.94
train: step: 3934, loss: 0.003619915572926402, acc: 1.0, recall: 1.0, precision: 1.0, f_beta: 1.0
train: step: 3935, loss: 0.03181251138448715, acc: 0.96, recall: 0.96, precision: 0.96, f_beta: 0.96
train: step: 3936, loss: 0.032642822712659836, acc: 0.96, recall: 0.96, precision: 0.9607371794871795, f_beta: 0.959983993597439
train: step: 3937, loss: 0.055161863565444946, acc: 0.94, recall: 0.94, precision: 0.94, f_beta: 0.94
train: step: 3938, loss: 0.054730698466300964, acc: 0.92, recall: 0.9199999999999999, precision: 0.9206730769230769, f_beta: 0.919967987194878
train: step: 3939, loss: 0.016232524067163467, acc: 0.99, recall: 0.99, precision: 0.9901960784313726, f_beta: 0.98999899989999
train: step: 3940, loss: 0.021547341719269753, acc: 0.98, recall: 0.98, precision: 0.98, f_beta: 0.98
train: step: 3941, loss: 0.028879303485155106, acc: 0.96, recall: 0.96, precision: 0.962962962962963, f_beta: 0.9599358974358974
train: step: 3942, loss: 0.013095414265990257, acc: 0.98, recall: 0.98, precision: 0.9807692307692308, f_beta: 0.9799919967987194
train: step: 3943, loss: 0.054410286247730255, acc: 0.92, recall: 0.9199999999999999, precision: 0.9227053140096618, f_beta: 0.9198717948717948
train: step: 3944, loss: 0.03045143559575081, acc: 0.96, recall: 0.96, precision: 0.9607371794871795, f_beta: 0.959983993597439
train: step: 3945, loss: 0.028383826836943626, acc: 0.96, recall: 0.96, precision: 0.962962962962963, f_beta: 0.9599358974358974
train: step: 3946, loss: 0.017711788415908813, acc: 0.98, recall: 0.98, precision: 0.9807692307692308, f_beta: 0.9799919967987194
train: step: 3947, loss: 0.03431917354464531, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 3948, loss: 0.017531849443912506, acc: 0.98, recall: 0.98, precision: 0.9807692307692308, f_beta: 0.9799919967987194
train: step: 3949, loss: 0.023387465626001358, acc: 0.98, recall: 0.98, precision: 0.98, f_beta: 0.98
train: step: 3950, loss: 0.013949982821941376, acc: 0.99, recall: 0.99, precision: 0.9901960784313726, f_beta: 0.98999899989999
train: step: 3951, loss: 0.051398735493421555, acc: 0.94, recall: 0.94, precision: 0.94, f_beta: 0.94
train: step: 3952, loss: 0.024877188727259636, acc: 0.97, recall: 0.97, precision: 0.9716981132075472, f_beta: 0.9699729756781104
train: step: 3953, loss: 0.03834279999136925, acc: 0.96, recall: 0.96, precision: 0.962962962962963, f_beta: 0.9599358974358974
train: step: 3954, loss: 0.028724387288093567, acc: 0.96, recall: 0.96, precision: 0.9607371794871795, f_beta: 0.959983993597439
train: step: 3955, loss: 0.039461154490709305, acc: 0.96, recall: 0.96, precision: 0.96, f_beta: 0.96
train: step: 3956, loss: 0.019636040553450584, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 3957, loss: 0.01844501681625843, acc: 0.98, recall: 0.98, precision: 0.9807692307692308, f_beta: 0.9799919967987194
train: step: 3958, loss: 0.0570363886654377, acc: 0.93, recall: 0.9299999999999999, precision: 0.9343434343434343, f_beta: 0.9298245614035088
train: step: 3959, loss: 0.008439753204584122, acc: 1.0, recall: 1.0, precision: 1.0, f_beta: 1.0
train: step: 3960, loss: 0.042065706104040146, acc: 0.95, recall: 0.95, precision: 0.9516258530710557, f_beta: 0.949954959463517
train: step: 3961, loss: 0.0272050853818655, acc: 0.96, recall: 0.96, precision: 0.9607371794871795, f_beta: 0.959983993597439
train: step: 3962, loss: 0.005500080529600382, acc: 0.99, recall: 0.99, precision: 0.9901960784313726, f_beta: 0.98999899989999
train: step: 3963, loss: 0.03441426903009415, acc: 0.95, recall: 0.95, precision: 0.9545454545454546, f_beta: 0.949874686716792
train: step: 3964, loss: 0.023618604987859726, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 3965, loss: 0.00448004063218832, acc: 1.0, recall: 1.0, precision: 1.0, f_beta: 1.0
train: step: 3966, loss: 0.0012417170219123363, acc: 1.0, recall: 1.0, precision: 1.0, f_beta: 1.0
train: step: 3967, loss: 0.039305612444877625, acc: 0.95, recall: 0.95, precision: 0.9501800720288115, f_beta: 0.9499949994999499
train: step: 3968, loss: 0.018736250698566437, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 3969, loss: 0.0024205518420785666, acc: 1.0, recall: 1.0, precision: 1.0, f_beta: 1.0
train: step: 3970, loss: 0.08173567056655884, acc: 0.91, recall: 0.91, precision: 0.9101640656262505, f_beta: 0.90999099909991
train: step: 3971, loss: 0.04736848920583725, acc: 0.93, recall: 0.9299999999999999, precision: 0.930172068827531, f_beta: 0.9299929992999301
train: step: 3972, loss: 0.008445638231933117, acc: 0.99, recall: 0.99, precision: 0.9901960784313726, f_beta: 0.98999899989999
train: step: 3973, loss: 0.06882453709840775, acc: 0.91, recall: 0.9099999999999999, precision: 0.9141414141414141, f_beta: 0.9097744360902256
train: step: 3974, loss: 0.012723280116915703, acc: 0.98, recall: 0.98, precision: 0.9807692307692308, f_beta: 0.9799919967987194
train: step: 3975, loss: 0.02353069745004177, acc: 0.98, recall: 0.98, precision: 0.98, f_beta: 0.98
train: step: 3976, loss: 0.002616133075207472, acc: 1.0, recall: 1.0, precision: 1.0, f_beta: 1.0
train: step: 3977, loss: 0.03368400037288666, acc: 0.96, recall: 0.96, precision: 0.962962962962963, f_beta: 0.9599358974358974
train: step: 3978, loss: 0.026969589293003082, acc: 0.97, recall: 0.97, precision: 0.9716981132075472, f_beta: 0.9699729756781104
train: step: 3979, loss: 0.020428262650966644, acc: 0.98, recall: 0.98, precision: 0.98, f_beta: 0.98
train: step: 3980, loss: 0.036671243607997894, acc: 0.96, recall: 0.96, precision: 0.9607371794871795, f_beta: 0.959983993597439
train: step: 3981, loss: 0.07562853395938873, acc: 0.92, recall: 0.92, precision: 0.92, f_beta: 0.92
train: step: 3982, loss: 0.00955099519342184, acc: 0.99, recall: 0.99, precision: 0.9901960784313726, f_beta: 0.98999899989999
train: step: 3983, loss: 0.05373160541057587, acc: 0.95, recall: 0.95, precision: 0.9501800720288115, f_beta: 0.9499949994999499
train: step: 3984, loss: 0.05927666649222374, acc: 0.93, recall: 0.9299999999999999, precision: 0.930172068827531, f_beta: 0.9299929992999301
train: step: 3985, loss: 0.060576438903808594, acc: 0.9, recall: 0.9, precision: 0.9006410256410255, f_beta: 0.8999599839935974
train: step: 3986, loss: 0.043157681822776794, acc: 0.94, recall: 0.94, precision: 0.9428341384863124, f_beta: 0.9399038461538463
train: step: 3987, loss: 0.006718541495501995, acc: 0.99, recall: 0.99, precision: 0.9901960784313726, f_beta: 0.98999899989999
train: step: 3988, loss: 0.04600561782717705, acc: 0.94, recall: 0.94, precision: 0.9407051282051282, f_beta: 0.9399759903961584
train: step: 3989, loss: 0.023176999762654305, acc: 0.98, recall: 0.98, precision: 0.98, f_beta: 0.98
train: step: 3990, loss: 0.02574421837925911, acc: 0.96, recall: 0.96, precision: 0.9607371794871795, f_beta: 0.959983993597439
train: step: 3991, loss: 0.05726052448153496, acc: 0.93, recall: 0.9299999999999999, precision: 0.9343434343434343, f_beta: 0.9298245614035088
train: step: 3992, loss: 0.014662543311715126, acc: 0.98, recall: 0.98, precision: 0.9807692307692308, f_beta: 0.9799919967987194
train: step: 3993, loss: 0.048153143376111984, acc: 0.92, recall: 0.92, precision: 0.92, f_beta: 0.92
train: step: 3994, loss: 0.03329584747552872, acc: 0.96, recall: 0.96, precision: 0.9607371794871795, f_beta: 0.959983993597439
train: step: 3995, loss: 0.04640955105423927, acc: 0.93, recall: 0.9299999999999999, precision: 0.930172068827531, f_beta: 0.9299929992999301
train: step: 3996, loss: 0.028298310935497284, acc: 0.97, recall: 0.97, precision: 0.9716981132075472, f_beta: 0.9699729756781104
train: step: 3997, loss: 0.03145037963986397, acc: 0.96, recall: 0.96, precision: 0.9607371794871795, f_beta: 0.959983993597439
train: step: 3998, loss: 0.046737030148506165, acc: 0.94, recall: 0.94, precision: 0.9428341384863124, f_beta: 0.9399038461538463
train: step: 3999, loss: 0.011568212881684303, acc: 0.98, recall: 0.98, precision: 0.98, f_beta: 0.98
train: step: 4000, loss: 0.03147893026471138, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 4001, loss: 0.04317798092961311, acc: 0.94, recall: 0.94, precision: 0.94, f_beta: 0.94
train: step: 4002, loss: 0.02771860919892788, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 4003, loss: 0.02272922918200493, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 4004, loss: 0.04331030696630478, acc: 0.94, recall: 0.94, precision: 0.94, f_beta: 0.94
train: step: 4005, loss: 0.021215878427028656, acc: 0.98, recall: 0.98, precision: 0.98, f_beta: 0.98
train: step: 4006, loss: 0.052770040929317474, acc: 0.94, recall: 0.94, precision: 0.9428341384863124, f_beta: 0.9399038461538463
train: step: 4007, loss: 0.036282435059547424, acc: 0.96, recall: 0.96, precision: 0.9607371794871795, f_beta: 0.959983993597439
train: step: 4008, loss: 0.028321336954832077, acc: 0.97, recall: 0.97, precision: 0.9716981132075472, f_beta: 0.9699729756781104
train: step: 4009, loss: 0.024238187819719315, acc: 0.97, recall: 0.97, precision: 0.9716981132075472, f_beta: 0.9699729756781104
train: step: 4010, loss: 0.02417195774614811, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 4011, loss: 0.01609697751700878, acc: 0.98, recall: 0.98, precision: 0.9807692307692308, f_beta: 0.9799919967987194
train: step: 4012, loss: 0.03316570818424225, acc: 0.96, recall: 0.96, precision: 0.96, f_beta: 0.96
train: step: 4013, loss: 0.046397674828767776, acc: 0.94, recall: 0.94, precision: 0.9407051282051282, f_beta: 0.9399759903961584
train: step: 4014, loss: 0.03744306415319443, acc: 0.96, recall: 0.96, precision: 0.9607371794871795, f_beta: 0.959983993597439
train: step: 4015, loss: 0.036917686462402344, acc: 0.95, recall: 0.95, precision: 0.9501800720288115, f_beta: 0.9499949994999499
train: step: 4016, loss: 0.03904132917523384, acc: 0.96, recall: 0.96, precision: 0.9607371794871795, f_beta: 0.959983993597439
train: step: 4017, loss: 0.08539816737174988, acc: 0.89, recall: 0.8899999999999999, precision: 0.8939393939393939, f_beta: 0.8897243107769424
train: step: 4018, loss: 0.048024844378232956, acc: 0.93, recall: 0.9299999999999999, precision: 0.9315535929345644, f_beta: 0.9299369432489241
train: step: 4019, loss: 0.019349951297044754, acc: 0.98, recall: 0.98, precision: 0.98, f_beta: 0.98
train: step: 4020, loss: 0.011272776871919632, acc: 0.98, recall: 0.98, precision: 0.9807692307692308, f_beta: 0.9799919967987194
train: step: 4021, loss: 0.03246507793664932, acc: 0.96, recall: 0.96, precision: 0.96, f_beta: 0.96
train: step: 4022, loss: 0.0340188704431057, acc: 0.96, recall: 0.96, precision: 0.962962962962963, f_beta: 0.9599358974358974
train: step: 4023, loss: 0.016534924507141113, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 4024, loss: 0.028432130813598633, acc: 0.96, recall: 0.96, precision: 0.96, f_beta: 0.96
train: step: 4025, loss: 0.05309634655714035, acc: 0.95, recall: 0.95, precision: 0.9516258530710557, f_beta: 0.949954959463517
train: step: 4026, loss: 0.02536989189684391, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 4027, loss: 0.05605030059814453, acc: 0.95, recall: 0.95, precision: 0.9516258530710557, f_beta: 0.949954959463517
train: step: 4028, loss: 0.028750360012054443, acc: 0.96, recall: 0.96, precision: 0.9607371794871795, f_beta: 0.959983993597439
train: step: 4029, loss: 0.022677922621369362, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 4030, loss: 0.03603078052401543, acc: 0.95, recall: 0.95, precision: 0.9501800720288115, f_beta: 0.9499949994999499
train: step: 4031, loss: 0.02289564162492752, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 4032, loss: 0.026870232075452805, acc: 0.98, recall: 0.98, precision: 0.9807692307692308, f_beta: 0.9799919967987194
train: step: 4033, loss: 0.010300254449248314, acc: 0.99, recall: 0.99, precision: 0.9901960784313726, f_beta: 0.98999899989999
train: step: 4034, loss: 0.026587389409542084, acc: 0.96, recall: 0.96, precision: 0.96, f_beta: 0.96
train: step: 4035, loss: 0.0503881461918354, acc: 0.94, recall: 0.94, precision: 0.9428341384863124, f_beta: 0.9399038461538463
train: step: 4036, loss: 0.06296858191490173, acc: 0.92, recall: 0.9199999999999999, precision: 0.9261363636363636, f_beta: 0.9197109594540345
train: step: 4037, loss: 0.019178573042154312, acc: 0.99, recall: 0.99, precision: 0.9901960784313726, f_beta: 0.98999899989999
train: step: 4038, loss: 0.028009934350848198, acc: 0.96, recall: 0.96, precision: 0.9607371794871795, f_beta: 0.959983993597439
train: step: 4039, loss: 0.03247340768575668, acc: 0.96, recall: 0.96, precision: 0.9607371794871795, f_beta: 0.959983993597439
train: step: 4040, loss: 0.02918335236608982, acc: 0.97, recall: 0.97, precision: 0.9716981132075472, f_beta: 0.9699729756781104
train: step: 4041, loss: 0.013905070722103119, acc: 0.98, recall: 0.98, precision: 0.9807692307692308, f_beta: 0.9799919967987194
train: step: 4042, loss: 0.02857806161046028, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 4043, loss: 0.04984453320503235, acc: 0.94, recall: 0.94, precision: 0.9428341384863124, f_beta: 0.9399038461538463
train: step: 4044, loss: 0.04637628421187401, acc: 0.94, recall: 0.94, precision: 0.9407051282051282, f_beta: 0.9399759903961584
train: step: 4045, loss: 0.04177842289209366, acc: 0.96, recall: 0.96, precision: 0.9607371794871795, f_beta: 0.959983993597439
train: step: 4046, loss: 0.03347473591566086, acc: 0.95, recall: 0.95, precision: 0.9516258530710557, f_beta: 0.949954959463517
train: step: 4047, loss: 0.027176236733794212, acc: 0.98, recall: 0.98, precision: 0.9807692307692308, f_beta: 0.9799919967987194
train: step: 4048, loss: 0.025634167715907097, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 4049, loss: 0.0203980952501297, acc: 0.98, recall: 0.98, precision: 0.98, f_beta: 0.98
train: step: 4050, loss: 0.047440603375434875, acc: 0.95, recall: 0.95, precision: 0.9501800720288115, f_beta: 0.9499949994999499
train: step: 4051, loss: 0.03210309147834778, acc: 0.95, recall: 0.95, precision: 0.9501800720288115, f_beta: 0.9499949994999499
train: step: 4052, loss: 0.039228472858667374, acc: 0.96, recall: 0.96, precision: 0.9607371794871795, f_beta: 0.959983993597439
train: step: 4053, loss: 0.028458308428525925, acc: 0.95, recall: 0.95, precision: 0.9516258530710557, f_beta: 0.949954959463517
train: step: 4054, loss: 0.02851267158985138, acc: 0.96, recall: 0.96, precision: 0.96, f_beta: 0.96
train: step: 4055, loss: 0.012678766623139381, acc: 0.98, recall: 0.98, precision: 0.98, f_beta: 0.98
train: step: 4056, loss: 0.028912553563714027, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 4057, loss: 0.021970491856336594, acc: 0.97, recall: 0.97, precision: 0.9716981132075472, f_beta: 0.9699729756781104
train: step: 4058, loss: 0.03922700509428978, acc: 0.96, recall: 0.96, precision: 0.9607371794871795, f_beta: 0.959983993597439
train: step: 4059, loss: 0.06219428405165672, acc: 0.93, recall: 0.9299999999999999, precision: 0.9315535929345644, f_beta: 0.9299369432489241
train: step: 4060, loss: 0.034691933542490005, acc: 0.95, recall: 0.95, precision: 0.9516258530710557, f_beta: 0.949954959463517
train: step: 4061, loss: 0.020329680293798447, acc: 0.98, recall: 0.98, precision: 0.98, f_beta: 0.98
train: step: 4062, loss: 0.016257867217063904, acc: 0.98, recall: 0.98, precision: 0.98, f_beta: 0.98
train: step: 4063, loss: 0.042900387197732925, acc: 0.94, recall: 0.94, precision: 0.9428341384863124, f_beta: 0.9399038461538463
train: step: 4064, loss: 0.022551894187927246, acc: 0.98, recall: 0.98, precision: 0.98, f_beta: 0.98
train: step: 4065, loss: 0.02460065670311451, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 4066, loss: 0.06018739566206932, acc: 0.9, recall: 0.8999999999999999, precision: 0.9058441558441559, f_beta: 0.8996386993175431
train: step: 4067, loss: 0.04363115876913071, acc: 0.95, recall: 0.95, precision: 0.9516258530710557, f_beta: 0.949954959463517
train: step: 4068, loss: 0.01553378626704216, acc: 0.96, recall: 0.96, precision: 0.9607371794871795, f_beta: 0.959983993597439
train: step: 4069, loss: 0.016593819484114647, acc: 0.98, recall: 0.98, precision: 0.9807692307692308, f_beta: 0.9799919967987194
train: step: 4070, loss: 0.027376150712370872, acc: 0.96, recall: 0.96, precision: 0.9607371794871795, f_beta: 0.959983993597439
train: step: 4071, loss: 0.05794281139969826, acc: 0.93, recall: 0.9299999999999999, precision: 0.930172068827531, f_beta: 0.9299929992999301
train: step: 4072, loss: 0.023646153509616852, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 4073, loss: 0.03137054294347763, acc: 0.97, recall: 0.97, precision: 0.9716981132075472, f_beta: 0.9699729756781104
train: step: 4074, loss: 0.05002536624670029, acc: 0.94, recall: 0.94, precision: 0.94, f_beta: 0.94
train: step: 4075, loss: 0.01214086078107357, acc: 0.98, recall: 0.98, precision: 0.98, f_beta: 0.98
train: step: 4076, loss: 0.00042411641334183514, acc: 1.0, recall: 1.0, precision: 1.0, f_beta: 1.0
train: step: 4077, loss: 0.016484759747982025, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 4078, loss: 0.03899242356419563, acc: 0.95, recall: 0.95, precision: 0.9501800720288115, f_beta: 0.9499949994999499
train: step: 4079, loss: 0.013547544367611408, acc: 0.99, recall: 0.99, precision: 0.9901960784313726, f_beta: 0.98999899989999
train: step: 4080, loss: 0.0340815968811512, acc: 0.97, recall: 0.97, precision: 0.9716981132075472, f_beta: 0.9699729756781104
train: step: 4081, loss: 0.033471450209617615, acc: 0.96, recall: 0.96, precision: 0.9607371794871795, f_beta: 0.959983993597439
train: step: 4082, loss: 0.06179864704608917, acc: 0.94, recall: 0.94, precision: 0.9464285714285714, f_beta: 0.939783219590526
train: step: 4083, loss: 0.01177480723708868, acc: 0.98, recall: 0.98, precision: 0.9807692307692308, f_beta: 0.9799919967987194
train: step: 4084, loss: 0.03202545642852783, acc: 0.96, recall: 0.96, precision: 0.96, f_beta: 0.96
train: step: 4085, loss: 0.03576503321528435, acc: 0.96, recall: 0.96, precision: 0.96, f_beta: 0.96
train: step: 4086, loss: 0.010046265088021755, acc: 0.99, recall: 0.99, precision: 0.9901960784313726, f_beta: 0.98999899989999
train: step: 4087, loss: 0.011547774076461792, acc: 0.99, recall: 0.99, precision: 0.9901960784313726, f_beta: 0.98999899989999
train: step: 4088, loss: 0.025712875649333, acc: 0.97, recall: 0.97, precision: 0.9716981132075472, f_beta: 0.9699729756781104
train: step: 4089, loss: 0.03964376822113991, acc: 0.96, recall: 0.96, precision: 0.9607371794871795, f_beta: 0.959983993597439
train: step: 4090, loss: 0.028217125684022903, acc: 0.95, recall: 0.95, precision: 0.9501800720288115, f_beta: 0.9499949994999499
train: step: 4091, loss: 0.0649755671620369, acc: 0.9, recall: 0.9, precision: 0.9006410256410255, f_beta: 0.8999599839935974
train: step: 4092, loss: 0.028076518326997757, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 4093, loss: 0.017420364543795586, acc: 0.97, recall: 0.97, precision: 0.9716981132075472, f_beta: 0.9699729756781104
train: step: 4094, loss: 0.029818575829267502, acc: 0.96, recall: 0.96, precision: 0.9607371794871795, f_beta: 0.959983993597439
train: step: 4095, loss: 0.01881159096956253, acc: 0.98, recall: 0.98, precision: 0.9807692307692308, f_beta: 0.9799919967987194
train: step: 4096, loss: 0.04713807255029678, acc: 0.95, recall: 0.95, precision: 0.9501800720288115, f_beta: 0.9499949994999499
train: step: 4097, loss: 0.012962833978235722, acc: 0.99, recall: 0.99, precision: 0.9901960784313726, f_beta: 0.98999899989999
train: step: 4098, loss: 0.0167486984282732, acc: 0.99, recall: 0.99, precision: 0.9901960784313726, f_beta: 0.98999899989999
train: step: 4099, loss: 0.025780046358704567, acc: 0.96, recall: 0.96, precision: 0.9607371794871795, f_beta: 0.959983993597439
train: step: 4100, loss: 0.025147072970867157, acc: 0.98, recall: 0.98, precision: 0.9807692307692308, f_beta: 0.9799919967987194
train: step: 4101, loss: 0.02607782557606697, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 4102, loss: 0.03608658164739609, acc: 0.96, recall: 0.96, precision: 0.962962962962963, f_beta: 0.9599358974358974
train: step: 4103, loss: 0.017412150278687477, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 4104, loss: 0.02942037023603916, acc: 0.96, recall: 0.96, precision: 0.96, f_beta: 0.96
train: step: 4105, loss: 0.019346026703715324, acc: 0.99, recall: 0.99, precision: 0.9901960784313726, f_beta: 0.98999899989999
train: step: 4106, loss: 0.01855391636490822, acc: 0.98, recall: 0.98, precision: 0.9807692307692308, f_beta: 0.9799919967987194
train: step: 4107, loss: 0.04122885689139366, acc: 0.95, recall: 0.95, precision: 0.9545454545454546, f_beta: 0.949874686716792
train: step: 4108, loss: 0.0374407023191452, acc: 0.96, recall: 0.96, precision: 0.96, f_beta: 0.96
train: step: 4109, loss: 0.04201874881982803, acc: 0.94, recall: 0.94, precision: 0.94, f_beta: 0.94
train: step: 4110, loss: 0.03191271424293518, acc: 0.96, recall: 0.96, precision: 0.962962962962963, f_beta: 0.9599358974358974
train: step: 4111, loss: 0.04761984199285507, acc: 0.94, recall: 0.94, precision: 0.9428341384863124, f_beta: 0.9399038461538463
train: step: 4112, loss: 0.009196222759783268, acc: 0.99, recall: 0.99, precision: 0.9901960784313726, f_beta: 0.98999899989999
train: step: 4113, loss: 0.02209950052201748, acc: 0.96, recall: 0.96, precision: 0.962962962962963, f_beta: 0.9599358974358974
train: step: 4114, loss: 0.025588154792785645, acc: 0.96, recall: 0.96, precision: 0.9607371794871795, f_beta: 0.959983993597439
train: step: 4115, loss: 0.048123445361852646, acc: 0.94, recall: 0.94, precision: 0.94, f_beta: 0.94
train: step: 4116, loss: 0.05801921710371971, acc: 0.93, recall: 0.9299999999999999, precision: 0.930172068827531, f_beta: 0.9299929992999301
train: step: 4117, loss: 0.0326506681740284, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 4118, loss: 0.020614556968212128, acc: 0.98, recall: 0.98, precision: 0.98, f_beta: 0.98
train: step: 4119, loss: 0.04741944372653961, acc: 0.94, recall: 0.94, precision: 0.94, f_beta: 0.94
train: step: 4120, loss: 0.03457263484597206, acc: 0.96, recall: 0.96, precision: 0.9607371794871795, f_beta: 0.959983993597439
train: step: 4121, loss: 0.028935031965374947, acc: 0.96, recall: 0.96, precision: 0.9607371794871795, f_beta: 0.959983993597439
train: step: 4122, loss: 0.028476206585764885, acc: 0.97, recall: 0.97, precision: 0.9716981132075472, f_beta: 0.9699729756781104
train: step: 4123, loss: 0.032396283000707626, acc: 0.96, recall: 0.96, precision: 0.9607371794871795, f_beta: 0.959983993597439
train: step: 4124, loss: 0.053381968289613724, acc: 0.92, recall: 0.9199999999999999, precision: 0.9206730769230769, f_beta: 0.919967987194878
train: step: 4125, loss: 0.028235342353582382, acc: 0.96, recall: 0.96, precision: 0.9607371794871795, f_beta: 0.959983993597439
train: step: 4126, loss: 0.005630145780742168, acc: 1.0, recall: 1.0, precision: 1.0, f_beta: 1.0
train: step: 4127, loss: 0.0476650707423687, acc: 0.95, recall: 0.95, precision: 0.9501800720288115, f_beta: 0.9499949994999499
train: step: 4128, loss: 0.013104711659252644, acc: 0.99, recall: 0.99, precision: 0.9901960784313726, f_beta: 0.98999899989999
train: step: 4129, loss: 0.01802791655063629, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 4130, loss: 0.015103157609701157, acc: 0.98, recall: 0.98, precision: 0.9807692307692308, f_beta: 0.9799919967987194
train: step: 4131, loss: 0.04862003028392792, acc: 0.95, recall: 0.95, precision: 0.9501800720288115, f_beta: 0.9499949994999499
train: step: 4132, loss: 0.03326218202710152, acc: 0.95, recall: 0.95, precision: 0.9501800720288115, f_beta: 0.9499949994999499
train: step: 4133, loss: 0.0341670885682106, acc: 0.96, recall: 0.96, precision: 0.96, f_beta: 0.96
train: step: 4134, loss: 0.023651354014873505, acc: 0.98, recall: 0.98, precision: 0.98, f_beta: 0.98
train: step: 4135, loss: 0.048647135496139526, acc: 0.92, recall: 0.92, precision: 0.92, f_beta: 0.92
train: step: 4136, loss: 0.05691349878907204, acc: 0.93, recall: 0.9299999999999999, precision: 0.930172068827531, f_beta: 0.9299929992999301
train: step: 4137, loss: 0.025574201717972755, acc: 0.96, recall: 0.96, precision: 0.9607371794871795, f_beta: 0.959983993597439
train: step: 4138, loss: 0.013655329123139381, acc: 0.98, recall: 0.98, precision: 0.98, f_beta: 0.98
train: step: 4139, loss: 0.03230960667133331, acc: 0.96, recall: 0.96, precision: 0.9607371794871795, f_beta: 0.959983993597439
train: step: 4140, loss: 0.01743548922240734, acc: 0.98, recall: 0.98, precision: 0.9807692307692308, f_beta: 0.9799919967987194
train: step: 4141, loss: 0.03380533680319786, acc: 0.95, recall: 0.95, precision: 0.9501800720288115, f_beta: 0.9499949994999499
train: step: 4142, loss: 0.06665652990341187, acc: 0.92, recall: 0.9199999999999999, precision: 0.9261363636363636, f_beta: 0.9197109594540345
train: step: 4143, loss: 0.023219265043735504, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 4144, loss: 0.03833216428756714, acc: 0.96, recall: 0.96, precision: 0.96, f_beta: 0.96
train: step: 4145, loss: 0.012654098682105541, acc: 1.0, recall: 1.0, precision: 1.0, f_beta: 1.0
train: step: 4146, loss: 0.004710648208856583, acc: 1.0, recall: 1.0, precision: 1.0, f_beta: 1.0
train: step: 4147, loss: 0.0029908930882811546, acc: 1.0, recall: 1.0, precision: 1.0, f_beta: 1.0
train: step: 4148, loss: 0.002301017753779888, acc: 1.0, recall: 1.0, precision: 1.0, f_beta: 1.0
train: step: 4149, loss: 0.05058947205543518, acc: 0.94, recall: 0.94, precision: 0.94, f_beta: 0.94
train: step: 4150, loss: 0.026778249070048332, acc: 0.98, recall: 0.98, precision: 0.98, f_beta: 0.98
train: step: 4151, loss: 0.014581628143787384, acc: 0.98, recall: 0.98, precision: 0.9807692307692308, f_beta: 0.9799919967987194
train: step: 4152, loss: 0.016215171664953232, acc: 0.98, recall: 0.98, precision: 0.9807692307692308, f_beta: 0.9799919967987194
train: step: 4153, loss: 0.036267753690481186, acc: 0.95, recall: 0.95, precision: 0.9501800720288115, f_beta: 0.9499949994999499
train: step: 4154, loss: 0.023321256041526794, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 4155, loss: 0.0177425779402256, acc: 0.98, recall: 0.98, precision: 0.98, f_beta: 0.98
train: step: 4156, loss: 0.018406562507152557, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 4157, loss: 0.028147397562861443, acc: 0.96, recall: 0.96, precision: 0.96, f_beta: 0.96
train: step: 4158, loss: 0.0590490996837616, acc: 0.93, recall: 0.9299999999999999, precision: 0.930172068827531, f_beta: 0.9299929992999301
train: step: 4159, loss: 0.03594011068344116, acc: 0.96, recall: 0.96, precision: 0.96, f_beta: 0.96
train: step: 4160, loss: 0.012490796856582165, acc: 0.97, recall: 0.97, precision: 0.9716981132075472, f_beta: 0.9699729756781104
train: step: 4161, loss: 0.03802432864904404, acc: 0.96, recall: 0.96, precision: 0.962962962962963, f_beta: 0.9599358974358974
train: step: 4162, loss: 0.00033449503825977445, acc: 1.0, recall: 1.0, precision: 1.0, f_beta: 1.0
train: step: 4163, loss: 0.003461451269686222, acc: 1.0, recall: 1.0, precision: 1.0, f_beta: 1.0
train: step: 4164, loss: 0.030951451510190964, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 4165, loss: 0.050161585211753845, acc: 0.94, recall: 0.94, precision: 0.94, f_beta: 0.94
train: step: 4166, loss: 0.014485490508377552, acc: 0.98, recall: 0.98, precision: 0.98, f_beta: 0.98
train: step: 4167, loss: 0.0032327400986105204, acc: 1.0, recall: 1.0, precision: 1.0, f_beta: 1.0
train: step: 4168, loss: 0.0012822620337828994, acc: 1.0, recall: 1.0, precision: 1.0, f_beta: 1.0
train: step: 4169, loss: 0.040368273854255676, acc: 0.95, recall: 0.95, precision: 0.9501800720288115, f_beta: 0.9499949994999499
train: step: 4170, loss: 0.0392356738448143, acc: 0.95, recall: 0.95, precision: 0.9501800720288115, f_beta: 0.9499949994999499
train: step: 4171, loss: 0.030026154592633247, acc: 0.96, recall: 0.96, precision: 0.96, f_beta: 0.96
train: step: 4172, loss: 0.04892957583069801, acc: 0.95, recall: 0.95, precision: 0.9516258530710557, f_beta: 0.949954959463517
train: step: 4173, loss: 0.03605974093079567, acc: 0.96, recall: 0.96, precision: 0.962962962962963, f_beta: 0.9599358974358974
train: step: 4174, loss: 0.040774811059236526, acc: 0.96, recall: 0.96, precision: 0.9607371794871795, f_beta: 0.959983993597439
train: step: 4175, loss: 0.024217450991272926, acc: 0.98, recall: 0.98, precision: 0.9807692307692308, f_beta: 0.9799919967987194
train: step: 4176, loss: 0.0319255106151104, acc: 0.98, recall: 0.98, precision: 0.9807692307692308, f_beta: 0.9799919967987194
train: step: 4177, loss: 0.031716786324977875, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 4178, loss: 0.022717686370015144, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 4179, loss: 0.027905354276299477, acc: 0.98, recall: 0.98, precision: 0.9807692307692308, f_beta: 0.9799919967987194
train: step: 4180, loss: 0.007698348723351955, acc: 0.99, recall: 0.99, precision: 0.9901960784313726, f_beta: 0.98999899989999
train: step: 4181, loss: 0.04010652378201485, acc: 0.95, recall: 0.95, precision: 0.9501800720288115, f_beta: 0.9499949994999499
train: step: 4182, loss: 0.012921149842441082, acc: 0.99, recall: 0.99, precision: 0.9901960784313726, f_beta: 0.98999899989999
train: step: 4183, loss: 0.01714649423956871, acc: 0.98, recall: 0.98, precision: 0.9807692307692308, f_beta: 0.9799919967987194
train: step: 4184, loss: 0.027532126754522324, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 4185, loss: 0.03253853693604469, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 4186, loss: 0.032743293792009354, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 4187, loss: 0.019696872681379318, acc: 0.98, recall: 0.98, precision: 0.9807692307692308, f_beta: 0.9799919967987194
train: step: 4188, loss: 0.004473600536584854, acc: 0.99, recall: 0.99, precision: 0.9901960784313726, f_beta: 0.98999899989999
train: step: 4189, loss: 0.01507424097508192, acc: 0.98, recall: 0.98, precision: 0.9807692307692308, f_beta: 0.9799919967987194
train: step: 4190, loss: 0.03518485650420189, acc: 0.95, recall: 0.95, precision: 0.9501800720288115, f_beta: 0.9499949994999499
train: step: 4191, loss: 0.05837243050336838, acc: 0.92, recall: 0.92, precision: 0.92, f_beta: 0.92
train: step: 4192, loss: 0.02610715851187706, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 4193, loss: 0.0170621145516634, acc: 0.98, recall: 0.98, precision: 0.9807692307692308, f_beta: 0.9799919967987194
train: step: 4194, loss: 0.006635195575654507, acc: 0.99, recall: 0.99, precision: 0.9901960784313726, f_beta: 0.98999899989999
train: step: 4195, loss: 0.015667006373405457, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 4196, loss: 0.009678758680820465, acc: 0.99, recall: 0.99, precision: 0.9901960784313726, f_beta: 0.98999899989999
train: step: 4197, loss: 0.04759747162461281, acc: 0.94, recall: 0.94, precision: 0.9407051282051282, f_beta: 0.9399759903961584
train: step: 4198, loss: 0.024413937702775, acc: 0.98, recall: 0.98, precision: 0.98, f_beta: 0.98
train: step: 4199, loss: 0.013000944629311562, acc: 0.98, recall: 0.98, precision: 0.9807692307692308, f_beta: 0.9799919967987194
train: step: 4200, loss: 0.011090580374002457, acc: 0.99, recall: 0.99, precision: 0.9901960784313726, f_beta: 0.98999899989999
train: step: 4201, loss: 0.039665695279836655, acc: 0.96, recall: 0.96, precision: 0.96, f_beta: 0.96
train: step: 4202, loss: 0.0194026380777359, acc: 0.98, recall: 0.98, precision: 0.9807692307692308, f_beta: 0.9799919967987194
train: step: 4203, loss: 0.044613294303417206, acc: 0.93, recall: 0.9299999999999999, precision: 0.9315535929345644, f_beta: 0.9299369432489241
train: step: 4204, loss: 0.009920495562255383, acc: 0.99, recall: 0.99, precision: 0.9901960784313726, f_beta: 0.98999899989999
train: step: 4205, loss: 0.04288465157151222, acc: 0.96, recall: 0.96, precision: 0.96, f_beta: 0.96
train: step: 4206, loss: 0.008217581547796726, acc: 0.99, recall: 0.99, precision: 0.9901960784313726, f_beta: 0.98999899989999
train: step: 4207, loss: 0.030231047421693802, acc: 0.97, recall: 0.97, precision: 0.9716981132075472, f_beta: 0.9699729756781104
train: step: 4208, loss: 0.020569458603858948, acc: 0.98, recall: 0.98, precision: 0.98, f_beta: 0.98
train: step: 4209, loss: 0.021336723119020462, acc: 0.98, recall: 0.98, precision: 0.98, f_beta: 0.98
train: step: 4210, loss: 0.01207982562482357, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 4211, loss: 0.015620499849319458, acc: 0.98, recall: 0.98, precision: 0.9807692307692308, f_beta: 0.9799919967987194
train: step: 4212, loss: 0.033049728721380234, acc: 0.95, recall: 0.95, precision: 0.9501800720288115, f_beta: 0.9499949994999499
train: step: 4213, loss: 0.03848721832036972, acc: 0.94, recall: 0.94, precision: 0.9428341384863124, f_beta: 0.9399038461538463
train: step: 4214, loss: 0.02137782983481884, acc: 0.98, recall: 0.98, precision: 0.98, f_beta: 0.98
train: step: 4215, loss: 0.021799052134156227, acc: 0.98, recall: 0.98, precision: 0.9807692307692308, f_beta: 0.9799919967987194
train: step: 4216, loss: 0.020859666168689728, acc: 0.97, recall: 0.97, precision: 0.9716981132075472, f_beta: 0.9699729756781104
train: step: 4217, loss: 0.04382892698049545, acc: 0.94, recall: 0.94, precision: 0.9407051282051282, f_beta: 0.9399759903961584
train: step: 4218, loss: 0.021305035799741745, acc: 0.98, recall: 0.98, precision: 0.9807692307692308, f_beta: 0.9799919967987194
train: step: 4219, loss: 0.01954653300344944, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 4220, loss: 0.013788040727376938, acc: 0.99, recall: 0.99, precision: 0.9901960784313726, f_beta: 0.98999899989999
train: step: 4221, loss: 0.04680406674742699, acc: 0.95, recall: 0.95, precision: 0.9501800720288115, f_beta: 0.9499949994999499
train: step: 4222, loss: 0.0508689247071743, acc: 0.94, recall: 0.94, precision: 0.9428341384863124, f_beta: 0.9399038461538463
train: step: 4223, loss: 0.028991220518946648, acc: 0.96, recall: 0.96, precision: 0.9607371794871795, f_beta: 0.959983993597439
train: step: 4224, loss: 0.0005154895479790866, acc: 1.0, recall: 1.0, precision: 1.0, f_beta: 1.0
train: step: 4225, loss: 0.010900645516812801, acc: 0.99, recall: 0.99, precision: 0.9901960784313726, f_beta: 0.98999899989999
train: step: 4226, loss: 0.00607892032712698, acc: 0.99, recall: 0.99, precision: 0.9901960784313726, f_beta: 0.98999899989999
train: step: 4227, loss: 0.035334695130586624, acc: 0.95, recall: 0.95, precision: 0.9501800720288115, f_beta: 0.9499949994999499
train: step: 4228, loss: 0.04606720805168152, acc: 0.94, recall: 0.94, precision: 0.9428341384863124, f_beta: 0.9399038461538463
train: step: 4229, loss: 0.005615738686174154, acc: 0.99, recall: 0.99, precision: 0.9901960784313726, f_beta: 0.98999899989999
train: step: 4230, loss: 0.03847464546561241, acc: 0.96, recall: 0.96, precision: 0.962962962962963, f_beta: 0.9599358974358974
train: step: 4231, loss: 0.013417700305581093, acc: 0.98, recall: 0.98, precision: 0.9807692307692308, f_beta: 0.9799919967987194
train: step: 4232, loss: 0.02814711630344391, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 4233, loss: 0.05440909415483475, acc: 0.93, recall: 0.9299999999999999, precision: 0.9343434343434343, f_beta: 0.9298245614035088
train: step: 4234, loss: 0.0040094912983477116, acc: 0.99, recall: 0.99, precision: 0.9901960784313726, f_beta: 0.98999899989999
train: step: 4235, loss: 0.016003336757421494, acc: 0.99, recall: 0.99, precision: 0.9901960784313726, f_beta: 0.98999899989999
train: step: 4236, loss: 0.02897699549794197, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 4237, loss: 0.021707093343138695, acc: 0.96, recall: 0.96, precision: 0.9607371794871795, f_beta: 0.959983993597439
train: step: 4238, loss: 0.010542389936745167, acc: 0.99, recall: 0.99, precision: 0.9901960784313726, f_beta: 0.98999899989999
train: step: 4239, loss: 0.036660172045230865, acc: 0.95, recall: 0.95, precision: 0.9516258530710557, f_beta: 0.949954959463517
train: step: 4240, loss: 0.026923324912786484, acc: 0.98, recall: 0.98, precision: 0.98, f_beta: 0.98
train: step: 4241, loss: 0.041957203298807144, acc: 0.96, recall: 0.96, precision: 0.96, f_beta: 0.96
train: step: 4242, loss: 0.02033829689025879, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 4243, loss: 0.003433789359405637, acc: 1.0, recall: 1.0, precision: 1.0, f_beta: 1.0
train: step: 4244, loss: 0.033160820603370667, acc: 0.96, recall: 0.96, precision: 0.9607371794871795, f_beta: 0.959983993597439
train: step: 4245, loss: 0.04219256713986397, acc: 0.95, recall: 0.95, precision: 0.9516258530710557, f_beta: 0.949954959463517
train: step: 4246, loss: 0.04419829323887825, acc: 0.95, recall: 0.95, precision: 0.9501800720288115, f_beta: 0.9499949994999499
train: step: 4247, loss: 0.03888033330440521, acc: 0.96, recall: 0.96, precision: 0.96, f_beta: 0.96
train: step: 4248, loss: 0.04824800416827202, acc: 0.94, recall: 0.94, precision: 0.9407051282051282, f_beta: 0.9399759903961584
train: step: 4249, loss: 0.032760217785835266, acc: 0.96, recall: 0.96, precision: 0.9607371794871795, f_beta: 0.959983993597439
train: step: 4250, loss: 0.01738503947854042, acc: 0.98, recall: 0.98, precision: 0.9807692307692308, f_beta: 0.9799919967987194
train: step: 4251, loss: 0.05149438977241516, acc: 0.92, recall: 0.9199999999999999, precision: 0.9227053140096618, f_beta: 0.9198717948717948
train: step: 4252, loss: 0.05124742537736893, acc: 0.95, recall: 0.95, precision: 0.9501800720288115, f_beta: 0.9499949994999499
train: step: 4253, loss: 0.0174625962972641, acc: 0.98, recall: 0.98, precision: 0.9807692307692308, f_beta: 0.9799919967987194
train: step: 4254, loss: 0.051683779805898666, acc: 0.94, recall: 0.94, precision: 0.9407051282051282, f_beta: 0.9399759903961584
train: step: 4255, loss: 0.025293024256825447, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 4256, loss: 0.04187021404504776, acc: 0.94, recall: 0.94, precision: 0.94, f_beta: 0.94
train: step: 4257, loss: 0.06809106469154358, acc: 0.91, recall: 0.9099999999999999, precision: 0.9141414141414141, f_beta: 0.9097744360902256
train: step: 4258, loss: 0.058360692113637924, acc: 0.93, recall: 0.9299999999999999, precision: 0.930172068827531, f_beta: 0.9299929992999301
train: step: 4259, loss: 0.008280710317194462, acc: 1.0, recall: 1.0, precision: 1.0, f_beta: 1.0
train: step: 4260, loss: 0.029688065871596336, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 4261, loss: 0.010520591400563717, acc: 0.99, recall: 0.99, precision: 0.9901960784313726, f_beta: 0.98999899989999
train: step: 4262, loss: 0.03637176379561424, acc: 0.96, recall: 0.96, precision: 0.9607371794871795, f_beta: 0.959983993597439
train: step: 4263, loss: 0.039063725620508194, acc: 0.96, recall: 0.96, precision: 0.9607371794871795, f_beta: 0.959983993597439
train: step: 4264, loss: 0.03213543817400932, acc: 0.95, recall: 0.95, precision: 0.9501800720288115, f_beta: 0.9499949994999499
train: step: 4265, loss: 0.02592054195702076, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 4266, loss: 0.03946352377533913, acc: 0.96, recall: 0.96, precision: 0.96, f_beta: 0.96
train: step: 4267, loss: 0.02667689137160778, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 4268, loss: 0.019000889733433723, acc: 0.98, recall: 0.98, precision: 0.98, f_beta: 0.98
train: step: 4269, loss: 0.007837814278900623, acc: 0.99, recall: 0.99, precision: 0.9901960784313726, f_beta: 0.98999899989999
train: step: 4270, loss: 0.04749755933880806, acc: 0.94, recall: 0.94, precision: 0.9428341384863124, f_beta: 0.9399038461538463
train: step: 4271, loss: 0.026192698627710342, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 4272, loss: 0.006731927394866943, acc: 0.99, recall: 0.99, precision: 0.9901960784313726, f_beta: 0.98999899989999
train: step: 4273, loss: 0.0058275614865124226, acc: 0.99, recall: 0.99, precision: 0.9901960784313726, f_beta: 0.98999899989999
train: step: 4274, loss: 0.012390597723424435, acc: 0.98, recall: 0.98, precision: 0.98, f_beta: 0.98
train: step: 4275, loss: 0.02979210764169693, acc: 0.96, recall: 0.96, precision: 0.9607371794871795, f_beta: 0.959983993597439
train: step: 4276, loss: 0.041481874883174896, acc: 0.96, recall: 0.96, precision: 0.96, f_beta: 0.96
train: step: 4277, loss: 0.013774042017757893, acc: 0.99, recall: 0.99, precision: 0.9901960784313726, f_beta: 0.98999899989999
train: step: 4278, loss: 0.028968794271349907, acc: 0.96, recall: 0.96, precision: 0.96, f_beta: 0.96
train: step: 4279, loss: 0.045753490179777145, acc: 0.95, recall: 0.95, precision: 0.9516258530710557, f_beta: 0.949954959463517
train: step: 4280, loss: 0.008059006184339523, acc: 0.99, recall: 0.99, precision: 0.9901960784313726, f_beta: 0.98999899989999
train: step: 4281, loss: 0.022144891321659088, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 4282, loss: 0.005027486477047205, acc: 0.99, recall: 0.99, precision: 0.9901960784313726, f_beta: 0.98999899989999
train: step: 4283, loss: 0.0441315732896328, acc: 0.93, recall: 0.9299999999999999, precision: 0.9315535929345644, f_beta: 0.9299369432489241
train: step: 4284, loss: 0.00920441746711731, acc: 0.99, recall: 0.99, precision: 0.9901960784313726, f_beta: 0.98999899989999
train: step: 4285, loss: 0.0325724259018898, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 4286, loss: 0.01873713731765747, acc: 0.98, recall: 0.98, precision: 0.98, f_beta: 0.98
train: step: 4287, loss: 0.012278185226023197, acc: 0.99, recall: 0.99, precision: 0.9901960784313726, f_beta: 0.98999899989999
train: step: 4288, loss: 0.011884456500411034, acc: 0.99, recall: 0.99, precision: 0.9901960784313726, f_beta: 0.98999899989999
train: step: 4289, loss: 0.05746506154537201, acc: 0.94, recall: 0.94, precision: 0.9407051282051282, f_beta: 0.9399759903961584
train: step: 4290, loss: 0.01871109940111637, acc: 0.98, recall: 0.98, precision: 0.9807692307692308, f_beta: 0.9799919967987194
train: step: 4291, loss: 0.022390738129615784, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 4292, loss: 0.04374951496720314, acc: 0.95, recall: 0.95, precision: 0.9501800720288115, f_beta: 0.9499949994999499
train: step: 4293, loss: 0.002654225565493107, acc: 1.0, recall: 1.0, precision: 1.0, f_beta: 1.0
train: step: 4294, loss: 0.007556793745607138, acc: 0.99, recall: 0.99, precision: 0.9901960784313726, f_beta: 0.98999899989999
train: step: 4295, loss: 0.012422532774508, acc: 0.99, recall: 0.99, precision: 0.9901960784313726, f_beta: 0.98999899989999
train: step: 4296, loss: 0.0010015827137976885, acc: 1.0, recall: 1.0, precision: 1.0, f_beta: 1.0
train: step: 4297, loss: 0.022193390876054764, acc: 0.97, recall: 0.97, precision: 0.9716981132075472, f_beta: 0.9699729756781104
train: step: 4298, loss: 0.044875144958496094, acc: 0.96, recall: 0.96, precision: 0.962962962962963, f_beta: 0.9599358974358974
train: step: 4299, loss: 0.007062089629471302, acc: 0.99, recall: 0.99, precision: 0.9901960784313726, f_beta: 0.98999899989999
train: step: 4300, loss: 0.027425475418567657, acc: 0.96, recall: 0.96, precision: 0.96, f_beta: 0.96
train: step: 4301, loss: 0.04070641100406647, acc: 0.94, recall: 0.94, precision: 0.94, f_beta: 0.94
train: step: 4302, loss: 0.02157130278646946, acc: 0.96, recall: 0.96, precision: 0.9607371794871795, f_beta: 0.959983993597439
train: step: 4303, loss: 0.02197466418147087, acc: 0.97, recall: 0.97, precision: 0.9716981132075472, f_beta: 0.9699729756781104
train: step: 4304, loss: 0.030352838337421417, acc: 0.97, recall: 0.97, precision: 0.9716981132075472, f_beta: 0.9699729756781104
train: step: 4305, loss: 0.009146622382104397, acc: 0.98, recall: 0.98, precision: 0.9807692307692308, f_beta: 0.9799919967987194
train: step: 4306, loss: 0.008250873535871506, acc: 0.99, recall: 0.99, precision: 0.9901960784313726, f_beta: 0.98999899989999
train: step: 4307, loss: 0.021935371682047844, acc: 0.98, recall: 0.98, precision: 0.9807692307692308, f_beta: 0.9799919967987194
train: step: 4308, loss: 0.06605329364538193, acc: 0.9, recall: 0.8999999999999999, precision: 0.9025764895330113, f_beta: 0.8998397435897436
train: step: 4309, loss: 0.023315219208598137, acc: 0.97, recall: 0.97, precision: 0.9716981132075472, f_beta: 0.9699729756781104
train: step: 4310, loss: 0.05258357897400856, acc: 0.94, recall: 0.94, precision: 0.94, f_beta: 0.94
train: step: 4311, loss: 0.008814835920929909, acc: 0.99, recall: 0.99, precision: 0.9901960784313726, f_beta: 0.98999899989999
train: step: 4312, loss: 0.06431130319833755, acc: 0.93, recall: 0.9299999999999999, precision: 0.9315535929345644, f_beta: 0.9299369432489241
train: step: 4313, loss: 0.026540078222751617, acc: 0.97, recall: 0.97, precision: 0.9716981132075472, f_beta: 0.9699729756781104
train: step: 4314, loss: 0.02833029255270958, acc: 0.96, recall: 0.96, precision: 0.96, f_beta: 0.96
train: step: 4315, loss: 0.021931104362010956, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 4316, loss: 0.028818350285291672, acc: 0.97, recall: 0.97, precision: 0.9716981132075472, f_beta: 0.9699729756781104
train: step: 4317, loss: 0.010325665585696697, acc: 0.99, recall: 0.99, precision: 0.9901960784313726, f_beta: 0.98999899989999
train: step: 4318, loss: 0.012842943891882896, acc: 0.98, recall: 0.98, precision: 0.98, f_beta: 0.98
train: step: 4319, loss: 0.07961556315422058, acc: 0.89, recall: 0.89, precision: 0.8914090726615818, f_beta: 0.8899009108197378
train: step: 4320, loss: 0.011400614865124226, acc: 0.98, recall: 0.98, precision: 0.98, f_beta: 0.98
train: step: 4321, loss: 0.05209682509303093, acc: 0.93, recall: 0.9299999999999999, precision: 0.9385964912280702, f_beta: 0.9296553110240177
train: step: 4322, loss: 0.02262110635638237, acc: 0.98, recall: 0.98, precision: 0.9807692307692308, f_beta: 0.9799919967987194
train: step: 4323, loss: 0.01917257346212864, acc: 0.96, recall: 0.96, precision: 0.96, f_beta: 0.96
train: step: 4324, loss: 0.050636839121580124, acc: 0.93, recall: 0.9299999999999999, precision: 0.930172068827531, f_beta: 0.9299929992999301
train: step: 4325, loss: 0.03853253275156021, acc: 0.96, recall: 0.96, precision: 0.96, f_beta: 0.96
train: step: 4326, loss: 0.037744756788015366, acc: 0.95, recall: 0.95, precision: 0.9516258530710557, f_beta: 0.949954959463517
train: step: 4327, loss: 0.026936674490571022, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 4328, loss: 0.06450240314006805, acc: 0.92, recall: 0.9199999999999999, precision: 0.9227053140096618, f_beta: 0.9198717948717948
train: step: 4329, loss: 0.022516828030347824, acc: 0.98, recall: 0.98, precision: 0.98, f_beta: 0.98
train: step: 4330, loss: 0.04798182472586632, acc: 0.94, recall: 0.94, precision: 0.9407051282051282, f_beta: 0.9399759903961584
train: step: 4331, loss: 0.03364640846848488, acc: 0.96, recall: 0.96, precision: 0.96, f_beta: 0.96
train: step: 4332, loss: 0.04564763233065605, acc: 0.94, recall: 0.94, precision: 0.94, f_beta: 0.94
train: step: 4333, loss: 0.024230962619185448, acc: 0.98, recall: 0.98, precision: 0.98, f_beta: 0.98
train: step: 4334, loss: 0.0345250703394413, acc: 0.95, recall: 0.95, precision: 0.9516258530710557, f_beta: 0.949954959463517
train: step: 4335, loss: 0.012002719566226006, acc: 0.98, recall: 0.98, precision: 0.9807692307692308, f_beta: 0.9799919967987194
train: step: 4336, loss: 0.010799610987305641, acc: 0.98, recall: 0.98, precision: 0.9807692307692308, f_beta: 0.9799919967987194
train: step: 4337, loss: 0.029297027736902237, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 4338, loss: 0.04534591734409332, acc: 0.94, recall: 0.94, precision: 0.9407051282051282, f_beta: 0.9399759903961584
train: step: 4339, loss: 0.010911770164966583, acc: 0.99, recall: 0.99, precision: 0.9901960784313726, f_beta: 0.98999899989999
train: step: 4340, loss: 0.01775394007563591, acc: 0.98, recall: 0.98, precision: 0.98, f_beta: 0.98
train: step: 4341, loss: 0.035748373717069626, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 4342, loss: 0.015794988721609116, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 4343, loss: 0.022700736299157143, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 4344, loss: 0.0362003818154335, acc: 0.95, recall: 0.95, precision: 0.9501800720288115, f_beta: 0.9499949994999499
train: step: 4345, loss: 0.047772180289030075, acc: 0.98, recall: 0.98, precision: 0.9807692307692308, f_beta: 0.9799919967987194
train: step: 4346, loss: 0.03800742328166962, acc: 0.94, recall: 0.94, precision: 0.94, f_beta: 0.94
train: step: 4347, loss: 0.021199679002165794, acc: 0.98, recall: 0.98, precision: 0.98, f_beta: 0.98
train: step: 4348, loss: 0.030969832092523575, acc: 0.96, recall: 0.96, precision: 0.962962962962963, f_beta: 0.9599358974358974
train: step: 4349, loss: 0.011667082086205482, acc: 0.99, recall: 0.99, precision: 0.9901960784313726, f_beta: 0.98999899989999
train: step: 4350, loss: 0.01981646567583084, acc: 0.98, recall: 0.98, precision: 0.98, f_beta: 0.98
train: step: 4351, loss: 0.032115038484334946, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 4352, loss: 0.010528621263802052, acc: 0.99, recall: 0.99, precision: 0.9901960784313726, f_beta: 0.98999899989999
train: step: 4353, loss: 0.04084663465619087, acc: 0.94, recall: 0.94, precision: 0.9407051282051282, f_beta: 0.9399759903961584
train: step: 4354, loss: 0.034042712301015854, acc: 0.96, recall: 0.96, precision: 0.9607371794871795, f_beta: 0.959983993597439
train: step: 4355, loss: 0.022721383720636368, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 4356, loss: 0.040482986718416214, acc: 0.96, recall: 0.96, precision: 0.9607371794871795, f_beta: 0.959983993597439
train: step: 4357, loss: 0.026035437360405922, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 4358, loss: 0.0377926342189312, acc: 0.95, recall: 0.95, precision: 0.9501800720288115, f_beta: 0.9499949994999499
train: step: 4359, loss: 0.05531717836856842, acc: 0.92, recall: 0.9199999999999999, precision: 0.9206730769230769, f_beta: 0.919967987194878
train: step: 4360, loss: 0.05474701523780823, acc: 0.93, recall: 0.9299999999999999, precision: 0.930172068827531, f_beta: 0.9299929992999301
train: step: 4361, loss: 0.03380917012691498, acc: 0.94, recall: 0.94, precision: 0.9407051282051282, f_beta: 0.9399759903961584
train: step: 4362, loss: 0.02293364331126213, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 4363, loss: 0.011951157823204994, acc: 0.98, recall: 0.98, precision: 0.9807692307692308, f_beta: 0.9799919967987194
train: step: 4364, loss: 0.022467026486992836, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 4365, loss: 0.02135467529296875, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 4366, loss: 0.02758147567510605, acc: 0.96, recall: 0.96, precision: 0.96, f_beta: 0.96
train: step: 4367, loss: 0.03310577571392059, acc: 0.95, recall: 0.95, precision: 0.9501800720288115, f_beta: 0.9499949994999499
train: step: 4368, loss: 0.012018534354865551, acc: 0.99, recall: 0.99, precision: 0.9901960784313726, f_beta: 0.98999899989999
train: step: 4369, loss: 0.01832299865782261, acc: 0.98, recall: 0.98, precision: 0.98, f_beta: 0.98
train: step: 4370, loss: 0.039121758192777634, acc: 0.95, recall: 0.95, precision: 0.9501800720288115, f_beta: 0.9499949994999499
train: step: 4371, loss: 0.0565146841108799, acc: 0.94, recall: 0.94, precision: 0.94, f_beta: 0.94
train: step: 4372, loss: 0.01876101642847061, acc: 0.98, recall: 0.98, precision: 0.98, f_beta: 0.98
train: step: 4373, loss: 0.027982229366898537, acc: 0.96, recall: 0.96, precision: 0.96, f_beta: 0.96
train: step: 4374, loss: 0.029109997674822807, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 4375, loss: 0.019623495638370514, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 4376, loss: 0.006194363813847303, acc: 0.99, recall: 0.99, precision: 0.9901960784313726, f_beta: 0.98999899989999
train: step: 4377, loss: 0.017783787101507187, acc: 0.98, recall: 0.98, precision: 0.98, f_beta: 0.98
train: step: 4378, loss: 0.05157877877354622, acc: 0.93, recall: 0.9299999999999999, precision: 0.9315535929345644, f_beta: 0.9299369432489241
train: step: 4379, loss: 0.030575621873140335, acc: 0.96, recall: 0.96, precision: 0.96, f_beta: 0.96
train: step: 4380, loss: 0.018557338044047356, acc: 0.98, recall: 0.98, precision: 0.98, f_beta: 0.98
train: step: 4381, loss: 0.01923566684126854, acc: 0.98, recall: 0.98, precision: 0.98, f_beta: 0.98
train: step: 4382, loss: 0.05582236871123314, acc: 0.93, recall: 0.9299999999999999, precision: 0.930172068827531, f_beta: 0.9299929992999301
train: step: 4383, loss: 0.025370707735419273, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 4384, loss: 0.0539093017578125, acc: 0.93, recall: 0.9299999999999999, precision: 0.930172068827531, f_beta: 0.9299929992999301
train: step: 4385, loss: 0.035937272012233734, acc: 0.96, recall: 0.96, precision: 0.96, f_beta: 0.96
train: step: 4386, loss: 0.01768288016319275, acc: 0.98, recall: 0.98, precision: 0.9807692307692308, f_beta: 0.9799919967987194
train: step: 4387, loss: 0.0034689379390329123, acc: 1.0, recall: 1.0, precision: 1.0, f_beta: 1.0
train: step: 4388, loss: 0.0384042002260685, acc: 0.96, recall: 0.96, precision: 0.9607371794871795, f_beta: 0.959983993597439
train: step: 4389, loss: 0.04116634279489517, acc: 0.94, recall: 0.94, precision: 0.9428341384863124, f_beta: 0.9399038461538463
train: step: 4390, loss: 0.029020145535469055, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 4391, loss: 0.0026210451032966375, acc: 1.0, recall: 1.0, precision: 1.0, f_beta: 1.0
train: step: 4392, loss: 0.013197965919971466, acc: 0.99, recall: 0.99, precision: 0.9901960784313726, f_beta: 0.98999899989999
train: step: 4393, loss: 0.01946961134672165, acc: 0.98, recall: 0.98, precision: 0.9807692307692308, f_beta: 0.9799919967987194
train: step: 4394, loss: 0.0680396556854248, acc: 0.92, recall: 0.9199999999999999, precision: 0.9261363636363636, f_beta: 0.9197109594540345
train: step: 4395, loss: 0.0028961538337171078, acc: 1.0, recall: 1.0, precision: 1.0, f_beta: 1.0
train: step: 4396, loss: 0.04542925953865051, acc: 0.94, recall: 0.94, precision: 0.9428341384863124, f_beta: 0.9399038461538463
train: step: 4397, loss: 0.035161495208740234, acc: 0.97, recall: 0.97, precision: 0.9716981132075472, f_beta: 0.9699729756781104
train: step: 4398, loss: 0.06203949451446533, acc: 0.92, recall: 0.9199999999999999, precision: 0.9206730769230769, f_beta: 0.919967987194878
train: step: 4399, loss: 0.030582604929804802, acc: 0.97, recall: 0.97, precision: 0.9716981132075472, f_beta: 0.9699729756781104
train: step: 4400, loss: 0.02100183442234993, acc: 0.98, recall: 0.98, precision: 0.9807692307692308, f_beta: 0.9799919967987194
train: step: 4401, loss: 0.01733637973666191, acc: 0.98, recall: 0.98, precision: 0.98, f_beta: 0.98
train: step: 4402, loss: 0.01710878126323223, acc: 0.98, recall: 0.98, precision: 0.9807692307692308, f_beta: 0.9799919967987194
train: step: 4403, loss: 0.0159948468208313, acc: 0.98, recall: 0.98, precision: 0.9807692307692308, f_beta: 0.9799919967987194
train: step: 4404, loss: 0.02570328675210476, acc: 0.98, recall: 0.98, precision: 0.9807692307692308, f_beta: 0.9799919967987194
train: step: 4405, loss: 0.01163577288389206, acc: 0.99, recall: 0.99, precision: 0.9901960784313726, f_beta: 0.98999899989999
train: step: 4406, loss: 0.04181087389588356, acc: 0.93, recall: 0.9299999999999999, precision: 0.930172068827531, f_beta: 0.9299929992999301
train: step: 4407, loss: 0.03928164392709732, acc: 0.95, recall: 0.95, precision: 0.9545454545454546, f_beta: 0.949874686716792
train: step: 4408, loss: 0.05298461392521858, acc: 0.94, recall: 0.94, precision: 0.94, f_beta: 0.94
train: step: 4409, loss: 0.020342469215393066, acc: 0.98, recall: 0.98, precision: 0.9807692307692308, f_beta: 0.9799919967987194
train: step: 4410, loss: 0.02235320582985878, acc: 0.97, recall: 0.97, precision: 0.9716981132075472, f_beta: 0.9699729756781104
train: step: 4411, loss: 0.03153064846992493, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 4412, loss: 0.028574123978614807, acc: 0.97, recall: 0.97, precision: 0.9716981132075472, f_beta: 0.9699729756781104
train: step: 4413, loss: 0.03098791092634201, acc: 0.96, recall: 0.96, precision: 0.9607371794871795, f_beta: 0.959983993597439
train: step: 4414, loss: 0.03231415897607803, acc: 0.95, recall: 0.95, precision: 0.9516258530710557, f_beta: 0.949954959463517
train: step: 4415, loss: 0.03990115597844124, acc: 0.95, recall: 0.95, precision: 0.9516258530710557, f_beta: 0.949954959463517
train: step: 4416, loss: 0.04531189799308777, acc: 0.95, recall: 0.95, precision: 0.9516258530710557, f_beta: 0.949954959463517
train: step: 4417, loss: 0.0035922634415328503, acc: 1.0, recall: 1.0, precision: 1.0, f_beta: 1.0
train: step: 4418, loss: 0.013876751996576786, acc: 0.98, recall: 0.98, precision: 0.98, f_beta: 0.98
train: step: 4419, loss: 0.011981707066297531, acc: 0.98, recall: 0.98, precision: 0.98, f_beta: 0.98
train: step: 4420, loss: 0.024431724101305008, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 4421, loss: 0.037877559661865234, acc: 0.96, recall: 0.96, precision: 0.96, f_beta: 0.96
train: step: 4422, loss: 0.024832215160131454, acc: 0.98, recall: 0.98, precision: 0.98, f_beta: 0.98
train: step: 4423, loss: 0.02818402089178562, acc: 0.97, recall: 0.97, precision: 0.9716981132075472, f_beta: 0.9699729756781104
train: step: 4424, loss: 0.02654610201716423, acc: 0.97, recall: 0.97, precision: 0.9716981132075472, f_beta: 0.9699729756781104
train: step: 4425, loss: 0.05146928131580353, acc: 0.93, recall: 0.9299999999999999, precision: 0.930172068827531, f_beta: 0.9299929992999301
train: step: 4426, loss: 0.014066112227737904, acc: 0.98, recall: 0.98, precision: 0.98, f_beta: 0.98
train: step: 4427, loss: 0.04176904633641243, acc: 0.95, recall: 0.95, precision: 0.9501800720288115, f_beta: 0.9499949994999499
train: step: 4428, loss: 0.029621973633766174, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 4429, loss: 0.032867059111595154, acc: 0.96, recall: 0.96, precision: 0.9607371794871795, f_beta: 0.959983993597439
train: step: 4430, loss: 0.004369918256998062, acc: 1.0, recall: 1.0, precision: 1.0, f_beta: 1.0
train: step: 4431, loss: 0.0362330824136734, acc: 0.96, recall: 0.96, precision: 0.96, f_beta: 0.96
train: step: 4432, loss: 0.03578822314739227, acc: 0.96, recall: 0.96, precision: 0.9607371794871795, f_beta: 0.959983993597439
train: step: 4433, loss: 0.04763238504528999, acc: 0.92, recall: 0.92, precision: 0.92, f_beta: 0.92
train: step: 4434, loss: 0.03387569636106491, acc: 0.96, recall: 0.96, precision: 0.9607371794871795, f_beta: 0.959983993597439
train: step: 4435, loss: 0.02100169099867344, acc: 0.98, recall: 0.98, precision: 0.98, f_beta: 0.98
train: step: 4436, loss: 0.008181581273674965, acc: 0.99, recall: 0.99, precision: 0.9901960784313726, f_beta: 0.98999899989999
train: step: 4437, loss: 0.03959111124277115, acc: 0.95, recall: 0.95, precision: 0.9501800720288115, f_beta: 0.9499949994999499
train: step: 4438, loss: 0.056194037199020386, acc: 0.93, recall: 0.9299999999999999, precision: 0.930172068827531, f_beta: 0.9299929992999301
train: step: 4439, loss: 0.042021192610263824, acc: 0.95, recall: 0.95, precision: 0.9516258530710557, f_beta: 0.949954959463517
train: step: 4440, loss: 0.07245758175849915, acc: 0.9, recall: 0.8999999999999999, precision: 0.9105090311986863, f_beta: 0.8993558776167472
train: step: 4441, loss: 0.07097119837999344, acc: 0.99, recall: 0.99, precision: 0.9901960784313726, f_beta: 0.98999899989999
train: step: 4442, loss: 0.04203520715236664, acc: 0.96, recall: 0.96, precision: 0.9607371794871795, f_beta: 0.959983993597439
train: step: 4443, loss: 0.025775529444217682, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 4444, loss: 0.02679535374045372, acc: 0.96, recall: 0.96, precision: 0.9607371794871795, f_beta: 0.959983993597439
train: step: 4445, loss: 0.018481336534023285, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 4446, loss: 0.06919599324464798, acc: 0.92, recall: 0.9199999999999999, precision: 0.9227053140096618, f_beta: 0.9198717948717948
train: step: 4447, loss: 0.05148522928357124, acc: 0.95, recall: 0.95, precision: 0.9516258530710557, f_beta: 0.949954959463517
train: step: 4448, loss: 0.04646627604961395, acc: 0.94, recall: 0.94, precision: 0.9464285714285714, f_beta: 0.939783219590526
train: step: 4449, loss: 0.0488174706697464, acc: 0.95, recall: 0.95, precision: 0.9516258530710557, f_beta: 0.949954959463517
train: step: 4450, loss: 0.03197908774018288, acc: 0.96, recall: 0.96, precision: 0.96, f_beta: 0.96
train: step: 4451, loss: 0.032547060400247574, acc: 0.97, recall: 0.97, precision: 0.9716981132075472, f_beta: 0.9699729756781104
train: step: 4452, loss: 0.018811428919434547, acc: 0.98, recall: 0.98, precision: 0.9807692307692308, f_beta: 0.9799919967987194
train: step: 4453, loss: 0.025663428008556366, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 4454, loss: 0.0533907413482666, acc: 0.94, recall: 0.94, precision: 0.9428341384863124, f_beta: 0.9399038461538463
train: step: 4455, loss: 0.04652281105518341, acc: 0.95, recall: 0.95, precision: 0.9516258530710557, f_beta: 0.949954959463517
train: step: 4456, loss: 0.04485538601875305, acc: 0.94, recall: 0.94, precision: 0.9464285714285714, f_beta: 0.939783219590526
train: step: 4457, loss: 0.07955456525087357, acc: 0.91, recall: 0.9099999999999999, precision: 0.923728813559322, f_beta: 0.9092650468797258
train: step: 4458, loss: 0.005229596048593521, acc: 1.0, recall: 1.0, precision: 1.0, f_beta: 1.0
train: step: 4459, loss: 0.05175526440143585, acc: 0.94, recall: 0.94, precision: 0.9428341384863124, f_beta: 0.9399038461538463
train: step: 4460, loss: 0.04418252781033516, acc: 0.95, recall: 0.95, precision: 0.9501800720288115, f_beta: 0.9499949994999499
train: step: 4461, loss: 0.03219297528266907, acc: 0.98, recall: 0.98, precision: 0.98, f_beta: 0.98
train: step: 4462, loss: 0.0258480217307806, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 4463, loss: 0.013100676238536835, acc: 0.98, recall: 0.98, precision: 0.98, f_beta: 0.98
train: step: 4464, loss: 0.01838367059826851, acc: 0.99, recall: 0.99, precision: 0.9901960784313726, f_beta: 0.98999899989999
train: step: 4465, loss: 0.02691742405295372, acc: 0.98, recall: 0.98, precision: 0.98, f_beta: 0.98
train: step: 4466, loss: 0.03666180744767189, acc: 0.96, recall: 0.96, precision: 0.9607371794871795, f_beta: 0.959983993597439
train: step: 4467, loss: 0.044386133551597595, acc: 0.94, recall: 0.94, precision: 0.9428341384863124, f_beta: 0.9399038461538463
train: step: 4468, loss: 0.055376939475536346, acc: 0.93, recall: 0.9299999999999999, precision: 0.930172068827531, f_beta: 0.9299929992999301
train: step: 4469, loss: 0.02841910906136036, acc: 0.97, recall: 0.97, precision: 0.9716981132075472, f_beta: 0.9699729756781104
train: step: 4470, loss: 0.07913357019424438, acc: 0.9, recall: 0.8999999999999999, precision: 0.9025764895330113, f_beta: 0.8998397435897436
train: step: 4471, loss: 0.03502814099192619, acc: 0.96, recall: 0.96, precision: 0.96, f_beta: 0.96
train: step: 4472, loss: 0.01638701930642128, acc: 0.98, recall: 0.98, precision: 0.9807692307692308, f_beta: 0.9799919967987194
train: step: 4473, loss: 0.03782159462571144, acc: 0.96, recall: 0.96, precision: 0.962962962962963, f_beta: 0.9599358974358974
train: step: 4474, loss: 0.029601087793707848, acc: 0.96, recall: 0.96, precision: 0.9607371794871795, f_beta: 0.959983993597439
train: step: 4475, loss: 0.030299458652734756, acc: 0.97, recall: 0.97, precision: 0.9716981132075472, f_beta: 0.9699729756781104
train: step: 4476, loss: 0.0332312285900116, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 4477, loss: 0.02117062173783779, acc: 0.98, recall: 0.98, precision: 0.98, f_beta: 0.98
train: step: 4478, loss: 0.018924817442893982, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 4479, loss: 0.05125073343515396, acc: 0.94, recall: 0.94, precision: 0.9407051282051282, f_beta: 0.9399759903961584
train: step: 4480, loss: 0.03229144215583801, acc: 0.96, recall: 0.96, precision: 0.96, f_beta: 0.96
train: step: 4481, loss: 0.045237451791763306, acc: 0.96, recall: 0.96, precision: 0.96, f_beta: 0.96
train: step: 4482, loss: 0.033348776400089264, acc: 0.95, recall: 0.95, precision: 0.9545454545454546, f_beta: 0.949874686716792
train: step: 4483, loss: 0.05930647999048233, acc: 0.92, recall: 0.9199999999999999, precision: 0.9310344827586207, f_beta: 0.9194847020933978
train: step: 4484, loss: 0.015464247204363346, acc: 0.99, recall: 0.99, precision: 0.9901960784313726, f_beta: 0.98999899989999
train: step: 4485, loss: 0.030028490349650383, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 4486, loss: 0.035930339246988297, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 4487, loss: 0.024435315281152725, acc: 0.98, recall: 0.98, precision: 0.98, f_beta: 0.98
train: step: 4488, loss: 0.04429011791944504, acc: 0.94, recall: 0.94, precision: 0.9407051282051282, f_beta: 0.9399759903961584
train: step: 4489, loss: 0.03565850108861923, acc: 0.95, recall: 0.95, precision: 0.9516258530710557, f_beta: 0.949954959463517
train: step: 4490, loss: 0.0353519544005394, acc: 0.96, recall: 0.96, precision: 0.96, f_beta: 0.96
train: step: 4491, loss: 0.012690850533545017, acc: 0.98, recall: 0.98, precision: 0.9807692307692308, f_beta: 0.9799919967987194
train: step: 4492, loss: 0.010555782355368137, acc: 0.99, recall: 0.99, precision: 0.9901960784313726, f_beta: 0.98999899989999
train: step: 4493, loss: 0.06257592886686325, acc: 0.93, recall: 0.9299999999999999, precision: 0.930172068827531, f_beta: 0.9299929992999301
train: step: 4494, loss: 0.06106915324926376, acc: 0.93, recall: 0.9299999999999999, precision: 0.930172068827531, f_beta: 0.9299929992999301
train: step: 4495, loss: 0.023462647572159767, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 4496, loss: 0.042476411908864975, acc: 0.95, recall: 0.95, precision: 0.9545454545454546, f_beta: 0.949874686716792
train: step: 4497, loss: 0.03530547022819519, acc: 0.96, recall: 0.96, precision: 0.9607371794871795, f_beta: 0.959983993597439
train: step: 4498, loss: 0.037513524293899536, acc: 0.96, recall: 0.96, precision: 0.9607371794871795, f_beta: 0.959983993597439
train: step: 4499, loss: 0.06693176180124283, acc: 0.93, recall: 0.9299999999999999, precision: 0.930172068827531, f_beta: 0.9299929992999301
train: step: 4500, loss: 0.025006961077451706, acc: 0.97, recall: 0.97, precision: 0.9716981132075472, f_beta: 0.9699729756781104
train: step: 4501, loss: 0.01695149578154087, acc: 0.98, recall: 0.98, precision: 0.98, f_beta: 0.98
train: step: 4502, loss: 0.009204915724694729, acc: 0.99, recall: 0.99, precision: 0.9901960784313726, f_beta: 0.98999899989999
train: step: 4503, loss: 0.06047375500202179, acc: 0.94, recall: 0.94, precision: 0.9407051282051282, f_beta: 0.9399759903961584
train: step: 4504, loss: 0.022068863734602928, acc: 0.98, recall: 0.98, precision: 0.98, f_beta: 0.98
train: step: 4505, loss: 0.010462725535035133, acc: 0.99, recall: 0.99, precision: 0.9901960784313726, f_beta: 0.98999899989999
train: step: 4506, loss: 0.027991484850645065, acc: 0.96, recall: 0.96, precision: 0.9607371794871795, f_beta: 0.959983993597439
train: step: 4507, loss: 0.026332154870033264, acc: 0.97, recall: 0.97, precision: 0.9716981132075472, f_beta: 0.9699729756781104
train: step: 4508, loss: 0.018417906016111374, acc: 0.98, recall: 0.98, precision: 0.9807692307692308, f_beta: 0.9799919967987194
train: step: 4509, loss: 0.03893425315618515, acc: 0.94, recall: 0.94, precision: 0.94, f_beta: 0.94
train: step: 4510, loss: 0.00936799868941307, acc: 1.0, recall: 1.0, precision: 1.0, f_beta: 1.0
train: step: 4511, loss: 0.032133251428604126, acc: 0.96, recall: 0.96, precision: 0.96, f_beta: 0.96
train: step: 4512, loss: 0.006149915512651205, acc: 0.99, recall: 0.99, precision: 0.9901960784313726, f_beta: 0.98999899989999
train: step: 4513, loss: 0.05156298726797104, acc: 0.93, recall: 0.9299999999999999, precision: 0.9315535929345644, f_beta: 0.9299369432489241
train: step: 4514, loss: 0.043577007949352264, acc: 0.94, recall: 0.94, precision: 0.9464285714285714, f_beta: 0.939783219590526
train: step: 4515, loss: 0.031627170741558075, acc: 0.96, recall: 0.96, precision: 0.962962962962963, f_beta: 0.9599358974358974
train: step: 4516, loss: 0.0537121407687664, acc: 0.92, recall: 0.92, precision: 0.92, f_beta: 0.92
train: step: 4517, loss: 0.018596641719341278, acc: 0.99, recall: 0.99, precision: 0.9901960784313726, f_beta: 0.98999899989999
train: step: 4518, loss: 0.026876123622059822, acc: 0.98, recall: 0.98, precision: 0.9807692307692308, f_beta: 0.9799919967987194
train: step: 4519, loss: 0.03367866575717926, acc: 0.96, recall: 0.96, precision: 0.9607371794871795, f_beta: 0.959983993597439
train: step: 4520, loss: 0.014827419072389603, acc: 0.98, recall: 0.98, precision: 0.98, f_beta: 0.98
train: step: 4521, loss: 0.04417596757411957, acc: 0.95, recall: 0.95, precision: 0.9516258530710557, f_beta: 0.949954959463517
train: step: 4522, loss: 0.0396689772605896, acc: 0.96, recall: 0.96, precision: 0.96, f_beta: 0.96
train: step: 4523, loss: 0.016590707004070282, acc: 0.98, recall: 0.98, precision: 0.9807692307692308, f_beta: 0.9799919967987194
train: step: 4524, loss: 0.016396349295973778, acc: 0.98, recall: 0.98, precision: 0.98, f_beta: 0.98
train: step: 4525, loss: 0.01951690763235092, acc: 0.98, recall: 0.98, precision: 0.98, f_beta: 0.98
train: step: 4526, loss: 0.0463365837931633, acc: 0.94, recall: 0.94, precision: 0.9407051282051282, f_beta: 0.9399759903961584
train: step: 4527, loss: 0.06058989465236664, acc: 0.92, recall: 0.92, precision: 0.92, f_beta: 0.92
train: step: 4528, loss: 0.01987336203455925, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 4529, loss: 0.014920910820364952, acc: 0.99, recall: 0.99, precision: 0.9901960784313726, f_beta: 0.98999899989999
train: step: 4530, loss: 0.024627098813652992, acc: 0.97, recall: 0.97, precision: 0.9716981132075472, f_beta: 0.9699729756781104
train: step: 4531, loss: 0.03860662132501602, acc: 0.95, recall: 0.95, precision: 0.9501800720288115, f_beta: 0.9499949994999499
train: step: 4532, loss: 0.05898910015821457, acc: 0.93, recall: 0.9299999999999999, precision: 0.930172068827531, f_beta: 0.9299929992999301
train: step: 4533, loss: 0.016163360327482224, acc: 0.98, recall: 0.98, precision: 0.9807692307692308, f_beta: 0.9799919967987194
train: step: 4534, loss: 0.024067319929599762, acc: 0.97, recall: 0.97, precision: 0.9716981132075472, f_beta: 0.9699729756781104
train: step: 4535, loss: 0.05240674316883087, acc: 0.95, recall: 0.95, precision: 0.9501800720288115, f_beta: 0.9499949994999499
train: step: 4536, loss: 0.013833362609148026, acc: 0.98, recall: 0.98, precision: 0.98, f_beta: 0.98
train: step: 4537, loss: 0.06561221927404404, acc: 0.91, recall: 0.91, precision: 0.9101640656262505, f_beta: 0.90999099909991
train: step: 4538, loss: 0.03679654747247696, acc: 0.95, recall: 0.95, precision: 0.9516258530710557, f_beta: 0.949954959463517
train: step: 4539, loss: 0.03542105108499527, acc: 0.95, recall: 0.95, precision: 0.9501800720288115, f_beta: 0.9499949994999499
train: step: 4540, loss: 0.04943159222602844, acc: 0.94, recall: 0.94, precision: 0.94, f_beta: 0.94
train: step: 4541, loss: 0.020428547635674477, acc: 0.97, recall: 0.97, precision: 0.9716981132075472, f_beta: 0.9699729756781104
train: step: 4542, loss: 0.031590692698955536, acc: 0.96, recall: 0.96, precision: 0.96, f_beta: 0.96
train: step: 4543, loss: 0.03033413738012314, acc: 0.96, recall: 0.96, precision: 0.96, f_beta: 0.96
train: step: 4544, loss: 0.03358268365263939, acc: 0.95, recall: 0.95, precision: 0.9501800720288115, f_beta: 0.9499949994999499
train: step: 4545, loss: 0.012070495635271072, acc: 0.99, recall: 0.99, precision: 0.9901960784313726, f_beta: 0.98999899989999
train: step: 4546, loss: 0.02680385112762451, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 4547, loss: 0.019902227446436882, acc: 0.98, recall: 0.98, precision: 0.98, f_beta: 0.98
train: step: 4548, loss: 0.023129701614379883, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 4549, loss: 0.04224327206611633, acc: 0.95, recall: 0.95, precision: 0.9501800720288115, f_beta: 0.9499949994999499
train: step: 4550, loss: 0.0323454849421978, acc: 0.96, recall: 0.96, precision: 0.96, f_beta: 0.96
train: step: 4551, loss: 0.017036406323313713, acc: 0.98, recall: 0.98, precision: 0.9807692307692308, f_beta: 0.9799919967987194
train: step: 4552, loss: 0.046144552528858185, acc: 0.95, recall: 0.95, precision: 0.9516258530710557, f_beta: 0.949954959463517
train: step: 4553, loss: 0.03964568302035332, acc: 0.95, recall: 0.95, precision: 0.9501800720288115, f_beta: 0.9499949994999499
train: step: 4554, loss: 0.033768005669116974, acc: 0.96, recall: 0.96, precision: 0.96, f_beta: 0.96
train: step: 4555, loss: 0.04869283735752106, acc: 0.95, recall: 0.95, precision: 0.9501800720288115, f_beta: 0.9499949994999499
train: step: 4556, loss: 0.022156791761517525, acc: 0.98, recall: 0.98, precision: 0.9807692307692308, f_beta: 0.9799919967987194
train: step: 4557, loss: 0.020314477384090424, acc: 0.98, recall: 0.98, precision: 0.9807692307692308, f_beta: 0.9799919967987194
train: step: 4558, loss: 0.05479101091623306, acc: 0.94, recall: 0.94, precision: 0.9428341384863124, f_beta: 0.9399038461538463
train: step: 4559, loss: 0.001933407736942172, acc: 1.0, recall: 1.0, precision: 1.0, f_beta: 1.0
train: step: 4560, loss: 0.006533673498779535, acc: 1.0, recall: 1.0, precision: 1.0, f_beta: 1.0
train: step: 4561, loss: 0.01947622373700142, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 4562, loss: 0.005064040422439575, acc: 0.99, recall: 0.99, precision: 0.9901960784313726, f_beta: 0.98999899989999
train: step: 4563, loss: 0.04117140173912048, acc: 0.95, recall: 0.95, precision: 0.9516258530710557, f_beta: 0.949954959463517
train: step: 4564, loss: 0.053005337715148926, acc: 0.93, recall: 0.9299999999999999, precision: 0.9315535929345644, f_beta: 0.9299369432489241
train: step: 4565, loss: 0.03310041502118111, acc: 0.96, recall: 0.96, precision: 0.9607371794871795, f_beta: 0.959983993597439
train: step: 4566, loss: 0.021477971225976944, acc: 0.97, recall: 0.97, precision: 0.9716981132075472, f_beta: 0.9699729756781104
train: step: 4567, loss: 0.052824363112449646, acc: 0.93, recall: 0.9299999999999999, precision: 0.930172068827531, f_beta: 0.9299929992999301
train: step: 4568, loss: 0.02475898340344429, acc: 0.97, recall: 0.97, precision: 0.9716981132075472, f_beta: 0.9699729756781104
train: step: 4569, loss: 0.035712841898202896, acc: 0.95, recall: 0.95, precision: 0.9516258530710557, f_beta: 0.949954959463517
train: step: 4570, loss: 0.01751016080379486, acc: 0.98, recall: 0.98, precision: 0.98, f_beta: 0.98
train: step: 4571, loss: 0.021343475207686424, acc: 0.98, recall: 0.98, precision: 0.98, f_beta: 0.98
train: step: 4572, loss: 0.05422142893075943, acc: 0.93, recall: 0.9299999999999999, precision: 0.9315535929345644, f_beta: 0.9299369432489241
train: step: 4573, loss: 0.04264431446790695, acc: 0.96, recall: 0.96, precision: 0.96, f_beta: 0.96
train: step: 4574, loss: 0.053322333842515945, acc: 0.93, recall: 0.9299999999999999, precision: 0.930172068827531, f_beta: 0.9299929992999301
train: step: 4575, loss: 0.06125899404287338, acc: 0.92, recall: 0.9199999999999999, precision: 0.9206730769230769, f_beta: 0.919967987194878
train: step: 4576, loss: 0.052166957408189774, acc: 0.95, recall: 0.95, precision: 0.9501800720288115, f_beta: 0.9499949994999499
train: step: 4577, loss: 0.046353720128536224, acc: 0.95, recall: 0.95, precision: 0.9501800720288115, f_beta: 0.9499949994999499
train: step: 4578, loss: 0.029167385771870613, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 4579, loss: 0.04746244475245476, acc: 0.95, recall: 0.95, precision: 0.9501800720288115, f_beta: 0.9499949994999499
train: step: 4580, loss: 0.0320332869887352, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 4581, loss: 0.029582347720861435, acc: 0.96, recall: 0.96, precision: 0.96, f_beta: 0.96
train: step: 4582, loss: 0.018778160214424133, acc: 0.98, recall: 0.98, precision: 0.9807692307692308, f_beta: 0.9799919967987194
train: step: 4583, loss: 0.030085710808634758, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 4584, loss: 0.018586808815598488, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 4585, loss: 0.038230765610933304, acc: 0.95, recall: 0.95, precision: 0.9516258530710557, f_beta: 0.949954959463517
train: step: 4586, loss: 0.012561088427901268, acc: 0.99, recall: 0.99, precision: 0.9901960784313726, f_beta: 0.98999899989999
train: step: 4587, loss: 0.047946806997060776, acc: 0.95, recall: 0.95, precision: 0.9501800720288115, f_beta: 0.9499949994999499
train: step: 4588, loss: 0.07385462522506714, acc: 0.91, recall: 0.9099999999999999, precision: 0.9114813327980731, f_beta: 0.9099189270343309
train: step: 4589, loss: 0.003282860154286027, acc: 1.0, recall: 1.0, precision: 1.0, f_beta: 1.0
train: step: 4590, loss: 0.026536274701356888, acc: 0.97, recall: 0.97, precision: 0.9716981132075472, f_beta: 0.9699729756781104
train: step: 4591, loss: 0.04491442069411278, acc: 0.95, recall: 0.95, precision: 0.9501800720288115, f_beta: 0.9499949994999499
train: step: 4592, loss: 0.021742044016718864, acc: 0.98, recall: 0.98, precision: 0.98, f_beta: 0.98
train: step: 4593, loss: 0.017018280923366547, acc: 0.98, recall: 0.98, precision: 0.98, f_beta: 0.98
train: step: 4594, loss: 0.04119010269641876, acc: 0.95, recall: 0.95, precision: 0.9516258530710557, f_beta: 0.949954959463517
train: step: 4595, loss: 0.021216683089733124, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 4596, loss: 0.00387919251807034, acc: 0.99, recall: 0.99, precision: 0.9901960784313726, f_beta: 0.98999899989999
train: step: 4597, loss: 0.044963158667087555, acc: 0.95, recall: 0.95, precision: 0.9501800720288115, f_beta: 0.9499949994999499
train: step: 4598, loss: 0.02961900644004345, acc: 0.96, recall: 0.96, precision: 0.96, f_beta: 0.96
train: step: 4599, loss: 0.030332336202263832, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 4600, loss: 0.03271428495645523, acc: 0.96, recall: 0.96, precision: 0.96, f_beta: 0.96
train: step: 4601, loss: 0.00851504411548376, acc: 0.99, recall: 0.99, precision: 0.9901960784313726, f_beta: 0.98999899989999
train: step: 4602, loss: 0.05047222226858139, acc: 0.94, recall: 0.94, precision: 0.9428341384863124, f_beta: 0.9399038461538463
train: step: 4603, loss: 0.04379093274474144, acc: 0.95, recall: 0.95, precision: 0.9516258530710557, f_beta: 0.949954959463517
train: step: 4604, loss: 0.06746792793273926, acc: 0.93, recall: 0.9299999999999999, precision: 0.930172068827531, f_beta: 0.9299929992999301
train: step: 4605, loss: 0.05210765451192856, acc: 0.95, recall: 0.95, precision: 0.9501800720288115, f_beta: 0.9499949994999499
train: step: 4606, loss: 0.018438545987010002, acc: 0.98, recall: 0.98, precision: 0.98, f_beta: 0.98
train: step: 4607, loss: 0.003124251728877425, acc: 1.0, recall: 1.0, precision: 1.0, f_beta: 1.0
train: step: 4608, loss: 0.004680536687374115, acc: 1.0, recall: 1.0, precision: 1.0, f_beta: 1.0
train: step: 4609, loss: 0.019561752676963806, acc: 0.97, recall: 0.97, precision: 0.9716981132075472, f_beta: 0.9699729756781104
train: step: 4610, loss: 0.011588845402002335, acc: 0.99, recall: 0.99, precision: 0.9901960784313726, f_beta: 0.98999899989999
train: step: 4611, loss: 0.009840729646384716, acc: 0.99, recall: 0.99, precision: 0.9901960784313726, f_beta: 0.98999899989999
train: step: 4612, loss: 0.019647473469376564, acc: 0.98, recall: 0.98, precision: 0.98, f_beta: 0.98
train: step: 4613, loss: 0.02967826835811138, acc: 0.96, recall: 0.96, precision: 0.962962962962963, f_beta: 0.9599358974358974
train: step: 4614, loss: 0.020639007911086082, acc: 0.98, recall: 0.98, precision: 0.9807692307692308, f_beta: 0.9799919967987194
train: step: 4615, loss: 0.017076492309570312, acc: 0.99, recall: 0.99, precision: 0.9901960784313726, f_beta: 0.98999899989999
train: step: 4616, loss: 0.01846633478999138, acc: 0.98, recall: 0.98, precision: 0.98, f_beta: 0.98
train: step: 4617, loss: 0.015408839099109173, acc: 0.98, recall: 0.98, precision: 0.9807692307692308, f_beta: 0.9799919967987194
train: step: 4618, loss: 0.04026121646165848, acc: 0.95, recall: 0.95, precision: 0.9516258530710557, f_beta: 0.949954959463517
train: step: 4619, loss: 0.0163844283670187, acc: 0.96, recall: 0.96, precision: 0.9607371794871795, f_beta: 0.959983993597439
train: step: 4620, loss: 0.012384810484945774, acc: 0.99, recall: 0.99, precision: 0.9901960784313726, f_beta: 0.98999899989999
train: step: 4621, loss: 0.019821979105472565, acc: 0.98, recall: 0.98, precision: 0.9807692307692308, f_beta: 0.9799919967987194
train: step: 4622, loss: 0.03670976310968399, acc: 0.95, recall: 0.95, precision: 0.9501800720288115, f_beta: 0.9499949994999499
train: step: 4623, loss: 0.08561567962169647, acc: 0.89, recall: 0.89, precision: 0.89015606242497, f_beta: 0.88998899889989
train: step: 4624, loss: 0.026706062257289886, acc: 0.97, recall: 0.97, precision: 0.9716981132075472, f_beta: 0.9699729756781104
train: step: 4625, loss: 0.03557547554373741, acc: 0.96, recall: 0.96, precision: 0.9607371794871795, f_beta: 0.959983993597439
train: step: 4626, loss: 0.04359322413802147, acc: 0.94, recall: 0.94, precision: 0.94, f_beta: 0.94
train: step: 4627, loss: 0.006418859586119652, acc: 1.0, recall: 1.0, precision: 1.0, f_beta: 1.0
train: step: 4628, loss: 0.014295860193669796, acc: 0.99, recall: 0.99, precision: 0.9901960784313726, f_beta: 0.98999899989999
train: step: 4629, loss: 0.026256317272782326, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 4630, loss: 0.020786188542842865, acc: 0.98, recall: 0.98, precision: 0.9807692307692308, f_beta: 0.9799919967987194
train: step: 4631, loss: 0.0389251634478569, acc: 0.95, recall: 0.95, precision: 0.9501800720288115, f_beta: 0.9499949994999499
train: step: 4632, loss: 0.04266636073589325, acc: 0.96, recall: 0.96, precision: 0.9607371794871795, f_beta: 0.959983993597439
train: step: 4633, loss: 0.04818154498934746, acc: 0.95, recall: 0.95, precision: 0.9501800720288115, f_beta: 0.9499949994999499
train: step: 4634, loss: 0.031413134187459946, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 4635, loss: 0.049516722559928894, acc: 0.94, recall: 0.94, precision: 0.9407051282051282, f_beta: 0.9399759903961584
train: step: 4636, loss: 0.041311513632535934, acc: 0.96, recall: 0.96, precision: 0.962962962962963, f_beta: 0.9599358974358974
train: step: 4637, loss: 0.014554529450833797, acc: 0.98, recall: 0.98, precision: 0.98, f_beta: 0.98
train: step: 4638, loss: 0.01927800662815571, acc: 0.98, recall: 0.98, precision: 0.98, f_beta: 0.98
train: step: 4639, loss: 0.016261495649814606, acc: 0.96, recall: 0.96, precision: 0.962962962962963, f_beta: 0.9599358974358974
train: step: 4640, loss: 0.04827652871608734, acc: 0.94, recall: 0.94, precision: 0.9464285714285714, f_beta: 0.939783219590526
train: step: 4641, loss: 0.0287963654845953, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 4642, loss: 0.05525876209139824, acc: 0.95, recall: 0.95, precision: 0.9516258530710557, f_beta: 0.949954959463517
train: step: 4643, loss: 0.03087794966995716, acc: 0.95, recall: 0.95, precision: 0.9516258530710557, f_beta: 0.949954959463517
train: step: 4644, loss: 0.04132276400923729, acc: 0.96, recall: 0.96, precision: 0.9607371794871795, f_beta: 0.959983993597439
train: step: 4645, loss: 0.004153146408498287, acc: 1.0, recall: 1.0, precision: 1.0, f_beta: 1.0
train: step: 4646, loss: 0.05329308658838272, acc: 0.96, recall: 0.96, precision: 0.9607371794871795, f_beta: 0.959983993597439
train: step: 4647, loss: 0.038751352578401566, acc: 0.94, recall: 0.94, precision: 0.9407051282051282, f_beta: 0.9399759903961584
train: step: 4648, loss: 0.026975518092513084, acc: 0.98, recall: 0.98, precision: 0.9807692307692308, f_beta: 0.9799919967987194
train: step: 4649, loss: 0.020207105204463005, acc: 0.97, recall: 0.97, precision: 0.9716981132075472, f_beta: 0.9699729756781104
train: step: 4650, loss: 0.04472029581665993, acc: 0.96, recall: 0.96, precision: 0.96, f_beta: 0.96
train: step: 4651, loss: 0.006117359269410372, acc: 0.99, recall: 0.99, precision: 0.9901960784313726, f_beta: 0.98999899989999
train: step: 4652, loss: 0.021220635622739792, acc: 0.98, recall: 0.98, precision: 0.98, f_beta: 0.98
train: step: 4653, loss: 0.032378505915403366, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 4654, loss: 0.013901624828577042, acc: 0.99, recall: 0.99, precision: 0.9901960784313726, f_beta: 0.98999899989999
train: step: 4655, loss: 0.0011129719205200672, acc: 1.0, recall: 1.0, precision: 1.0, f_beta: 1.0
train: step: 4656, loss: 0.012330731377005577, acc: 0.99, recall: 0.99, precision: 0.9901960784313726, f_beta: 0.98999899989999
train: step: 4657, loss: 0.0596877858042717, acc: 0.92, recall: 0.9199999999999999, precision: 0.9206730769230769, f_beta: 0.919967987194878
train: step: 4658, loss: 0.0036655666772276163, acc: 1.0, recall: 1.0, precision: 1.0, f_beta: 1.0
train: step: 4659, loss: 0.02411988191306591, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 4660, loss: 0.018241658806800842, acc: 0.98, recall: 0.98, precision: 0.9807692307692308, f_beta: 0.9799919967987194
train: step: 4661, loss: 0.05605144053697586, acc: 0.93, recall: 0.9299999999999999, precision: 0.930172068827531, f_beta: 0.9299929992999301
train: step: 4662, loss: 0.05134997516870499, acc: 0.95, recall: 0.95, precision: 0.9501800720288115, f_beta: 0.9499949994999499
train: step: 4663, loss: 0.03178533539175987, acc: 0.95, recall: 0.95, precision: 0.9516258530710557, f_beta: 0.949954959463517
train: step: 4664, loss: 0.020296819508075714, acc: 0.98, recall: 0.98, precision: 0.98, f_beta: 0.98
train: step: 4665, loss: 0.03430749103426933, acc: 0.96, recall: 0.96, precision: 0.962962962962963, f_beta: 0.9599358974358974
train: step: 4666, loss: 0.028334731236100197, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 4667, loss: 0.015580186620354652, acc: 0.98, recall: 0.98, precision: 0.9807692307692308, f_beta: 0.9799919967987194
train: step: 4668, loss: 0.027637748047709465, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 4669, loss: 0.02722058817744255, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 4670, loss: 0.0452733039855957, acc: 0.95, recall: 0.95, precision: 0.9516258530710557, f_beta: 0.949954959463517
train: step: 4671, loss: 0.011407161131501198, acc: 0.99, recall: 0.99, precision: 0.9901960784313726, f_beta: 0.98999899989999
train: step: 4672, loss: 0.014468357898294926, acc: 0.98, recall: 0.98, precision: 0.98, f_beta: 0.98
train: step: 4673, loss: 0.02756514959037304, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 4674, loss: 0.03021220676600933, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 4675, loss: 0.030145656317472458, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 4676, loss: 0.02427741140127182, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 4677, loss: 0.048169780522584915, acc: 0.95, recall: 0.95, precision: 0.9501800720288115, f_beta: 0.9499949994999499
train: step: 4678, loss: 0.002346734283491969, acc: 1.0, recall: 1.0, precision: 1.0, f_beta: 1.0
train: step: 4679, loss: 0.029333967715501785, acc: 0.96, recall: 0.96, precision: 0.962962962962963, f_beta: 0.9599358974358974
train: step: 4680, loss: 0.025847138836979866, acc: 0.98, recall: 0.98, precision: 0.9807692307692308, f_beta: 0.9799919967987194
train: step: 4681, loss: 0.08157100528478622, acc: 0.91, recall: 0.9099999999999999, precision: 0.9141414141414141, f_beta: 0.9097744360902256
train: step: 4682, loss: 0.026310889050364494, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 4683, loss: 0.010146832093596458, acc: 0.99, recall: 0.99, precision: 0.9901960784313726, f_beta: 0.98999899989999
train: step: 4684, loss: 0.01848951168358326, acc: 0.98, recall: 0.98, precision: 0.98, f_beta: 0.98
train: step: 4685, loss: 0.0221357773989439, acc: 0.97, recall: 0.97, precision: 0.9716981132075472, f_beta: 0.9699729756781104
train: step: 4686, loss: 0.04089362919330597, acc: 0.96, recall: 0.96, precision: 0.9607371794871795, f_beta: 0.959983993597439
train: step: 4687, loss: 0.052278369665145874, acc: 0.94, recall: 0.94, precision: 0.9407051282051282, f_beta: 0.9399759903961584
train: step: 4688, loss: 0.04649585857987404, acc: 0.96, recall: 0.96, precision: 0.962962962962963, f_beta: 0.9599358974358974
train: step: 4689, loss: 0.08476085960865021, acc: 0.9, recall: 0.9, precision: 0.9, f_beta: 0.9
train: step: 4690, loss: 0.05385279655456543, acc: 0.93, recall: 0.9299999999999999, precision: 0.9315535929345644, f_beta: 0.9299369432489241
train: step: 4691, loss: 0.02936229296028614, acc: 0.97, recall: 0.97, precision: 0.9716981132075472, f_beta: 0.9699729756781104
train: step: 4692, loss: 0.018132291734218597, acc: 0.98, recall: 0.98, precision: 0.9807692307692308, f_beta: 0.9799919967987194
train: step: 4693, loss: 0.07495632022619247, acc: 0.92, recall: 0.9199999999999999, precision: 0.9206730769230769, f_beta: 0.919967987194878
train: step: 4694, loss: 0.00282390508800745, acc: 1.0, recall: 1.0, precision: 1.0, f_beta: 1.0
train: step: 4695, loss: 0.003422809299081564, acc: 0.99, recall: 0.99, precision: 0.9901960784313726, f_beta: 0.98999899989999
train: step: 4696, loss: 0.025440674275159836, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 4697, loss: 0.05081559345126152, acc: 0.93, recall: 0.9299999999999999, precision: 0.9315535929345644, f_beta: 0.9299369432489241
train: step: 4698, loss: 0.035774167627096176, acc: 0.96, recall: 0.96, precision: 0.962962962962963, f_beta: 0.9599358974358974
train: step: 4699, loss: 0.08417585492134094, acc: 0.89, recall: 0.8899999999999999, precision: 0.8939393939393939, f_beta: 0.8897243107769424
train: step: 4700, loss: 0.02906860038638115, acc: 0.96, recall: 0.96, precision: 0.9607371794871795, f_beta: 0.959983993597439
train: step: 4701, loss: 0.0584840402007103, acc: 0.92, recall: 0.9199999999999999, precision: 0.9261363636363636, f_beta: 0.9197109594540345
train: step: 4702, loss: 0.058108989149332047, acc: 0.94, recall: 0.94, precision: 0.9428341384863124, f_beta: 0.9399038461538463
train: step: 4703, loss: 0.02343830093741417, acc: 0.97, recall: 0.97, precision: 0.9716981132075472, f_beta: 0.9699729756781104
train: step: 4704, loss: 0.044703930616378784, acc: 0.94, recall: 0.94, precision: 0.94, f_beta: 0.94
train: step: 4705, loss: 0.024176673963665962, acc: 0.96, recall: 0.96, precision: 0.9607371794871795, f_beta: 0.959983993597439
train: step: 4706, loss: 0.044626399874687195, acc: 0.95, recall: 0.95, precision: 0.9501800720288115, f_beta: 0.9499949994999499
train: step: 4707, loss: 0.04494545981287956, acc: 0.94, recall: 0.94, precision: 0.94, f_beta: 0.94
train: step: 4708, loss: 0.024832332506775856, acc: 0.96, recall: 0.96, precision: 0.96, f_beta: 0.96
train: step: 4709, loss: 0.022374169901013374, acc: 0.98, recall: 0.98, precision: 0.98, f_beta: 0.98
train: step: 4710, loss: 0.020614853128790855, acc: 0.98, recall: 0.98, precision: 0.98, f_beta: 0.98
train: step: 4711, loss: 0.039131101220846176, acc: 0.96, recall: 0.96, precision: 0.96, f_beta: 0.96
train: step: 4712, loss: 0.019821874797344208, acc: 0.98, recall: 0.98, precision: 0.9807692307692308, f_beta: 0.9799919967987194
train: step: 4713, loss: 0.013197495602071285, acc: 0.99, recall: 0.99, precision: 0.9901960784313726, f_beta: 0.98999899989999
train: step: 4714, loss: 0.013426227495074272, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 4715, loss: 0.019526202231645584, acc: 0.99, recall: 0.99, precision: 0.9901960784313726, f_beta: 0.98999899989999
train: step: 4716, loss: 0.046873778104782104, acc: 0.94, recall: 0.94, precision: 0.9407051282051282, f_beta: 0.9399759903961584
train: step: 4717, loss: 0.001409983029589057, acc: 1.0, recall: 1.0, precision: 1.0, f_beta: 1.0
train: step: 4718, loss: 0.017346518114209175, acc: 0.98, recall: 0.98, precision: 0.98, f_beta: 0.98
train: step: 4719, loss: 0.014396156184375286, acc: 0.98, recall: 0.98, precision: 0.9807692307692308, f_beta: 0.9799919967987194
train: step: 4720, loss: 0.05573757737874985, acc: 0.93, recall: 0.9299999999999999, precision: 0.9315535929345644, f_beta: 0.9299369432489241
train: step: 4721, loss: 0.0011578056728467345, acc: 1.0, recall: 1.0, precision: 1.0, f_beta: 1.0
train: step: 4722, loss: 0.0031072632409632206, acc: 1.0, recall: 1.0, precision: 1.0, f_beta: 1.0
train: step: 4723, loss: 0.010093118995428085, acc: 0.98, recall: 0.98, precision: 0.9807692307692308, f_beta: 0.9799919967987194
train: step: 4724, loss: 0.025815820321440697, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 4725, loss: 0.0404670350253582, acc: 0.95, recall: 0.95, precision: 0.9516258530710557, f_beta: 0.949954959463517
train: step: 4726, loss: 0.026054389774799347, acc: 0.96, recall: 0.96, precision: 0.962962962962963, f_beta: 0.9599358974358974
train: step: 4727, loss: 0.008299709297716618, acc: 0.99, recall: 0.99, precision: 0.9901960784313726, f_beta: 0.98999899989999
train: step: 4728, loss: 0.014757633209228516, acc: 0.99, recall: 0.99, precision: 0.9901960784313726, f_beta: 0.98999899989999
train: step: 4729, loss: 0.03573774918913841, acc: 0.93, recall: 0.9299999999999999, precision: 0.9343434343434343, f_beta: 0.9298245614035088
train: step: 4730, loss: 0.025333905592560768, acc: 0.98, recall: 0.98, precision: 0.98, f_beta: 0.98
train: step: 4731, loss: 0.027211444452404976, acc: 0.96, recall: 0.96, precision: 0.962962962962963, f_beta: 0.9599358974358974
train: step: 4732, loss: 0.0552191361784935, acc: 0.94, recall: 0.94, precision: 0.94, f_beta: 0.94
train: step: 4733, loss: 0.025088440626859665, acc: 0.97, recall: 0.97, precision: 0.9716981132075472, f_beta: 0.9699729756781104
train: step: 4734, loss: 0.03948839008808136, acc: 0.95, recall: 0.95, precision: 0.9516258530710557, f_beta: 0.949954959463517
train: step: 4735, loss: 0.0022602060344070196, acc: 1.0, recall: 1.0, precision: 1.0, f_beta: 1.0
train: step: 4736, loss: 0.03605039045214653, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 4737, loss: 0.017940782010555267, acc: 0.98, recall: 0.98, precision: 0.98, f_beta: 0.98
train: step: 4738, loss: 0.03667604550719261, acc: 0.96, recall: 0.96, precision: 0.96, f_beta: 0.96
train: step: 4739, loss: 0.0005378508358262479, acc: 1.0, recall: 1.0, precision: 1.0, f_beta: 1.0
train: step: 4740, loss: 0.03148307278752327, acc: 0.97, recall: 0.97, precision: 0.9716981132075472, f_beta: 0.9699729756781104
train: step: 4741, loss: 0.05408409982919693, acc: 0.95, recall: 0.95, precision: 0.9501800720288115, f_beta: 0.9499949994999499
train: step: 4742, loss: 0.02536289021372795, acc: 0.96, recall: 0.96, precision: 0.9607371794871795, f_beta: 0.959983993597439
train: step: 4743, loss: 0.006401002872735262, acc: 0.99, recall: 0.99, precision: 0.9901960784313726, f_beta: 0.98999899989999
train: step: 4744, loss: 0.05708314850926399, acc: 0.93, recall: 0.9299999999999999, precision: 0.930172068827531, f_beta: 0.9299929992999301
train: step: 4745, loss: 0.018652522936463356, acc: 0.98, recall: 0.98, precision: 0.98, f_beta: 0.98
train: step: 4746, loss: 0.015533512458205223, acc: 0.98, recall: 0.98, precision: 0.9807692307692308, f_beta: 0.9799919967987194
train: step: 4747, loss: 0.0255187526345253, acc: 0.95, recall: 0.95, precision: 0.9501800720288115, f_beta: 0.9499949994999499
train: step: 4748, loss: 0.021909594535827637, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 4749, loss: 0.03905601054430008, acc: 0.94, recall: 0.94, precision: 0.9407051282051282, f_beta: 0.9399759903961584
train: step: 4750, loss: 0.03133867681026459, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 4751, loss: 0.04955538362264633, acc: 0.93, recall: 0.9299999999999999, precision: 0.9385964912280702, f_beta: 0.9296553110240177
train: step: 4752, loss: 0.0349728949368, acc: 0.97, recall: 0.97, precision: 0.9716981132075472, f_beta: 0.9699729756781104
train: step: 4753, loss: 0.02957144007086754, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 4754, loss: 0.009125635959208012, acc: 0.99, recall: 0.99, precision: 0.9901960784313726, f_beta: 0.98999899989999
train: step: 4755, loss: 0.03144240006804466, acc: 0.95, recall: 0.95, precision: 0.9516258530710557, f_beta: 0.949954959463517
train: step: 4756, loss: 0.017340265214443207, acc: 0.98, recall: 0.98, precision: 0.9807692307692308, f_beta: 0.9799919967987194
train: step: 4757, loss: 0.013837141916155815, acc: 0.99, recall: 0.99, precision: 0.9901960784313726, f_beta: 0.98999899989999
train: step: 4758, loss: 0.04131397232413292, acc: 0.96, recall: 0.96, precision: 0.9607371794871795, f_beta: 0.959983993597439
train: step: 4759, loss: 0.00961910281330347, acc: 0.99, recall: 0.99, precision: 0.9901960784313726, f_beta: 0.98999899989999
train: step: 4760, loss: 0.03727998584508896, acc: 0.96, recall: 0.96, precision: 0.9607371794871795, f_beta: 0.959983993597439
train: step: 4761, loss: 0.0421258844435215, acc: 0.96, recall: 0.96, precision: 0.9607371794871795, f_beta: 0.959983993597439
train: step: 4762, loss: 0.008871478959918022, acc: 0.99, recall: 0.99, precision: 0.9901960784313726, f_beta: 0.98999899989999
train: step: 4763, loss: 0.03724949434399605, acc: 0.97, recall: 0.97, precision: 0.9716981132075472, f_beta: 0.9699729756781104
train: step: 4764, loss: 0.021356461569666862, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 4765, loss: 0.016970472410321236, acc: 0.98, recall: 0.98, precision: 0.9807692307692308, f_beta: 0.9799919967987194
train: step: 4766, loss: 0.020860005170106888, acc: 0.98, recall: 0.98, precision: 0.98, f_beta: 0.98
train: step: 4767, loss: 0.015194029547274113, acc: 0.98, recall: 0.98, precision: 0.98, f_beta: 0.98
train: step: 4768, loss: 0.033326275646686554, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 4769, loss: 0.02416263148188591, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 4770, loss: 0.014714193530380726, acc: 0.98, recall: 0.98, precision: 0.9807692307692308, f_beta: 0.9799919967987194
train: step: 4771, loss: 0.007786455098539591, acc: 1.0, recall: 1.0, precision: 1.0, f_beta: 1.0
train: step: 4772, loss: 0.01073546800762415, acc: 1.0, recall: 1.0, precision: 1.0, f_beta: 1.0
train: step: 4773, loss: 0.020112691447138786, acc: 0.98, recall: 0.98, precision: 0.9807692307692308, f_beta: 0.9799919967987194
train: step: 4774, loss: 0.03220207244157791, acc: 0.96, recall: 0.96, precision: 0.9607371794871795, f_beta: 0.959983993597439
train: step: 4775, loss: 0.011213326826691628, acc: 0.99, recall: 0.99, precision: 0.9901960784313726, f_beta: 0.98999899989999
train: step: 4776, loss: 0.0001994155754800886, acc: 1.0, recall: 1.0, precision: 1.0, f_beta: 1.0
train: step: 4777, loss: 0.01485376339405775, acc: 0.98, recall: 0.98, precision: 0.9807692307692308, f_beta: 0.9799919967987194
train: step: 4778, loss: 0.03537867218255997, acc: 0.95, recall: 0.95, precision: 0.9501800720288115, f_beta: 0.9499949994999499
train: step: 4779, loss: 0.051764827221632004, acc: 0.93, recall: 0.9299999999999999, precision: 0.930172068827531, f_beta: 0.9299929992999301
train: step: 4780, loss: 0.026101795956492424, acc: 0.98, recall: 0.98, precision: 0.9807692307692308, f_beta: 0.9799919967987194
train: step: 4781, loss: 0.01052544079720974, acc: 0.99, recall: 0.99, precision: 0.9901960784313726, f_beta: 0.98999899989999
train: step: 4782, loss: 0.02161262556910515, acc: 0.98, recall: 0.98, precision: 0.9807692307692308, f_beta: 0.9799919967987194
train: step: 4783, loss: 0.029100365936756134, acc: 0.96, recall: 0.96, precision: 0.9607371794871795, f_beta: 0.959983993597439
train: step: 4784, loss: 0.02205815725028515, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 4785, loss: 0.012437181547284126, acc: 0.98, recall: 0.98, precision: 0.9807692307692308, f_beta: 0.9799919967987194
train: step: 4786, loss: 0.03144111484289169, acc: 0.96, recall: 0.96, precision: 0.9607371794871795, f_beta: 0.959983993597439
train: step: 4787, loss: 0.030354924499988556, acc: 0.96, recall: 0.96, precision: 0.96, f_beta: 0.96
train: step: 4788, loss: 0.05568494647741318, acc: 0.94, recall: 0.94, precision: 0.94, f_beta: 0.94
train: step: 4789, loss: 0.026571597903966904, acc: 0.96, recall: 0.96, precision: 0.9607371794871795, f_beta: 0.959983993597439
train: step: 4790, loss: 0.014081722125411034, acc: 0.99, recall: 0.99, precision: 0.9901960784313726, f_beta: 0.98999899989999
train: step: 4791, loss: 0.06274394690990448, acc: 0.93, recall: 0.9299999999999999, precision: 0.930172068827531, f_beta: 0.9299929992999301
train: step: 4792, loss: 0.039136748760938644, acc: 0.96, recall: 0.96, precision: 0.9607371794871795, f_beta: 0.959983993597439
train: step: 4793, loss: 0.01920953392982483, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 4794, loss: 0.03861798718571663, acc: 0.96, recall: 0.96, precision: 0.962962962962963, f_beta: 0.9599358974358974
train: step: 4795, loss: 0.08998581022024155, acc: 0.91, recall: 0.91, precision: 0.9101640656262505, f_beta: 0.90999099909991
train: step: 4796, loss: 0.009686780162155628, acc: 0.99, recall: 0.99, precision: 0.9901960784313726, f_beta: 0.98999899989999
train: step: 4797, loss: 0.011022926308214664, acc: 0.98, recall: 0.98, precision: 0.98, f_beta: 0.98
train: step: 4798, loss: 0.020775428041815758, acc: 0.98, recall: 0.98, precision: 0.98, f_beta: 0.98
train: step: 4799, loss: 0.030049247667193413, acc: 0.96, recall: 0.96, precision: 0.9607371794871795, f_beta: 0.959983993597439
train: step: 4800, loss: 0.05566886067390442, acc: 0.93, recall: 0.9299999999999999, precision: 0.9315535929345644, f_beta: 0.9299369432489241
train: step: 4801, loss: 0.03239918127655983, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 4802, loss: 0.0067637707106769085, acc: 0.99, recall: 0.99, precision: 0.9901960784313726, f_beta: 0.98999899989999
train: step: 4803, loss: 0.0002461379626765847, acc: 1.0, recall: 1.0, precision: 1.0, f_beta: 1.0
train: step: 4804, loss: 0.005912497639656067, acc: 0.99, recall: 0.99, precision: 0.9901960784313726, f_beta: 0.98999899989999
train: step: 4805, loss: 0.0219725351780653, acc: 0.97, recall: 0.97, precision: 0.9716981132075472, f_beta: 0.9699729756781104
train: step: 4806, loss: 0.07760201394557953, acc: 0.92, recall: 0.9199999999999999, precision: 0.9261363636363636, f_beta: 0.9197109594540345
train: step: 4807, loss: 0.03682241961359978, acc: 0.95, recall: 0.95, precision: 0.9516258530710557, f_beta: 0.949954959463517
train: step: 4808, loss: 0.0004187444574199617, acc: 1.0, recall: 1.0, precision: 1.0, f_beta: 1.0
train: step: 4809, loss: 0.0132592236623168, acc: 0.99, recall: 0.99, precision: 0.9901960784313726, f_beta: 0.98999899989999
train: step: 4810, loss: 0.02121087536215782, acc: 0.98, recall: 0.98, precision: 0.9807692307692308, f_beta: 0.9799919967987194
train: step: 4811, loss: 0.03769011050462723, acc: 0.96, recall: 0.96, precision: 0.9607371794871795, f_beta: 0.959983993597439
train: step: 4812, loss: 0.042860355228185654, acc: 0.95, recall: 0.95, precision: 0.9501800720288115, f_beta: 0.9499949994999499
train: step: 4813, loss: 0.029007764533162117, acc: 0.96, recall: 0.96, precision: 0.9607371794871795, f_beta: 0.959983993597439
train: step: 4814, loss: 0.02883998677134514, acc: 0.96, recall: 0.96, precision: 0.96, f_beta: 0.96
train: step: 4815, loss: 0.05692671239376068, acc: 0.94, recall: 0.94, precision: 0.9428341384863124, f_beta: 0.9399038461538463
train: step: 4816, loss: 0.043199628591537476, acc: 0.95, recall: 0.95, precision: 0.9545454545454546, f_beta: 0.949874686716792
train: step: 4817, loss: 0.03512785583734512, acc: 0.96, recall: 0.96, precision: 0.962962962962963, f_beta: 0.9599358974358974
train: step: 4818, loss: 0.03207140043377876, acc: 0.96, recall: 0.96, precision: 0.9607371794871795, f_beta: 0.959983993597439
train: step: 4819, loss: 0.06003044918179512, acc: 0.92, recall: 0.9199999999999999, precision: 0.9206730769230769, f_beta: 0.919967987194878
train: step: 4820, loss: 0.046651944518089294, acc: 0.94, recall: 0.94, precision: 0.9428341384863124, f_beta: 0.9399038461538463
train: step: 4821, loss: 0.03775842487812042, acc: 0.95, recall: 0.95, precision: 0.9516258530710557, f_beta: 0.949954959463517
train: step: 4822, loss: 0.024369029328227043, acc: 0.97, recall: 0.97, precision: 0.9716981132075472, f_beta: 0.9699729756781104
train: step: 4823, loss: 0.03401046618819237, acc: 0.96, recall: 0.96, precision: 0.962962962962963, f_beta: 0.9599358974358974
train: step: 4824, loss: 0.02849752828478813, acc: 0.97, recall: 0.97, precision: 0.9716981132075472, f_beta: 0.9699729756781104
train: step: 4825, loss: 0.005664963275194168, acc: 1.0, recall: 1.0, precision: 1.0, f_beta: 1.0
train: step: 4826, loss: 0.00021212002320680767, acc: 1.0, recall: 1.0, precision: 1.0, f_beta: 1.0
train: step: 4827, loss: 0.047872208058834076, acc: 0.94, recall: 0.94, precision: 0.9407051282051282, f_beta: 0.9399759903961584
train: step: 4828, loss: 0.036842986941337585, acc: 0.96, recall: 0.96, precision: 0.962962962962963, f_beta: 0.9599358974358974
train: step: 4829, loss: 0.05346256121993065, acc: 0.93, recall: 0.9299999999999999, precision: 0.9343434343434343, f_beta: 0.9298245614035088
train: step: 4830, loss: 0.020748315379023552, acc: 0.98, recall: 0.98, precision: 0.98, f_beta: 0.98
train: step: 4831, loss: 0.01794782653450966, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 4832, loss: 0.03668580204248428, acc: 0.96, recall: 0.96, precision: 0.9607371794871795, f_beta: 0.959983993597439
train: step: 4833, loss: 0.031148605048656464, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 4834, loss: 0.06719576567411423, acc: 0.92, recall: 0.9199999999999999, precision: 0.9206730769230769, f_beta: 0.919967987194878
train: step: 4835, loss: 0.016602523624897003, acc: 0.97, recall: 0.97, precision: 0.9716981132075472, f_beta: 0.9699729756781104
train: step: 4836, loss: 0.022597838193178177, acc: 0.97, recall: 0.97, precision: 0.9716981132075472, f_beta: 0.9699729756781104
train: step: 4837, loss: 0.03308044373989105, acc: 0.97, recall: 0.97, precision: 0.9716981132075472, f_beta: 0.9699729756781104
train: step: 4838, loss: 0.018081020563840866, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 4839, loss: 0.04271872341632843, acc: 0.94, recall: 0.94, precision: 0.9428341384863124, f_beta: 0.9399038461538463
train: step: 4840, loss: 0.044878195971250534, acc: 0.94, recall: 0.94, precision: 0.94, f_beta: 0.94
train: step: 4841, loss: 0.05857520177960396, acc: 0.93, recall: 0.9299999999999999, precision: 0.930172068827531, f_beta: 0.9299929992999301
train: step: 4842, loss: 0.03598809987306595, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 4843, loss: 0.028160717338323593, acc: 0.96, recall: 0.96, precision: 0.962962962962963, f_beta: 0.9599358974358974
train: step: 4844, loss: 0.01672888547182083, acc: 0.99, recall: 0.99, precision: 0.9901960784313726, f_beta: 0.98999899989999
train: step: 4845, loss: 0.009152830578386784, acc: 0.99, recall: 0.99, precision: 0.9901960784313726, f_beta: 0.98999899989999
train: step: 4846, loss: 0.04187776520848274, acc: 0.96, recall: 0.96, precision: 0.9607371794871795, f_beta: 0.959983993597439
train: step: 4847, loss: 0.026312341913580894, acc: 0.97, recall: 0.97, precision: 0.9716981132075472, f_beta: 0.9699729756781104
train: step: 4848, loss: 0.024982742965221405, acc: 0.96, recall: 0.96, precision: 0.96, f_beta: 0.96
train: step: 4849, loss: 0.03697507455945015, acc: 0.94, recall: 0.94, precision: 0.9407051282051282, f_beta: 0.9399759903961584
train: step: 4850, loss: 0.00952799990773201, acc: 0.98, recall: 0.98, precision: 0.9807692307692308, f_beta: 0.9799919967987194
train: step: 4851, loss: 0.002128807594999671, acc: 1.0, recall: 1.0, precision: 1.0, f_beta: 1.0
train: step: 4852, loss: 0.009810165502130985, acc: 0.99, recall: 0.99, precision: 0.9901960784313726, f_beta: 0.98999899989999
train: step: 4853, loss: 0.06203700229525566, acc: 0.94, recall: 0.94, precision: 0.94, f_beta: 0.94
train: step: 4854, loss: 0.02428254671394825, acc: 0.96, recall: 0.96, precision: 0.9607371794871795, f_beta: 0.959983993597439
train: step: 4855, loss: 0.029979191720485687, acc: 0.96, recall: 0.96, precision: 0.96, f_beta: 0.96
train: step: 4856, loss: 0.04876969754695892, acc: 0.95, recall: 0.95, precision: 0.9516258530710557, f_beta: 0.949954959463517
train: step: 4857, loss: 0.03482012823224068, acc: 0.96, recall: 0.96, precision: 0.962962962962963, f_beta: 0.9599358974358974
train: step: 4858, loss: 0.016801489517092705, acc: 0.99, recall: 0.99, precision: 0.9901960784313726, f_beta: 0.98999899989999
train: step: 4859, loss: 0.06528624892234802, acc: 0.91, recall: 0.9099999999999999, precision: 0.9114813327980731, f_beta: 0.9099189270343309
train: step: 4860, loss: 0.001510815811343491, acc: 1.0, recall: 1.0, precision: 1.0, f_beta: 1.0
train: step: 4861, loss: 0.019655052572488785, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 4862, loss: 0.003889536950737238, acc: 1.0, recall: 1.0, precision: 1.0, f_beta: 1.0
train: step: 4863, loss: 0.023607194423675537, acc: 0.98, recall: 0.98, precision: 0.9807692307692308, f_beta: 0.9799919967987194
train: step: 4864, loss: 0.03961896523833275, acc: 0.96, recall: 0.96, precision: 0.96, f_beta: 0.96
train: step: 4865, loss: 0.016919219866394997, acc: 0.98, recall: 0.98, precision: 0.98, f_beta: 0.98
train: step: 4866, loss: 0.010247718542814255, acc: 0.99, recall: 0.99, precision: 0.9901960784313726, f_beta: 0.98999899989999
train: step: 4867, loss: 0.04769854247570038, acc: 0.94, recall: 0.94, precision: 0.9407051282051282, f_beta: 0.9399759903961584
train: step: 4868, loss: 0.01775539480149746, acc: 0.98, recall: 0.98, precision: 0.9807692307692308, f_beta: 0.9799919967987194
train: step: 4869, loss: 0.029696207493543625, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 4870, loss: 0.011655449867248535, acc: 0.99, recall: 0.99, precision: 0.9901960784313726, f_beta: 0.98999899989999
train: step: 4871, loss: 0.024125583469867706, acc: 0.96, recall: 0.96, precision: 0.962962962962963, f_beta: 0.9599358974358974
train: step: 4872, loss: 0.009235676378011703, acc: 0.99, recall: 0.99, precision: 0.9901960784313726, f_beta: 0.98999899989999
train: step: 4873, loss: 0.010275889188051224, acc: 0.98, recall: 0.98, precision: 0.9807692307692308, f_beta: 0.9799919967987194
train: step: 4874, loss: 0.01961563527584076, acc: 0.98, recall: 0.98, precision: 0.98, f_beta: 0.98
train: step: 4875, loss: 0.004223417025059462, acc: 1.0, recall: 1.0, precision: 1.0, f_beta: 1.0
train: step: 4876, loss: 0.04986109584569931, acc: 0.93, recall: 0.9299999999999999, precision: 0.9343434343434343, f_beta: 0.9298245614035088
train: step: 4877, loss: 0.026733295992016792, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 4878, loss: 0.01911570131778717, acc: 0.97, recall: 0.97, precision: 0.9716981132075472, f_beta: 0.9699729756781104
train: step: 4879, loss: 0.028480054810643196, acc: 0.96, recall: 0.96, precision: 0.9607371794871795, f_beta: 0.959983993597439
train: step: 4880, loss: 0.04911305382847786, acc: 0.94, recall: 0.94, precision: 0.94, f_beta: 0.94
train: step: 4881, loss: 0.0239780992269516, acc: 0.96, recall: 0.96, precision: 0.9607371794871795, f_beta: 0.959983993597439
train: step: 4882, loss: 0.004000685643404722, acc: 0.99, recall: 0.99, precision: 0.9901960784313726, f_beta: 0.98999899989999
train: step: 4883, loss: 0.013334984891116619, acc: 0.99, recall: 0.99, precision: 0.9901960784313726, f_beta: 0.98999899989999
train: step: 4884, loss: 0.055530987679958344, acc: 0.92, recall: 0.9199999999999999, precision: 0.9227053140096618, f_beta: 0.9198717948717948
train: step: 4885, loss: 0.013502409681677818, acc: 0.99, recall: 0.99, precision: 0.9901960784313726, f_beta: 0.98999899989999
train: step: 4886, loss: 0.0277436301112175, acc: 0.96, recall: 0.96, precision: 0.9607371794871795, f_beta: 0.959983993597439
train: step: 4887, loss: 0.017420504242181778, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 4888, loss: 0.004096943885087967, acc: 0.99, recall: 0.99, precision: 0.9901960784313726, f_beta: 0.98999899989999
train: step: 4889, loss: 0.0022583655081689358, acc: 1.0, recall: 1.0, precision: 1.0, f_beta: 1.0
train: step: 4890, loss: 0.024543117731809616, acc: 0.96, recall: 0.96, precision: 0.9607371794871795, f_beta: 0.959983993597439
train: step: 4891, loss: 0.027371695265173912, acc: 0.96, recall: 0.96, precision: 0.962962962962963, f_beta: 0.9599358974358974
train: step: 4892, loss: 0.017492661252617836, acc: 0.96, recall: 0.96, precision: 0.9607371794871795, f_beta: 0.959983993597439
train: step: 4893, loss: 0.05034724250435829, acc: 0.95, recall: 0.95, precision: 0.9545454545454546, f_beta: 0.949874686716792
train: step: 4894, loss: 0.045291412621736526, acc: 0.94, recall: 0.94, precision: 0.9428341384863124, f_beta: 0.9399038461538463
train: step: 4895, loss: 0.05195571854710579, acc: 0.94, recall: 0.94, precision: 0.9428341384863124, f_beta: 0.9399038461538463
train: step: 4896, loss: 0.0257693100720644, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 4897, loss: 0.048372235149145126, acc: 0.94, recall: 0.94, precision: 0.9407051282051282, f_beta: 0.9399759903961584
train: step: 4898, loss: 0.03099115937948227, acc: 0.94, recall: 0.94, precision: 0.94, f_beta: 0.94
train: step: 4899, loss: 0.006576042156666517, acc: 0.99, recall: 0.99, precision: 0.9901960784313726, f_beta: 0.98999899989999
train: step: 4900, loss: 0.016553232446312904, acc: 0.98, recall: 0.98, precision: 0.98, f_beta: 0.98
train: step: 4901, loss: 0.011692493222653866, acc: 0.99, recall: 0.99, precision: 0.9901960784313726, f_beta: 0.98999899989999
train: step: 4902, loss: 0.010328453034162521, acc: 0.98, recall: 0.98, precision: 0.9807692307692308, f_beta: 0.9799919967987194
train: step: 4903, loss: 0.00426129437983036, acc: 0.99, recall: 0.99, precision: 0.9901960784313726, f_beta: 0.98999899989999
train: step: 4904, loss: 0.012171699665486813, acc: 0.99, recall: 0.99, precision: 0.9901960784313726, f_beta: 0.98999899989999
train: step: 4905, loss: 0.03003622591495514, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 4906, loss: 0.06154826283454895, acc: 0.93, recall: 0.9299999999999999, precision: 0.9315535929345644, f_beta: 0.9299369432489241
train: step: 4907, loss: 0.016703829169273376, acc: 0.98, recall: 0.98, precision: 0.98, f_beta: 0.98
train: step: 4908, loss: 0.02716972678899765, acc: 0.96, recall: 0.96, precision: 0.9607371794871795, f_beta: 0.959983993597439
train: step: 4909, loss: 0.015267555601894855, acc: 0.98, recall: 0.98, precision: 0.9807692307692308, f_beta: 0.9799919967987194
train: step: 4910, loss: 0.03768624737858772, acc: 0.96, recall: 0.96, precision: 0.9607371794871795, f_beta: 0.959983993597439
train: step: 4911, loss: 0.0863666757941246, acc: 0.89, recall: 0.89, precision: 0.8914090726615818, f_beta: 0.8899009108197378
train: step: 4912, loss: 0.0382690355181694, acc: 0.95, recall: 0.95, precision: 0.9501800720288115, f_beta: 0.9499949994999499
train: step: 4913, loss: 0.03246656060218811, acc: 0.95, recall: 0.95, precision: 0.9545454545454546, f_beta: 0.949874686716792
train: step: 4914, loss: 0.02385132759809494, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 4915, loss: 0.058771539479494095, acc: 0.93, recall: 0.9299999999999999, precision: 0.930172068827531, f_beta: 0.9299929992999301
train: step: 4916, loss: 0.013560734689235687, acc: 0.98, recall: 0.98, precision: 0.9807692307692308, f_beta: 0.9799919967987194
train: step: 4917, loss: 0.04005299508571625, acc: 0.94, recall: 0.94, precision: 0.9407051282051282, f_beta: 0.9399759903961584
train: step: 4918, loss: 0.020363593474030495, acc: 0.98, recall: 0.98, precision: 0.9807692307692308, f_beta: 0.9799919967987194
train: step: 4919, loss: 0.03326425701379776, acc: 0.96, recall: 0.96, precision: 0.9607371794871795, f_beta: 0.959983993597439
train: step: 4920, loss: 0.009337374940514565, acc: 0.99, recall: 0.99, precision: 0.9901960784313726, f_beta: 0.98999899989999
train: step: 4921, loss: 0.03082510270178318, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 4922, loss: 0.014616096392273903, acc: 0.98, recall: 0.98, precision: 0.98, f_beta: 0.98
train: step: 4923, loss: 0.026808440685272217, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 4924, loss: 0.00870589166879654, acc: 0.99, recall: 0.99, precision: 0.9901960784313726, f_beta: 0.98999899989999
train: step: 4925, loss: 0.030984744429588318, acc: 0.94, recall: 0.94, precision: 0.9407051282051282, f_beta: 0.9399759903961584
train: step: 4926, loss: 0.011972400359809399, acc: 0.98, recall: 0.98, precision: 0.98, f_beta: 0.98
train: step: 4927, loss: 0.03440745547413826, acc: 0.96, recall: 0.96, precision: 0.96, f_beta: 0.96
train: step: 4928, loss: 0.01622256450355053, acc: 0.98, recall: 0.98, precision: 0.9807692307692308, f_beta: 0.9799919967987194
train: step: 4929, loss: 0.0383869893848896, acc: 0.95, recall: 0.95, precision: 0.9516258530710557, f_beta: 0.949954959463517
train: step: 4930, loss: 0.03603114187717438, acc: 0.97, recall: 0.97, precision: 0.9716981132075472, f_beta: 0.9699729756781104
train: step: 4931, loss: 0.023772526532411575, acc: 0.97, recall: 0.97, precision: 0.9716981132075472, f_beta: 0.9699729756781104
train: step: 4932, loss: 0.02872578799724579, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 4933, loss: 0.03665971755981445, acc: 0.95, recall: 0.95, precision: 0.9516258530710557, f_beta: 0.949954959463517
train: step: 4934, loss: 0.015508213080465794, acc: 0.99, recall: 0.99, precision: 0.9901960784313726, f_beta: 0.98999899989999
train: step: 4935, loss: 0.02292133867740631, acc: 0.98, recall: 0.98, precision: 0.98, f_beta: 0.98
train: step: 4936, loss: 0.030917424708604813, acc: 0.96, recall: 0.96, precision: 0.9607371794871795, f_beta: 0.959983993597439
train: step: 4937, loss: 0.04586507007479668, acc: 0.95, recall: 0.95, precision: 0.9501800720288115, f_beta: 0.9499949994999499
train: step: 4938, loss: 0.011255220510065556, acc: 0.99, recall: 0.99, precision: 0.9901960784313726, f_beta: 0.98999899989999
train: step: 4939, loss: 0.012491622008383274, acc: 0.99, recall: 0.99, precision: 0.9901960784313726, f_beta: 0.98999899989999
train: step: 4940, loss: 0.05068855732679367, acc: 0.95, recall: 0.95, precision: 0.9516258530710557, f_beta: 0.949954959463517
train: step: 4941, loss: 0.017962727695703506, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 4942, loss: 0.006485658697783947, acc: 0.99, recall: 0.99, precision: 0.9901960784313726, f_beta: 0.98999899989999
train: step: 4943, loss: 0.021626848727464676, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 4944, loss: 0.0356891043484211, acc: 0.94, recall: 0.94, precision: 0.9428341384863124, f_beta: 0.9399038461538463
train: step: 4945, loss: 0.016917649656534195, acc: 0.98, recall: 0.98, precision: 0.9807692307692308, f_beta: 0.9799919967987194
train: step: 4946, loss: 0.010360832326114178, acc: 0.98, recall: 0.98, precision: 0.98, f_beta: 0.98
train: step: 4947, loss: 0.024740509688854218, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 4948, loss: 0.011925309896469116, acc: 0.98, recall: 0.98, precision: 0.98, f_beta: 0.98
train: step: 4949, loss: 0.024251962080597878, acc: 0.97, recall: 0.97, precision: 0.9716981132075472, f_beta: 0.9699729756781104
train: step: 4950, loss: 0.03151033818721771, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 4951, loss: 0.012901000678539276, acc: 0.99, recall: 0.99, precision: 0.9901960784313726, f_beta: 0.98999899989999
train: step: 4952, loss: 0.026749221608042717, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 4953, loss: 0.0601046197116375, acc: 0.94, recall: 0.94, precision: 0.94, f_beta: 0.94
train: step: 4954, loss: 0.026225075125694275, acc: 0.96, recall: 0.96, precision: 0.9607371794871795, f_beta: 0.959983993597439
train: step: 4955, loss: 0.023702621459960938, acc: 0.96, recall: 0.96, precision: 0.9607371794871795, f_beta: 0.959983993597439
train: step: 4956, loss: 0.022838521748781204, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 4957, loss: 0.02113884687423706, acc: 0.98, recall: 0.98, precision: 0.98, f_beta: 0.98
train: step: 4958, loss: 0.04485301673412323, acc: 0.96, recall: 0.96, precision: 0.9607371794871795, f_beta: 0.959983993597439
train: step: 4959, loss: 0.007817353121936321, acc: 0.99, recall: 0.99, precision: 0.9901960784313726, f_beta: 0.98999899989999
train: step: 4960, loss: 0.03767373040318489, acc: 0.96, recall: 0.96, precision: 0.962962962962963, f_beta: 0.9599358974358974
train: step: 4961, loss: 0.0359685905277729, acc: 0.96, recall: 0.96, precision: 0.9607371794871795, f_beta: 0.959983993597439
train: step: 4962, loss: 0.04627081751823425, acc: 0.94, recall: 0.94, precision: 0.94, f_beta: 0.94
train: step: 4963, loss: 0.0478326790034771, acc: 0.95, recall: 0.95, precision: 0.9501800720288115, f_beta: 0.9499949994999499
train: step: 4964, loss: 0.010627679526805878, acc: 0.98, recall: 0.98, precision: 0.98, f_beta: 0.98
train: step: 4965, loss: 0.04412369802594185, acc: 0.93, recall: 0.9299999999999999, precision: 0.9343434343434343, f_beta: 0.9298245614035088
train: step: 4966, loss: 0.019613854587078094, acc: 0.98, recall: 0.98, precision: 0.9807692307692308, f_beta: 0.9799919967987194
train: step: 4967, loss: 0.06206991523504257, acc: 0.91, recall: 0.91, precision: 0.9101640656262505, f_beta: 0.90999099909991
train: step: 4968, loss: 0.00794007908552885, acc: 0.99, recall: 0.99, precision: 0.9901960784313726, f_beta: 0.98999899989999
train: step: 4969, loss: 0.03905629739165306, acc: 0.97, recall: 0.97, precision: 0.9716981132075472, f_beta: 0.9699729756781104
train: step: 4970, loss: 0.011870053596794605, acc: 0.99, recall: 0.99, precision: 0.9901960784313726, f_beta: 0.98999899989999
train: step: 4971, loss: 0.02089039236307144, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 4972, loss: 0.03856894001364708, acc: 0.95, recall: 0.95, precision: 0.9545454545454546, f_beta: 0.949874686716792
train: step: 4973, loss: 0.025252042338252068, acc: 0.98, recall: 0.98, precision: 0.9807692307692308, f_beta: 0.9799919967987194
train: step: 4974, loss: 0.027002710849046707, acc: 0.97, recall: 0.97, precision: 0.9716981132075472, f_beta: 0.9699729756781104
train: step: 4975, loss: 0.03696603327989578, acc: 0.96, recall: 0.96, precision: 0.9607371794871795, f_beta: 0.959983993597439
train: step: 4976, loss: 0.03733868896961212, acc: 0.95, recall: 0.95, precision: 0.9501800720288115, f_beta: 0.9499949994999499
train: step: 4977, loss: 0.025210125371813774, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 4978, loss: 0.024968480691313744, acc: 0.96, recall: 0.96, precision: 0.9607371794871795, f_beta: 0.959983993597439
train: step: 4979, loss: 0.027401309460401535, acc: 0.96, recall: 0.96, precision: 0.96, f_beta: 0.96
train: step: 4980, loss: 0.017314236611127853, acc: 0.98, recall: 0.98, precision: 0.98, f_beta: 0.98
train: step: 4981, loss: 0.006264273077249527, acc: 0.99, recall: 0.99, precision: 0.9901960784313726, f_beta: 0.98999899989999
train: step: 4982, loss: 0.06452661752700806, acc: 0.92, recall: 0.9199999999999999, precision: 0.9227053140096618, f_beta: 0.9198717948717948
train: step: 4983, loss: 0.0034808272030204535, acc: 1.0, recall: 1.0, precision: 1.0, f_beta: 1.0
train: step: 4984, loss: 0.031021341681480408, acc: 0.96, recall: 0.96, precision: 0.96, f_beta: 0.96
train: step: 4985, loss: 0.03888249769806862, acc: 0.96, recall: 0.96, precision: 0.96, f_beta: 0.96
train: step: 4986, loss: 0.019338997080922127, acc: 0.98, recall: 0.98, precision: 0.9807692307692308, f_beta: 0.9799919967987194
train: step: 4987, loss: 0.03055242821574211, acc: 0.95, recall: 0.95, precision: 0.9516258530710557, f_beta: 0.949954959463517
train: step: 4988, loss: 0.020909862592816353, acc: 0.98, recall: 0.98, precision: 0.98, f_beta: 0.98
train: step: 4989, loss: 0.023043513298034668, acc: 0.98, recall: 0.98, precision: 0.98, f_beta: 0.98
train: step: 4990, loss: 0.018021738156676292, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 4991, loss: 0.022270094603300095, acc: 0.97, recall: 0.97, precision: 0.9716981132075472, f_beta: 0.9699729756781104
train: step: 4992, loss: 0.029274707660079002, acc: 0.95, recall: 0.95, precision: 0.9501800720288115, f_beta: 0.9499949994999499
train: step: 4993, loss: 0.05516265705227852, acc: 0.94, recall: 0.94, precision: 0.9464285714285714, f_beta: 0.939783219590526
train: step: 4994, loss: 0.026902131736278534, acc: 0.96, recall: 0.96, precision: 0.96, f_beta: 0.96
train: step: 4995, loss: 0.011734341271221638, acc: 0.98, recall: 0.98, precision: 0.9807692307692308, f_beta: 0.9799919967987194
train: step: 4996, loss: 0.04428381472826004, acc: 0.93, recall: 0.9299999999999999, precision: 0.9315535929345644, f_beta: 0.9299369432489241
train: step: 4997, loss: 0.03981710970401764, acc: 0.96, recall: 0.96, precision: 0.9607371794871795, f_beta: 0.959983993597439
train: step: 4998, loss: 0.024222686886787415, acc: 0.98, recall: 0.98, precision: 0.98, f_beta: 0.98
train: step: 4999, loss: 0.010211505927145481, acc: 0.99, recall: 0.99, precision: 0.9901960784313726, f_beta: 0.98999899989999
train: step: 5000, loss: 0.04418328404426575, acc: 0.94, recall: 0.94, precision: 0.9407051282051282, f_beta: 0.9399759903961584
train: step: 5001, loss: 0.003345702076330781, acc: 1.0, recall: 1.0, precision: 1.0, f_beta: 1.0
train: step: 5002, loss: 0.03234962001442909, acc: 0.97, recall: 0.97, precision: 0.9716981132075472, f_beta: 0.9699729756781104
train: step: 5003, loss: 0.020264456048607826, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 5004, loss: 0.036402929574251175, acc: 0.96, recall: 0.96, precision: 0.9607371794871795, f_beta: 0.959983993597439
train: step: 5005, loss: 0.05504002422094345, acc: 0.93, recall: 0.9299999999999999, precision: 0.9315535929345644, f_beta: 0.9299369432489241
train: step: 5006, loss: 0.019794993102550507, acc: 0.98, recall: 0.98, precision: 0.9807692307692308, f_beta: 0.9799919967987194
train: step: 5007, loss: 0.025381121784448624, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 5008, loss: 0.03614151105284691, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 5009, loss: 0.023714646697044373, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 5010, loss: 0.028765760362148285, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 5011, loss: 0.022296065464615822, acc: 0.98, recall: 0.98, precision: 0.9807692307692308, f_beta: 0.9799919967987194
train: step: 5012, loss: 0.001424675341695547, acc: 1.0, recall: 1.0, precision: 1.0, f_beta: 1.0
train: step: 5013, loss: 0.011033752001821995, acc: 0.99, recall: 0.99, precision: 0.9901960784313726, f_beta: 0.98999899989999
train: step: 5014, loss: 0.007297055795788765, acc: 0.99, recall: 0.99, precision: 0.9901960784313726, f_beta: 0.98999899989999
train: step: 5015, loss: 0.048979587852954865, acc: 0.95, recall: 0.95, precision: 0.9516258530710557, f_beta: 0.949954959463517
train: step: 5016, loss: 0.042761191725730896, acc: 0.96, recall: 0.96, precision: 0.9607371794871795, f_beta: 0.959983993597439
train: step: 5017, loss: 0.040454406291246414, acc: 0.95, recall: 0.95, precision: 0.9501800720288115, f_beta: 0.9499949994999499
train: step: 5018, loss: 0.012847722508013248, acc: 0.99, recall: 0.99, precision: 0.9901960784313726, f_beta: 0.98999899989999
train: step: 5019, loss: 0.02174992486834526, acc: 0.98, recall: 0.98, precision: 0.9807692307692308, f_beta: 0.9799919967987194
train: step: 5020, loss: 0.03766724467277527, acc: 0.95, recall: 0.95, precision: 0.9516258530710557, f_beta: 0.949954959463517
train: step: 5021, loss: 0.023686202242970467, acc: 0.98, recall: 0.98, precision: 0.9807692307692308, f_beta: 0.9799919967987194
train: step: 5022, loss: 0.03701871261000633, acc: 0.95, recall: 0.95, precision: 0.9516258530710557, f_beta: 0.949954959463517
train: step: 5023, loss: 0.04435193911194801, acc: 0.94, recall: 0.94, precision: 0.9407051282051282, f_beta: 0.9399759903961584
train: step: 5024, loss: 0.0347452387213707, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 5025, loss: 0.035363152623176575, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 5026, loss: 0.03712897002696991, acc: 0.96, recall: 0.96, precision: 0.962962962962963, f_beta: 0.9599358974358974
train: step: 5027, loss: 0.008820246905088425, acc: 0.99, recall: 0.99, precision: 0.9901960784313726, f_beta: 0.98999899989999
train: step: 5028, loss: 0.009781165979802608, acc: 0.99, recall: 0.99, precision: 0.9901960784313726, f_beta: 0.98999899989999
train: step: 5029, loss: 0.020384125411510468, acc: 0.98, recall: 0.98, precision: 0.98, f_beta: 0.98
train: step: 5030, loss: 0.008829046040773392, acc: 0.99, recall: 0.99, precision: 0.9901960784313726, f_beta: 0.98999899989999
train: step: 5031, loss: 0.03482685983181, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 5032, loss: 0.053629759699106216, acc: 0.93, recall: 0.9299999999999999, precision: 0.9315535929345644, f_beta: 0.9299369432489241
train: step: 5033, loss: 0.01110498420894146, acc: 0.99, recall: 0.99, precision: 0.9901960784313726, f_beta: 0.98999899989999
train: step: 5034, loss: 0.025443976745009422, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 5035, loss: 0.010017765685915947, acc: 0.99, recall: 0.99, precision: 0.9901960784313726, f_beta: 0.98999899989999
train: step: 5036, loss: 0.022850623354315758, acc: 0.98, recall: 0.98, precision: 0.98, f_beta: 0.98
train: step: 5037, loss: 0.011716613546013832, acc: 0.98, recall: 0.98, precision: 0.9807692307692308, f_beta: 0.9799919967987194
train: step: 5038, loss: 0.037223413586616516, acc: 0.96, recall: 0.96, precision: 0.96, f_beta: 0.96
train: step: 5039, loss: 0.007814389653503895, acc: 0.99, recall: 0.99, precision: 0.9901960784313726, f_beta: 0.98999899989999
train: step: 5040, loss: 0.0549921840429306, acc: 0.92, recall: 0.9199999999999999, precision: 0.9227053140096618, f_beta: 0.9198717948717948
train: step: 5041, loss: 0.03500868007540703, acc: 0.96, recall: 0.96, precision: 0.9607371794871795, f_beta: 0.959983993597439
train: step: 5042, loss: 0.03526750206947327, acc: 0.97, recall: 0.97, precision: 0.9716981132075472, f_beta: 0.9699729756781104
train: step: 5043, loss: 0.012690122239291668, acc: 0.99, recall: 0.99, precision: 0.9901960784313726, f_beta: 0.98999899989999
train: step: 5044, loss: 0.04188213497400284, acc: 0.96, recall: 0.96, precision: 0.9607371794871795, f_beta: 0.959983993597439
train: step: 5045, loss: 0.013630094937980175, acc: 0.99, recall: 0.99, precision: 0.9901960784313726, f_beta: 0.98999899989999
train: step: 5046, loss: 0.03854828700423241, acc: 0.96, recall: 0.96, precision: 0.962962962962963, f_beta: 0.9599358974358974
train: step: 5047, loss: 0.01114265713840723, acc: 0.99, recall: 0.99, precision: 0.9901960784313726, f_beta: 0.98999899989999
train: step: 5048, loss: 0.03683669865131378, acc: 0.95, recall: 0.95, precision: 0.9545454545454546, f_beta: 0.949874686716792
train: step: 5049, loss: 0.007595356088131666, acc: 0.99, recall: 0.99, precision: 0.9901960784313726, f_beta: 0.98999899989999
train: step: 5050, loss: 0.0699850544333458, acc: 0.92, recall: 0.9199999999999999, precision: 0.9206730769230769, f_beta: 0.919967987194878
train: step: 5051, loss: 0.01690235175192356, acc: 0.98, recall: 0.98, precision: 0.98, f_beta: 0.98
train: step: 5052, loss: 0.02126910164952278, acc: 0.96, recall: 0.96, precision: 0.96, f_beta: 0.96
train: step: 5053, loss: 0.001404043403454125, acc: 1.0, recall: 1.0, precision: 1.0, f_beta: 1.0
train: step: 5054, loss: 0.02329517900943756, acc: 0.97, recall: 0.97, precision: 0.9716981132075472, f_beta: 0.9699729756781104
train: step: 5055, loss: 0.006565813906490803, acc: 0.99, recall: 0.99, precision: 0.9901960784313726, f_beta: 0.98999899989999
train: step: 5056, loss: 0.014114818535745144, acc: 0.98, recall: 0.98, precision: 0.98, f_beta: 0.98
train: step: 5057, loss: 0.006537273991852999, acc: 0.99, recall: 0.99, precision: 0.9901960784313726, f_beta: 0.98999899989999
train: step: 5058, loss: 0.027653880417346954, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 5059, loss: 0.03696384280920029, acc: 0.95, recall: 0.95, precision: 0.9501800720288115, f_beta: 0.9499949994999499
train: step: 5060, loss: 0.0012588384561240673, acc: 1.0, recall: 1.0, precision: 1.0, f_beta: 1.0
train: step: 5061, loss: 0.009121591225266457, acc: 0.99, recall: 0.99, precision: 0.9901960784313726, f_beta: 0.98999899989999
train: step: 5062, loss: 0.011375555768609047, acc: 0.99, recall: 0.99, precision: 0.9901960784313726, f_beta: 0.98999899989999
train: step: 5063, loss: 0.01742449775338173, acc: 0.99, recall: 0.99, precision: 0.9901960784313726, f_beta: 0.98999899989999
train: step: 5064, loss: 0.056783027946949005, acc: 0.94, recall: 0.94, precision: 0.9407051282051282, f_beta: 0.9399759903961584
train: step: 5065, loss: 0.01267437543720007, acc: 0.99, recall: 0.99, precision: 0.9901960784313726, f_beta: 0.98999899989999
train: step: 5066, loss: 0.011648640967905521, acc: 0.99, recall: 0.99, precision: 0.9901960784313726, f_beta: 0.98999899989999
train: step: 5067, loss: 0.011969182640314102, acc: 0.99, recall: 0.99, precision: 0.9901960784313726, f_beta: 0.98999899989999
train: step: 5068, loss: 0.0008938865503296256, acc: 1.0, recall: 1.0, precision: 1.0, f_beta: 1.0
train: step: 5069, loss: 0.01431923359632492, acc: 0.98, recall: 0.98, precision: 0.98, f_beta: 0.98
train: step: 5070, loss: 0.03396204859018326, acc: 0.97, recall: 0.97, precision: 0.9716981132075472, f_beta: 0.9699729756781104
train: step: 5071, loss: 0.0620112381875515, acc: 0.93, recall: 0.9299999999999999, precision: 0.930172068827531, f_beta: 0.9299929992999301
train: step: 5072, loss: 0.057239044457674026, acc: 0.92, recall: 0.9199999999999999, precision: 0.9227053140096618, f_beta: 0.9198717948717948
train: step: 5073, loss: 0.035946980118751526, acc: 0.96, recall: 0.96, precision: 0.96, f_beta: 0.96
train: step: 5074, loss: 0.03520813584327698, acc: 0.96, recall: 0.96, precision: 0.96, f_beta: 0.96
train: step: 5075, loss: 0.05628373101353645, acc: 0.95, recall: 0.95, precision: 0.9516258530710557, f_beta: 0.949954959463517
train: step: 5076, loss: 0.11478570848703384, acc: 0.87, recall: 0.87, precision: 0.8888188314417822, f_beta: 0.868407733576273
train: step: 5077, loss: 0.024851014837622643, acc: 0.96, recall: 0.96, precision: 0.962962962962963, f_beta: 0.9599358974358974
train: step: 5078, loss: 0.07712596654891968, acc: 0.93, recall: 0.9299999999999999, precision: 0.9385964912280702, f_beta: 0.9296553110240177
train: step: 5079, loss: 0.0951397493481636, acc: 0.89, recall: 0.8899999999999999, precision: 0.8977968176254589, f_beta: 0.8894583458948848
train: step: 5080, loss: 0.025768589228391647, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 5081, loss: 0.05154895782470703, acc: 0.94, recall: 0.94, precision: 0.94, f_beta: 0.94
train: step: 5082, loss: 0.03336268290877342, acc: 0.96, recall: 0.96, precision: 0.962962962962963, f_beta: 0.9599358974358974
train: step: 5083, loss: 0.020849552005529404, acc: 0.98, recall: 0.98, precision: 0.9807692307692308, f_beta: 0.9799919967987194
train: step: 5084, loss: 0.05953245609998703, acc: 0.93, recall: 0.9299999999999999, precision: 0.9343434343434343, f_beta: 0.9298245614035088
train: step: 5085, loss: 0.020863177254796028, acc: 0.97, recall: 0.97, precision: 0.9716981132075472, f_beta: 0.9699729756781104
train: step: 5086, loss: 0.043604716658592224, acc: 0.95, recall: 0.95, precision: 0.9501800720288115, f_beta: 0.9499949994999499
train: step: 5087, loss: 0.016853880137205124, acc: 0.99, recall: 0.99, precision: 0.9901960784313726, f_beta: 0.98999899989999
train: step: 5088, loss: 0.044449079781770706, acc: 0.97, recall: 0.97, precision: 0.9716981132075472, f_beta: 0.9699729756781104
train: step: 5089, loss: 0.04114885255694389, acc: 0.95, recall: 0.95, precision: 0.9545454545454546, f_beta: 0.949874686716792
train: step: 5090, loss: 0.008348440751433372, acc: 0.99, recall: 0.99, precision: 0.9901960784313726, f_beta: 0.98999899989999
train: step: 5091, loss: 0.022581107914447784, acc: 0.98, recall: 0.98, precision: 0.9807692307692308, f_beta: 0.9799919967987194
train: step: 5092, loss: 0.038299646228551865, acc: 0.94, recall: 0.94, precision: 0.9407051282051282, f_beta: 0.9399759903961584
train: step: 5093, loss: 0.05018370598554611, acc: 0.94, recall: 0.94, precision: 0.9464285714285714, f_beta: 0.939783219590526
train: step: 5094, loss: 0.0005168665084056556, acc: 1.0, recall: 1.0, precision: 1.0, f_beta: 1.0
train: step: 5095, loss: 0.016046226024627686, acc: 0.98, recall: 0.98, precision: 0.98, f_beta: 0.98
train: step: 5096, loss: 0.015077512711286545, acc: 0.98, recall: 0.98, precision: 0.98, f_beta: 0.98
train: step: 5097, loss: 0.03832387551665306, acc: 0.95, recall: 0.95, precision: 0.9516258530710557, f_beta: 0.949954959463517
train: step: 5098, loss: 0.02141067571938038, acc: 0.98, recall: 0.98, precision: 0.9807692307692308, f_beta: 0.9799919967987194
train: step: 5099, loss: 0.06898210197687149, acc: 0.92, recall: 0.9199999999999999, precision: 0.9206730769230769, f_beta: 0.919967987194878
train: step: 5100, loss: 0.011307265609502792, acc: 0.99, recall: 0.99, precision: 0.9901960784313726, f_beta: 0.98999899989999
train: step: 5101, loss: 0.0243483018130064, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 5102, loss: 0.005834486335515976, acc: 0.99, recall: 0.99, precision: 0.9901960784313726, f_beta: 0.98999899989999
train: step: 5103, loss: 0.030345287173986435, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 5104, loss: 0.0529751256108284, acc: 0.96, recall: 0.96, precision: 0.96, f_beta: 0.96
train: step: 5105, loss: 0.029626835137605667, acc: 0.96, recall: 0.96, precision: 0.9607371794871795, f_beta: 0.959983993597439
train: step: 5106, loss: 0.04276911914348602, acc: 0.95, recall: 0.95, precision: 0.9501800720288115, f_beta: 0.9499949994999499
train: step: 5107, loss: 0.028698574751615524, acc: 0.98, recall: 0.98, precision: 0.98, f_beta: 0.98
train: step: 5108, loss: 0.04393056407570839, acc: 0.95, recall: 0.95, precision: 0.9501800720288115, f_beta: 0.9499949994999499
train: step: 5109, loss: 0.02140003629028797, acc: 0.98, recall: 0.98, precision: 0.9807692307692308, f_beta: 0.9799919967987194
train: step: 5110, loss: 0.040747612714767456, acc: 0.95, recall: 0.95, precision: 0.9501800720288115, f_beta: 0.9499949994999499
train: step: 5111, loss: 0.058594271540641785, acc: 0.93, recall: 0.9299999999999999, precision: 0.930172068827531, f_beta: 0.9299929992999301
train: step: 5112, loss: 0.02366713434457779, acc: 0.97, recall: 0.97, precision: 0.9716981132075472, f_beta: 0.9699729756781104
train: step: 5113, loss: 0.046511758118867874, acc: 0.95, recall: 0.95, precision: 0.9501800720288115, f_beta: 0.9499949994999499
train: step: 5114, loss: 0.03439055755734444, acc: 0.96, recall: 0.96, precision: 0.9607371794871795, f_beta: 0.959983993597439
train: step: 5115, loss: 0.04672884941101074, acc: 0.95, recall: 0.95, precision: 0.9516258530710557, f_beta: 0.949954959463517
train: step: 5116, loss: 0.01972784474492073, acc: 0.98, recall: 0.98, precision: 0.9807692307692308, f_beta: 0.9799919967987194
train: step: 5117, loss: 0.03151492401957512, acc: 0.96, recall: 0.96, precision: 0.9607371794871795, f_beta: 0.959983993597439
train: step: 5118, loss: 0.04375062882900238, acc: 0.95, recall: 0.95, precision: 0.9501800720288115, f_beta: 0.9499949994999499
train: step: 5119, loss: 0.03790568187832832, acc: 0.96, recall: 0.96, precision: 0.9607371794871795, f_beta: 0.959983993597439
train: step: 5120, loss: 0.029926562681794167, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 5121, loss: 0.02275433950126171, acc: 0.96, recall: 0.96, precision: 0.9607371794871795, f_beta: 0.959983993597439
train: step: 5122, loss: 0.03600279986858368, acc: 0.94, recall: 0.94, precision: 0.94, f_beta: 0.94
train: step: 5123, loss: 0.033345505595207214, acc: 0.96, recall: 0.96, precision: 0.96, f_beta: 0.96
train: step: 5124, loss: 0.04993274807929993, acc: 0.92, recall: 0.9199999999999999, precision: 0.9206730769230769, f_beta: 0.919967987194878
train: step: 5125, loss: 0.023304034024477005, acc: 0.98, recall: 0.98, precision: 0.9807692307692308, f_beta: 0.9799919967987194
train: step: 5126, loss: 0.02143958769738674, acc: 0.98, recall: 0.98, precision: 0.9807692307692308, f_beta: 0.9799919967987194
train: step: 5127, loss: 0.012876307591795921, acc: 0.99, recall: 0.99, precision: 0.9901960784313726, f_beta: 0.98999899989999
train: step: 5128, loss: 0.0143641522154212, acc: 0.98, recall: 0.98, precision: 0.9807692307692308, f_beta: 0.9799919967987194
train: step: 5129, loss: 0.03090658225119114, acc: 0.96, recall: 0.96, precision: 0.9607371794871795, f_beta: 0.959983993597439
train: step: 5130, loss: 0.023636065423488617, acc: 0.96, recall: 0.96, precision: 0.9607371794871795, f_beta: 0.959983993597439
train: step: 5131, loss: 0.013553273864090443, acc: 0.99, recall: 0.99, precision: 0.9901960784313726, f_beta: 0.98999899989999
train: step: 5132, loss: 0.02120601199567318, acc: 0.98, recall: 0.98, precision: 0.98, f_beta: 0.98
train: step: 5133, loss: 0.02291017584502697, acc: 0.98, recall: 0.98, precision: 0.98, f_beta: 0.98
train: step: 5134, loss: 0.004942551255226135, acc: 1.0, recall: 1.0, precision: 1.0, f_beta: 1.0
train: step: 5135, loss: 0.020635223016142845, acc: 0.98, recall: 0.98, precision: 0.98, f_beta: 0.98
train: step: 5136, loss: 0.07344657182693481, acc: 0.9, recall: 0.8999999999999999, precision: 0.9105090311986863, f_beta: 0.8993558776167472
train: step: 5137, loss: 0.033402908593416214, acc: 0.96, recall: 0.96, precision: 0.96, f_beta: 0.96
train: step: 5138, loss: 0.022659512236714363, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 5139, loss: 0.019578905776143074, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 5140, loss: 0.02030712179839611, acc: 0.98, recall: 0.98, precision: 0.98, f_beta: 0.98
train: step: 5141, loss: 0.039827924221754074, acc: 0.95, recall: 0.95, precision: 0.9501800720288115, f_beta: 0.9499949994999499
train: step: 5142, loss: 0.06996090710163116, acc: 0.92, recall: 0.9199999999999999, precision: 0.9206730769230769, f_beta: 0.919967987194878
train: step: 5143, loss: 0.04741533100605011, acc: 0.94, recall: 0.94, precision: 0.9464285714285714, f_beta: 0.939783219590526
train: step: 5144, loss: 0.05604934319853783, acc: 0.93, recall: 0.9299999999999999, precision: 0.930172068827531, f_beta: 0.9299929992999301
train: step: 5145, loss: 0.056394826620817184, acc: 0.93, recall: 0.9299999999999999, precision: 0.930172068827531, f_beta: 0.9299929992999301
train: step: 5146, loss: 0.03122791089117527, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 5147, loss: 0.03581226244568825, acc: 0.95, recall: 0.95, precision: 0.9501800720288115, f_beta: 0.9499949994999499
train: step: 5148, loss: 0.02154867723584175, acc: 0.96, recall: 0.96, precision: 0.9607371794871795, f_beta: 0.959983993597439
train: step: 5149, loss: 0.028191177174448967, acc: 0.95, recall: 0.95, precision: 0.9501800720288115, f_beta: 0.9499949994999499
train: step: 5150, loss: 0.029119279235601425, acc: 0.96, recall: 0.96, precision: 0.96, f_beta: 0.96
train: step: 5151, loss: 0.035593319684267044, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 5152, loss: 0.02000044845044613, acc: 0.97, recall: 0.97, precision: 0.9716981132075472, f_beta: 0.9699729756781104
train: step: 5153, loss: 0.0033671497367322445, acc: 1.0, recall: 1.0, precision: 1.0, f_beta: 1.0
train: step: 5154, loss: 0.03992896154522896, acc: 0.94, recall: 0.94, precision: 0.9407051282051282, f_beta: 0.9399759903961584
train: step: 5155, loss: 0.01005925890058279, acc: 0.99, recall: 0.99, precision: 0.9901960784313726, f_beta: 0.98999899989999
train: step: 5156, loss: 0.009521543979644775, acc: 0.99, recall: 0.99, precision: 0.9901960784313726, f_beta: 0.98999899989999
train: step: 5157, loss: 0.026625942438840866, acc: 0.97, recall: 0.97, precision: 0.9716981132075472, f_beta: 0.9699729756781104
train: step: 5158, loss: 0.017087042331695557, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 5159, loss: 0.02988104335963726, acc: 0.95, recall: 0.95, precision: 0.9501800720288115, f_beta: 0.9499949994999499
train: step: 5160, loss: 0.015068450011312962, acc: 0.98, recall: 0.98, precision: 0.9807692307692308, f_beta: 0.9799919967987194
train: step: 5161, loss: 0.05090325325727463, acc: 0.95, recall: 0.95, precision: 0.9516258530710557, f_beta: 0.949954959463517
train: step: 5162, loss: 0.02436644583940506, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 5163, loss: 0.010612568818032742, acc: 0.98, recall: 0.98, precision: 0.98, f_beta: 0.98
train: step: 5164, loss: 0.042581912130117416, acc: 0.96, recall: 0.96, precision: 0.96, f_beta: 0.96
train: step: 5165, loss: 0.0007648433675058186, acc: 1.0, recall: 1.0, precision: 1.0, f_beta: 1.0
train: step: 5166, loss: 0.0027608394157141447, acc: 1.0, recall: 1.0, precision: 1.0, f_beta: 1.0
train: step: 5167, loss: 0.03604099899530411, acc: 0.96, recall: 0.96, precision: 0.96, f_beta: 0.96
train: step: 5168, loss: 0.025842096656560898, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 5169, loss: 0.05122135952115059, acc: 0.93, recall: 0.9299999999999999, precision: 0.9315535929345644, f_beta: 0.9299369432489241
train: step: 5170, loss: 0.011867428198456764, acc: 0.99, recall: 0.99, precision: 0.9901960784313726, f_beta: 0.98999899989999
train: step: 5171, loss: 0.038240883499383926, acc: 0.94, recall: 0.94, precision: 0.9407051282051282, f_beta: 0.9399759903961584
train: step: 5172, loss: 0.03273032605648041, acc: 0.96, recall: 0.96, precision: 0.9607371794871795, f_beta: 0.959983993597439
train: step: 5173, loss: 0.012349823489785194, acc: 0.99, recall: 0.99, precision: 0.9901960784313726, f_beta: 0.98999899989999
train: step: 5174, loss: 0.07251103222370148, acc: 0.91, recall: 0.9099999999999999, precision: 0.9114813327980731, f_beta: 0.9099189270343309
train: step: 5175, loss: 0.02198942005634308, acc: 0.98, recall: 0.98, precision: 0.9807692307692308, f_beta: 0.9799919967987194
train: step: 5176, loss: 0.016894252970814705, acc: 0.98, recall: 0.98, precision: 0.9807692307692308, f_beta: 0.9799919967987194
train: step: 5177, loss: 0.03547096624970436, acc: 0.96, recall: 0.96, precision: 0.962962962962963, f_beta: 0.9599358974358974
train: step: 5178, loss: 0.06305699795484543, acc: 0.94, recall: 0.94, precision: 0.94, f_beta: 0.94
train: step: 5179, loss: 0.04366859421133995, acc: 0.95, recall: 0.95, precision: 0.9501800720288115, f_beta: 0.9499949994999499
train: step: 5180, loss: 0.04423540458083153, acc: 0.94, recall: 0.94, precision: 0.94, f_beta: 0.94
train: step: 5181, loss: 0.011854703538119793, acc: 0.99, recall: 0.99, precision: 0.9901960784313726, f_beta: 0.98999899989999
train: step: 5182, loss: 0.06498254835605621, acc: 0.92, recall: 0.9199999999999999, precision: 0.9206730769230769, f_beta: 0.919967987194878
train: step: 5183, loss: 0.034075792878866196, acc: 0.96, recall: 0.96, precision: 0.96, f_beta: 0.96
train: step: 5184, loss: 0.011033189482986927, acc: 0.99, recall: 0.99, precision: 0.9901960784313726, f_beta: 0.98999899989999
train: step: 5185, loss: 0.009752094745635986, acc: 0.99, recall: 0.99, precision: 0.9901960784313726, f_beta: 0.98999899989999
train: step: 5186, loss: 0.032455459237098694, acc: 0.97, recall: 0.97, precision: 0.9716981132075472, f_beta: 0.9699729756781104
train: step: 5187, loss: 0.012301605194807053, acc: 0.99, recall: 0.99, precision: 0.9901960784313726, f_beta: 0.98999899989999
train: step: 5188, loss: 0.02824079431593418, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 5189, loss: 0.033252522349357605, acc: 0.96, recall: 0.96, precision: 0.9607371794871795, f_beta: 0.959983993597439
train: step: 5190, loss: 0.02910248562693596, acc: 0.97, recall: 0.97, precision: 0.9716981132075472, f_beta: 0.9699729756781104
train: step: 5191, loss: 0.022396814078092575, acc: 0.98, recall: 0.98, precision: 0.98, f_beta: 0.98
train: step: 5192, loss: 0.008164634928107262, acc: 0.99, recall: 0.99, precision: 0.9901960784313726, f_beta: 0.98999899989999
train: step: 5193, loss: 0.04593874514102936, acc: 0.95, recall: 0.95, precision: 0.9501800720288115, f_beta: 0.9499949994999499
train: step: 5194, loss: 0.05870337411761284, acc: 0.94, recall: 0.94, precision: 0.9407051282051282, f_beta: 0.9399759903961584
train: step: 5195, loss: 0.0713336169719696, acc: 0.91, recall: 0.91, precision: 0.9101640656262505, f_beta: 0.90999099909991
train: step: 5196, loss: 0.019374966621398926, acc: 0.98, recall: 0.98, precision: 0.98, f_beta: 0.98
train: step: 5197, loss: 0.041647039353847504, acc: 0.95, recall: 0.95, precision: 0.9501800720288115, f_beta: 0.9499949994999499
train: step: 5198, loss: 0.03459581732749939, acc: 0.96, recall: 0.96, precision: 0.96, f_beta: 0.96
train: step: 5199, loss: 0.03535047918558121, acc: 0.96, recall: 0.96, precision: 0.9607371794871795, f_beta: 0.959983993597439
train: step: 5200, loss: 0.027189375832676888, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 5201, loss: 0.019973479211330414, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 5202, loss: 0.011426533572375774, acc: 0.99, recall: 0.99, precision: 0.9901960784313726, f_beta: 0.98999899989999
train: step: 5203, loss: 0.016365692019462585, acc: 0.98, recall: 0.98, precision: 0.9807692307692308, f_beta: 0.9799919967987194
train: step: 5204, loss: 0.019363736733794212, acc: 0.98, recall: 0.98, precision: 0.9807692307692308, f_beta: 0.9799919967987194
train: step: 5205, loss: 0.009935376234352589, acc: 0.99, recall: 0.99, precision: 0.9901960784313726, f_beta: 0.98999899989999
train: step: 5206, loss: 0.01217939518392086, acc: 0.99, recall: 0.99, precision: 0.9901960784313726, f_beta: 0.98999899989999
train: step: 5207, loss: 0.03110501356422901, acc: 0.95, recall: 0.95, precision: 0.9501800720288115, f_beta: 0.9499949994999499
train: step: 5208, loss: 0.04523109272122383, acc: 0.94, recall: 0.94, precision: 0.9407051282051282, f_beta: 0.9399759903961584
train: step: 5209, loss: 0.017119579017162323, acc: 0.98, recall: 0.98, precision: 0.98, f_beta: 0.98
train: step: 5210, loss: 0.022553734481334686, acc: 0.97, recall: 0.97, precision: 0.9716981132075472, f_beta: 0.9699729756781104
train: step: 5211, loss: 0.03277080878615379, acc: 0.96, recall: 0.96, precision: 0.9607371794871795, f_beta: 0.959983993597439
train: step: 5212, loss: 0.01329460646957159, acc: 0.99, recall: 0.99, precision: 0.9901960784313726, f_beta: 0.98999899989999
train: step: 5213, loss: 0.02255185693502426, acc: 0.97, recall: 0.97, precision: 0.9716981132075472, f_beta: 0.9699729756781104
train: step: 5214, loss: 0.04789750277996063, acc: 0.94, recall: 0.94, precision: 0.9407051282051282, f_beta: 0.9399759903961584
train: step: 5215, loss: 0.024947259575128555, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 5216, loss: 0.011218729428946972, acc: 0.99, recall: 0.99, precision: 0.9901960784313726, f_beta: 0.98999899989999
train: step: 5217, loss: 0.018243245780467987, acc: 0.98, recall: 0.98, precision: 0.98, f_beta: 0.98
train: step: 5218, loss: 0.03923596814274788, acc: 0.96, recall: 0.96, precision: 0.96, f_beta: 0.96
train: step: 5219, loss: 0.021594513207674026, acc: 0.98, recall: 0.98, precision: 0.98, f_beta: 0.98
train: step: 5220, loss: 0.04729042202234268, acc: 0.94, recall: 0.94, precision: 0.94, f_beta: 0.94
train: step: 5221, loss: 0.01953822746872902, acc: 0.97, recall: 0.97, precision: 0.9716981132075472, f_beta: 0.9699729756781104
train: step: 5222, loss: 0.056285325437784195, acc: 0.94, recall: 0.94, precision: 0.94, f_beta: 0.94
train: step: 5223, loss: 0.052207790315151215, acc: 0.93, recall: 0.9299999999999999, precision: 0.9315535929345644, f_beta: 0.9299369432489241
train: step: 5224, loss: 0.047232963144779205, acc: 0.95, recall: 0.95, precision: 0.9516258530710557, f_beta: 0.949954959463517
train: step: 5225, loss: 0.0560314804315567, acc: 0.94, recall: 0.94, precision: 0.94, f_beta: 0.94
train: step: 5226, loss: 0.04370376095175743, acc: 0.95, recall: 0.95, precision: 0.9501800720288115, f_beta: 0.9499949994999499
train: step: 5227, loss: 0.009971466846764088, acc: 0.99, recall: 0.99, precision: 0.9901960784313726, f_beta: 0.98999899989999
train: step: 5228, loss: 0.0293307825922966, acc: 0.96, recall: 0.96, precision: 0.96, f_beta: 0.96
train: step: 5229, loss: 0.020002316683530807, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 5230, loss: 0.05225425586104393, acc: 0.93, recall: 0.9299999999999999, precision: 0.930172068827531, f_beta: 0.9299929992999301
train: step: 5231, loss: 0.059208501130342484, acc: 0.92, recall: 0.9199999999999999, precision: 0.9227053140096618, f_beta: 0.9198717948717948
train: step: 5232, loss: 0.04156629741191864, acc: 0.95, recall: 0.95, precision: 0.9516258530710557, f_beta: 0.949954959463517
train: step: 5233, loss: 0.042625393718481064, acc: 0.96, recall: 0.96, precision: 0.96, f_beta: 0.96
train: step: 5234, loss: 0.025487437844276428, acc: 0.98, recall: 0.98, precision: 0.98, f_beta: 0.98
train: step: 5235, loss: 0.008742546662688255, acc: 0.99, recall: 0.99, precision: 0.9901960784313726, f_beta: 0.98999899989999
train: step: 5236, loss: 0.023518593981862068, acc: 0.98, recall: 0.98, precision: 0.98, f_beta: 0.98
train: step: 5237, loss: 0.044629935175180435, acc: 0.95, recall: 0.95, precision: 0.9516258530710557, f_beta: 0.949954959463517
train: step: 5238, loss: 0.02487862855195999, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 5239, loss: 0.04173249751329422, acc: 0.96, recall: 0.96, precision: 0.962962962962963, f_beta: 0.9599358974358974
train: step: 5240, loss: 0.008540669456124306, acc: 0.99, recall: 0.99, precision: 0.9901960784313726, f_beta: 0.98999899989999
train: step: 5241, loss: 0.017220599576830864, acc: 0.99, recall: 0.99, precision: 0.9901960784313726, f_beta: 0.98999899989999
train: step: 5242, loss: 0.03776821121573448, acc: 0.96, recall: 0.96, precision: 0.96, f_beta: 0.96
train: step: 5243, loss: 0.00722684757784009, acc: 0.99, recall: 0.99, precision: 0.9901960784313726, f_beta: 0.98999899989999
train: step: 5244, loss: 0.01830858178436756, acc: 0.98, recall: 0.98, precision: 0.98, f_beta: 0.98
train: step: 5245, loss: 0.017909588292241096, acc: 0.98, recall: 0.98, precision: 0.9807692307692308, f_beta: 0.9799919967987194
train: step: 5246, loss: 0.017217740416526794, acc: 0.98, recall: 0.98, precision: 0.9807692307692308, f_beta: 0.9799919967987194
train: step: 5247, loss: 0.01819564960896969, acc: 0.98, recall: 0.98, precision: 0.98, f_beta: 0.98
train: step: 5248, loss: 0.03960124030709267, acc: 0.94, recall: 0.94, precision: 0.94, f_beta: 0.94
train: step: 5249, loss: 0.044947050511837006, acc: 0.95, recall: 0.95, precision: 0.9516258530710557, f_beta: 0.949954959463517
train: step: 5250, loss: 0.03644583374261856, acc: 0.95, recall: 0.95, precision: 0.9516258530710557, f_beta: 0.949954959463517
train: step: 5251, loss: 0.04208920896053314, acc: 0.95, recall: 0.95, precision: 0.9501800720288115, f_beta: 0.9499949994999499
train: step: 5252, loss: 0.028092747554183006, acc: 0.96, recall: 0.96, precision: 0.96, f_beta: 0.96
train: step: 5253, loss: 0.017300236970186234, acc: 0.98, recall: 0.98, precision: 0.9807692307692308, f_beta: 0.9799919967987194
train: step: 5254, loss: 0.028920255601406097, acc: 0.96, recall: 0.96, precision: 0.9607371794871795, f_beta: 0.959983993597439
train: step: 5255, loss: 0.02677948959171772, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 5256, loss: 0.015376131981611252, acc: 0.98, recall: 0.98, precision: 0.98, f_beta: 0.98
train: step: 5257, loss: 0.024258442223072052, acc: 0.98, recall: 0.98, precision: 0.9807692307692308, f_beta: 0.9799919967987194
train: step: 5258, loss: 0.04178411513566971, acc: 0.95, recall: 0.95, precision: 0.9516258530710557, f_beta: 0.949954959463517
train: step: 5259, loss: 0.032498739659786224, acc: 0.95, recall: 0.95, precision: 0.9516258530710557, f_beta: 0.949954959463517
train: step: 5260, loss: 0.01955842226743698, acc: 0.99, recall: 0.99, precision: 0.9901960784313726, f_beta: 0.98999899989999
train: step: 5261, loss: 0.03869849070906639, acc: 0.93, recall: 0.9299999999999999, precision: 0.9343434343434343, f_beta: 0.9298245614035088
train: step: 5262, loss: 0.022229747846722603, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 5263, loss: 0.02862674929201603, acc: 0.95, recall: 0.95, precision: 0.9516258530710557, f_beta: 0.949954959463517
train: step: 5264, loss: 0.002629535272717476, acc: 1.0, recall: 1.0, precision: 1.0, f_beta: 1.0
train: step: 5265, loss: 0.02489563450217247, acc: 0.96, recall: 0.96, precision: 0.96, f_beta: 0.96
train: step: 5266, loss: 0.030771765857934952, acc: 0.97, recall: 0.97, precision: 0.9716981132075472, f_beta: 0.9699729756781104
train: step: 5267, loss: 0.014876157976686954, acc: 0.98, recall: 0.98, precision: 0.98, f_beta: 0.98
train: step: 5268, loss: 0.02683654986321926, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 5269, loss: 0.013303368352353573, acc: 0.98, recall: 0.98, precision: 0.9807692307692308, f_beta: 0.9799919967987194
train: step: 5270, loss: 0.03232724592089653, acc: 0.96, recall: 0.96, precision: 0.9607371794871795, f_beta: 0.959983993597439
train: step: 5271, loss: 0.010312321595847607, acc: 0.99, recall: 0.99, precision: 0.9901960784313726, f_beta: 0.98999899989999
train: step: 5272, loss: 0.026875615119934082, acc: 0.96, recall: 0.96, precision: 0.96, f_beta: 0.96
train: step: 5273, loss: 0.018163155764341354, acc: 0.98, recall: 0.98, precision: 0.9807692307692308, f_beta: 0.9799919967987194
train: step: 5274, loss: 0.02374458871781826, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 5275, loss: 0.018536880612373352, acc: 0.98, recall: 0.98, precision: 0.9807692307692308, f_beta: 0.9799919967987194
train: step: 5276, loss: 0.039357248693704605, acc: 0.95, recall: 0.95, precision: 0.9516258530710557, f_beta: 0.949954959463517
train: step: 5277, loss: 0.026740245521068573, acc: 0.98, recall: 0.98, precision: 0.98, f_beta: 0.98
train: step: 5278, loss: 0.035069890320301056, acc: 0.96, recall: 0.96, precision: 0.9607371794871795, f_beta: 0.959983993597439
train: step: 5279, loss: 0.05608101934194565, acc: 0.94, recall: 0.94, precision: 0.9407051282051282, f_beta: 0.9399759903961584
train: step: 5280, loss: 0.023834170773625374, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 5281, loss: 0.007882270030677319, acc: 1.0, recall: 1.0, precision: 1.0, f_beta: 1.0
train: step: 5282, loss: 0.017664415761828423, acc: 0.98, recall: 0.98, precision: 0.9807692307692308, f_beta: 0.9799919967987194
train: step: 5283, loss: 0.009412932209670544, acc: 0.99, recall: 0.99, precision: 0.9901960784313726, f_beta: 0.98999899989999
train: step: 5284, loss: 0.02297925390303135, acc: 0.98, recall: 0.98, precision: 0.9807692307692308, f_beta: 0.9799919967987194
train: step: 5285, loss: 0.029623908922076225, acc: 0.96, recall: 0.96, precision: 0.96, f_beta: 0.96
train: step: 5286, loss: 0.023654909804463387, acc: 0.98, recall: 0.98, precision: 0.98, f_beta: 0.98
train: step: 5287, loss: 0.03521692380309105, acc: 0.96, recall: 0.96, precision: 0.96, f_beta: 0.96
train: step: 5288, loss: 0.03872361034154892, acc: 0.95, recall: 0.95, precision: 0.9545454545454546, f_beta: 0.949874686716792
train: step: 5289, loss: 0.016482964158058167, acc: 0.97, recall: 0.97, precision: 0.9716981132075472, f_beta: 0.9699729756781104
train: step: 5290, loss: 0.02760501578450203, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 5291, loss: 0.018017450347542763, acc: 0.98, recall: 0.98, precision: 0.98, f_beta: 0.98
train: step: 5292, loss: 0.019914302974939346, acc: 0.98, recall: 0.98, precision: 0.98, f_beta: 0.98
train: step: 5293, loss: 0.03693772479891777, acc: 0.95, recall: 0.95, precision: 0.9516258530710557, f_beta: 0.949954959463517
train: step: 5294, loss: 0.011669082567095757, acc: 0.98, recall: 0.98, precision: 0.9807692307692308, f_beta: 0.9799919967987194
train: step: 5295, loss: 0.039592813700437546, acc: 0.96, recall: 0.96, precision: 0.9607371794871795, f_beta: 0.959983993597439
train: step: 5296, loss: 0.015992680564522743, acc: 0.98, recall: 0.98, precision: 0.9807692307692308, f_beta: 0.9799919967987194
train: step: 5297, loss: 0.0025419420562684536, acc: 1.0, recall: 1.0, precision: 1.0, f_beta: 1.0
train: step: 5298, loss: 0.034866686910390854, acc: 0.96, recall: 0.96, precision: 0.962962962962963, f_beta: 0.9599358974358974
train: step: 5299, loss: 0.021641308441758156, acc: 0.98, recall: 0.98, precision: 0.98, f_beta: 0.98
train: step: 5300, loss: 0.010238545015454292, acc: 0.99, recall: 0.99, precision: 0.9901960784313726, f_beta: 0.98999899989999
train: step: 5301, loss: 0.02907908521592617, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 5302, loss: 0.04309191554784775, acc: 0.92, recall: 0.9199999999999999, precision: 0.9206730769230769, f_beta: 0.919967987194878
train: step: 5303, loss: 0.04025460407137871, acc: 0.96, recall: 0.96, precision: 0.9607371794871795, f_beta: 0.959983993597439
train: step: 5304, loss: 0.020963206887245178, acc: 0.98, recall: 0.98, precision: 0.98, f_beta: 0.98
train: step: 5305, loss: 0.015427546575665474, acc: 0.98, recall: 0.98, precision: 0.98, f_beta: 0.98
train: step: 5306, loss: 0.033794280141592026, acc: 0.95, recall: 0.95, precision: 0.9516258530710557, f_beta: 0.949954959463517
train: step: 5307, loss: 0.005484648514539003, acc: 0.99, recall: 0.99, precision: 0.9901960784313726, f_beta: 0.98999899989999
train: step: 5308, loss: 0.02282930538058281, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 5309, loss: 0.01894034817814827, acc: 0.99, recall: 0.99, precision: 0.9901960784313726, f_beta: 0.98999899989999
train: step: 5310, loss: 0.03182234242558479, acc: 0.97, recall: 0.97, precision: 0.9716981132075472, f_beta: 0.9699729756781104
train: step: 5311, loss: 0.028597939759492874, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 5312, loss: 0.005455427337437868, acc: 1.0, recall: 1.0, precision: 1.0, f_beta: 1.0
train: step: 5313, loss: 0.04100685194134712, acc: 0.95, recall: 0.95, precision: 0.9501800720288115, f_beta: 0.9499949994999499
train: step: 5314, loss: 0.02790665812790394, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 5315, loss: 0.016593873500823975, acc: 0.98, recall: 0.98, precision: 0.9807692307692308, f_beta: 0.9799919967987194
train: step: 5316, loss: 0.03621237352490425, acc: 0.95, recall: 0.95, precision: 0.9516258530710557, f_beta: 0.949954959463517
train: step: 5317, loss: 0.04180065542459488, acc: 0.95, recall: 0.95, precision: 0.9501800720288115, f_beta: 0.9499949994999499
train: step: 5318, loss: 0.004951341077685356, acc: 1.0, recall: 1.0, precision: 1.0, f_beta: 1.0
train: step: 5319, loss: 0.019929509609937668, acc: 0.97, recall: 0.97, precision: 0.9716981132075472, f_beta: 0.9699729756781104
train: step: 5320, loss: 0.022028183564543724, acc: 0.98, recall: 0.98, precision: 0.9807692307692308, f_beta: 0.9799919967987194
train: step: 5321, loss: 0.019490107893943787, acc: 0.98, recall: 0.98, precision: 0.98, f_beta: 0.98
train: step: 5322, loss: 0.018938861787319183, acc: 0.98, recall: 0.98, precision: 0.98, f_beta: 0.98
train: step: 5323, loss: 0.04009728133678436, acc: 0.96, recall: 0.96, precision: 0.96, f_beta: 0.96
train: step: 5324, loss: 0.020551590248942375, acc: 0.98, recall: 0.98, precision: 0.98, f_beta: 0.98
train: step: 5325, loss: 0.0555952750146389, acc: 0.94, recall: 0.94, precision: 0.9428341384863124, f_beta: 0.9399038461538463
train: step: 5326, loss: 0.02574928104877472, acc: 0.95, recall: 0.95, precision: 0.9516258530710557, f_beta: 0.949954959463517
train: step: 5327, loss: 0.0018867702456191182, acc: 1.0, recall: 1.0, precision: 1.0, f_beta: 1.0
train: step: 5328, loss: 0.02603953331708908, acc: 0.96, recall: 0.96, precision: 0.962962962962963, f_beta: 0.9599358974358974
train: step: 5329, loss: 0.03723635524511337, acc: 0.96, recall: 0.96, precision: 0.962962962962963, f_beta: 0.9599358974358974
train: step: 5330, loss: 0.0216952133923769, acc: 0.98, recall: 0.98, precision: 0.98, f_beta: 0.98
train: step: 5331, loss: 0.043488845229148865, acc: 0.96, recall: 0.96, precision: 0.96, f_beta: 0.96
train: step: 5332, loss: 0.02477964013814926, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 5333, loss: 0.0346955768764019, acc: 0.95, recall: 0.95, precision: 0.9501800720288115, f_beta: 0.9499949994999499
train: step: 5334, loss: 0.035427678376436234, acc: 0.96, recall: 0.96, precision: 0.9607371794871795, f_beta: 0.959983993597439
train: step: 5335, loss: 0.02003699354827404, acc: 0.96, recall: 0.96, precision: 0.9607371794871795, f_beta: 0.959983993597439
train: step: 5336, loss: 0.013215099461376667, acc: 0.99, recall: 0.99, precision: 0.9901960784313726, f_beta: 0.98999899989999
train: step: 5337, loss: 0.02882891148328781, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 5338, loss: 0.01635732688009739, acc: 0.98, recall: 0.98, precision: 0.98, f_beta: 0.98
train: step: 5339, loss: 0.03239302709698677, acc: 0.97, recall: 0.97, precision: 0.9716981132075472, f_beta: 0.9699729756781104
train: step: 5340, loss: 0.0375860370695591, acc: 0.94, recall: 0.94, precision: 0.9407051282051282, f_beta: 0.9399759903961584
train: step: 5341, loss: 0.02231718972325325, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 5342, loss: 0.025954946875572205, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 5343, loss: 0.07899965345859528, acc: 0.87, recall: 0.87, precision: 0.8737373737373737, f_beta: 0.8696741854636592
train: step: 5344, loss: 0.045590583235025406, acc: 0.96, recall: 0.96, precision: 0.9607371794871795, f_beta: 0.959983993597439
train: step: 5345, loss: 0.0300150103867054, acc: 0.97, recall: 0.97, precision: 0.9716981132075472, f_beta: 0.9699729756781104
train: step: 5346, loss: 0.02186773344874382, acc: 0.97, recall: 0.97, precision: 0.9716981132075472, f_beta: 0.9699729756781104
train: step: 5347, loss: 0.029078243300318718, acc: 0.96, recall: 0.96, precision: 0.9607371794871795, f_beta: 0.959983993597439
train: step: 5348, loss: 0.019970402121543884, acc: 0.98, recall: 0.98, precision: 0.98, f_beta: 0.98
train: step: 5349, loss: 0.03396512195467949, acc: 0.96, recall: 0.96, precision: 0.962962962962963, f_beta: 0.9599358974358974
train: step: 5350, loss: 0.008243745192885399, acc: 0.99, recall: 0.99, precision: 0.9901960784313726, f_beta: 0.98999899989999
train: step: 5351, loss: 0.02377045899629593, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 5352, loss: 0.0481356717646122, acc: 0.93, recall: 0.9299999999999999, precision: 0.930172068827531, f_beta: 0.9299929992999301
train: step: 5353, loss: 0.04853590950369835, acc: 0.94, recall: 0.94, precision: 0.9407051282051282, f_beta: 0.9399759903961584
train: step: 5354, loss: 0.027629012241959572, acc: 0.97, recall: 0.97, precision: 0.9716981132075472, f_beta: 0.9699729756781104
train: step: 5355, loss: 0.012637244537472725, acc: 0.99, recall: 0.99, precision: 0.9901960784313726, f_beta: 0.98999899989999
train: step: 5356, loss: 0.009671520441770554, acc: 0.99, recall: 0.99, precision: 0.9901960784313726, f_beta: 0.98999899989999
train: step: 5357, loss: 0.042994193732738495, acc: 0.95, recall: 0.95, precision: 0.9516258530710557, f_beta: 0.949954959463517
train: step: 5358, loss: 0.027503423392772675, acc: 0.96, recall: 0.96, precision: 0.962962962962963, f_beta: 0.9599358974358974
train: step: 5359, loss: 0.02478315681219101, acc: 0.96, recall: 0.96, precision: 0.96, f_beta: 0.96
train: step: 5360, loss: 0.011145871132612228, acc: 0.99, recall: 0.99, precision: 0.9901960784313726, f_beta: 0.98999899989999
train: step: 5361, loss: 0.01657325215637684, acc: 0.98, recall: 0.98, precision: 0.9807692307692308, f_beta: 0.9799919967987194
train: step: 5362, loss: 0.021014360710978508, acc: 0.98, recall: 0.98, precision: 0.98, f_beta: 0.98
train: step: 5363, loss: 0.040474966168403625, acc: 0.95, recall: 0.95, precision: 0.9545454545454546, f_beta: 0.949874686716792
train: step: 5364, loss: 0.022451283410191536, acc: 0.98, recall: 0.98, precision: 0.9807692307692308, f_beta: 0.9799919967987194
train: step: 5365, loss: 0.008885657414793968, acc: 0.99, recall: 0.99, precision: 0.9901960784313726, f_beta: 0.98999899989999
train: step: 5366, loss: 0.013354177586734295, acc: 0.99, recall: 0.99, precision: 0.9901960784313726, f_beta: 0.98999899989999
train: step: 5367, loss: 0.012485296465456486, acc: 0.98, recall: 0.98, precision: 0.9807692307692308, f_beta: 0.9799919967987194
train: step: 5368, loss: 0.02091275155544281, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 5369, loss: 0.020045606419444084, acc: 0.97, recall: 0.97, precision: 0.9716981132075472, f_beta: 0.9699729756781104
train: step: 5370, loss: 0.011235921643674374, acc: 0.98, recall: 0.98, precision: 0.98, f_beta: 0.98
train: step: 5371, loss: 0.012798606418073177, acc: 0.99, recall: 0.99, precision: 0.9901960784313726, f_beta: 0.98999899989999
train: step: 5372, loss: 0.024070464074611664, acc: 0.98, recall: 0.98, precision: 0.9807692307692308, f_beta: 0.9799919967987194
train: step: 5373, loss: 0.04614829644560814, acc: 0.96, recall: 0.96, precision: 0.9607371794871795, f_beta: 0.959983993597439
train: step: 5374, loss: 0.021339815109968185, acc: 0.98, recall: 0.98, precision: 0.9807692307692308, f_beta: 0.9799919967987194
train: step: 5375, loss: 0.011443152092397213, acc: 0.98, recall: 0.98, precision: 0.98, f_beta: 0.98
train: step: 5376, loss: 0.024966955184936523, acc: 0.96, recall: 0.96, precision: 0.9607371794871795, f_beta: 0.959983993597439
train: step: 5377, loss: 0.02816050499677658, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 5378, loss: 0.0343693271279335, acc: 0.96, recall: 0.96, precision: 0.9607371794871795, f_beta: 0.959983993597439
train: step: 5379, loss: 0.03305933624505997, acc: 0.96, recall: 0.96, precision: 0.9607371794871795, f_beta: 0.959983993597439
train: step: 5380, loss: 0.021482594311237335, acc: 0.98, recall: 0.98, precision: 0.98, f_beta: 0.98
train: step: 5381, loss: 0.02288658544421196, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 5382, loss: 0.023485884070396423, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 5383, loss: 0.004103322979062796, acc: 1.0, recall: 1.0, precision: 1.0, f_beta: 1.0
train: step: 5384, loss: 0.05574057623744011, acc: 0.93, recall: 0.9299999999999999, precision: 0.9315535929345644, f_beta: 0.9299369432489241
train: step: 5385, loss: 0.028827829286456108, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 5386, loss: 0.015333286486566067, acc: 0.98, recall: 0.98, precision: 0.9807692307692308, f_beta: 0.9799919967987194
train: step: 5387, loss: 0.01217668317258358, acc: 0.99, recall: 0.99, precision: 0.9901960784313726, f_beta: 0.98999899989999
train: step: 5388, loss: 0.027876410633325577, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 5389, loss: 0.013190108351409435, acc: 0.99, recall: 0.99, precision: 0.9901960784313726, f_beta: 0.98999899989999
train: step: 5390, loss: 0.019793257117271423, acc: 0.98, recall: 0.98, precision: 0.98, f_beta: 0.98
train: step: 5391, loss: 0.034318313002586365, acc: 0.96, recall: 0.96, precision: 0.9607371794871795, f_beta: 0.959983993597439
train: step: 5392, loss: 0.008929317817091942, acc: 0.99, recall: 0.99, precision: 0.9901960784313726, f_beta: 0.98999899989999
train: step: 5393, loss: 0.061396222561597824, acc: 0.93, recall: 0.9299999999999999, precision: 0.930172068827531, f_beta: 0.9299929992999301
train: step: 5394, loss: 0.02480095811188221, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 5395, loss: 0.027621343731880188, acc: 0.96, recall: 0.96, precision: 0.9607371794871795, f_beta: 0.959983993597439
train: step: 5396, loss: 0.0549498051404953, acc: 0.94, recall: 0.94, precision: 0.9428341384863124, f_beta: 0.9399038461538463
train: step: 5397, loss: 0.04170401394367218, acc: 0.96, recall: 0.96, precision: 0.9607371794871795, f_beta: 0.959983993597439
train: step: 5398, loss: 0.04696274921298027, acc: 0.93, recall: 0.9299999999999999, precision: 0.930172068827531, f_beta: 0.9299929992999301
train: step: 5399, loss: 0.037212371826171875, acc: 0.95, recall: 0.95, precision: 0.9501800720288115, f_beta: 0.9499949994999499
train: step: 5400, loss: 0.0413384810090065, acc: 0.96, recall: 0.96, precision: 0.96, f_beta: 0.96
train: step: 5401, loss: 0.024550290778279305, acc: 0.98, recall: 0.98, precision: 0.98, f_beta: 0.98
train: step: 5402, loss: 0.02705431915819645, acc: 0.96, recall: 0.96, precision: 0.96, f_beta: 0.96
train: step: 5403, loss: 0.04643592983484268, acc: 0.95, recall: 0.95, precision: 0.9545454545454546, f_beta: 0.949874686716792
train: step: 5404, loss: 0.02597353421151638, acc: 0.97, recall: 0.97, precision: 0.9716981132075472, f_beta: 0.9699729756781104
train: step: 5405, loss: 0.03315914422273636, acc: 0.96, recall: 0.96, precision: 0.962962962962963, f_beta: 0.9599358974358974
train: step: 5406, loss: 0.04201088845729828, acc: 0.95, recall: 0.95, precision: 0.9501800720288115, f_beta: 0.9499949994999499
train: step: 5407, loss: 0.010738428682088852, acc: 0.99, recall: 0.99, precision: 0.9901960784313726, f_beta: 0.98999899989999
train: step: 5408, loss: 0.007356202695518732, acc: 0.99, recall: 0.99, precision: 0.9901960784313726, f_beta: 0.98999899989999
train: step: 5409, loss: 0.020172815769910812, acc: 0.98, recall: 0.98, precision: 0.98, f_beta: 0.98
train: step: 5410, loss: 0.019684376195073128, acc: 0.98, recall: 0.98, precision: 0.9807692307692308, f_beta: 0.9799919967987194
train: step: 5411, loss: 0.0007481935899704695, acc: 1.0, recall: 1.0, precision: 1.0, f_beta: 1.0
train: step: 5412, loss: 0.02700650878250599, acc: 0.97, recall: 0.97, precision: 0.9716981132075472, f_beta: 0.9699729756781104
train: step: 5413, loss: 0.008440502919256687, acc: 0.98, recall: 0.98, precision: 0.9807692307692308, f_beta: 0.9799919967987194
train: step: 5414, loss: 0.011059274896979332, acc: 0.99, recall: 0.99, precision: 0.9901960784313726, f_beta: 0.98999899989999
train: step: 5415, loss: 0.01819692552089691, acc: 0.98, recall: 0.98, precision: 0.9807692307692308, f_beta: 0.9799919967987194
train: step: 5416, loss: 0.019714143127202988, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 5417, loss: 0.02019052766263485, acc: 0.97, recall: 0.97, precision: 0.9716981132075472, f_beta: 0.9699729756781104
train: step: 5418, loss: 0.03932555019855499, acc: 0.96, recall: 0.96, precision: 0.9607371794871795, f_beta: 0.959983993597439
train: step: 5419, loss: 0.02316724695265293, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 5420, loss: 0.020234856754541397, acc: 0.98, recall: 0.98, precision: 0.98, f_beta: 0.98
train: step: 5421, loss: 0.04169698804616928, acc: 0.95, recall: 0.95, precision: 0.9501800720288115, f_beta: 0.9499949994999499
train: step: 5422, loss: 0.047443509101867676, acc: 0.95, recall: 0.95, precision: 0.9516258530710557, f_beta: 0.949954959463517
train: step: 5423, loss: 0.022925175726413727, acc: 0.98, recall: 0.98, precision: 0.98, f_beta: 0.98
train: step: 5424, loss: 0.03580440953373909, acc: 0.94, recall: 0.94, precision: 0.9428341384863124, f_beta: 0.9399038461538463
train: step: 5425, loss: 0.030443381518125534, acc: 0.96, recall: 0.96, precision: 0.9607371794871795, f_beta: 0.959983993597439
train: step: 5426, loss: 0.014087634161114693, acc: 0.98, recall: 0.98, precision: 0.98, f_beta: 0.98
train: step: 5427, loss: 0.028632523491978645, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 5428, loss: 0.05000012367963791, acc: 0.95, recall: 0.95, precision: 0.9516258530710557, f_beta: 0.949954959463517
train: step: 5429, loss: 0.011182413436472416, acc: 0.99, recall: 0.99, precision: 0.9901960784313726, f_beta: 0.98999899989999
train: step: 5430, loss: 0.035382069647312164, acc: 0.96, recall: 0.96, precision: 0.9607371794871795, f_beta: 0.959983993597439
train: step: 5431, loss: 0.008630798198282719, acc: 0.99, recall: 0.99, precision: 0.9901960784313726, f_beta: 0.98999899989999
train: step: 5432, loss: 0.007692183367908001, acc: 0.99, recall: 0.99, precision: 0.9901960784313726, f_beta: 0.98999899989999
train: step: 5433, loss: 0.026574477553367615, acc: 0.97, recall: 0.97, precision: 0.9716981132075472, f_beta: 0.9699729756781104
train: step: 5434, loss: 0.033604174852371216, acc: 0.96, recall: 0.96, precision: 0.9607371794871795, f_beta: 0.959983993597439
train: step: 5435, loss: 0.007137390319257975, acc: 0.99, recall: 0.99, precision: 0.9901960784313726, f_beta: 0.98999899989999
train: step: 5436, loss: 0.006641793064773083, acc: 0.99, recall: 0.99, precision: 0.9901960784313726, f_beta: 0.98999899989999
train: step: 5437, loss: 0.032778993248939514, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 5438, loss: 0.029337262734770775, acc: 0.96, recall: 0.96, precision: 0.9607371794871795, f_beta: 0.959983993597439
train: step: 5439, loss: 0.03942351043224335, acc: 0.96, recall: 0.96, precision: 0.96, f_beta: 0.96
train: step: 5440, loss: 0.03206339478492737, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 5441, loss: 0.021301914006471634, acc: 0.98, recall: 0.98, precision: 0.9807692307692308, f_beta: 0.9799919967987194
train: step: 5442, loss: 0.030875248834490776, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 5443, loss: 0.013050215318799019, acc: 0.98, recall: 0.98, precision: 0.98, f_beta: 0.98
train: step: 5444, loss: 0.04286296293139458, acc: 0.95, recall: 0.95, precision: 0.9516258530710557, f_beta: 0.949954959463517
train: step: 5445, loss: 0.03935720771551132, acc: 0.95, recall: 0.95, precision: 0.9516258530710557, f_beta: 0.949954959463517
train: step: 5446, loss: 0.01577099785208702, acc: 0.99, recall: 0.99, precision: 0.9901960784313726, f_beta: 0.98999899989999
train: step: 5447, loss: 0.015653379261493683, acc: 0.98, recall: 0.98, precision: 0.9807692307692308, f_beta: 0.9799919967987194
train: step: 5448, loss: 0.0362619124352932, acc: 0.95, recall: 0.95, precision: 0.9545454545454546, f_beta: 0.949874686716792
train: step: 5449, loss: 0.0004088008136022836, acc: 1.0, recall: 1.0, precision: 1.0, f_beta: 1.0
train: step: 5450, loss: 0.029556021094322205, acc: 0.96, recall: 0.96, precision: 0.9607371794871795, f_beta: 0.959983993597439
train: step: 5451, loss: 0.011603880673646927, acc: 0.98, recall: 0.98, precision: 0.9807692307692308, f_beta: 0.9799919967987194
train: step: 5452, loss: 0.012004401534795761, acc: 0.98, recall: 0.98, precision: 0.98, f_beta: 0.98
train: step: 5453, loss: 0.003220153506845236, acc: 0.99, recall: 0.99, precision: 0.9901960784313726, f_beta: 0.98999899989999
train: step: 5454, loss: 0.01103852316737175, acc: 0.99, recall: 0.99, precision: 0.9901960784313726, f_beta: 0.98999899989999
train: step: 5455, loss: 0.0265825018286705, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 5456, loss: 0.007199503015726805, acc: 0.99, recall: 0.99, precision: 0.9901960784313726, f_beta: 0.98999899989999
train: step: 5457, loss: 0.0067918505519628525, acc: 0.99, recall: 0.99, precision: 0.9901960784313726, f_beta: 0.98999899989999
train: step: 5458, loss: 0.03013201244175434, acc: 0.96, recall: 0.96, precision: 0.9607371794871795, f_beta: 0.959983993597439
train: step: 5459, loss: 0.021440768614411354, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 5460, loss: 0.023154502734541893, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 5461, loss: 0.03603288531303406, acc: 0.96, recall: 0.96, precision: 0.9607371794871795, f_beta: 0.959983993597439
train: step: 5462, loss: 0.03210882842540741, acc: 0.95, recall: 0.95, precision: 0.9545454545454546, f_beta: 0.949874686716792
train: step: 5463, loss: 0.008929800242185593, acc: 0.99, recall: 0.99, precision: 0.9901960784313726, f_beta: 0.98999899989999
train: step: 5464, loss: 0.04091433063149452, acc: 0.94, recall: 0.94, precision: 0.94, f_beta: 0.94
train: step: 5465, loss: 0.026545356959104538, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 5466, loss: 0.01719219982624054, acc: 0.98, recall: 0.98, precision: 0.98, f_beta: 0.98
train: step: 5467, loss: 0.0014029216254130006, acc: 1.0, recall: 1.0, precision: 1.0, f_beta: 1.0
train: step: 5468, loss: 0.027733124792575836, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 5469, loss: 0.020139820873737335, acc: 0.97, recall: 0.97, precision: 0.9716981132075472, f_beta: 0.9699729756781104
train: step: 5470, loss: 0.02909543737769127, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 5471, loss: 0.03589965030550957, acc: 0.95, recall: 0.95, precision: 0.9516258530710557, f_beta: 0.949954959463517
train: step: 5472, loss: 0.041499920189380646, acc: 0.95, recall: 0.95, precision: 0.9501800720288115, f_beta: 0.9499949994999499
train: step: 5473, loss: 0.019371358677744865, acc: 0.98, recall: 0.98, precision: 0.9807692307692308, f_beta: 0.9799919967987194
train: step: 5474, loss: 0.020458487793803215, acc: 0.96, recall: 0.96, precision: 0.96, f_beta: 0.96
train: step: 5475, loss: 0.016678009182214737, acc: 0.98, recall: 0.98, precision: 0.98, f_beta: 0.98
train: step: 5476, loss: 0.056618832051754, acc: 0.94, recall: 0.94, precision: 0.9407051282051282, f_beta: 0.9399759903961584
train: step: 5477, loss: 0.02381599135696888, acc: 0.96, recall: 0.96, precision: 0.962962962962963, f_beta: 0.9599358974358974
train: step: 5478, loss: 0.06721703708171844, acc: 0.92, recall: 0.9199999999999999, precision: 0.9227053140096618, f_beta: 0.9198717948717948
train: step: 5479, loss: 0.012072613462805748, acc: 0.98, recall: 0.98, precision: 0.98, f_beta: 0.98
train: step: 5480, loss: 0.05142415687441826, acc: 0.94, recall: 0.94, precision: 0.94, f_beta: 0.94
train: step: 5481, loss: 0.0287784431129694, acc: 0.96, recall: 0.96, precision: 0.9607371794871795, f_beta: 0.959983993597439
train: step: 5482, loss: 0.05475514382123947, acc: 0.94, recall: 0.94, precision: 0.94, f_beta: 0.94
train: step: 5483, loss: 0.02420019544661045, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 5484, loss: 0.06139172241091728, acc: 0.91, recall: 0.9099999999999999, precision: 0.9114813327980731, f_beta: 0.9099189270343309
train: step: 5485, loss: 0.0024690567515790462, acc: 1.0, recall: 1.0, precision: 1.0, f_beta: 1.0
train: step: 5486, loss: 0.03473873436450958, acc: 0.96, recall: 0.96, precision: 0.9607371794871795, f_beta: 0.959983993597439
train: step: 5487, loss: 0.018338028341531754, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 5488, loss: 0.01917640119791031, acc: 0.98, recall: 0.98, precision: 0.9807692307692308, f_beta: 0.9799919967987194
train: step: 5489, loss: 0.041326865553855896, acc: 0.96, recall: 0.96, precision: 0.9607371794871795, f_beta: 0.959983993597439
train: step: 5490, loss: 0.01697031408548355, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 5491, loss: 0.011300847865641117, acc: 0.99, recall: 0.99, precision: 0.9901960784313726, f_beta: 0.98999899989999
train: step: 5492, loss: 0.023234792053699493, acc: 0.98, recall: 0.98, precision: 0.9807692307692308, f_beta: 0.9799919967987194
train: step: 5493, loss: 0.013616375625133514, acc: 0.98, recall: 0.98, precision: 0.9807692307692308, f_beta: 0.9799919967987194
train: step: 5494, loss: 0.027180256322026253, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 5495, loss: 0.03564984351396561, acc: 0.96, recall: 0.96, precision: 0.9607371794871795, f_beta: 0.959983993597439
train: step: 5496, loss: 0.007250335067510605, acc: 0.99, recall: 0.99, precision: 0.9901960784313726, f_beta: 0.98999899989999
train: step: 5497, loss: 0.023944668471813202, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 5498, loss: 0.026212530210614204, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 5499, loss: 0.011697695590555668, acc: 0.99, recall: 0.99, precision: 0.9901960784313726, f_beta: 0.98999899989999
train: step: 5500, loss: 0.02375280112028122, acc: 0.98, recall: 0.98, precision: 0.98, f_beta: 0.98
train: step: 5501, loss: 0.012867304496467113, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 5502, loss: 0.030112288892269135, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 5503, loss: 0.040529560297727585, acc: 0.96, recall: 0.96, precision: 0.96, f_beta: 0.96
train: step: 5504, loss: 0.014832518063485622, acc: 0.99, recall: 0.99, precision: 0.9901960784313726, f_beta: 0.98999899989999
train: step: 5505, loss: 0.031531430780887604, acc: 0.95, recall: 0.95, precision: 0.9501800720288115, f_beta: 0.9499949994999499
train: step: 5506, loss: 0.03595099598169327, acc: 0.96, recall: 0.96, precision: 0.96, f_beta: 0.96
train: step: 5507, loss: 0.03238653019070625, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 5508, loss: 0.016177933663129807, acc: 0.98, recall: 0.98, precision: 0.98, f_beta: 0.98
train: step: 5509, loss: 0.03455642610788345, acc: 0.96, recall: 0.96, precision: 0.9607371794871795, f_beta: 0.959983993597439
train: step: 5510, loss: 0.024792203679680824, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 5511, loss: 0.011940275318920612, acc: 0.99, recall: 0.99, precision: 0.9901960784313726, f_beta: 0.98999899989999
train: step: 5512, loss: 0.013768324628472328, acc: 0.98, recall: 0.98, precision: 0.9807692307692308, f_beta: 0.9799919967987194
train: step: 5513, loss: 0.005931835155934095, acc: 0.99, recall: 0.99, precision: 0.9901960784313726, f_beta: 0.98999899989999
train: step: 5514, loss: 0.01981276459991932, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 5515, loss: 0.032567836344242096, acc: 0.96, recall: 0.96, precision: 0.9607371794871795, f_beta: 0.959983993597439
train: step: 5516, loss: 0.02115209773182869, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 5517, loss: 0.03649834543466568, acc: 0.96, recall: 0.96, precision: 0.96, f_beta: 0.96
train: step: 5518, loss: 0.02183685265481472, acc: 0.96, recall: 0.96, precision: 0.962962962962963, f_beta: 0.9599358974358974
train: step: 5519, loss: 0.026053283363580704, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 5520, loss: 0.011978383176028728, acc: 0.99, recall: 0.99, precision: 0.9901960784313726, f_beta: 0.98999899989999
train: step: 5521, loss: 0.013305163010954857, acc: 0.99, recall: 0.99, precision: 0.9901960784313726, f_beta: 0.98999899989999
train: step: 5522, loss: 0.02730017714202404, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 5523, loss: 0.035788144916296005, acc: 0.96, recall: 0.96, precision: 0.9607371794871795, f_beta: 0.959983993597439
train: step: 5524, loss: 0.024801179766654968, acc: 0.96, recall: 0.96, precision: 0.962962962962963, f_beta: 0.9599358974358974
train: step: 5525, loss: 0.010221959091722965, acc: 0.99, recall: 0.99, precision: 0.9901960784313726, f_beta: 0.98999899989999
train: step: 5526, loss: 0.013168266043066978, acc: 0.99, recall: 0.99, precision: 0.9901960784313726, f_beta: 0.98999899989999
train: step: 5527, loss: 0.017354179173707962, acc: 0.97, recall: 0.97, precision: 0.9716981132075472, f_beta: 0.9699729756781104
train: step: 5528, loss: 0.04809168726205826, acc: 0.95, recall: 0.95, precision: 0.9501800720288115, f_beta: 0.9499949994999499
train: step: 5529, loss: 0.04277436435222626, acc: 0.94, recall: 0.94, precision: 0.9407051282051282, f_beta: 0.9399759903961584
train: step: 5530, loss: 0.03798985108733177, acc: 0.95, recall: 0.95, precision: 0.9501800720288115, f_beta: 0.9499949994999499
train: step: 5531, loss: 0.02382105402648449, acc: 0.97, recall: 0.97, precision: 0.9716981132075472, f_beta: 0.9699729756781104
train: step: 5532, loss: 0.0305807925760746, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 5533, loss: 0.031764984130859375, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 5534, loss: 0.01859404891729355, acc: 0.98, recall: 0.98, precision: 0.9807692307692308, f_beta: 0.9799919967987194
train: step: 5535, loss: 0.02518017217516899, acc: 0.98, recall: 0.98, precision: 0.98, f_beta: 0.98
train: step: 5536, loss: 0.016293032094836235, acc: 0.98, recall: 0.98, precision: 0.9807692307692308, f_beta: 0.9799919967987194
train: step: 5537, loss: 0.036199506372213364, acc: 0.95, recall: 0.95, precision: 0.9501800720288115, f_beta: 0.9499949994999499
train: step: 5538, loss: 0.04161800444126129, acc: 0.96, recall: 0.96, precision: 0.96, f_beta: 0.96
train: step: 5539, loss: 0.04150475561618805, acc: 0.94, recall: 0.94, precision: 0.9428341384863124, f_beta: 0.9399038461538463
train: step: 5540, loss: 0.053416699171066284, acc: 0.94, recall: 0.94, precision: 0.9407051282051282, f_beta: 0.9399759903961584
train: step: 5541, loss: 0.005415547639131546, acc: 1.0, recall: 1.0, precision: 1.0, f_beta: 1.0
train: step: 5542, loss: 0.032444678246974945, acc: 0.96, recall: 0.96, precision: 0.9607371794871795, f_beta: 0.959983993597439
train: step: 5543, loss: 0.010828482918441296, acc: 0.99, recall: 0.99, precision: 0.9901960784313726, f_beta: 0.98999899989999
train: step: 5544, loss: 0.02993001975119114, acc: 0.97, recall: 0.97, precision: 0.9716981132075472, f_beta: 0.9699729756781104
train: step: 5545, loss: 0.00957863125950098, acc: 0.98, recall: 0.98, precision: 0.9807692307692308, f_beta: 0.9799919967987194
train: step: 5546, loss: 0.04554158076643944, acc: 0.95, recall: 0.95, precision: 0.9516258530710557, f_beta: 0.949954959463517
train: step: 5547, loss: 0.04542243480682373, acc: 0.94, recall: 0.94, precision: 0.9428341384863124, f_beta: 0.9399038461538463
train: step: 5548, loss: 0.016431748867034912, acc: 0.98, recall: 0.98, precision: 0.98, f_beta: 0.98
train: step: 5549, loss: 0.015699559822678566, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 5550, loss: 0.0030948901548981667, acc: 1.0, recall: 1.0, precision: 1.0, f_beta: 1.0
train: step: 5551, loss: 0.006523588672280312, acc: 0.99, recall: 0.99, precision: 0.9901960784313726, f_beta: 0.98999899989999
train: step: 5552, loss: 0.030059104785323143, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 5553, loss: 0.021067971363663673, acc: 0.98, recall: 0.98, precision: 0.9807692307692308, f_beta: 0.9799919967987194
train: step: 5554, loss: 0.048049502074718475, acc: 0.94, recall: 0.94, precision: 0.9428341384863124, f_beta: 0.9399038461538463
train: step: 5555, loss: 0.003560941433534026, acc: 1.0, recall: 1.0, precision: 1.0, f_beta: 1.0
train: step: 5556, loss: 0.02107667736709118, acc: 0.97, recall: 0.97, precision: 0.9716981132075472, f_beta: 0.9699729756781104
train: step: 5557, loss: 0.024258337914943695, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 5558, loss: 0.03154149651527405, acc: 0.95, recall: 0.95, precision: 0.9516258530710557, f_beta: 0.949954959463517
train: step: 5559, loss: 0.035238318145275116, acc: 0.96, recall: 0.96, precision: 0.9607371794871795, f_beta: 0.959983993597439
train: step: 5560, loss: 0.042426660656929016, acc: 0.95, recall: 0.95, precision: 0.9501800720288115, f_beta: 0.9499949994999499
train: step: 5561, loss: 0.015463517047464848, acc: 0.99, recall: 0.99, precision: 0.9901960784313726, f_beta: 0.98999899989999
train: step: 5562, loss: 0.002981129800900817, acc: 1.0, recall: 1.0, precision: 1.0, f_beta: 1.0
train: step: 5563, loss: 0.013637997210025787, acc: 0.99, recall: 0.99, precision: 0.9901960784313726, f_beta: 0.98999899989999
train: step: 5564, loss: 0.02016034536063671, acc: 0.98, recall: 0.98, precision: 0.9807692307692308, f_beta: 0.9799919967987194
train: step: 5565, loss: 0.017257312312722206, acc: 0.98, recall: 0.98, precision: 0.98, f_beta: 0.98
train: step: 5566, loss: 0.026698995381593704, acc: 0.98, recall: 0.98, precision: 0.98, f_beta: 0.98
train: step: 5567, loss: 0.029290886595845222, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 5568, loss: 0.007477486040443182, acc: 0.99, recall: 0.99, precision: 0.9901960784313726, f_beta: 0.98999899989999
train: step: 5569, loss: 0.013907389715313911, acc: 0.99, recall: 0.99, precision: 0.9901960784313726, f_beta: 0.98999899989999
train: step: 5570, loss: 0.03150894492864609, acc: 0.96, recall: 0.96, precision: 0.9607371794871795, f_beta: 0.959983993597439
train: step: 5571, loss: 0.012482195161283016, acc: 0.99, recall: 0.99, precision: 0.9901960784313726, f_beta: 0.98999899989999
train: step: 5572, loss: 0.025583282113075256, acc: 0.97, recall: 0.97, precision: 0.9716981132075472, f_beta: 0.9699729756781104
train: step: 5573, loss: 0.013655470684170723, acc: 0.99, recall: 0.99, precision: 0.9901960784313726, f_beta: 0.98999899989999
train: step: 5574, loss: 0.012128814123570919, acc: 0.99, recall: 0.99, precision: 0.9901960784313726, f_beta: 0.98999899989999
train: step: 5575, loss: 0.03445091471076012, acc: 0.96, recall: 0.96, precision: 0.962962962962963, f_beta: 0.9599358974358974
train: step: 5576, loss: 0.04128948226571083, acc: 0.96, recall: 0.96, precision: 0.962962962962963, f_beta: 0.9599358974358974
train: step: 5577, loss: 0.054174043238162994, acc: 0.94, recall: 0.94, precision: 0.9407051282051282, f_beta: 0.9399759903961584
train: step: 5578, loss: 0.018763117492198944, acc: 0.98, recall: 0.98, precision: 0.98, f_beta: 0.98
train: step: 5579, loss: 0.005111119709908962, acc: 0.99, recall: 0.99, precision: 0.9901960784313726, f_beta: 0.98999899989999
train: step: 5580, loss: 0.011445419862866402, acc: 0.99, recall: 0.99, precision: 0.9901960784313726, f_beta: 0.98999899989999
train: step: 5581, loss: 0.02412053570151329, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 5582, loss: 0.013689293526113033, acc: 0.98, recall: 0.98, precision: 0.98, f_beta: 0.98
train: step: 5583, loss: 0.043091896921396255, acc: 0.95, recall: 0.95, precision: 0.9516258530710557, f_beta: 0.949954959463517
train: step: 5584, loss: 0.0185256190598011, acc: 0.98, recall: 0.98, precision: 0.98, f_beta: 0.98
train: step: 5585, loss: 0.03233851492404938, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 5586, loss: 0.019871093332767487, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 5587, loss: 0.010626520961523056, acc: 0.99, recall: 0.99, precision: 0.9901960784313726, f_beta: 0.98999899989999
train: step: 5588, loss: 0.0007935176836326718, acc: 1.0, recall: 1.0, precision: 1.0, f_beta: 1.0
train: step: 5589, loss: 0.010372191667556763, acc: 0.99, recall: 0.99, precision: 0.9901960784313726, f_beta: 0.98999899989999
train: step: 5590, loss: 0.012005217373371124, acc: 0.99, recall: 0.99, precision: 0.9901960784313726, f_beta: 0.98999899989999
train: step: 5591, loss: 0.0003235882904846221, acc: 1.0, recall: 1.0, precision: 1.0, f_beta: 1.0
train: step: 5592, loss: 0.013016870245337486, acc: 0.99, recall: 0.99, precision: 0.9901960784313726, f_beta: 0.98999899989999
train: step: 5593, loss: 0.03246532380580902, acc: 0.95, recall: 0.95, precision: 0.9545454545454546, f_beta: 0.949874686716792
train: step: 5594, loss: 0.020208287984132767, acc: 0.96, recall: 0.96, precision: 0.962962962962963, f_beta: 0.9599358974358974
train: step: 5595, loss: 0.0066359019838273525, acc: 0.99, recall: 0.99, precision: 0.9901960784313726, f_beta: 0.98999899989999
train: step: 5596, loss: 0.003701575333252549, acc: 1.0, recall: 1.0, precision: 1.0, f_beta: 1.0
train: step: 5597, loss: 0.024546276777982712, acc: 0.96, recall: 0.96, precision: 0.962962962962963, f_beta: 0.9599358974358974
train: step: 5598, loss: 0.0016829577507451177, acc: 1.0, recall: 1.0, precision: 1.0, f_beta: 1.0
train: step: 5599, loss: 0.02587760053575039, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 5600, loss: 0.019468432292342186, acc: 0.97, recall: 0.97, precision: 0.9716981132075472, f_beta: 0.9699729756781104
train: step: 5601, loss: 0.022652048617601395, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 5602, loss: 0.051593117415905, acc: 0.94, recall: 0.94, precision: 0.9407051282051282, f_beta: 0.9399759903961584
train: step: 5603, loss: 0.056696753948926926, acc: 0.93, recall: 0.9299999999999999, precision: 0.9315535929345644, f_beta: 0.9299369432489241
train: step: 5604, loss: 0.03257245570421219, acc: 0.94, recall: 0.94, precision: 0.9407051282051282, f_beta: 0.9399759903961584
train: step: 5605, loss: 0.00018352169718127698, acc: 1.0, recall: 1.0, precision: 1.0, f_beta: 1.0
train: step: 5606, loss: 0.013021527789533138, acc: 0.99, recall: 0.99, precision: 0.9901960784313726, f_beta: 0.98999899989999
train: step: 5607, loss: 0.029561512172222137, acc: 0.97, recall: 0.97, precision: 0.9716981132075472, f_beta: 0.9699729756781104
train: step: 5608, loss: 0.015881381928920746, acc: 0.98, recall: 0.98, precision: 0.98, f_beta: 0.98
train: step: 5609, loss: 0.055034711956977844, acc: 0.93, recall: 0.9299999999999999, precision: 0.9315535929345644, f_beta: 0.9299369432489241
train: step: 5610, loss: 0.010408228263258934, acc: 0.99, recall: 0.99, precision: 0.9901960784313726, f_beta: 0.98999899989999
train: step: 5611, loss: 0.03223491087555885, acc: 0.96, recall: 0.96, precision: 0.96, f_beta: 0.96
train: step: 5612, loss: 0.018367744982242584, acc: 0.98, recall: 0.98, precision: 0.98, f_beta: 0.98
train: step: 5613, loss: 0.027191057801246643, acc: 0.96, recall: 0.96, precision: 0.9607371794871795, f_beta: 0.959983993597439
train: step: 5614, loss: 0.02359556220471859, acc: 0.98, recall: 0.98, precision: 0.98, f_beta: 0.98
train: step: 5615, loss: 0.010501958429813385, acc: 0.99, recall: 0.99, precision: 0.9901960784313726, f_beta: 0.98999899989999
train: step: 5616, loss: 0.025139860808849335, acc: 0.98, recall: 0.98, precision: 0.98, f_beta: 0.98
train: step: 5617, loss: 0.01559380255639553, acc: 0.98, recall: 0.98, precision: 0.98, f_beta: 0.98
train: step: 5618, loss: 0.025098757818341255, acc: 0.96, recall: 0.96, precision: 0.9607371794871795, f_beta: 0.959983993597439
train: step: 5619, loss: 0.011447494849562645, acc: 0.99, recall: 0.99, precision: 0.9901960784313726, f_beta: 0.98999899989999
train: step: 5620, loss: 0.04513225704431534, acc: 0.96, recall: 0.96, precision: 0.9607371794871795, f_beta: 0.959983993597439
train: step: 5621, loss: 0.00675380090251565, acc: 0.99, recall: 0.99, precision: 0.9901960784313726, f_beta: 0.98999899989999
train: step: 5622, loss: 0.019867224618792534, acc: 0.97, recall: 0.97, precision: 0.9716981132075472, f_beta: 0.9699729756781104
train: step: 5623, loss: 0.011067142710089684, acc: 0.99, recall: 0.99, precision: 0.9901960784313726, f_beta: 0.98999899989999
train: step: 5624, loss: 0.04467380419373512, acc: 0.95, recall: 0.95, precision: 0.9516258530710557, f_beta: 0.949954959463517
train: step: 5625, loss: 0.013801122084259987, acc: 0.98, recall: 0.98, precision: 0.98, f_beta: 0.98
train: step: 5626, loss: 0.03261527419090271, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 5627, loss: 0.044751472771167755, acc: 0.95, recall: 0.95, precision: 0.9516258530710557, f_beta: 0.949954959463517
train: step: 5628, loss: 0.014694484882056713, acc: 0.99, recall: 0.99, precision: 0.9901960784313726, f_beta: 0.98999899989999
train: step: 5629, loss: 0.04654671996831894, acc: 0.95, recall: 0.95, precision: 0.9516258530710557, f_beta: 0.949954959463517
train: step: 5630, loss: 0.037227813154459, acc: 0.94, recall: 0.94, precision: 0.9428341384863124, f_beta: 0.9399038461538463
train: step: 5631, loss: 0.052923522889614105, acc: 0.94, recall: 0.94, precision: 0.9407051282051282, f_beta: 0.9399759903961584
train: step: 5632, loss: 0.02290026657283306, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 5633, loss: 0.028521446511149406, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 5634, loss: 0.01904529705643654, acc: 0.98, recall: 0.98, precision: 0.9807692307692308, f_beta: 0.9799919967987194
train: step: 5635, loss: 0.03168309107422829, acc: 0.96, recall: 0.96, precision: 0.96, f_beta: 0.96
train: step: 5636, loss: 0.029984254390001297, acc: 0.97, recall: 0.97, precision: 0.9716981132075472, f_beta: 0.9699729756781104
train: step: 5637, loss: 0.005773233249783516, acc: 0.99, recall: 0.99, precision: 0.9901960784313726, f_beta: 0.98999899989999
train: step: 5638, loss: 0.021678833290934563, acc: 0.98, recall: 0.98, precision: 0.9807692307692308, f_beta: 0.9799919967987194
train: step: 5639, loss: 0.015917189419269562, acc: 0.99, recall: 0.99, precision: 0.9901960784313726, f_beta: 0.98999899989999
train: step: 5640, loss: 0.06212712451815605, acc: 0.92, recall: 0.9199999999999999, precision: 0.9227053140096618, f_beta: 0.9198717948717948
train: step: 5641, loss: 0.012382418848574162, acc: 0.98, recall: 0.98, precision: 0.9807692307692308, f_beta: 0.9799919967987194
train: step: 5642, loss: 0.004923961590975523, acc: 0.99, recall: 0.99, precision: 0.9901960784313726, f_beta: 0.98999899989999
train: step: 5643, loss: 0.04112536087632179, acc: 0.95, recall: 0.95, precision: 0.9501800720288115, f_beta: 0.9499949994999499
train: step: 5644, loss: 0.01662290282547474, acc: 0.97, recall: 0.97, precision: 0.9716981132075472, f_beta: 0.9699729756781104
train: step: 5645, loss: 0.047652941197156906, acc: 0.94, recall: 0.94, precision: 0.94, f_beta: 0.94
train: step: 5646, loss: 0.02067442424595356, acc: 0.97, recall: 0.97, precision: 0.9716981132075472, f_beta: 0.9699729756781104
train: step: 5647, loss: 0.020454054698348045, acc: 0.98, recall: 0.98, precision: 0.98, f_beta: 0.98
train: step: 5648, loss: 0.03340453281998634, acc: 0.96, recall: 0.96, precision: 0.9607371794871795, f_beta: 0.959983993597439
train: step: 5649, loss: 0.021696925163269043, acc: 0.98, recall: 0.98, precision: 0.98, f_beta: 0.98
train: step: 5650, loss: 0.028103427961468697, acc: 0.96, recall: 0.96, precision: 0.962962962962963, f_beta: 0.9599358974358974
train: step: 5651, loss: 0.014459737576544285, acc: 0.98, recall: 0.98, precision: 0.9807692307692308, f_beta: 0.9799919967987194
train: step: 5652, loss: 0.007981035858392715, acc: 0.99, recall: 0.99, precision: 0.9901960784313726, f_beta: 0.98999899989999
train: step: 5653, loss: 0.004142672289162874, acc: 1.0, recall: 1.0, precision: 1.0, f_beta: 1.0
train: step: 5654, loss: 0.020722348242998123, acc: 0.98, recall: 0.98, precision: 0.98, f_beta: 0.98
train: step: 5655, loss: 0.03823713958263397, acc: 0.95, recall: 0.95, precision: 0.9516258530710557, f_beta: 0.949954959463517
train: step: 5656, loss: 0.027507038787007332, acc: 0.97, recall: 0.97, precision: 0.9716981132075472, f_beta: 0.9699729756781104
train: step: 5657, loss: 0.047423653304576874, acc: 0.94, recall: 0.94, precision: 0.94, f_beta: 0.94
train: step: 5658, loss: 0.062430571764707565, acc: 0.91, recall: 0.9099999999999999, precision: 0.9114813327980731, f_beta: 0.9099189270343309
train: step: 5659, loss: 0.018515942618250847, acc: 0.98, recall: 0.98, precision: 0.9807692307692308, f_beta: 0.9799919967987194
train: step: 5660, loss: 0.047161735594272614, acc: 0.94, recall: 0.94, precision: 0.9407051282051282, f_beta: 0.9399759903961584
train: step: 5661, loss: 0.018221383914351463, acc: 0.98, recall: 0.98, precision: 0.9807692307692308, f_beta: 0.9799919967987194
train: step: 5662, loss: 0.009925267659127712, acc: 0.99, recall: 0.99, precision: 0.9901960784313726, f_beta: 0.98999899989999
train: step: 5663, loss: 0.008331049233675003, acc: 0.99, recall: 0.99, precision: 0.9901960784313726, f_beta: 0.98999899989999
train: step: 5664, loss: 0.03898491710424423, acc: 0.95, recall: 0.95, precision: 0.9501800720288115, f_beta: 0.9499949994999499
train: step: 5665, loss: 0.027271460741758347, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 5666, loss: 0.017778605222702026, acc: 0.98, recall: 0.98, precision: 0.9807692307692308, f_beta: 0.9799919967987194
train: step: 5667, loss: 0.05244440212845802, acc: 0.94, recall: 0.94, precision: 0.9428341384863124, f_beta: 0.9399038461538463
train: step: 5668, loss: 0.003510458394885063, acc: 1.0, recall: 1.0, precision: 1.0, f_beta: 1.0
train: step: 5669, loss: 0.011771533638238907, acc: 0.98, recall: 0.98, precision: 0.9807692307692308, f_beta: 0.9799919967987194
train: step: 5670, loss: 0.022110821679234505, acc: 0.98, recall: 0.98, precision: 0.98, f_beta: 0.98
train: step: 5671, loss: 0.05566249042749405, acc: 0.94, recall: 0.94, precision: 0.9407051282051282, f_beta: 0.9399759903961584
train: step: 5672, loss: 0.023392414674162865, acc: 0.97, recall: 0.97, precision: 0.9716981132075472, f_beta: 0.9699729756781104
train: step: 5673, loss: 0.01897299848496914, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 5674, loss: 0.019634777680039406, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 5675, loss: 0.02395012602210045, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 5676, loss: 0.03747120127081871, acc: 0.96, recall: 0.96, precision: 0.9607371794871795, f_beta: 0.959983993597439
train: step: 5677, loss: 0.0422464944422245, acc: 0.95, recall: 0.95, precision: 0.9516258530710557, f_beta: 0.949954959463517
train: step: 5678, loss: 0.039874568581581116, acc: 0.95, recall: 0.95, precision: 0.9545454545454546, f_beta: 0.949874686716792
train: step: 5679, loss: 0.027206692844629288, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 5680, loss: 0.040923167020082474, acc: 0.94, recall: 0.94, precision: 0.94, f_beta: 0.94
train: step: 5681, loss: 0.04205102100968361, acc: 0.94, recall: 0.94, precision: 0.9407051282051282, f_beta: 0.9399759903961584
train: step: 5682, loss: 0.021010423079133034, acc: 0.98, recall: 0.98, precision: 0.9807692307692308, f_beta: 0.9799919967987194
train: step: 5683, loss: 0.0074212695471942425, acc: 0.99, recall: 0.99, precision: 0.9901960784313726, f_beta: 0.98999899989999
train: step: 5684, loss: 0.03410940617322922, acc: 0.97, recall: 0.97, precision: 0.9716981132075472, f_beta: 0.9699729756781104
train: step: 5685, loss: 0.015538768842816353, acc: 0.99, recall: 0.99, precision: 0.9901960784313726, f_beta: 0.98999899989999
train: step: 5686, loss: 0.012171314097940922, acc: 0.98, recall: 0.98, precision: 0.98, f_beta: 0.98
train: step: 5687, loss: 0.020252924412488937, acc: 0.98, recall: 0.98, precision: 0.98, f_beta: 0.98
train: step: 5688, loss: 0.02520279958844185, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 5689, loss: 0.037427376955747604, acc: 0.95, recall: 0.95, precision: 0.9501800720288115, f_beta: 0.9499949994999499
train: step: 5690, loss: 0.027770470827817917, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 5691, loss: 0.018087616190314293, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 5692, loss: 0.025822997093200684, acc: 0.96, recall: 0.96, precision: 0.9607371794871795, f_beta: 0.959983993597439
train: step: 5693, loss: 0.02802453003823757, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 5694, loss: 0.010574369691312313, acc: 0.99, recall: 0.99, precision: 0.9901960784313726, f_beta: 0.98999899989999
train: step: 5695, loss: 0.019213397055864334, acc: 0.98, recall: 0.98, precision: 0.9807692307692308, f_beta: 0.9799919967987194
train: step: 5696, loss: 0.053447410464286804, acc: 0.94, recall: 0.94, precision: 0.9407051282051282, f_beta: 0.9399759903961584
train: step: 5697, loss: 0.00029167544562369585, acc: 1.0, recall: 1.0, precision: 1.0, f_beta: 1.0
train: step: 5698, loss: 0.026596534997224808, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 5699, loss: 0.004607108887284994, acc: 0.99, recall: 0.99, precision: 0.9901960784313726, f_beta: 0.98999899989999
train: step: 5700, loss: 0.003184120636433363, acc: 1.0, recall: 1.0, precision: 1.0, f_beta: 1.0
train: step: 5701, loss: 0.0101137924939394, acc: 0.98, recall: 0.98, precision: 0.98, f_beta: 0.98
train: step: 5702, loss: 0.04237351939082146, acc: 0.95, recall: 0.95, precision: 0.9501800720288115, f_beta: 0.9499949994999499
train: step: 5703, loss: 0.008556663990020752, acc: 0.99, recall: 0.99, precision: 0.9901960784313726, f_beta: 0.98999899989999
train: step: 5704, loss: 0.029691820964217186, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 5705, loss: 0.010853278450667858, acc: 0.99, recall: 0.99, precision: 0.9901960784313726, f_beta: 0.98999899989999
train: step: 5706, loss: 0.039862457662820816, acc: 0.95, recall: 0.95, precision: 0.9501800720288115, f_beta: 0.9499949994999499
train: step: 5707, loss: 0.016000591218471527, acc: 0.98, recall: 0.98, precision: 0.98, f_beta: 0.98
train: step: 5708, loss: 0.012856431305408478, acc: 0.99, recall: 0.99, precision: 0.9901960784313726, f_beta: 0.98999899989999
train: step: 5709, loss: 0.012600510381162167, acc: 0.99, recall: 0.99, precision: 0.9901960784313726, f_beta: 0.98999899989999
train: step: 5710, loss: 0.04420344531536102, acc: 0.95, recall: 0.95, precision: 0.9516258530710557, f_beta: 0.949954959463517
train: step: 5711, loss: 0.021093493327498436, acc: 0.97, recall: 0.97, precision: 0.9716981132075472, f_beta: 0.9699729756781104
train: step: 5712, loss: 0.026197977364063263, acc: 0.97, recall: 0.97, precision: 0.9716981132075472, f_beta: 0.9699729756781104
train: step: 5713, loss: 0.013722389005124569, acc: 0.99, recall: 0.99, precision: 0.9901960784313726, f_beta: 0.98999899989999
train: step: 5714, loss: 0.034649480134248734, acc: 0.96, recall: 0.96, precision: 0.96, f_beta: 0.96
train: step: 5715, loss: 0.039150405675172806, acc: 0.96, recall: 0.96, precision: 0.962962962962963, f_beta: 0.9599358974358974
train: step: 5716, loss: 0.030394960194826126, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 5717, loss: 0.0524294339120388, acc: 0.93, recall: 0.9299999999999999, precision: 0.930172068827531, f_beta: 0.9299929992999301
train: step: 5718, loss: 0.03600452095270157, acc: 0.96, recall: 0.96, precision: 0.96, f_beta: 0.96
train: step: 5719, loss: 0.010065295733511448, acc: 0.99, recall: 0.99, precision: 0.9901960784313726, f_beta: 0.98999899989999
train: step: 5720, loss: 0.010585447773337364, acc: 0.99, recall: 0.99, precision: 0.9901960784313726, f_beta: 0.98999899989999
train: step: 5721, loss: 0.009958572685718536, acc: 0.99, recall: 0.99, precision: 0.9901960784313726, f_beta: 0.98999899989999
train: step: 5722, loss: 0.009693223051726818, acc: 0.99, recall: 0.99, precision: 0.9901960784313726, f_beta: 0.98999899989999
train: step: 5723, loss: 0.04014144092798233, acc: 0.95, recall: 0.95, precision: 0.9516258530710557, f_beta: 0.949954959463517
train: step: 5724, loss: 0.002664490370079875, acc: 1.0, recall: 1.0, precision: 1.0, f_beta: 1.0
train: step: 5725, loss: 0.018518583849072456, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 5726, loss: 0.02696928195655346, acc: 0.97, recall: 0.97, precision: 0.9716981132075472, f_beta: 0.9699729756781104
train: step: 5727, loss: 0.014529986307024956, acc: 0.98, recall: 0.98, precision: 0.98, f_beta: 0.98
train: step: 5728, loss: 0.01716945879161358, acc: 0.98, recall: 0.98, precision: 0.98, f_beta: 0.98
train: step: 5729, loss: 9.321777179138735e-05, acc: 1.0, recall: 1.0, precision: 1.0, f_beta: 1.0
train: step: 5730, loss: 0.021391134709119797, acc: 0.96, recall: 0.96, precision: 0.962962962962963, f_beta: 0.9599358974358974
train: step: 5731, loss: 0.021806707605719566, acc: 0.98, recall: 0.98, precision: 0.98, f_beta: 0.98
train: step: 5732, loss: 0.014864897355437279, acc: 0.98, recall: 0.98, precision: 0.9807692307692308, f_beta: 0.9799919967987194
train: step: 5733, loss: 0.04667629301548004, acc: 0.95, recall: 0.95, precision: 0.9501800720288115, f_beta: 0.9499949994999499
train: step: 5734, loss: 0.02129381150007248, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 5735, loss: 0.018352141603827477, acc: 0.98, recall: 0.98, precision: 0.98, f_beta: 0.98
train: step: 5736, loss: 0.01684202067553997, acc: 0.97, recall: 0.97, precision: 0.9716981132075472, f_beta: 0.9699729756781104
train: step: 5737, loss: 0.011365575715899467, acc: 0.98, recall: 0.98, precision: 0.98, f_beta: 0.98
train: step: 5738, loss: 0.013638513162732124, acc: 0.98, recall: 0.98, precision: 0.9807692307692308, f_beta: 0.9799919967987194
train: step: 5739, loss: 0.039148230105638504, acc: 0.96, recall: 0.96, precision: 0.9607371794871795, f_beta: 0.959983993597439
train: step: 5740, loss: 0.013913700357079506, acc: 0.98, recall: 0.98, precision: 0.98, f_beta: 0.98
train: step: 5741, loss: 0.024620484560728073, acc: 0.96, recall: 0.96, precision: 0.96, f_beta: 0.96
train: step: 5742, loss: 0.04241020604968071, acc: 0.95, recall: 0.95, precision: 0.9501800720288115, f_beta: 0.9499949994999499
train: step: 5743, loss: 0.021917754784226418, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 5744, loss: 0.023983092978596687, acc: 0.98, recall: 0.98, precision: 0.9807692307692308, f_beta: 0.9799919967987194
train: step: 5745, loss: 0.0054419804364442825, acc: 0.99, recall: 0.99, precision: 0.9901960784313726, f_beta: 0.98999899989999
train: step: 5746, loss: 0.0034496814478188753, acc: 1.0, recall: 1.0, precision: 1.0, f_beta: 1.0
train: step: 5747, loss: 0.010198934003710747, acc: 0.99, recall: 0.99, precision: 0.9901960784313726, f_beta: 0.98999899989999
train: step: 5748, loss: 0.019248254597187042, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 5749, loss: 0.009164372459053993, acc: 0.99, recall: 0.99, precision: 0.9901960784313726, f_beta: 0.98999899989999
train: step: 5750, loss: 0.029898252338171005, acc: 0.95, recall: 0.95, precision: 0.9516258530710557, f_beta: 0.949954959463517
train: step: 5751, loss: 0.011729937046766281, acc: 0.99, recall: 0.99, precision: 0.9901960784313726, f_beta: 0.98999899989999
train: step: 5752, loss: 0.019043127074837685, acc: 0.98, recall: 0.98, precision: 0.98, f_beta: 0.98
train: step: 5753, loss: 0.03841301053762436, acc: 0.95, recall: 0.95, precision: 0.9516258530710557, f_beta: 0.949954959463517
train: step: 5754, loss: 0.004057505168020725, acc: 1.0, recall: 1.0, precision: 1.0, f_beta: 1.0
train: step: 5755, loss: 0.012151582166552544, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 5756, loss: 0.020682893693447113, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 5757, loss: 0.02312961034476757, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 5758, loss: 0.01316270511597395, acc: 0.98, recall: 0.98, precision: 0.9807692307692308, f_beta: 0.9799919967987194
train: step: 5759, loss: 0.01618819497525692, acc: 0.98, recall: 0.98, precision: 0.9807692307692308, f_beta: 0.9799919967987194
train: step: 5760, loss: 0.023617148399353027, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 5761, loss: 0.024856891483068466, acc: 0.96, recall: 0.96, precision: 0.9607371794871795, f_beta: 0.959983993597439
train: step: 5762, loss: 0.04023199528455734, acc: 0.96, recall: 0.96, precision: 0.9607371794871795, f_beta: 0.959983993597439
train: step: 5763, loss: 0.04780836030840874, acc: 0.93, recall: 0.9299999999999999, precision: 0.9315535929345644, f_beta: 0.9299369432489241
train: step: 5764, loss: 0.0018412311328575015, acc: 1.0, recall: 1.0, precision: 1.0, f_beta: 1.0
train: step: 5765, loss: 0.06069697439670563, acc: 0.93, recall: 0.9299999999999999, precision: 0.9343434343434343, f_beta: 0.9298245614035088
train: step: 5766, loss: 0.040677908807992935, acc: 0.96, recall: 0.96, precision: 0.962962962962963, f_beta: 0.9599358974358974
train: step: 5767, loss: 0.0557732954621315, acc: 0.91, recall: 0.9099999999999999, precision: 0.9114813327980731, f_beta: 0.9099189270343309
train: step: 5768, loss: 0.010982956737279892, acc: 0.99, recall: 0.99, precision: 0.9901960784313726, f_beta: 0.98999899989999
train: step: 5769, loss: 0.01996416226029396, acc: 0.98, recall: 0.98, precision: 0.98, f_beta: 0.98
train: step: 5770, loss: 0.013974891044199467, acc: 0.98, recall: 0.98, precision: 0.9807692307692308, f_beta: 0.9799919967987194
train: step: 5771, loss: 0.029230168089270592, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 5772, loss: 0.013724608346819878, acc: 0.98, recall: 0.98, precision: 0.98, f_beta: 0.98
train: step: 5773, loss: 0.0693088173866272, acc: 0.92, recall: 0.92, precision: 0.92, f_beta: 0.92
train: step: 5774, loss: 0.02101508341729641, acc: 0.96, recall: 0.96, precision: 0.96, f_beta: 0.96
train: step: 5775, loss: 0.06345941871404648, acc: 0.92, recall: 0.92, precision: 0.92, f_beta: 0.92
train: step: 5776, loss: 0.00582456961274147, acc: 0.99, recall: 0.99, precision: 0.9901960784313726, f_beta: 0.98999899989999
train: step: 5777, loss: 0.028409942984580994, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 5778, loss: 0.0009084220509976149, acc: 1.0, recall: 1.0, precision: 1.0, f_beta: 1.0
train: step: 5779, loss: 0.018913662061095238, acc: 0.98, recall: 0.98, precision: 0.9807692307692308, f_beta: 0.9799919967987194
train: step: 5780, loss: 0.0216730497777462, acc: 0.98, recall: 0.98, precision: 0.98, f_beta: 0.98
train: step: 5781, loss: 0.02992750145494938, acc: 0.96, recall: 0.96, precision: 0.96, f_beta: 0.96
train: step: 5782, loss: 0.03683603182435036, acc: 0.94, recall: 0.94, precision: 0.94, f_beta: 0.94
train: step: 5783, loss: 0.008718602359294891, acc: 0.99, recall: 0.99, precision: 0.9901960784313726, f_beta: 0.98999899989999
train: step: 5784, loss: 0.010390695184469223, acc: 0.99, recall: 0.99, precision: 0.9901960784313726, f_beta: 0.98999899989999
train: step: 5785, loss: 0.000727592094335705, acc: 1.0, recall: 1.0, precision: 1.0, f_beta: 1.0
train: step: 5786, loss: 0.030465537682175636, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 5787, loss: 0.010890436358749866, acc: 0.99, recall: 0.99, precision: 0.9901960784313726, f_beta: 0.98999899989999
train: step: 5788, loss: 0.057577118277549744, acc: 0.93, recall: 0.9299999999999999, precision: 0.9343434343434343, f_beta: 0.9298245614035088
train: step: 5789, loss: 0.03576137125492096, acc: 0.94, recall: 0.94, precision: 0.9428341384863124, f_beta: 0.9399038461538463
train: step: 5790, loss: 0.018029309809207916, acc: 0.98, recall: 0.98, precision: 0.9807692307692308, f_beta: 0.9799919967987194
train: step: 5791, loss: 0.020356401801109314, acc: 0.98, recall: 0.98, precision: 0.9807692307692308, f_beta: 0.9799919967987194
train: step: 5792, loss: 0.017459847033023834, acc: 0.97, recall: 0.97, precision: 0.9716981132075472, f_beta: 0.9699729756781104
train: step: 5793, loss: 0.02050820179283619, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 5794, loss: 0.007275507319718599, acc: 0.98, recall: 0.98, precision: 0.9807692307692308, f_beta: 0.9799919967987194
train: step: 5795, loss: 0.03988927975296974, acc: 0.94, recall: 0.94, precision: 0.9428341384863124, f_beta: 0.9399038461538463
train: step: 5796, loss: 0.010125899687409401, acc: 0.99, recall: 0.99, precision: 0.9901960784313726, f_beta: 0.98999899989999
train: step: 5797, loss: 0.06107933074235916, acc: 0.93, recall: 0.9299999999999999, precision: 0.9343434343434343, f_beta: 0.9298245614035088
train: step: 5798, loss: 0.03810659795999527, acc: 0.94, recall: 0.94, precision: 0.9407051282051282, f_beta: 0.9399759903961584
train: step: 5799, loss: 0.05996336042881012, acc: 0.92, recall: 0.9199999999999999, precision: 0.9206730769230769, f_beta: 0.919967987194878
train: step: 5800, loss: 0.0022744613233953714, acc: 1.0, recall: 1.0, precision: 1.0, f_beta: 1.0
train: step: 5801, loss: 0.03844518959522247, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 5802, loss: 0.010185393504798412, acc: 0.99, recall: 0.99, precision: 0.9901960784313726, f_beta: 0.98999899989999
train: step: 5803, loss: 0.02497924119234085, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 5804, loss: 0.001810278045013547, acc: 1.0, recall: 1.0, precision: 1.0, f_beta: 1.0
train: step: 5805, loss: 0.03109346702694893, acc: 0.96, recall: 0.96, precision: 0.9607371794871795, f_beta: 0.959983993597439
train: step: 5806, loss: 0.010962401516735554, acc: 0.99, recall: 0.99, precision: 0.9901960784313726, f_beta: 0.98999899989999
train: step: 5807, loss: 0.023198986425995827, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 5808, loss: 0.01704835705459118, acc: 0.98, recall: 0.98, precision: 0.9807692307692308, f_beta: 0.9799919967987194
train: step: 5809, loss: 0.0471235066652298, acc: 0.95, recall: 0.95, precision: 0.9516258530710557, f_beta: 0.949954959463517
train: step: 5810, loss: 0.010252188891172409, acc: 0.98, recall: 0.98, precision: 0.98, f_beta: 0.98
train: step: 5811, loss: 0.046592775732278824, acc: 0.94, recall: 0.94, precision: 0.9407051282051282, f_beta: 0.9399759903961584
train: step: 5812, loss: 0.03217150643467903, acc: 0.96, recall: 0.96, precision: 0.9607371794871795, f_beta: 0.959983993597439
train: step: 5813, loss: 0.00977803859859705, acc: 0.99, recall: 0.99, precision: 0.9901960784313726, f_beta: 0.98999899989999
train: step: 5814, loss: 0.01334135327488184, acc: 0.99, recall: 0.99, precision: 0.9901960784313726, f_beta: 0.98999899989999
train: step: 5815, loss: 0.06207110360264778, acc: 0.94, recall: 0.94, precision: 0.9428341384863124, f_beta: 0.9399038461538463
train: step: 5816, loss: 0.0028890306130051613, acc: 1.0, recall: 1.0, precision: 1.0, f_beta: 1.0
train: step: 5817, loss: 0.03561076521873474, acc: 0.97, recall: 0.97, precision: 0.9716981132075472, f_beta: 0.9699729756781104
train: step: 5818, loss: 0.013595457188785076, acc: 0.99, recall: 0.99, precision: 0.9901960784313726, f_beta: 0.98999899989999
train: step: 5819, loss: 0.030878204852342606, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 5820, loss: 0.01218552328646183, acc: 0.98, recall: 0.98, precision: 0.9807692307692308, f_beta: 0.9799919967987194
train: step: 5821, loss: 0.026420865207910538, acc: 0.97, recall: 0.97, precision: 0.9716981132075472, f_beta: 0.9699729756781104
train: step: 5822, loss: 0.0119966184720397, acc: 0.99, recall: 0.99, precision: 0.9901960784313726, f_beta: 0.98999899989999
train: step: 5823, loss: 0.011704130098223686, acc: 0.99, recall: 0.99, precision: 0.9901960784313726, f_beta: 0.98999899989999
train: step: 5824, loss: 0.012282013893127441, acc: 0.99, recall: 0.99, precision: 0.9901960784313726, f_beta: 0.98999899989999
train: step: 5825, loss: 0.031691040843725204, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 5826, loss: 0.040666721761226654, acc: 0.96, recall: 0.96, precision: 0.9607371794871795, f_beta: 0.959983993597439
train: step: 5827, loss: 0.028664248064160347, acc: 0.97, recall: 0.97, precision: 0.9716981132075472, f_beta: 0.9699729756781104
train: step: 5828, loss: 0.01740466058254242, acc: 0.98, recall: 0.98, precision: 0.9807692307692308, f_beta: 0.9799919967987194
train: step: 5829, loss: 0.01740417443215847, acc: 0.98, recall: 0.98, precision: 0.9807692307692308, f_beta: 0.9799919967987194
train: step: 5830, loss: 0.0265592522919178, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 5831, loss: 0.021708624437451363, acc: 0.98, recall: 0.98, precision: 0.9807692307692308, f_beta: 0.9799919967987194
train: step: 5832, loss: 0.005635986104607582, acc: 0.99, recall: 0.99, precision: 0.9901960784313726, f_beta: 0.98999899989999
train: step: 5833, loss: 0.011954046785831451, acc: 0.98, recall: 0.98, precision: 0.9807692307692308, f_beta: 0.9799919967987194
train: step: 5834, loss: 0.013787376694381237, acc: 0.99, recall: 0.99, precision: 0.9901960784313726, f_beta: 0.98999899989999
train: step: 5835, loss: 0.03687090426683426, acc: 0.96, recall: 0.96, precision: 0.9607371794871795, f_beta: 0.959983993597439
train: step: 5836, loss: 0.00536624900996685, acc: 0.99, recall: 0.99, precision: 0.9901960784313726, f_beta: 0.98999899989999
train: step: 5837, loss: 0.023290829733014107, acc: 0.98, recall: 0.98, precision: 0.9807692307692308, f_beta: 0.9799919967987194
train: step: 5838, loss: 0.00890620332211256, acc: 0.99, recall: 0.99, precision: 0.9901960784313726, f_beta: 0.98999899989999
train: step: 5839, loss: 0.03037242963910103, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 5840, loss: 0.005020341370254755, acc: 0.99, recall: 0.99, precision: 0.9901960784313726, f_beta: 0.98999899989999
train: step: 5841, loss: 0.025502853095531464, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 5842, loss: 0.03027510643005371, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 5843, loss: 0.030889147892594337, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 5844, loss: 0.014543343335390091, acc: 1.0, recall: 1.0, precision: 1.0, f_beta: 1.0
train: step: 5845, loss: 0.018283428624272346, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 5846, loss: 0.027346841990947723, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 5847, loss: 0.005055911373347044, acc: 0.99, recall: 0.99, precision: 0.9901960784313726, f_beta: 0.98999899989999
train: step: 5848, loss: 0.03438971936702728, acc: 0.95, recall: 0.95, precision: 0.9516258530710557, f_beta: 0.949954959463517
train: step: 5849, loss: 0.02017386257648468, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 5850, loss: 0.0138402059674263, acc: 0.98, recall: 0.98, precision: 0.9807692307692308, f_beta: 0.9799919967987194
train: step: 5851, loss: 0.034484416246414185, acc: 0.96, recall: 0.96, precision: 0.9607371794871795, f_beta: 0.959983993597439
train: step: 5852, loss: 0.02419194206595421, acc: 0.96, recall: 0.96, precision: 0.9607371794871795, f_beta: 0.959983993597439
train: step: 5853, loss: 0.02457611635327339, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 5854, loss: 0.009242288768291473, acc: 0.99, recall: 0.99, precision: 0.9901960784313726, f_beta: 0.98999899989999
train: step: 5855, loss: 0.01718456670641899, acc: 0.98, recall: 0.98, precision: 0.9807692307692308, f_beta: 0.9799919967987194
train: step: 5856, loss: 0.02891474775969982, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 5857, loss: 0.03375943377614021, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 5858, loss: 0.01073330920189619, acc: 0.98, recall: 0.98, precision: 0.9807692307692308, f_beta: 0.9799919967987194
train: step: 5859, loss: 0.0160282664000988, acc: 0.98, recall: 0.98, precision: 0.98, f_beta: 0.98
train: step: 5860, loss: 0.026434021070599556, acc: 0.97, recall: 0.97, precision: 0.9716981132075472, f_beta: 0.9699729756781104
train: step: 5861, loss: 0.014565098099410534, acc: 0.98, recall: 0.98, precision: 0.98, f_beta: 0.98
train: step: 5862, loss: 0.024446258321404457, acc: 0.96, recall: 0.96, precision: 0.96, f_beta: 0.96
train: step: 5863, loss: 0.022391414269804955, acc: 0.98, recall: 0.98, precision: 0.9807692307692308, f_beta: 0.9799919967987194
train: step: 5864, loss: 0.029193168506026268, acc: 0.97, recall: 0.97, precision: 0.9716981132075472, f_beta: 0.9699729756781104
train: step: 5865, loss: 0.03563149273395538, acc: 0.95, recall: 0.95, precision: 0.9501800720288115, f_beta: 0.9499949994999499
train: step: 5866, loss: 0.025784490630030632, acc: 0.98, recall: 0.98, precision: 0.98, f_beta: 0.98
train: step: 5867, loss: 0.01442051026970148, acc: 0.98, recall: 0.98, precision: 0.98, f_beta: 0.98
train: step: 5868, loss: 0.008054617792367935, acc: 0.99, recall: 0.99, precision: 0.9901960784313726, f_beta: 0.98999899989999
train: step: 5869, loss: 0.03818097710609436, acc: 0.95, recall: 0.95, precision: 0.9516258530710557, f_beta: 0.949954959463517
train: step: 5870, loss: 0.02926846593618393, acc: 0.95, recall: 0.95, precision: 0.9501800720288115, f_beta: 0.9499949994999499
train: step: 5871, loss: 0.02942969836294651, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 5872, loss: 0.038809966295957565, acc: 0.96, recall: 0.96, precision: 0.96, f_beta: 0.96
train: step: 5873, loss: 0.010760868899524212, acc: 0.99, recall: 0.99, precision: 0.9901960784313726, f_beta: 0.98999899989999
train: step: 5874, loss: 0.030239352956414223, acc: 0.96, recall: 0.96, precision: 0.96, f_beta: 0.96
train: step: 5875, loss: 0.025696415454149246, acc: 0.96, recall: 0.96, precision: 0.9607371794871795, f_beta: 0.959983993597439
train: step: 5876, loss: 0.0388357974588871, acc: 0.96, recall: 0.96, precision: 0.962962962962963, f_beta: 0.9599358974358974
train: step: 5877, loss: 0.04311490058898926, acc: 0.94, recall: 0.94, precision: 0.9428341384863124, f_beta: 0.9399038461538463
train: step: 5878, loss: 0.03635932132601738, acc: 0.96, recall: 0.96, precision: 0.9607371794871795, f_beta: 0.959983993597439
train: step: 5879, loss: 0.030858254060149193, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 5880, loss: 0.044515401124954224, acc: 0.93, recall: 0.9299999999999999, precision: 0.9315535929345644, f_beta: 0.9299369432489241
train: step: 5881, loss: 0.0491303913295269, acc: 0.95, recall: 0.95, precision: 0.9516258530710557, f_beta: 0.949954959463517
train: step: 5882, loss: 0.04361076280474663, acc: 0.95, recall: 0.95, precision: 0.9516258530710557, f_beta: 0.949954959463517
train: step: 5883, loss: 0.024419672787189484, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 5884, loss: 0.02682969532907009, acc: 0.96, recall: 0.96, precision: 0.96, f_beta: 0.96
train: step: 5885, loss: 0.002237549517303705, acc: 1.0, recall: 1.0, precision: 1.0, f_beta: 1.0
train: step: 5886, loss: 0.017598910257220268, acc: 0.98, recall: 0.98, precision: 0.98, f_beta: 0.98
train: step: 5887, loss: 0.03828592598438263, acc: 0.96, recall: 0.96, precision: 0.962962962962963, f_beta: 0.9599358974358974
train: step: 5888, loss: 0.03617396950721741, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 5889, loss: 0.02673397958278656, acc: 0.98, recall: 0.98, precision: 0.98, f_beta: 0.98
train: step: 5890, loss: 0.021079614758491516, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 5891, loss: 0.022367196157574654, acc: 0.98, recall: 0.98, precision: 0.9807692307692308, f_beta: 0.9799919967987194
train: step: 5892, loss: 0.023691602051258087, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 5893, loss: 0.027302570641040802, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 5894, loss: 0.024853868409991264, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 5895, loss: 0.021899275481700897, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 5896, loss: 0.03295118734240532, acc: 0.96, recall: 0.96, precision: 0.9607371794871795, f_beta: 0.959983993597439
train: step: 5897, loss: 0.010156993754208088, acc: 0.99, recall: 0.99, precision: 0.9901960784313726, f_beta: 0.98999899989999
train: step: 5898, loss: 0.010391298681497574, acc: 0.98, recall: 0.98, precision: 0.98, f_beta: 0.98
train: step: 5899, loss: 0.01060466654598713, acc: 0.99, recall: 0.99, precision: 0.9901960784313726, f_beta: 0.98999899989999
train: step: 5900, loss: 0.013862869702279568, acc: 0.98, recall: 0.98, precision: 0.9807692307692308, f_beta: 0.9799919967987194
train: step: 5901, loss: 0.00864819623529911, acc: 0.99, recall: 0.99, precision: 0.9901960784313726, f_beta: 0.98999899989999
train: step: 5902, loss: 0.03937717527151108, acc: 0.95, recall: 0.95, precision: 0.9501800720288115, f_beta: 0.9499949994999499
train: step: 5903, loss: 0.013501989655196667, acc: 0.98, recall: 0.98, precision: 0.98, f_beta: 0.98
train: step: 5904, loss: 0.0004718781274277717, acc: 1.0, recall: 1.0, precision: 1.0, f_beta: 1.0
train: step: 5905, loss: 0.017592990770936012, acc: 0.98, recall: 0.98, precision: 0.98, f_beta: 0.98
train: step: 5906, loss: 0.0351223461329937, acc: 0.96, recall: 0.96, precision: 0.962962962962963, f_beta: 0.9599358974358974
train: step: 5907, loss: 0.0043114167638123035, acc: 0.99, recall: 0.99, precision: 0.9901960784313726, f_beta: 0.98999899989999
train: step: 5908, loss: 0.03716396912932396, acc: 0.95, recall: 0.95, precision: 0.9501800720288115, f_beta: 0.9499949994999499
train: step: 5909, loss: 0.008675482124090195, acc: 0.99, recall: 0.99, precision: 0.9901960784313726, f_beta: 0.98999899989999
train: step: 5910, loss: 0.016290752217173576, acc: 0.99, recall: 0.99, precision: 0.9901960784313726, f_beta: 0.98999899989999
train: step: 5911, loss: 0.03844144940376282, acc: 0.96, recall: 0.96, precision: 0.96, f_beta: 0.96
train: step: 5912, loss: 0.0005223244661465287, acc: 1.0, recall: 1.0, precision: 1.0, f_beta: 1.0
train: step: 5913, loss: 0.013136074878275394, acc: 0.98, recall: 0.98, precision: 0.9807692307692308, f_beta: 0.9799919967987194
train: step: 5914, loss: 0.03308708593249321, acc: 0.96, recall: 0.96, precision: 0.9607371794871795, f_beta: 0.959983993597439
train: step: 5915, loss: 0.059136275202035904, acc: 0.93, recall: 0.9299999999999999, precision: 0.9343434343434343, f_beta: 0.9298245614035088
train: step: 5916, loss: 0.021501101553440094, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 5917, loss: 0.030261944979429245, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 5918, loss: 0.016395559534430504, acc: 0.98, recall: 0.98, precision: 0.9807692307692308, f_beta: 0.9799919967987194
train: step: 5919, loss: 0.05058339983224869, acc: 0.94, recall: 0.94, precision: 0.9407051282051282, f_beta: 0.9399759903961584
train: step: 5920, loss: 0.0049524144269526005, acc: 0.99, recall: 0.99, precision: 0.9901960784313726, f_beta: 0.98999899989999
train: step: 5921, loss: 0.018454669043421745, acc: 0.96, recall: 0.96, precision: 0.962962962962963, f_beta: 0.9599358974358974
train: step: 5922, loss: 0.021954812109470367, acc: 0.98, recall: 0.98, precision: 0.9807692307692308, f_beta: 0.9799919967987194
train: step: 5923, loss: 0.025295862928032875, acc: 0.97, recall: 0.97, precision: 0.9716981132075472, f_beta: 0.9699729756781104
train: step: 5924, loss: 0.052884940057992935, acc: 0.94, recall: 0.94, precision: 0.9407051282051282, f_beta: 0.9399759903961584
train: step: 5925, loss: 0.025167953222990036, acc: 0.98, recall: 0.98, precision: 0.9807692307692308, f_beta: 0.9799919967987194
train: step: 5926, loss: 0.011024747975170612, acc: 0.99, recall: 0.99, precision: 0.9901960784313726, f_beta: 0.98999899989999
train: step: 5927, loss: 0.02084912732243538, acc: 0.98, recall: 0.98, precision: 0.9807692307692308, f_beta: 0.9799919967987194
train: step: 5928, loss: 0.03826417401432991, acc: 0.96, recall: 0.96, precision: 0.9607371794871795, f_beta: 0.959983993597439
train: step: 5929, loss: 0.01667199842631817, acc: 0.98, recall: 0.98, precision: 0.9807692307692308, f_beta: 0.9799919967987194
train: step: 5930, loss: 0.021292701363563538, acc: 0.98, recall: 0.98, precision: 0.9807692307692308, f_beta: 0.9799919967987194
train: step: 5931, loss: 0.0005744932568632066, acc: 1.0, recall: 1.0, precision: 1.0, f_beta: 1.0
train: step: 5932, loss: 0.07971177995204926, acc: 0.91, recall: 0.9099999999999999, precision: 0.9181966544267646, f_beta: 0.9095568284594513
train: step: 5933, loss: 0.03627513349056244, acc: 0.96, recall: 0.96, precision: 0.962962962962963, f_beta: 0.9599358974358974
train: step: 5934, loss: 0.0026727705262601376, acc: 1.0, recall: 1.0, precision: 1.0, f_beta: 1.0
train: step: 5935, loss: 0.013402659446001053, acc: 0.99, recall: 0.99, precision: 0.9901960784313726, f_beta: 0.98999899989999
train: step: 5936, loss: 0.015188081189990044, acc: 0.98, recall: 0.98, precision: 0.98, f_beta: 0.98
train: step: 5937, loss: 0.07887303084135056, acc: 0.9, recall: 0.8999999999999999, precision: 0.9025764895330113, f_beta: 0.8998397435897436
train: step: 5938, loss: 0.014631791040301323, acc: 0.99, recall: 0.99, precision: 0.9901960784313726, f_beta: 0.98999899989999
train: step: 5939, loss: 0.006990422029048204, acc: 0.99, recall: 0.99, precision: 0.9901960784313726, f_beta: 0.98999899989999
train: step: 5940, loss: 0.0076402598060667515, acc: 0.99, recall: 0.99, precision: 0.9901960784313726, f_beta: 0.98999899989999
train: step: 5941, loss: 0.07274199277162552, acc: 0.9, recall: 0.9, precision: 0.9006410256410255, f_beta: 0.8999599839935974
train: step: 5942, loss: 0.03685283288359642, acc: 0.95, recall: 0.95, precision: 0.9545454545454546, f_beta: 0.949874686716792
train: step: 5943, loss: 0.033934883773326874, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 5944, loss: 0.004091417416930199, acc: 1.0, recall: 1.0, precision: 1.0, f_beta: 1.0
train: step: 5945, loss: 0.04747328907251358, acc: 0.95, recall: 0.95, precision: 0.9516258530710557, f_beta: 0.949954959463517
train: step: 5946, loss: 0.014661402441561222, acc: 0.99, recall: 0.99, precision: 0.9901960784313726, f_beta: 0.98999899989999
train: step: 5947, loss: 0.0284917950630188, acc: 0.97, recall: 0.97, precision: 0.9716981132075472, f_beta: 0.9699729756781104
train: step: 5948, loss: 0.006934084929525852, acc: 0.99, recall: 0.99, precision: 0.9901960784313726, f_beta: 0.98999899989999
train: step: 5949, loss: 0.023350892588496208, acc: 0.98, recall: 0.98, precision: 0.98, f_beta: 0.98
train: step: 5950, loss: 0.012466148473322392, acc: 0.99, recall: 0.99, precision: 0.9901960784313726, f_beta: 0.98999899989999
train: step: 5951, loss: 0.057495471090078354, acc: 0.91, recall: 0.9099999999999999, precision: 0.9181966544267646, f_beta: 0.9095568284594513
train: step: 5952, loss: 0.036080893129110336, acc: 0.96, recall: 0.96, precision: 0.96, f_beta: 0.96
train: step: 5953, loss: 0.01824210397899151, acc: 0.98, recall: 0.98, precision: 0.98, f_beta: 0.98
train: step: 5954, loss: 0.01061265543103218, acc: 0.99, recall: 0.99, precision: 0.9901960784313726, f_beta: 0.98999899989999
train: step: 5955, loss: 0.016723141074180603, acc: 0.99, recall: 0.99, precision: 0.9901960784313726, f_beta: 0.98999899989999
train: step: 5956, loss: 0.040230970829725266, acc: 0.94, recall: 0.94, precision: 0.9428341384863124, f_beta: 0.9399038461538463
train: step: 5957, loss: 0.00255215703509748, acc: 1.0, recall: 1.0, precision: 1.0, f_beta: 1.0
train: step: 5958, loss: 0.03548453003168106, acc: 0.96, recall: 0.96, precision: 0.9607371794871795, f_beta: 0.959983993597439
train: step: 5959, loss: 0.02349913865327835, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 5960, loss: 0.004130272194743156, acc: 0.99, recall: 0.99, precision: 0.9901960784313726, f_beta: 0.98999899989999
train: step: 5961, loss: 0.010778870433568954, acc: 0.99, recall: 0.99, precision: 0.9901960784313726, f_beta: 0.98999899989999
train: step: 5962, loss: 0.023673515766859055, acc: 0.98, recall: 0.98, precision: 0.98, f_beta: 0.98
train: step: 5963, loss: 0.00514844199642539, acc: 0.99, recall: 0.99, precision: 0.9901960784313726, f_beta: 0.98999899989999
train: step: 5964, loss: 0.017970819026231766, acc: 0.98, recall: 0.98, precision: 0.98, f_beta: 0.98
train: step: 5965, loss: 0.06078391894698143, acc: 0.93, recall: 0.9299999999999999, precision: 0.9343434343434343, f_beta: 0.9298245614035088
train: step: 5966, loss: 0.07316096872091293, acc: 0.89, recall: 0.89, precision: 0.9098360655737705, f_beta: 0.8886526976414617
train: step: 5967, loss: 0.0007194052450358868, acc: 1.0, recall: 1.0, precision: 1.0, f_beta: 1.0
train: step: 5968, loss: 0.011072736233472824, acc: 0.99, recall: 0.99, precision: 0.9901960784313726, f_beta: 0.98999899989999
train: step: 5969, loss: 0.030274992808699608, acc: 0.97, recall: 0.97, precision: 0.9716981132075472, f_beta: 0.9699729756781104
train: step: 5970, loss: 0.021952448412775993, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 5971, loss: 0.02878311462700367, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 5972, loss: 0.013992740772664547, acc: 0.99, recall: 0.99, precision: 0.9901960784313726, f_beta: 0.98999899989999
train: step: 5973, loss: 0.005482513457536697, acc: 0.99, recall: 0.99, precision: 0.9901960784313726, f_beta: 0.98999899989999
train: step: 5974, loss: 0.0074350046925246716, acc: 0.99, recall: 0.99, precision: 0.9901960784313726, f_beta: 0.98999899989999
train: step: 5975, loss: 0.03712128475308418, acc: 0.96, recall: 0.96, precision: 0.9607371794871795, f_beta: 0.959983993597439
train: step: 5976, loss: 0.02726530097424984, acc: 0.96, recall: 0.96, precision: 0.962962962962963, f_beta: 0.9599358974358974
train: step: 5977, loss: 0.022052720189094543, acc: 0.96, recall: 0.96, precision: 0.962962962962963, f_beta: 0.9599358974358974
train: step: 5978, loss: 0.0030738692730665207, acc: 1.0, recall: 1.0, precision: 1.0, f_beta: 1.0
train: step: 5979, loss: 0.021984271705150604, acc: 0.98, recall: 0.98, precision: 0.9807692307692308, f_beta: 0.9799919967987194
train: step: 5980, loss: 0.04717153683304787, acc: 0.93, recall: 0.9299999999999999, precision: 0.9385964912280702, f_beta: 0.9296553110240177
train: step: 5981, loss: 0.010150277987122536, acc: 0.99, recall: 0.99, precision: 0.9901960784313726, f_beta: 0.98999899989999
train: step: 5982, loss: 0.023365568369627, acc: 0.98, recall: 0.98, precision: 0.98, f_beta: 0.98
train: step: 5983, loss: 0.04808269813656807, acc: 0.93, recall: 0.9299999999999999, precision: 0.930172068827531, f_beta: 0.9299929992999301
train: step: 5984, loss: 0.021001972258090973, acc: 0.98, recall: 0.98, precision: 0.9807692307692308, f_beta: 0.9799919967987194
train: step: 5985, loss: 0.02283194102346897, acc: 0.98, recall: 0.98, precision: 0.98, f_beta: 0.98
train: step: 5986, loss: 0.04758809879422188, acc: 0.93, recall: 0.9299999999999999, precision: 0.930172068827531, f_beta: 0.9299929992999301
train: step: 5987, loss: 0.020202169194817543, acc: 0.98, recall: 0.98, precision: 0.98, f_beta: 0.98
train: step: 5988, loss: 0.02981133945286274, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 5989, loss: 0.024293288588523865, acc: 0.98, recall: 0.98, precision: 0.9807692307692308, f_beta: 0.9799919967987194
train: step: 5990, loss: 0.010051616467535496, acc: 0.99, recall: 0.99, precision: 0.9901960784313726, f_beta: 0.98999899989999
train: step: 5991, loss: 0.02308371476829052, acc: 0.96, recall: 0.96, precision: 0.96, f_beta: 0.96
train: step: 5992, loss: 4.6608336560893804e-05, acc: 1.0, recall: 1.0, precision: 1.0, f_beta: 1.0
train: step: 5993, loss: 0.029345860704779625, acc: 0.95, recall: 0.95, precision: 0.9501800720288115, f_beta: 0.9499949994999499
train: step: 5994, loss: 0.00969190988689661, acc: 0.99, recall: 0.99, precision: 0.9901960784313726, f_beta: 0.98999899989999
train: step: 5995, loss: 0.020898230373859406, acc: 0.98, recall: 0.98, precision: 0.9807692307692308, f_beta: 0.9799919967987194
train: step: 5996, loss: 0.0312989316880703, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 5997, loss: 0.020024575293064117, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 5998, loss: 0.03619096428155899, acc: 0.95, recall: 0.95, precision: 0.9516258530710557, f_beta: 0.949954959463517
train: step: 5999, loss: 0.047091539949178696, acc: 0.95, recall: 0.95, precision: 0.9545454545454546, f_beta: 0.949874686716792
