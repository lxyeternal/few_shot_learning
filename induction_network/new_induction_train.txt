train: step: 0, loss: 0.37088271975517273, acc: 0.44, recall: 0.43999999999999995, precision: 0.41776315789473684, f_beta: 0.3993993993993994
train: step: 1, loss: 0.37073609232902527, acc: 0.5, recall: 0.5, precision: 0.25, f_beta: 0.3333333333333333
train: step: 2, loss: 0.3692548871040344, acc: 0.5, recall: 0.5, precision: 0.25, f_beta: 0.3333333333333333
train: step: 3, loss: 0.36506617069244385, acc: 0.5, recall: 0.5, precision: 0.25, f_beta: 0.3333333333333333
train: step: 4, loss: 0.2842061519622803, acc: 0.5, recall: 0.5, precision: 0.25, f_beta: 0.3333333333333333
train: step: 5, loss: 0.340591162443161, acc: 0.5, recall: 0.5, precision: 0.25, f_beta: 0.3333333333333333
train: step: 6, loss: 0.2870957553386688, acc: 0.5, recall: 0.5, precision: 0.25, f_beta: 0.3333333333333333
train: step: 7, loss: 0.31061241030693054, acc: 0.5, recall: 0.5, precision: 0.25, f_beta: 0.3333333333333333
train: step: 8, loss: 0.3601212799549103, acc: 0.5, recall: 0.5, precision: 0.25, f_beta: 0.3333333333333333
train: step: 9, loss: 0.3158285319805145, acc: 0.5, recall: 0.5, precision: 0.25, f_beta: 0.3333333333333333
train: step: 10, loss: 0.2700032889842987, acc: 0.5, recall: 0.5, precision: 0.25, f_beta: 0.3333333333333333
train: step: 11, loss: 0.26381242275238037, acc: 0.5, recall: 0.5, precision: 0.25, f_beta: 0.3333333333333333
train: step: 12, loss: 0.26973235607147217, acc: 0.5, recall: 0.5, precision: 0.25, f_beta: 0.3333333333333333
train: step: 13, loss: 0.2574518620967865, acc: 0.5, recall: 0.5, precision: 0.25, f_beta: 0.3333333333333333
train: step: 14, loss: 0.25207120180130005, acc: 0.5, recall: 0.5, precision: 0.25, f_beta: 0.3333333333333333
train: step: 15, loss: 0.2664237916469574, acc: 0.51, recall: 0.51, precision: 0.7525252525252526, f_beta: 0.3551783129359126
train: step: 16, loss: 0.2536216080188751, acc: 0.5, recall: 0.5, precision: 0.25, f_beta: 0.3333333333333333
train: step: 17, loss: 0.2580069601535797, acc: 0.69, recall: 0.69, precision: 0.690686471296668, f_beta: 0.6897207486738064
train: step: 18, loss: 0.25494807958602905, acc: 0.49, recall: 0.49, precision: 0.4473684210526316, f_beta: 0.3605015673981191
train: step: 19, loss: 0.25998929142951965, acc: 0.52, recall: 0.52, precision: 0.5679347826086957, f_beta: 0.4171928120446819
train: step: 20, loss: 0.2614365518093109, acc: 0.5, recall: 0.5, precision: 0.25, f_beta: 0.3333333333333333
train: step: 21, loss: 0.2559705972671509, acc: 0.5, recall: 0.5, precision: 0.25, f_beta: 0.3333333333333333
train: step: 22, loss: 0.28801026940345764, acc: 0.5, recall: 0.5, precision: 0.25, f_beta: 0.3333333333333333
train: step: 23, loss: 0.2520347833633423, acc: 0.66, recall: 0.6599999999999999, precision: 0.6838235294117647, f_beta: 0.6486151302190988
train: step: 24, loss: 0.2560429871082306, acc: 0.5, recall: 0.5, precision: 0.25, f_beta: 0.3333333333333333
train: step: 25, loss: 0.25537756085395813, acc: 0.58, recall: 0.5800000000000001, precision: 0.625, f_beta: 0.5384615384615385
train: step: 26, loss: 0.25576332211494446, acc: 0.74, recall: 0.74, precision: 0.767379679144385, f_beta: 0.7331691297208538
train: step: 27, loss: 0.2525637745857239, acc: 0.56, recall: 0.56, precision: 0.7659574468085106, f_beta: 0.4543650793650793
train: step: 28, loss: 0.25582653284072876, acc: 0.5, recall: 0.5, precision: 0.25, f_beta: 0.3333333333333333
train: step: 29, loss: 0.25505539774894714, acc: 0.79, recall: 0.79, precision: 0.7997106242248864, f_beta: 0.7882851093860268
train: step: 30, loss: 0.25748971104621887, acc: 0.55, recall: 0.55, precision: 0.6920122887864824, f_beta: 0.4479205005520795
train: step: 31, loss: 0.253734290599823, acc: 0.5, recall: 0.5, precision: 0.25, f_beta: 0.3333333333333333
train: step: 32, loss: 0.25373584032058716, acc: 0.51, recall: 0.51, precision: 0.7525252525252526, f_beta: 0.3551783129359126
train: step: 33, loss: 0.252646267414093, acc: 0.54, recall: 0.54, precision: 0.6358695652173914, f_beta: 0.4414764448761534
train: step: 34, loss: 0.2517591416835785, acc: 0.55, recall: 0.55, precision: 0.5666666666666667, f_beta: 0.52
train: step: 35, loss: 0.251730740070343, acc: 0.5, recall: 0.5, precision: 0.25, f_beta: 0.3333333333333333
train: step: 36, loss: 0.252402663230896, acc: 0.5, recall: 0.5, precision: 0.25, f_beta: 0.3333333333333333
train: step: 37, loss: 0.2512390911579132, acc: 0.56, recall: 0.56, precision: 0.6116071428571428, f_beta: 0.5024875621890548
train: step: 38, loss: 0.25112393498420715, acc: 0.32, recall: 0.32, precision: 0.2931985294117647, f_beta: 0.2972302604381976
train: step: 39, loss: 0.25462737679481506, acc: 0.5, recall: 0.5, precision: 0.25, f_beta: 0.3333333333333333
train: step: 40, loss: 0.2521556615829468, acc: 0.5, recall: 0.5, precision: 0.25, f_beta: 0.3333333333333333
train: step: 41, loss: 0.2550113797187805, acc: 0.51, recall: 0.51, precision: 0.7525252525252526, f_beta: 0.3551783129359126
train: step: 42, loss: 0.25174808502197266, acc: 0.59, recall: 0.5900000000000001, precision: 0.5917992656058751, f_beta: 0.5879811074263892
train: step: 43, loss: 0.2515685558319092, acc: 0.64, recall: 0.64, precision: 0.6736111111111112, f_beta: 0.6216897856242118
train: step: 44, loss: 0.25040149688720703, acc: 0.5, recall: 0.5, precision: 0.25, f_beta: 0.3333333333333333
train: step: 45, loss: 0.25131145119667053, acc: 0.5, recall: 0.5, precision: 0.25, f_beta: 0.3333333333333333
train: step: 46, loss: 0.250160276889801, acc: 0.57, recall: 0.5700000000000001, precision: 0.7688172043010753, f_beta: 0.4724573671942093
train: step: 47, loss: 0.2514016330242157, acc: 0.51, recall: 0.51, precision: 0.5859106529209621, f_beta: 0.3710691823899371
train: step: 48, loss: 0.25131726264953613, acc: 0.5, recall: 0.5, precision: 0.25, f_beta: 0.3333333333333333
train: step: 49, loss: 0.25061240792274475, acc: 0.5, recall: 0.5, precision: 0.25, f_beta: 0.3333333333333333
train: step: 50, loss: 0.25179359316825867, acc: 0.54, recall: 0.54, precision: 0.5946969696969697, f_beta: 0.46236559139784944
train: step: 51, loss: 0.25025472044944763, acc: 0.52, recall: 0.52, precision: 0.6302083333333333, f_beta: 0.39117199391172
train: step: 52, loss: 0.2515623867511749, acc: 0.65, recall: 0.65, precision: 0.6608751608751608, f_beta: 0.6439833180754755
train: step: 53, loss: 0.2505842447280884, acc: 0.68, recall: 0.6799999999999999, precision: 0.6826298701298701, f_beta: 0.6788438378161381
train: step: 54, loss: 0.2514384388923645, acc: 0.43, recall: 0.43, precision: 0.3945147679324894, f_beta: 0.37766131673763514
train: step: 55, loss: 0.25084322690963745, acc: 0.5, recall: 0.5, precision: 0.25, f_beta: 0.3333333333333333
train: step: 56, loss: 0.24725893139839172, acc: 0.59, recall: 0.59, precision: 0.7747252747252747, f_beta: 0.5071523019593701
train: step: 57, loss: 0.25191208720207214, acc: 0.68, recall: 0.6799999999999999, precision: 0.6826298701298701, f_beta: 0.6788438378161381
train: step: 58, loss: 0.2475593239068985, acc: 0.65, recall: 0.65, precision: 0.6902587519025876, f_beta: 0.630450849963045
train: step: 59, loss: 0.2505779564380646, acc: 0.5, recall: 0.5, precision: 0.25, f_beta: 0.3333333333333333
train: step: 60, loss: 0.24951137602329254, acc: 0.5, recall: 0.5, precision: 0.25, f_beta: 0.3333333333333333
train: step: 61, loss: 0.25258779525756836, acc: 0.5, recall: 0.5, precision: 0.25, f_beta: 0.3333333333333333
train: step: 62, loss: 0.2501990795135498, acc: 0.52, recall: 0.52, precision: 0.7551020408163265, f_beta: 0.3762993762993764
train: step: 63, loss: 0.24954994022846222, acc: 0.61, recall: 0.6100000000000001, precision: 0.6121991024071807, f_beta: 0.6080795899909557
train: step: 64, loss: 0.2502962052822113, acc: 0.55, recall: 0.55, precision: 0.5501806503412284, f_beta: 0.5495946351716544
train: step: 65, loss: 0.2498304694890976, acc: 0.53, recall: 0.53, precision: 0.6152073732718895, f_beta: 0.42338363390994965
train: step: 66, loss: 0.25336870551109314, acc: 0.39, recall: 0.39, precision: 0.3756218905472637, f_beta: 0.37184635979816705
train: step: 67, loss: 0.24623920023441315, acc: 0.59, recall: 0.59, precision: 0.6764705882352942, f_beta: 0.5327635327635327
train: step: 68, loss: 0.2441757321357727, acc: 0.62, recall: 0.62, precision: 0.6273344651952462, f_beta: 0.614448051948052
train: step: 69, loss: 0.2514953911304474, acc: 0.5, recall: 0.5, precision: 0.25, f_beta: 0.3333333333333333
train: step: 70, loss: 0.23866012692451477, acc: 0.63, recall: 0.63, precision: 0.6959011452682339, f_beta: 0.5960257670051317
train: step: 71, loss: 0.24544614553451538, acc: 0.63, recall: 0.63, precision: 0.7111760883690708, f_beta: 0.590662683925213
train: step: 72, loss: 0.24930459260940552, acc: 0.53, recall: 0.53, precision: 0.6152073732718895, f_beta: 0.42338363390994965
train: step: 73, loss: 0.25454476475715637, acc: 0.38, recall: 0.38, precision: 0.36979166666666663, f_beta: 0.3676050591595267
train: step: 74, loss: 0.26096615195274353, acc: 0.35, recall: 0.35, precision: 0.30000000000000004, f_beta: 0.30666666666666664
train: step: 75, loss: 0.25007298588752747, acc: 0.5, recall: 0.5, precision: 0.25, f_beta: 0.3333333333333333
train: step: 76, loss: 0.2507101893424988, acc: 0.52, recall: 0.52, precision: 0.6302083333333333, f_beta: 0.39117199391172
train: step: 77, loss: 0.25178593397140503, acc: 0.6, recall: 0.6, precision: 0.6041666666666667, f_beta: 0.595959595959596
train: step: 78, loss: 0.240622878074646, acc: 0.61, recall: 0.61, precision: 0.6948972360028349, f_beta: 0.5623386825272136
train: step: 79, loss: 0.24954017996788025, acc: 0.5, recall: 0.5, precision: 0.5, f_beta: 0.36580416032470825
train: step: 80, loss: 0.2507248818874359, acc: 0.5, recall: 0.5, precision: 0.25, f_beta: 0.3333333333333333
train: step: 81, loss: 0.24833372235298157, acc: 0.54, recall: 0.54, precision: 0.6773049645390071, f_beta: 0.4295634920634921
train: step: 82, loss: 0.23623737692832947, acc: 0.68, recall: 0.6799999999999999, precision: 0.6847290640394088, f_beta: 0.677938808373591
train: step: 83, loss: 0.2393096536397934, acc: 0.64, recall: 0.64, precision: 0.6666666666666666, f_beta: 0.625
train: step: 84, loss: 0.24622340500354767, acc: 0.59, recall: 0.59, precision: 0.729826353421859, f_beta: 0.516452411841019
train: step: 85, loss: 0.2511104941368103, acc: 0.66, recall: 0.66, precision: 0.7192982456140351, f_beta: 0.6353496353496353
train: step: 86, loss: 0.25003135204315186, acc: 0.57, recall: 0.57, precision: 0.6787538304392238, f_beta: 0.4928647246137516
train: step: 87, loss: 0.23286603391170502, acc: 0.64, recall: 0.64, precision: 0.6608455882352942, f_beta: 0.6279454319966928
train: step: 88, loss: 0.23887483775615692, acc: 0.64, recall: 0.64, precision: 0.71875, f_beta: 0.6043956043956045
train: step: 89, loss: 0.2558140456676483, acc: 0.56, recall: 0.56, precision: 0.5714285714285714, f_beta: 0.5416666666666666
train: step: 90, loss: 0.25311124324798584, acc: 0.66, recall: 0.6599999999999999, precision: 0.6610305958132046, f_beta: 0.6594551282051282
train: step: 91, loss: 0.24151939153671265, acc: 0.63, recall: 0.63, precision: 0.6343530384456387, f_beta: 0.6269785260610949
train: step: 92, loss: 0.25400978326797485, acc: 0.71, recall: 0.71, precision: 0.7100840336134454, f_beta: 0.7099709970997101
train: step: 93, loss: 0.22698448598384857, acc: 0.67, recall: 0.67, precision: 0.7156265854895991, f_beta: 0.6515679442508711
train: step: 94, loss: 0.2535940408706665, acc: 0.65, recall: 0.65, precision: 0.6648351648351649, f_beta: 0.6419437340153452
train: step: 95, loss: 0.19061999022960663, acc: 0.73, recall: 0.73, precision: 0.7688172043010753, f_beta: 0.7198879551820727
train: step: 96, loss: 0.2407580018043518, acc: 0.62, recall: 0.62, precision: 0.6488095238095238, f_beta: 0.6006725514922236
train: step: 97, loss: 0.17341893911361694, acc: 0.8, recall: 0.8, precision: 0.8004807692307692, f_beta: 0.7999199679871949
train: step: 98, loss: 0.18822145462036133, acc: 0.78, recall: 0.78, precision: 0.7840909090909092, f_beta: 0.779205138498595
train: step: 99, loss: 0.2612507939338684, acc: 0.7, recall: 0.7, precision: 0.7228163992869875, f_beta: 0.6921182266009853
train: step: 100, loss: 0.18532337248325348, acc: 0.73, recall: 0.73, precision: 0.7308309915696507, f_beta: 0.7297567811029926
train: step: 101, loss: 0.21197277307510376, acc: 0.66, recall: 0.66, precision: 0.6666666666666666, f_beta: 0.6565656565656566
train: step: 102, loss: 0.35312125086784363, acc: 0.57, recall: 0.57, precision: 0.6787538304392238, f_beta: 0.4928647246137516
train: step: 103, loss: 0.3418799936771393, acc: 0.5, recall: 0.5, precision: 0.5, f_beta: 0.39290917921321034
train: step: 104, loss: 0.3348459303379059, acc: 0.4, recall: 0.4, precision: 0.4, f_beta: 0.4000000000000001
train: step: 105, loss: 0.3334139287471771, acc: 0.36, recall: 0.36, precision: 0.3480902777777778, f_beta: 0.34720522235822115
train: step: 106, loss: 0.32032695412635803, acc: 0.26, recall: 0.26, precision: 0.2584541062801933, f_beta: 0.2588141025641026
train: step: 107, loss: 0.21250663697719574, acc: 0.72, recall: 0.72, precision: 0.7527573529411764, f_beta: 0.7106242248863166
train: step: 108, loss: 0.31665554642677307, acc: 0.64, recall: 0.64, precision: 0.681912681912682, f_beta: 0.6179966044142614
train: step: 109, loss: 0.2995079457759857, acc: 0.66, recall: 0.66, precision: 0.7079002079002079, f_beta: 0.6392190152801358
train: step: 110, loss: 0.2841196358203888, acc: 0.64, recall: 0.64, precision: 0.6666666666666666, f_beta: 0.625
train: step: 111, loss: 0.2469257414340973, acc: 0.5, recall: 0.5, precision: 0.25, f_beta: 0.3333333333333333
train: step: 112, loss: 0.2621626555919647, acc: 0.5, recall: 0.5, precision: 0.25, f_beta: 0.3333333333333333
train: step: 113, loss: 0.2548210918903351, acc: 0.5, recall: 0.5, precision: 0.25, f_beta: 0.3333333333333333
train: step: 114, loss: 0.2403569221496582, acc: 0.54, recall: 0.54, precision: 0.7604166666666667, f_beta: 0.4165398274987316
train: step: 115, loss: 0.24835701286792755, acc: 0.5, recall: 0.5, precision: 0.25, f_beta: 0.3333333333333333
train: step: 116, loss: 0.26663389801979065, acc: 0.61, recall: 0.6100000000000001, precision: 0.6111111111111112, f_beta: 0.6090225563909775
train: step: 117, loss: 0.2402305155992508, acc: 0.56, recall: 0.56, precision: 0.7659574468085106, f_beta: 0.4543650793650793
train: step: 118, loss: 0.2502762973308563, acc: 0.52, recall: 0.52, precision: 0.7551020408163265, f_beta: 0.3762993762993764
train: step: 119, loss: 0.25223714113235474, acc: 0.5, recall: 0.5, precision: 0.25, f_beta: 0.3333333333333333
train: step: 120, loss: 0.24515533447265625, acc: 0.72, recall: 0.72, precision: 0.7291666666666667, f_beta: 0.7171717171717171
train: step: 121, loss: 0.31663867831230164, acc: 0.31, recall: 0.31, precision: 0.27793361383824217, f_beta: 0.2841581076875194
train: step: 122, loss: 0.254456490278244, acc: 0.46, recall: 0.46, precision: 0.23958333333333334, f_beta: 0.31506849315068497
train: step: 123, loss: 0.20705266296863556, acc: 0.76, recall: 0.76, precision: 0.7758913412563667, f_beta: 0.7564935064935063
train: step: 124, loss: 0.22294872999191284, acc: 0.6, recall: 0.6, precision: 0.7777777777777778, f_beta: 0.5238095238095238
train: step: 125, loss: 0.21072174608707428, acc: 0.71, recall: 0.71, precision: 0.7121212121212122, f_beta: 0.7092731829573935
train: step: 126, loss: 0.20970480144023895, acc: 0.76, recall: 0.76, precision: 0.7668308702791462, f_beta: 0.7584541062801933
train: step: 127, loss: 0.211669459939003, acc: 0.71, recall: 0.71, precision: 0.78, f_beta: 0.6906666666666665
train: step: 128, loss: 0.22214870154857635, acc: 0.58, recall: 0.58, precision: 0.6661129568106312, f_beta: 0.5174632352941175
train: step: 129, loss: 0.24448932707309723, acc: 0.71, recall: 0.71, precision: 0.7307692307692308, f_beta: 0.7033248081841432
train: step: 130, loss: 0.27016881108283997, acc: 0.67, recall: 0.6699999999999999, precision: 0.7399774138904573, f_beta: 0.6440513428972063
train: step: 131, loss: 0.2068444788455963, acc: 0.77, recall: 0.77, precision: 0.805291723202171, f_beta: 0.7631551848419318
train: step: 132, loss: 0.163996160030365, acc: 0.75, recall: 0.75, precision: 0.8035454103933948, f_beta: 0.7384663667747673
train: step: 133, loss: 0.2818141579627991, acc: 0.44, recall: 0.44, precision: 0.2961956521739131, f_beta: 0.3200582807187955
train: step: 134, loss: 0.202591210603714, acc: 0.71, recall: 0.71, precision: 0.7170318313352625, f_beta: 0.7076318177235609
train: step: 135, loss: 0.16173534095287323, acc: 0.78, recall: 0.78, precision: 0.8038194444444444, f_beta: 0.7756017951856385
train: step: 136, loss: 0.18244582414627075, acc: 0.73, recall: 0.73, precision: 0.8066666666666666, f_beta: 0.712
train: step: 137, loss: 0.15854895114898682, acc: 0.78, recall: 0.78, precision: 0.8038194444444444, f_beta: 0.7756017951856385
train: step: 138, loss: 0.3143500089645386, acc: 0.42, recall: 0.42, precision: 0.375, f_beta: 0.3626373626373627
train: step: 139, loss: 0.3080492317676544, acc: 0.73, recall: 0.73, precision: 0.7323232323232323, f_beta: 0.7293233082706766
train: step: 140, loss: 0.14611585438251495, acc: 0.8, recall: 0.8, precision: 0.8004807692307692, f_beta: 0.7999199679871949
train: step: 141, loss: 0.2919958531856537, acc: 0.5, recall: 0.5, precision: 0.25, f_beta: 0.3333333333333333
train: step: 142, loss: 0.16207362711429596, acc: 0.83, recall: 0.8300000000000001, precision: 0.8333333333333334, f_beta: 0.8295739348370927
train: step: 143, loss: 0.17520874738693237, acc: 0.8, recall: 0.8, precision: 0.8, f_beta: 0.8000000000000002
train: step: 144, loss: 0.21202152967453003, acc: 0.7, recall: 0.7, precision: 0.7228163992869875, f_beta: 0.6921182266009853
train: step: 145, loss: 0.18356853723526, acc: 0.75, recall: 0.75, precision: 0.7501000400160064, f_beta: 0.7499749974997499
train: step: 146, loss: 0.1737595796585083, acc: 0.76, recall: 0.76, precision: 0.7708333333333333, f_beta: 0.7575757575757576
train: step: 147, loss: 0.17816047370433807, acc: 0.75, recall: 0.75, precision: 0.7747252747252746, f_beta: 0.7442455242966751
train: step: 148, loss: 0.17738854885101318, acc: 0.71, recall: 0.71, precision: 0.7252252252252251, f_beta: 0.7050147492625368
train: step: 149, loss: 0.17970234155654907, acc: 0.77, recall: 0.77, precision: 0.7709755118426336, f_beta: 0.769792813532179
train: step: 150, loss: 0.14588278532028198, acc: 0.77, recall: 0.77, precision: 0.7967032967032968, f_beta: 0.7647058823529411
train: step: 151, loss: 0.1678401231765747, acc: 0.77, recall: 0.77, precision: 0.7895752895752897, f_beta: 0.7660461804495982
train: step: 152, loss: 0.18863710761070251, acc: 0.73, recall: 0.73, precision: 0.7308309915696507, f_beta: 0.7297567811029926
train: step: 153, loss: 0.14519226551055908, acc: 0.82, recall: 0.8200000000000001, precision: 0.822061191626409, f_beta: 0.8197115384615385
train: step: 154, loss: 0.16441214084625244, acc: 0.77, recall: 0.77, precision: 0.7701080432172869, f_beta: 0.7699769976997699
train: step: 155, loss: 0.14341950416564941, acc: 0.78, recall: 0.78, precision: 0.780448717948718, f_beta: 0.7799119647859143
train: step: 156, loss: 0.1830807328224182, acc: 0.73, recall: 0.73, precision: 0.7466752466752467, f_beta: 0.7253585596582239
train: step: 157, loss: 0.1720862239599228, acc: 0.76, recall: 0.76, precision: 0.7637987012987013, f_beta: 0.7591328783621035
train: step: 158, loss: 0.10394337028265, acc: 0.88, recall: 0.88, precision: 0.88, f_beta: 0.88
train: step: 159, loss: 0.0983785092830658, acc: 0.87, recall: 0.87, precision: 0.8737373737373737, f_beta: 0.8696741854636592
train: step: 160, loss: 0.16652551293373108, acc: 0.79, recall: 0.79, precision: 0.7929292929292929, f_beta: 0.7894736842105263
train: step: 161, loss: 0.1678086817264557, acc: 0.8, recall: 0.8, precision: 0.8, f_beta: 0.8000000000000002
train: step: 162, loss: 0.11302647739648819, acc: 0.85, recall: 0.8500000000000001, precision: 0.8535353535353536, f_beta: 0.849624060150376
train: step: 163, loss: 0.15910722315311432, acc: 0.77, recall: 0.77, precision: 0.7837326607818411, f_beta: 0.767182913250329
train: step: 164, loss: 0.14531107246875763, acc: 0.84, recall: 0.84, precision: 0.84, f_beta: 0.8399999999999999
train: step: 165, loss: 0.18813201785087585, acc: 0.76, recall: 0.76, precision: 0.7616747181964574, f_beta: 0.7596153846153846
train: step: 166, loss: 0.11725445091724396, acc: 0.83, recall: 0.8300000000000001, precision: 0.8311922922521076, f_beta: 0.8298468621759585
train: step: 167, loss: 0.12932807207107544, acc: 0.81, recall: 0.81, precision: 0.8131313131313131, f_beta: 0.8095238095238095
train: step: 168, loss: 0.1665635108947754, acc: 0.76, recall: 0.76, precision: 0.7708333333333333, f_beta: 0.7575757575757576
train: step: 169, loss: 0.11875950545072556, acc: 0.84, recall: 0.8400000000000001, precision: 0.8421900161030595, f_beta: 0.8397435897435896
train: step: 170, loss: 0.14662744104862213, acc: 0.84, recall: 0.8400000000000001, precision: 0.8489326765188834, f_beta: 0.8389694041867956
train: step: 171, loss: 0.14961332082748413, acc: 0.76, recall: 0.76, precision: 0.76, f_beta: 0.76
train: step: 172, loss: 0.15731927752494812, acc: 0.81, recall: 0.81, precision: 0.8324753324753325, f_beta: 0.8067338012409724
train: step: 173, loss: 0.369121789932251, acc: 0.76, recall: 0.76, precision: 0.7604166666666667, f_beta: 0.7599039615846339
train: step: 174, loss: 0.1209571585059166, acc: 0.84, recall: 0.8400000000000001, precision: 0.8489326765188834, f_beta: 0.8389694041867956
train: step: 175, loss: 0.14255903661251068, acc: 0.81, recall: 0.81, precision: 0.8203803224472923, f_beta: 0.8084484323016432
train: step: 176, loss: 0.12904003262519836, acc: 0.85, recall: 0.8500000000000001, precision: 0.8535353535353536, f_beta: 0.849624060150376
train: step: 177, loss: 0.19631148874759674, acc: 0.74, recall: 0.74, precision: 0.74, f_beta: 0.74
train: step: 178, loss: 0.12564051151275635, acc: 0.84, recall: 0.8400000000000001, precision: 0.8449675324675325, f_beta: 0.839421918908069
train: step: 179, loss: 0.12758076190948486, acc: 0.81, recall: 0.81, precision: 0.8161974704202366, f_beta: 0.8090644156366193
train: step: 180, loss: 0.1212846115231514, acc: 0.83, recall: 0.8300000000000001, precision: 0.8311922922521076, f_beta: 0.8298468621759585
train: step: 181, loss: 0.20774832367897034, acc: 0.83, recall: 0.8300000000000001, precision: 0.8365973072215422, f_beta: 0.8291628982011858
train: step: 182, loss: 0.07806288450956345, acc: 0.91, recall: 0.91, precision: 0.9101640656262505, f_beta: 0.90999099909991
train: step: 183, loss: 0.09350874274969101, acc: 0.87, recall: 0.87, precision: 0.8773969808241534, f_beta: 0.8693598633303186
train: step: 184, loss: 0.18388549983501434, acc: 0.77, recall: 0.77, precision: 0.7790409260024803, f_beta: 0.7681217864704104
train: step: 185, loss: 0.11461511254310608, acc: 0.83, recall: 0.8300000000000001, precision: 0.8333333333333334, f_beta: 0.8295739348370927
train: step: 186, loss: 0.15818288922309875, acc: 0.8, recall: 0.8, precision: 0.8, f_beta: 0.8000000000000002
train: step: 187, loss: 0.16173894703388214, acc: 0.77, recall: 0.77, precision: 0.7709755118426336, f_beta: 0.769792813532179
train: step: 188, loss: 0.09501034021377563, acc: 0.89, recall: 0.89, precision: 0.89015606242497, f_beta: 0.88998899889989
train: step: 189, loss: 0.38553062081336975, acc: 0.63, recall: 0.63, precision: 0.6313131313131313, f_beta: 0.6290726817042607
train: step: 190, loss: 0.144912451505661, acc: 0.77, recall: 0.77, precision: 0.7753977968176254, f_beta: 0.7688674505074867
train: step: 191, loss: 0.3522571325302124, acc: 0.74, recall: 0.74, precision: 0.7403846153846154, f_beta: 0.7398959583833533
train: step: 192, loss: 0.13040322065353394, acc: 0.82, recall: 0.8200000000000001, precision: 0.8333333333333334, f_beta: 0.8181818181818181
train: step: 193, loss: 0.14955241978168488, acc: 0.78, recall: 0.78, precision: 0.7873563218390804, f_beta: 0.7785829307568439
train: step: 194, loss: 0.11326946318149567, acc: 0.84, recall: 0.84, precision: 0.8689236111111112, f_beta: 0.8368013055895552
train: step: 195, loss: 0.24045196175575256, acc: 0.75, recall: 0.75, precision: 0.7549979600163199, f_beta: 0.7487689679429204
train: step: 196, loss: 0.11315131932497025, acc: 0.87, recall: 0.87, precision: 0.8823894171145101, f_beta: 0.8689384010484928
train: step: 197, loss: 0.130896657705307, acc: 0.84, recall: 0.8400000000000001, precision: 0.8421900161030595, f_beta: 0.8397435897435896
train: step: 198, loss: 0.07838426530361176, acc: 0.89, recall: 0.8899999999999999, precision: 0.8977968176254589, f_beta: 0.8894583458948848
train: step: 199, loss: 0.1467856913805008, acc: 0.82, recall: 0.82, precision: 0.8395585738539898, f_beta: 0.8173701298701298
train: step: 200, loss: 0.14748884737491608, acc: 0.79, recall: 0.79, precision: 0.811025311025311, f_beta: 0.7863899908452854
train: step: 201, loss: 0.3807142376899719, acc: 0.78, recall: 0.78, precision: 0.780448717948718, f_beta: 0.7799119647859143
train: step: 202, loss: 0.17429806292057037, acc: 0.74, recall: 0.74, precision: 0.75, f_beta: 0.7373737373737373
train: step: 203, loss: 0.10052485764026642, acc: 0.87, recall: 0.87, precision: 0.8773969808241534, f_beta: 0.8693598633303186
train: step: 204, loss: 0.14877165853977203, acc: 0.8, recall: 0.8, precision: 0.8004807692307692, f_beta: 0.7999199679871949
train: step: 205, loss: 0.3239027261734009, acc: 0.76, recall: 0.76, precision: 0.7758913412563667, f_beta: 0.7564935064935063
train: step: 206, loss: 0.11793293058872223, acc: 0.85, recall: 0.85, precision: 0.867801597309794, f_beta: 0.848162769511084
train: step: 207, loss: 0.1656222939491272, acc: 0.79, recall: 0.79, precision: 0.7957976336189311, f_beta: 0.7889659330720531
train: step: 208, loss: 0.22161470353603363, acc: 0.67, recall: 0.6699999999999999, precision: 0.7561784207353828, f_beta: 0.6396986570586309
train: step: 209, loss: 0.16212129592895508, acc: 0.73, recall: 0.73, precision: 0.7527472527472527, f_beta: 0.7237851662404092
train: step: 210, loss: 0.15498651564121246, acc: 0.78, recall: 0.78, precision: 0.8216911764705883, f_beta: 0.7726333195535344
train: step: 211, loss: 0.1428758203983307, acc: 0.8, recall: 0.8, precision: 0.8446691176470589, f_beta: 0.7933030177759404
train: step: 212, loss: 0.29393187165260315, acc: 0.76, recall: 0.76, precision: 0.8224206349206349, f_beta: 0.7477931904161412
train: step: 213, loss: 0.10520924627780914, acc: 0.87, recall: 0.87, precision: 0.8701480592236894, f_beta: 0.8699869986998701
train: step: 214, loss: 0.1350942850112915, acc: 0.82, recall: 0.8200000000000001, precision: 0.8205128205128205, f_beta: 0.8199279711884755
train: step: 215, loss: 0.10248357057571411, acc: 0.86, recall: 0.86, precision: 0.8605769230769231, f_beta: 0.8599439775910365
train: step: 216, loss: 0.0742979347705841, acc: 0.91, recall: 0.91, precision: 0.9101640656262505, f_beta: 0.90999099909991
train: step: 217, loss: 0.1422988623380661, acc: 0.82, recall: 0.8200000000000001, precision: 0.8246753246753247, f_beta: 0.8193496587715776
train: step: 218, loss: 0.09695864468812943, acc: 0.88, recall: 0.8799999999999999, precision: 0.885551948051948, f_beta: 0.8795664391810518
train: step: 219, loss: 0.11519330739974976, acc: 0.84, recall: 0.84, precision: 0.8405448717948718, f_beta: 0.8399359743897559
train: step: 220, loss: 0.4040645658969879, acc: 0.11, recall: 0.11, precision: 0.10984393757503001, f_beta: 0.10991099109910991
train: step: 221, loss: 0.3146284818649292, acc: 0.67, recall: 0.67, precision: 0.7064108790675085, f_beta: 0.6547756041426929
train: step: 222, loss: 0.130401611328125, acc: 0.83, recall: 0.8300000000000001, precision: 0.8365973072215422, f_beta: 0.8291628982011858
train: step: 223, loss: 0.13835424184799194, acc: 0.82, recall: 0.82, precision: 0.82, f_beta: 0.82
train: step: 224, loss: 0.3619615435600281, acc: 0.77, recall: 0.77, precision: 0.805291723202171, f_beta: 0.7631551848419318
train: step: 225, loss: 0.3735290467739105, acc: 0.73, recall: 0.73, precision: 0.7917300862506342, f_beta: 0.7149192271143491
train: step: 226, loss: 0.36655959486961365, acc: 0.37, recall: 0.37, precision: 0.36868686868686873, f_beta: 0.368421052631579
train: step: 227, loss: 0.3371118903160095, acc: 0.78, recall: 0.78, precision: 0.8119429590017826, f_beta: 0.7742200328407225
train: step: 228, loss: 0.34562617540359497, acc: 0.44, recall: 0.44, precision: 0.43842364532019706, f_beta: 0.4363929146537842
train: step: 229, loss: 0.33941689133644104, acc: 0.42, recall: 0.42000000000000004, precision: 0.33388704318936874, f_beta: 0.3336397058823529
train: step: 230, loss: 0.29466086626052856, acc: 0.39, recall: 0.38999999999999996, precision: 0.3213125406107862, f_beta: 0.3251465870118376
train: step: 231, loss: 0.32339417934417725, acc: 0.51, recall: 0.51, precision: 0.5177179305457122, f_beta: 0.45011783189316573
train: step: 232, loss: 0.31153404712677, acc: 0.55, recall: 0.55, precision: 0.5812215724496426, f_beta: 0.5021573182874212
train: step: 233, loss: 0.30559033155441284, acc: 0.59, recall: 0.59, precision: 0.7747252747252747, f_beta: 0.5071523019593701
train: step: 234, loss: 0.2891042232513428, acc: 0.54, recall: 0.54, precision: 0.6358695652173914, f_beta: 0.4414764448761534
train: step: 235, loss: 0.2772991359233856, acc: 0.45, recall: 0.45, precision: 0.4114103472714387, f_beta: 0.38278532151273703
train: step: 236, loss: 0.28438428044319153, acc: 0.51, recall: 0.51, precision: 0.5384024577572964, f_beta: 0.3988467672678199
train: step: 237, loss: 0.2744535207748413, acc: 0.52, recall: 0.52, precision: 0.5415282392026578, f_beta: 0.4485294117647059
train: step: 238, loss: 0.2696284353733063, acc: 0.47, recall: 0.47000000000000003, precision: 0.3847926267281106, f_beta: 0.3497730339835603
train: step: 239, loss: 0.27189260721206665, acc: 0.45, recall: 0.45, precision: 0.34737484737484736, f_beta: 0.338862844091838
train: step: 240, loss: 0.2770911455154419, acc: 0.34, recall: 0.34, precision: 0.20238095238095238, f_beta: 0.2537313432835821
train: step: 241, loss: 0.27239781618118286, acc: 0.49, recall: 0.49, precision: 0.4473684210526316, f_beta: 0.3605015673981191
train: step: 242, loss: 0.26858291029930115, acc: 0.6, recall: 0.6, precision: 0.6860119047619048, f_beta: 0.5477159656264134
train: step: 243, loss: 0.2641090750694275, acc: 0.45, recall: 0.45, precision: 0.23684210526315788, f_beta: 0.3103448275862069
train: step: 244, loss: 0.26875799894332886, acc: 0.67, recall: 0.6699999999999999, precision: 0.7561784207353828, f_beta: 0.6396986570586309
train: step: 245, loss: 0.2686225473880768, acc: 0.42, recall: 0.42000000000000004, precision: 0.3106060606060606, f_beta: 0.3221131369798972
train: step: 246, loss: 0.2575167417526245, acc: 0.66, recall: 0.6599999999999999, precision: 0.6642036124794746, f_beta: 0.6578099838969405
train: step: 247, loss: 0.2646421790122986, acc: 0.33, recall: 0.33, precision: 0.3013090229079009, f_beta: 0.30490714804440294
train: step: 248, loss: 0.26411721110343933, acc: 0.38, recall: 0.38, precision: 0.25083056478405313, f_beta: 0.28768382352941174
train: step: 249, loss: 0.2518104016780853, acc: 0.73, recall: 0.73, precision: 0.7377015295576685, f_beta: 0.7277951406391774
train: step: 250, loss: 0.2569517195224762, acc: 0.53, recall: 0.53, precision: 0.6152073732718895, f_beta: 0.42338363390994965
train: step: 251, loss: 0.252506285905838, acc: 0.61, recall: 0.61, precision: 0.6657625075346594, f_beta: 0.5741893219783819
train: step: 252, loss: 0.2592676877975464, acc: 0.34, recall: 0.34, precision: 0.25, f_beta: 0.2747252747252747
train: step: 253, loss: 0.25262951850891113, acc: 0.64, recall: 0.64, precision: 0.7604166666666667, f_beta: 0.592944369063772
train: step: 254, loss: 0.2503305673599243, acc: 0.63, recall: 0.63, precision: 0.6959011452682339, f_beta: 0.5960257670051317
train: step: 255, loss: 0.2551406919956207, acc: 0.37, recall: 0.37, precision: 0.3040988547317661, f_beta: 0.3121519816573861
train: step: 256, loss: 0.25110435485839844, acc: 0.78, recall: 0.78, precision: 0.7840909090909092, f_beta: 0.779205138498595
train: step: 257, loss: 0.25316113233566284, acc: 0.41, recall: 0.41000000000000003, precision: 0.3010610079575597, f_beta: 0.31641756459274706
train: step: 258, loss: 0.2496468722820282, acc: 0.69, recall: 0.6900000000000001, precision: 0.7409944190766109, f_beta: 0.6726850385386971
train: step: 259, loss: 0.25211113691329956, acc: 0.5, recall: 0.5, precision: 0.25, f_beta: 0.3333333333333333
train: step: 260, loss: 0.2494421750307083, acc: 0.59, recall: 0.59, precision: 0.6989389920424403, f_beta: 0.5249681381068243
train: step: 261, loss: 0.2515484690666199, acc: 0.5, recall: 0.5, precision: 0.25, f_beta: 0.3333333333333333
train: step: 262, loss: 0.24943962693214417, acc: 0.6, recall: 0.6, precision: 0.7777777777777778, f_beta: 0.5238095238095238
train: step: 263, loss: 0.24904169142246246, acc: 0.76, recall: 0.76, precision: 0.7758913412563667, f_beta: 0.7564935064935063
train: step: 264, loss: 0.25191956758499146, acc: 0.41, recall: 0.41, precision: 0.22527472527472528, f_beta: 0.2907801418439716
train: step: 265, loss: 0.250181645154953, acc: 0.51, recall: 0.51, precision: 0.5113071008593397, f_beta: 0.49541756770672435
train: step: 266, loss: 0.2492852658033371, acc: 0.5, recall: 0.5, precision: 0.25, f_beta: 0.3333333333333333
train: step: 267, loss: 0.24961024522781372, acc: 0.77, recall: 0.77, precision: 0.7727272727272727, f_beta: 0.7694235588972431
train: step: 268, loss: 0.25147268176078796, acc: 0.39, recall: 0.39, precision: 0.33423749246534057, f_beta: 0.33398842668413586
train: step: 269, loss: 0.25188329815864563, acc: 0.42, recall: 0.42000000000000004, precision: 0.33388704318936874, f_beta: 0.3336397058823529
train: step: 270, loss: 0.24853309988975525, acc: 0.54, recall: 0.54, precision: 0.6358695652173914, f_beta: 0.4414764448761534
train: step: 271, loss: 0.24904146790504456, acc: 0.5, recall: 0.5, precision: 0.25, f_beta: 0.3333333333333333
train: step: 272, loss: 0.25106820464134216, acc: 0.51, recall: 0.51, precision: 0.7525252525252526, f_beta: 0.3551783129359126
train: step: 273, loss: 0.2512603998184204, acc: 0.5, recall: 0.5, precision: 0.25, f_beta: 0.3333333333333333
train: step: 274, loss: 0.2522653043270111, acc: 0.44, recall: 0.44, precision: 0.43990384615384615, f_beta: 0.4397759103641457
train: step: 275, loss: 0.24994085729122162, acc: 0.5, recall: 0.5, precision: 0.25, f_beta: 0.3333333333333333
train: step: 276, loss: 0.24640272557735443, acc: 0.81, recall: 0.81, precision: 0.8623188405797102, f_beta: 0.8028841166096068
train: step: 277, loss: 0.24930036067962646, acc: 0.62, recall: 0.62, precision: 0.6378676470588236, f_beta: 0.6072757337742869
train: step: 278, loss: 0.24908983707427979, acc: 0.52, recall: 0.52, precision: 0.7551020408163265, f_beta: 0.3762993762993764
train: step: 279, loss: 0.25005096197128296, acc: 0.5, recall: 0.5, precision: 0.25, f_beta: 0.3333333333333333
train: step: 280, loss: 0.24910873174667358, acc: 0.5, recall: 0.5, precision: 0.25, f_beta: 0.3333333333333333
train: step: 281, loss: 0.24892254173755646, acc: 0.68, recall: 0.6799999999999999, precision: 0.6953125, f_beta: 0.6736026111791106
train: step: 282, loss: 0.2503378987312317, acc: 0.5, recall: 0.5, precision: 0.25, f_beta: 0.3333333333333333
train: step: 283, loss: 0.2512151002883911, acc: 0.5, recall: 0.5, precision: 0.25, f_beta: 0.3333333333333333
train: step: 284, loss: 0.24955588579177856, acc: 0.5, recall: 0.5, precision: 0.25, f_beta: 0.3333333333333333
train: step: 285, loss: 0.25059154629707336, acc: 0.5, recall: 0.5, precision: 0.25, f_beta: 0.3333333333333333
train: step: 286, loss: 0.24876534938812256, acc: 0.79, recall: 0.79, precision: 0.7929292929292929, f_beta: 0.7894736842105263
train: step: 287, loss: 0.24966947734355927, acc: 0.5, recall: 0.5, precision: 0.25, f_beta: 0.3333333333333333
train: step: 288, loss: 0.2486407458782196, acc: 0.54, recall: 0.54, precision: 0.6773049645390071, f_beta: 0.4295634920634921
train: step: 289, loss: 0.24730736017227173, acc: 0.59, recall: 0.59, precision: 0.7747252747252747, f_beta: 0.5071523019593701
train: step: 290, loss: 0.25153180956840515, acc: 0.5, recall: 0.5, precision: 0.25, f_beta: 0.3333333333333333
train: step: 291, loss: 0.24866455793380737, acc: 0.64, recall: 0.64, precision: 0.7371273712737128, f_beta: 0.5989304812834224
train: step: 292, loss: 0.2506752908229828, acc: 0.65, recall: 0.6499999999999999, precision: 0.669606512890095, f_beta: 0.6395839769333744
train: step: 293, loss: 0.24942617118358612, acc: 0.5, recall: 0.5, precision: 0.25, f_beta: 0.3333333333333333
train: step: 294, loss: 0.2534571588039398, acc: 0.5, recall: 0.5, precision: 0.25, f_beta: 0.3333333333333333
train: step: 295, loss: 0.2473331093788147, acc: 0.5, recall: 0.5, precision: 0.25, f_beta: 0.3333333333333333
train: step: 296, loss: 0.2499060034751892, acc: 0.5, recall: 0.5, precision: 0.25, f_beta: 0.3333333333333333
train: step: 297, loss: 0.2510019838809967, acc: 0.52, recall: 0.52, precision: 0.7551020408163265, f_beta: 0.3762993762993764
train: step: 298, loss: 0.24926315248012543, acc: 0.67, recall: 0.6699999999999999, precision: 0.7399774138904573, f_beta: 0.6440513428972063
train: step: 299, loss: 0.24617762863636017, acc: 0.5, recall: 0.5, precision: 0.25, f_beta: 0.3333333333333333
train: step: 300, loss: 0.24657097458839417, acc: 0.5, recall: 0.5, precision: 0.25, f_beta: 0.3333333333333333
train: step: 301, loss: 0.24916166067123413, acc: 0.51, recall: 0.51, precision: 0.7525252525252526, f_beta: 0.3551783129359126
train: step: 302, loss: 0.24698616564273834, acc: 0.5, recall: 0.5, precision: 0.25, f_beta: 0.3333333333333333
train: step: 303, loss: 0.25352776050567627, acc: 0.41, recall: 0.41000000000000003, precision: 0.27017364657814097, f_beta: 0.3041632267956127
train: step: 304, loss: 0.2485990673303604, acc: 0.52, recall: 0.52, precision: 0.7551020408163265, f_beta: 0.3762993762993764
train: step: 305, loss: 0.2493458390235901, acc: 0.81, recall: 0.81, precision: 0.8257671290458175, f_beta: 0.8076728413807065
train: step: 306, loss: 0.2531663179397583, acc: 0.49, recall: 0.49, precision: 0.4140893470790378, f_beta: 0.3453985367731998
train: step: 307, loss: 0.25026071071624756, acc: 0.58, recall: 0.58, precision: 0.6661129568106312, f_beta: 0.5174632352941175
train: step: 308, loss: 0.2474626898765564, acc: 0.71, recall: 0.71, precision: 0.8164556962025317, f_beta: 0.6833715471121302
train: step: 309, loss: 0.24853366613388062, acc: 0.5, recall: 0.5, precision: 0.25, f_beta: 0.3333333333333333
train: step: 310, loss: 0.24764373898506165, acc: 0.67, recall: 0.6699999999999999, precision: 0.7561784207353828, f_beta: 0.6396986570586309
train: step: 311, loss: 0.24024604260921478, acc: 0.81, recall: 0.81, precision: 0.8203803224472923, f_beta: 0.8084484323016432
train: step: 312, loss: 0.24771636724472046, acc: 0.65, recall: 0.65, precision: 0.7941176470588236, f_beta: 0.6011396011396011
train: step: 313, loss: 0.24832957983016968, acc: 0.74, recall: 0.74, precision: 0.7976190476190477, f_beta: 0.7267759562841529
train: step: 314, loss: 0.24908922612667084, acc: 0.5, recall: 0.5, precision: 0.25, f_beta: 0.3333333333333333
train: step: 315, loss: 0.24814586341381073, acc: 0.5, recall: 0.5, precision: 0.25, f_beta: 0.3333333333333333
train: step: 316, loss: 0.24707461893558502, acc: 0.52, recall: 0.52, precision: 0.7551020408163265, f_beta: 0.3762993762993764
train: step: 317, loss: 0.25043565034866333, acc: 0.44, recall: 0.44000000000000006, precision: 0.4363327674023769, f_beta: 0.43181818181818177
train: step: 318, loss: 0.24504874646663666, acc: 0.62, recall: 0.62, precision: 0.7840909090909092, f_beta: 0.5558672276764843
train: step: 319, loss: 0.2519173324108124, acc: 0.37, recall: 0.37, precision: 0.28882391163092913, f_beta: 0.3030202456023897
train: step: 320, loss: 0.2528002858161926, acc: 0.28, recall: 0.28, precision: 0.22718253968253968, f_beta: 0.24337957124842372
train: step: 321, loss: 0.24221307039260864, acc: 0.81, recall: 0.81, precision: 0.8131313131313131, f_beta: 0.8095238095238095
train: step: 322, loss: 0.24712562561035156, acc: 0.5, recall: 0.5, precision: 0.25, f_beta: 0.3333333333333333
train: step: 323, loss: 0.2467321902513504, acc: 0.83, recall: 0.8300000000000001, precision: 0.8410500206696983, f_beta: 0.8286117552172598
train: step: 324, loss: 0.24253986775875092, acc: 0.65, recall: 0.65, precision: 0.7117447769621683, f_beta: 0.6224786970121886
train: step: 325, loss: 0.23729747533798218, acc: 0.76, recall: 0.76, precision: 0.8224206349206349, f_beta: 0.7477931904161412
train: step: 326, loss: 0.244007408618927, acc: 0.72, recall: 0.72, precision: 0.7450980392156863, f_beta: 0.7126436781609196
train: step: 327, loss: 0.24000629782676697, acc: 0.65, recall: 0.65, precision: 0.7941176470588236, f_beta: 0.6011396011396011
train: step: 328, loss: 0.24229995906352997, acc: 0.75, recall: 0.75, precision: 0.7501000400160064, f_beta: 0.7499749974997499
train: step: 329, loss: 0.2517801523208618, acc: 0.51, recall: 0.51, precision: 0.7525252525252526, f_beta: 0.3551783129359126
train: step: 330, loss: 0.2482297569513321, acc: 0.5, recall: 0.5, precision: 0.25, f_beta: 0.3333333333333333
train: step: 331, loss: 0.25222885608673096, acc: 0.5, recall: 0.5, precision: 0.25, f_beta: 0.3333333333333333
train: step: 332, loss: 0.24948149919509888, acc: 0.72, recall: 0.72, precision: 0.7214170692431562, f_beta: 0.7195512820512822
train: step: 333, loss: 0.2478945553302765, acc: 0.63, recall: 0.63, precision: 0.6519401589527817, f_beta: 0.6161427533976553
train: step: 334, loss: 0.24858129024505615, acc: 0.8, recall: 0.8, precision: 0.8004807692307692, f_beta: 0.7999199679871949
train: step: 335, loss: 0.24061909317970276, acc: 0.58, recall: 0.58, precision: 0.7222222222222222, f_beta: 0.5
train: step: 336, loss: 0.24348540604114532, acc: 0.66, recall: 0.6599999999999999, precision: 0.75, f_beta: 0.6263736263736264
train: step: 337, loss: 0.24237611889839172, acc: 0.69, recall: 0.69, precision: 0.7533333333333334, f_beta: 0.6693333333333333
train: step: 338, loss: 0.2519065737724304, acc: 0.5, recall: 0.5, precision: 0.5, f_beta: 0.43464495703301675
train: step: 339, loss: 0.23821701109409332, acc: 0.69, recall: 0.69, precision: 0.808641975308642, f_beta: 0.6570417081535568
train: step: 340, loss: 0.23838871717453003, acc: 0.7, recall: 0.7, precision: 0.7083333333333333, f_beta: 0.6969696969696968
train: step: 341, loss: 0.23685112595558167, acc: 0.78, recall: 0.78, precision: 0.7916666666666666, f_beta: 0.7777777777777779
train: step: 342, loss: 0.2531687021255493, acc: 0.33, recall: 0.33, precision: 0.32660138718890247, f_beta: 0.32670083408702644
train: step: 343, loss: 0.24245868623256683, acc: 0.61, recall: 0.61, precision: 0.7808988764044944, f_beta: 0.5400400990682864
train: step: 344, loss: 0.24051323533058167, acc: 0.68, recall: 0.68, precision: 0.7232142857142857, f_beta: 0.6637242538881883
train: step: 345, loss: 0.2450881451368332, acc: 0.66, recall: 0.6599999999999999, precision: 0.67825311942959, f_beta: 0.6510673234811166
train: step: 346, loss: 0.2429473102092743, acc: 0.8, recall: 0.8, precision: 0.8125, f_beta: 0.797979797979798
train: step: 347, loss: 0.24198059737682343, acc: 0.72, recall: 0.72, precision: 0.7728174603174603, f_beta: 0.7057587221521648
train: step: 348, loss: 0.23116907477378845, acc: 0.68, recall: 0.68, precision: 0.7338877338877339, f_beta: 0.6604414261460101
train: step: 349, loss: 0.266228586435318, acc: 0.4, recall: 0.4, precision: 0.3700623700623701, f_beta: 0.3633276740237691
train: step: 350, loss: 0.22316434979438782, acc: 0.82, recall: 0.8200000000000001, precision: 0.8333333333333334, f_beta: 0.8181818181818181
train: step: 351, loss: 0.25131580233573914, acc: 0.56, recall: 0.56, precision: 0.7038043478260869, f_beta: 0.46576007770762506
train: step: 352, loss: 0.2300904244184494, acc: 0.79, recall: 0.79, precision: 0.7929292929292929, f_beta: 0.7894736842105263
train: step: 353, loss: 0.226023867726326, acc: 0.75, recall: 0.75, precision: 0.8170979198376458, f_beta: 0.736036321402175
train: step: 354, loss: 0.2624252140522003, acc: 0.33, recall: 0.33, precision: 0.24382157926461723, f_beta: 0.2684790916038869
train: step: 355, loss: 0.24724167585372925, acc: 0.75, recall: 0.75, precision: 0.7627154266498528, f_beta: 0.7469379491851402
train: step: 356, loss: 0.23093895614147186, acc: 0.75, recall: 0.75, precision: 0.7826775214834916, f_beta: 0.7425599835238389
train: step: 357, loss: 0.22890806198120117, acc: 0.66, recall: 0.6599999999999999, precision: 0.6642036124794746, f_beta: 0.6578099838969405
train: step: 358, loss: 0.22515727579593658, acc: 0.81, recall: 0.81, precision: 0.810124049619848, f_beta: 0.8099809980998101
train: step: 359, loss: 0.23251616954803467, acc: 0.75, recall: 0.75, precision: 0.7583712277800745, f_beta: 0.7479584635547939
train: step: 360, loss: 0.2239023596048355, acc: 0.85, recall: 0.85, precision: 0.8501400560224089, f_beta: 0.84998499849985
train: step: 361, loss: 0.20751334726810455, acc: 0.7, recall: 0.7, precision: 0.7170138888888888, f_beta: 0.6940024479804162
train: step: 362, loss: 0.22802764177322388, acc: 0.68, recall: 0.6799999999999999, precision: 0.7005347593582887, f_beta: 0.6715927750410509
train: step: 363, loss: 0.25609925389289856, acc: 0.19, recall: 0.19, precision: 0.17423287095418244, f_beta: 0.18007895535985424
train: step: 364, loss: 0.22021332383155823, acc: 0.76, recall: 0.76, precision: 0.7616747181964574, f_beta: 0.7596153846153846
train: step: 365, loss: 0.24745725095272064, acc: 0.61, recall: 0.61, precision: 0.6948972360028349, f_beta: 0.5623386825272136
train: step: 366, loss: 0.20882000029087067, acc: 0.79, recall: 0.79, precision: 0.7901160464185675, f_beta: 0.78997899789979
train: step: 367, loss: 0.2606238126754761, acc: 0.68, recall: 0.68, precision: 0.7338877338877339, f_beta: 0.6604414261460101
train: step: 368, loss: 0.26317355036735535, acc: 0.3, recall: 0.3, precision: 0.2619047619047619, f_beta: 0.2708333333333333
train: step: 369, loss: 0.22145414352416992, acc: 0.81, recall: 0.81, precision: 0.8203803224472923, f_beta: 0.8084484323016432
train: step: 370, loss: 0.2565556764602661, acc: 0.77, recall: 0.77, precision: 0.7837326607818411, f_beta: 0.767182913250329
train: step: 371, loss: 0.1865011751651764, acc: 0.86, recall: 0.86, precision: 0.8623188405797102, f_beta: 0.859775641025641
train: step: 372, loss: 0.21479667723178864, acc: 0.77, recall: 0.77, precision: 0.7967032967032968, f_beta: 0.7647058823529411
train: step: 373, loss: 0.21086129546165466, acc: 0.69, recall: 0.69, precision: 0.6900760304121649, f_beta: 0.68996899689969
train: step: 374, loss: 0.2726826071739197, acc: 0.81, recall: 0.81, precision: 0.8161974704202366, f_beta: 0.8090644156366193
train: step: 375, loss: 0.17982357740402222, acc: 0.86, recall: 0.86, precision: 0.8694581280788177, f_beta: 0.8590982286634461
train: step: 376, loss: 0.24734531342983246, acc: 0.73, recall: 0.73, precision: 0.7917300862506342, f_beta: 0.7149192271143491
train: step: 377, loss: 0.1683129370212555, acc: 0.82, recall: 0.82, precision: 0.8395585738539898, f_beta: 0.8173701298701298
train: step: 378, loss: 0.20606373250484467, acc: 0.75, recall: 0.75, precision: 0.7747252747252746, f_beta: 0.7442455242966751
train: step: 379, loss: 0.16173961758613586, acc: 0.78, recall: 0.78, precision: 0.7840909090909092, f_beta: 0.779205138498595
train: step: 380, loss: 0.17354725301265717, acc: 0.72, recall: 0.72, precision: 0.7257799671592775, f_beta: 0.7181964573268922
train: step: 381, loss: 0.34635189175605774, acc: 0.25, recall: 0.25, precision: 0.24989995998399359, f_beta: 0.24992499249924993
train: step: 382, loss: 0.25849583745002747, acc: 0.75, recall: 0.75, precision: 0.7501000400160064, f_beta: 0.7499749974997499
train: step: 383, loss: 0.19421203434467316, acc: 0.79, recall: 0.79, precision: 0.7901160464185675, f_beta: 0.78997899789979
train: step: 384, loss: 0.24743644893169403, acc: 0.86, recall: 0.86, precision: 0.875, f_beta: 0.8585858585858586
train: step: 385, loss: 0.23959413170814514, acc: 0.73, recall: 0.73, precision: 0.7688172043010753, f_beta: 0.7198879551820727
train: step: 386, loss: 0.1966978907585144, acc: 0.89, recall: 0.89, precision: 0.89015606242497, f_beta: 0.88998899889989
train: step: 387, loss: 0.21204817295074463, acc: 0.86, recall: 0.86, precision: 0.8623188405797102, f_beta: 0.859775641025641
train: step: 388, loss: 0.2173101305961609, acc: 0.72, recall: 0.72, precision: 0.7728174603174603, f_beta: 0.7057587221521648
train: step: 389, loss: 0.161956325173378, acc: 0.75, recall: 0.75, precision: 0.7525252525252526, f_beta: 0.7493734335839599
train: step: 390, loss: 0.18061986565589905, acc: 0.83, recall: 0.83, precision: 0.8301320528211285, f_beta: 0.8299829982998299
train: step: 391, loss: 0.3551096022129059, acc: 0.56, recall: 0.56, precision: 0.5874125874125874, f_beta: 0.5225694444444444
train: step: 392, loss: 0.13372904062271118, acc: 0.86, recall: 0.8600000000000001, precision: 0.8652597402597402, f_beta: 0.8594941790445605
train: step: 393, loss: 0.1925714910030365, acc: 0.79, recall: 0.79, precision: 0.7957976336189311, f_beta: 0.7889659330720531
train: step: 394, loss: 0.1314309537410736, acc: 0.83, recall: 0.83, precision: 0.8301320528211285, f_beta: 0.8299829982998299
train: step: 395, loss: 0.2931739091873169, acc: 0.71, recall: 0.71, precision: 0.7206809583858764, f_beta: 0.7064480210547626
train: step: 396, loss: 0.17956210672855377, acc: 0.7, recall: 0.7, precision: 0.7083333333333333, f_beta: 0.6969696969696968
train: step: 397, loss: 0.2812335193157196, acc: 0.83, recall: 0.83, precision: 0.853925353925354, f_beta: 0.8270776116366596
train: step: 398, loss: 0.11716989427804947, acc: 0.9, recall: 0.9, precision: 0.9166666666666667, f_beta: 0.898989898989899
train: step: 399, loss: 0.20718000829219818, acc: 0.67, recall: 0.6699999999999999, precision: 0.7561784207353828, f_beta: 0.6396986570586309
train: step: 400, loss: 0.24560323357582092, acc: 0.8, recall: 0.8, precision: 0.8125, f_beta: 0.797979797979798
train: step: 401, loss: 0.13973772525787354, acc: 0.86, recall: 0.86, precision: 0.8820033955857385, f_beta: 0.8579545454545454
train: step: 402, loss: 0.1750926375389099, acc: 0.76, recall: 0.76, precision: 0.76, f_beta: 0.76
train: step: 403, loss: 0.11846485733985901, acc: 0.86, recall: 0.86, precision: 0.8694581280788177, f_beta: 0.8590982286634461
train: step: 404, loss: 0.3305552303791046, acc: 0.63, recall: 0.63, precision: 0.6366120218579234, f_beta: 0.6254681647940075
train: step: 405, loss: 0.33346518874168396, acc: 0.3, recall: 0.3, precision: 0.29474548440065684, f_beta: 0.29549114331723025
train: step: 406, loss: 0.22237415611743927, acc: 0.74, recall: 0.74, precision: 0.767379679144385, f_beta: 0.7331691297208538
train: step: 407, loss: 0.12956660985946655, acc: 0.85, recall: 0.8500000000000001, precision: 0.8535353535353536, f_beta: 0.849624060150376
train: step: 408, loss: 0.13899517059326172, acc: 0.85, recall: 0.8500000000000001, precision: 0.8569971440228478, f_beta: 0.8492613807657521
train: step: 409, loss: 0.24549074470996857, acc: 0.83, recall: 0.83, precision: 0.8467843631778058, f_beta: 0.8279178054458953
train: step: 410, loss: 0.3331514298915863, acc: 0.56, recall: 0.5599999999999999, precision: 0.5822368421052632, f_beta: 0.528099528099528
train: step: 411, loss: 0.26576998829841614, acc: 0.85, recall: 0.8500000000000001, precision: 0.8569971440228478, f_beta: 0.8492613807657521
train: step: 412, loss: 0.29579639434814453, acc: 0.8, recall: 0.8, precision: 0.8043831168831169, f_beta: 0.7992773986350863
train: step: 413, loss: 0.2321193665266037, acc: 0.83, recall: 0.83, precision: 0.8301320528211285, f_beta: 0.8299829982998299
train: step: 414, loss: 0.15605498850345612, acc: 0.82, recall: 0.82, precision: 0.82, f_beta: 0.82
train: step: 415, loss: 0.17744678258895874, acc: 0.85, recall: 0.85, precision: 0.8501400560224089, f_beta: 0.84998499849985
train: step: 416, loss: 0.19694240391254425, acc: 0.68, recall: 0.6799999999999999, precision: 0.7068014705882353, f_beta: 0.6692848284415047
train: step: 417, loss: 0.2281130850315094, acc: 0.66, recall: 0.66, precision: 0.6984126984126984, f_beta: 0.6427070197562001
train: step: 418, loss: 0.21577446162700653, acc: 0.69, recall: 0.69, precision: 0.7220663861617578, f_beta: 0.6783898744683057
train: step: 419, loss: 0.20947769284248352, acc: 0.75, recall: 0.75, precision: 0.7627154266498528, f_beta: 0.7469379491851402
train: step: 420, loss: 0.17903371155261993, acc: 0.75, recall: 0.75, precision: 0.792192613370734, f_beta: 0.7406369955389562
train: step: 421, loss: 0.19178444147109985, acc: 0.76, recall: 0.76, precision: 0.8378378378378378, f_beta: 0.7453310696095077
train: step: 422, loss: 0.15706495940685272, acc: 0.86, recall: 0.86, precision: 0.8694581280788177, f_beta: 0.8590982286634461
train: step: 423, loss: 0.1795622706413269, acc: 0.78, recall: 0.78, precision: 0.8119429590017826, f_beta: 0.7742200328407225
train: step: 424, loss: 0.1812041699886322, acc: 0.79, recall: 0.79, precision: 0.7929292929292929, f_beta: 0.7894736842105263
train: step: 425, loss: 0.12630751729011536, acc: 0.88, recall: 0.88, precision: 0.88, f_beta: 0.88
train: step: 426, loss: 0.2340782880783081, acc: 0.83, recall: 0.8300000000000001, precision: 0.8410500206696983, f_beta: 0.8286117552172598
train: step: 427, loss: 0.17068901658058167, acc: 0.77, recall: 0.77, precision: 0.7837326607818411, f_beta: 0.767182913250329
train: step: 428, loss: 0.1234041377902031, acc: 0.87, recall: 0.87, precision: 0.8888188314417822, f_beta: 0.868407733576273
train: step: 429, loss: 0.2375069260597229, acc: 0.85, recall: 0.85, precision: 0.8501400560224089, f_beta: 0.84998499849985
train: step: 430, loss: 0.13343451917171478, acc: 0.8, recall: 0.8, precision: 0.8078817733990147, f_beta: 0.7987117552334944
train: step: 431, loss: 0.2506462335586548, acc: 0.87, recall: 0.87, precision: 0.8713368125250903, f_beta: 0.8698828946051447
train: step: 432, loss: 0.44922271370887756, acc: 0.18, recall: 0.18, precision: 0.17159277504105092, f_beta: 0.1747181964573269
train: step: 433, loss: 0.14576935768127441, acc: 0.81, recall: 0.81, precision: 0.8111200321156162, f_beta: 0.8098288459613652
train: step: 434, loss: 0.39567023515701294, acc: 0.79, recall: 0.79, precision: 0.852112676056338, f_beta: 0.7803117480908044
train: step: 435, loss: 0.1472623646259308, acc: 0.85, recall: 0.85, precision: 0.867801597309794, f_beta: 0.848162769511084
train: step: 436, loss: 0.16602753102779388, acc: 0.79, recall: 0.79, precision: 0.8186813186813187, f_beta: 0.7851662404092071
train: step: 437, loss: 0.14882516860961914, acc: 0.79, recall: 0.79, precision: 0.8047498949138294, f_beta: 0.7874278773155178
train: step: 438, loss: 0.16133105754852295, acc: 0.79, recall: 0.79, precision: 0.7997106242248864, f_beta: 0.7882851093860268
train: step: 439, loss: 0.13311731815338135, acc: 0.81, recall: 0.81, precision: 0.8161974704202366, f_beta: 0.8090644156366193
train: step: 440, loss: 0.1081715077161789, acc: 0.84, recall: 0.8400000000000001, precision: 0.8421900161030595, f_beta: 0.8397435897435896
train: step: 441, loss: 0.12498753517866135, acc: 0.82, recall: 0.8200000000000001, precision: 0.8246753246753247, f_beta: 0.8193496587715776
train: step: 442, loss: 0.1605304777622223, acc: 0.75, recall: 0.75, precision: 0.7525252525252526, f_beta: 0.7493734335839599
train: step: 443, loss: 0.16880542039871216, acc: 0.86, recall: 0.86, precision: 0.8605769230769231, f_beta: 0.8599439775910365
train: step: 444, loss: 0.11734328418970108, acc: 0.86, recall: 0.8600000000000001, precision: 0.8652597402597402, f_beta: 0.8594941790445605
train: step: 445, loss: 0.11606288701295853, acc: 0.84, recall: 0.84, precision: 0.84, f_beta: 0.8399999999999999
train: step: 446, loss: 0.14118728041648865, acc: 0.79, recall: 0.79, precision: 0.7901160464185675, f_beta: 0.78997899789979
train: step: 447, loss: 0.10648182034492493, acc: 0.86, recall: 0.86, precision: 0.86, f_beta: 0.8599999999999999
train: step: 448, loss: 0.07134216278791428, acc: 0.93, recall: 0.9299999999999999, precision: 0.930172068827531, f_beta: 0.9299929992999301
train: step: 449, loss: 0.12261999398469925, acc: 0.87, recall: 0.87, precision: 0.8701480592236894, f_beta: 0.8699869986998701
train: step: 450, loss: 0.35857143998146057, acc: 0.67, recall: 0.6699999999999999, precision: 0.6706142111601766, f_beta: 0.6697027324592133
train: step: 451, loss: 0.38098758459091187, acc: 0.72, recall: 0.72, precision: 0.7619047619047619, f_beta: 0.7083333333333334
train: step: 452, loss: 0.14020311832427979, acc: 0.82, recall: 0.8200000000000001, precision: 0.8205128205128205, f_beta: 0.8199279711884755
train: step: 453, loss: 0.13786984980106354, acc: 0.87, recall: 0.87, precision: 0.8823894171145101, f_beta: 0.8689384010484928
train: step: 454, loss: 0.14330020546913147, acc: 0.81, recall: 0.81, precision: 0.8111200321156162, f_beta: 0.8098288459613652
train: step: 455, loss: 0.3649258017539978, acc: 0.76, recall: 0.76, precision: 0.7821180555555556, f_beta: 0.755201958384333
train: step: 456, loss: 0.13558171689510345, acc: 0.8, recall: 0.8, precision: 0.8019323671497585, f_beta: 0.7996794871794872
train: step: 457, loss: 0.37158599495887756, acc: 0.73, recall: 0.73, precision: 0.7466752466752467, f_beta: 0.7253585596582239
train: step: 458, loss: 0.36348000168800354, acc: 0.68, recall: 0.6799999999999999, precision: 0.7467105263157895, f_beta: 0.6567996567996568
train: step: 459, loss: 0.3551467955112457, acc: 0.67, recall: 0.6699999999999999, precision: 0.7561784207353828, f_beta: 0.6396986570586309
train: step: 460, loss: 0.07260748744010925, acc: 0.94, recall: 0.94, precision: 0.9428341384863124, f_beta: 0.9399038461538463
train: step: 461, loss: 0.10713914781808853, acc: 0.86, recall: 0.86, precision: 0.8605769230769231, f_beta: 0.8599439775910365
train: step: 462, loss: 0.1855154037475586, acc: 0.81, recall: 0.81, precision: 0.810124049619848, f_beta: 0.8099809980998101
train: step: 463, loss: 0.10636597871780396, acc: 0.87, recall: 0.87, precision: 0.8713368125250903, f_beta: 0.8698828946051447
train: step: 464, loss: 0.3137585520744324, acc: 0.23, recall: 0.23, precision: 0.22902448815736653, f_beta: 0.22930637573816437
train: step: 465, loss: 0.1045365110039711, acc: 0.9, recall: 0.9, precision: 0.9, f_beta: 0.9
train: step: 466, loss: 0.10140824317932129, acc: 0.86, recall: 0.86, precision: 0.8820033955857385, f_beta: 0.8579545454545454
train: step: 467, loss: 0.13219106197357178, acc: 0.86, recall: 0.86, precision: 0.8623188405797102, f_beta: 0.859775641025641
train: step: 468, loss: 0.10733648389577866, acc: 0.86, recall: 0.86, precision: 0.86, f_beta: 0.8599999999999999
train: step: 469, loss: 0.1576659381389618, acc: 0.77, recall: 0.77, precision: 0.7701080432172869, f_beta: 0.7699769976997699
train: step: 470, loss: 0.11809928715229034, acc: 0.88, recall: 0.8799999999999999, precision: 0.885551948051948, f_beta: 0.8795664391810518
train: step: 471, loss: 0.08896920830011368, acc: 0.91, recall: 0.9099999999999999, precision: 0.9114813327980731, f_beta: 0.9099189270343309
train: step: 472, loss: 0.12778609991073608, acc: 0.84, recall: 0.8400000000000001, precision: 0.8421900161030595, f_beta: 0.8397435897435896
train: step: 473, loss: 0.32481080293655396, acc: 0.86, recall: 0.86, precision: 0.875, f_beta: 0.8585858585858586
train: step: 474, loss: 0.16442592442035675, acc: 0.76, recall: 0.76, precision: 0.7616747181964574, f_beta: 0.7596153846153846
train: step: 475, loss: 0.1645282506942749, acc: 0.75, recall: 0.75, precision: 0.750903251706142, f_beta: 0.7497747973175859
train: step: 476, loss: 0.1906730830669403, acc: 0.84, recall: 0.84, precision: 0.8405448717948718, f_beta: 0.8399359743897559
train: step: 477, loss: 0.08086126297712326, acc: 0.89, recall: 0.89, precision: 0.89015606242497, f_beta: 0.88998899889989
train: step: 478, loss: 0.14691388607025146, acc: 0.79, recall: 0.79, precision: 0.811025311025311, f_beta: 0.7863899908452854
train: step: 479, loss: 0.34991592168807983, acc: 0.81, recall: 0.81, precision: 0.8257671290458175, f_beta: 0.8076728413807065
train: step: 480, loss: 0.156369149684906, acc: 0.76, recall: 0.76, precision: 0.7758913412563667, f_beta: 0.7564935064935063
train: step: 481, loss: 0.08864141255617142, acc: 0.92, recall: 0.9199999999999999, precision: 0.9206730769230769, f_beta: 0.919967987194878
train: step: 482, loss: 0.16193737089633942, acc: 0.84, recall: 0.8400000000000001, precision: 0.8449675324675325, f_beta: 0.839421918908069
train: step: 483, loss: 0.09328001737594604, acc: 0.89, recall: 0.8899999999999999, precision: 0.8939393939393939, f_beta: 0.8897243107769424
train: step: 484, loss: 0.15789619088172913, acc: 0.77, recall: 0.77, precision: 0.7790409260024803, f_beta: 0.7681217864704104
train: step: 485, loss: 0.12860006093978882, acc: 0.81, recall: 0.81, precision: 0.8161974704202366, f_beta: 0.8090644156366193
train: step: 486, loss: 0.12085609138011932, acc: 0.83, recall: 0.8300000000000001, precision: 0.8311922922521076, f_beta: 0.8298468621759585
train: step: 487, loss: 0.22991417348384857, acc: 0.65, recall: 0.65, precision: 0.7941176470588236, f_beta: 0.6011396011396011
train: step: 488, loss: 0.1220313161611557, acc: 0.8, recall: 0.8, precision: 0.8125, f_beta: 0.797979797979798
train: step: 489, loss: 0.1721261590719223, acc: 0.76, recall: 0.76, precision: 0.8095238095238095, f_beta: 0.75
train: step: 490, loss: 0.15649372339248657, acc: 0.79, recall: 0.79, precision: 0.811025311025311, f_beta: 0.7863899908452854
train: step: 491, loss: 0.11565136164426804, acc: 0.83, recall: 0.8300000000000001, precision: 0.8333333333333334, f_beta: 0.8295739348370927
train: step: 492, loss: 0.1849082112312317, acc: 0.75, recall: 0.75, precision: 0.7525252525252526, f_beta: 0.7493734335839599
train: step: 493, loss: 0.12619967758655548, acc: 0.8, recall: 0.8, precision: 0.8183361629881154, f_beta: 0.797077922077922
train: step: 494, loss: 0.06482809782028198, acc: 0.93, recall: 0.9299999999999999, precision: 0.9315535929345644, f_beta: 0.9299369432489241
train: step: 495, loss: 0.1413402557373047, acc: 0.8, recall: 0.8, precision: 0.8019323671497585, f_beta: 0.7996794871794872
train: step: 496, loss: 0.07823284715414047, acc: 0.88, recall: 0.88, precision: 0.8806089743589745, f_beta: 0.879951980792317
train: step: 497, loss: 0.13439719378948212, acc: 0.81, recall: 0.81, precision: 0.8131313131313131, f_beta: 0.8095238095238095
train: step: 498, loss: 0.12734560668468475, acc: 0.83, recall: 0.83, precision: 0.8467843631778058, f_beta: 0.8279178054458953
train: step: 499, loss: 0.12734295427799225, acc: 0.85, recall: 0.85, precision: 0.8501400560224089, f_beta: 0.84998499849985
train: step: 500, loss: 0.09772183746099472, acc: 0.88, recall: 0.88, precision: 0.8824476650563606, f_beta: 0.8798076923076923
train: step: 501, loss: 0.05972406268119812, acc: 0.91, recall: 0.9099999999999999, precision: 0.9114813327980731, f_beta: 0.9099189270343309
train: step: 502, loss: 0.11266565322875977, acc: 0.85, recall: 0.85, precision: 0.851264552388599, f_beta: 0.8498648783905516
train: step: 503, loss: 0.10768181085586548, acc: 0.85, recall: 0.8500000000000001, precision: 0.8535353535353536, f_beta: 0.849624060150376
train: step: 504, loss: 0.12641678750514984, acc: 0.79, recall: 0.79, precision: 0.7901160464185675, f_beta: 0.78997899789979
train: step: 505, loss: 0.12720787525177002, acc: 0.82, recall: 0.8200000000000001, precision: 0.822061191626409, f_beta: 0.8197115384615385
train: step: 506, loss: 0.07303283363580704, acc: 0.91, recall: 0.91, precision: 0.9101640656262505, f_beta: 0.90999099909991
train: step: 507, loss: 0.1383064091205597, acc: 0.81, recall: 0.81, precision: 0.8161974704202366, f_beta: 0.8090644156366193
train: step: 508, loss: 0.07769634574651718, acc: 0.87, recall: 0.87, precision: 0.8773969808241534, f_beta: 0.8693598633303186
train: step: 509, loss: 0.11426663398742676, acc: 0.85, recall: 0.85, precision: 0.8501400560224089, f_beta: 0.84998499849985
train: step: 510, loss: 0.13268665969371796, acc: 0.81, recall: 0.81, precision: 0.8161974704202366, f_beta: 0.8090644156366193
train: step: 511, loss: 0.11180178821086884, acc: 0.85, recall: 0.8500000000000001, precision: 0.8535353535353536, f_beta: 0.849624060150376
train: step: 512, loss: 0.12665465474128723, acc: 0.83, recall: 0.83, precision: 0.8301320528211285, f_beta: 0.8299829982998299
train: step: 513, loss: 0.1373198926448822, acc: 0.83, recall: 0.83, precision: 0.8301320528211285, f_beta: 0.8299829982998299
train: step: 514, loss: 0.1266830861568451, acc: 0.79, recall: 0.79, precision: 0.7910477719791249, f_beta: 0.7898108297467722
train: step: 515, loss: 0.10430369526147842, acc: 0.87, recall: 0.87, precision: 0.8773969808241534, f_beta: 0.8693598633303186
train: step: 516, loss: 0.1214165985584259, acc: 0.83, recall: 0.8300000000000001, precision: 0.8410500206696983, f_beta: 0.8286117552172598
train: step: 517, loss: 0.1560983508825302, acc: 0.77, recall: 0.77, precision: 0.7837326607818411, f_beta: 0.767182913250329
train: step: 518, loss: 0.15898893773555756, acc: 0.78, recall: 0.78, precision: 0.7916666666666666, f_beta: 0.7777777777777779
train: step: 519, loss: 0.16555386781692505, acc: 0.76, recall: 0.76, precision: 0.7668308702791462, f_beta: 0.7584541062801933
train: step: 520, loss: 0.0894242450594902, acc: 0.9, recall: 0.8999999999999999, precision: 0.9025764895330113, f_beta: 0.8998397435897436
train: step: 521, loss: 0.17053523659706116, acc: 0.77, recall: 0.77, precision: 0.7753977968176254, f_beta: 0.7688674505074867
train: step: 522, loss: 0.10415729880332947, acc: 0.84, recall: 0.84, precision: 0.8405448717948718, f_beta: 0.8399359743897559
train: step: 523, loss: 0.11112900078296661, acc: 0.84, recall: 0.8400000000000001, precision: 0.8421900161030595, f_beta: 0.8397435897435896
train: step: 524, loss: 0.14868175983428955, acc: 0.75, recall: 0.75, precision: 0.7583712277800745, f_beta: 0.7479584635547939
train: step: 525, loss: 0.11140890419483185, acc: 0.85, recall: 0.8500000000000001, precision: 0.8569971440228478, f_beta: 0.8492613807657521
train: step: 526, loss: 0.10609542578458786, acc: 0.85, recall: 0.85, precision: 0.8617197188921042, f_beta: 0.8487750781328762
train: step: 527, loss: 0.1417437195777893, acc: 0.77, recall: 0.77, precision: 0.7701080432172869, f_beta: 0.7699769976997699
train: step: 528, loss: 0.13715556263923645, acc: 0.8, recall: 0.8, precision: 0.8, f_beta: 0.8000000000000002
train: step: 529, loss: 0.10397409647703171, acc: 0.88, recall: 0.88, precision: 0.8824476650563606, f_beta: 0.8798076923076923
train: step: 530, loss: 0.12704043090343475, acc: 0.81, recall: 0.81, precision: 0.8131313131313131, f_beta: 0.8095238095238095
train: step: 531, loss: 0.054468706250190735, acc: 0.96, recall: 0.96, precision: 0.962962962962963, f_beta: 0.9599358974358974
train: step: 532, loss: 0.10903611034154892, acc: 0.85, recall: 0.8500000000000001, precision: 0.8569971440228478, f_beta: 0.8492613807657521
train: step: 533, loss: 0.07879585027694702, acc: 0.9, recall: 0.8999999999999999, precision: 0.9025764895330113, f_beta: 0.8998397435897436
train: step: 534, loss: 0.09905890375375748, acc: 0.88, recall: 0.88, precision: 0.8806089743589745, f_beta: 0.879951980792317
train: step: 535, loss: 0.07200225442647934, acc: 0.91, recall: 0.91, precision: 0.9101640656262505, f_beta: 0.90999099909991
train: step: 536, loss: 0.08521686494350433, acc: 0.88, recall: 0.88, precision: 0.8958333333333333, f_beta: 0.8787878787878789
train: step: 537, loss: 0.11065702140331268, acc: 0.86, recall: 0.86, precision: 0.8605769230769231, f_beta: 0.8599439775910365
train: step: 538, loss: 0.0743219405412674, acc: 0.9, recall: 0.8999999999999999, precision: 0.9025764895330113, f_beta: 0.8998397435897436
train: step: 539, loss: 0.10090763121843338, acc: 0.88, recall: 0.88, precision: 0.88, f_beta: 0.88
train: step: 540, loss: 0.08624932914972305, acc: 0.87, recall: 0.87, precision: 0.8737373737373737, f_beta: 0.8696741854636592
train: step: 541, loss: 0.10286099463701248, acc: 0.86, recall: 0.86, precision: 0.86, f_beta: 0.8599999999999999
train: step: 542, loss: 0.1077614575624466, acc: 0.88, recall: 0.88, precision: 0.8806089743589745, f_beta: 0.879951980792317
train: step: 543, loss: 0.08022268116474152, acc: 0.88, recall: 0.88, precision: 0.8806089743589745, f_beta: 0.879951980792317
train: step: 544, loss: 0.08315490931272507, acc: 0.86, recall: 0.86, precision: 0.8623188405797102, f_beta: 0.859775641025641
train: step: 545, loss: 0.10715668648481369, acc: 0.85, recall: 0.85, precision: 0.851264552388599, f_beta: 0.8498648783905516
train: step: 546, loss: 0.1416974514722824, acc: 0.82, recall: 0.8200000000000001, precision: 0.822061191626409, f_beta: 0.8197115384615385
train: step: 547, loss: 0.134238138794899, acc: 0.82, recall: 0.82, precision: 0.82, f_beta: 0.82
train: step: 548, loss: 0.13176988065242767, acc: 0.81, recall: 0.81, precision: 0.810124049619848, f_beta: 0.8099809980998101
train: step: 549, loss: 0.10773725807666779, acc: 0.86, recall: 0.86, precision: 0.8605769230769231, f_beta: 0.8599439775910365
train: step: 550, loss: 0.1211150586605072, acc: 0.84, recall: 0.84, precision: 0.8541666666666667, f_beta: 0.8383838383838385
train: step: 551, loss: 0.06478650122880936, acc: 0.91, recall: 0.9099999999999999, precision: 0.9181966544267646, f_beta: 0.9095568284594513
train: step: 552, loss: 0.08556199073791504, acc: 0.88, recall: 0.88, precision: 0.88, f_beta: 0.88
train: step: 553, loss: 0.1366567611694336, acc: 0.79, recall: 0.79, precision: 0.8389434315100515, f_beta: 0.7821350762527233
train: step: 554, loss: 0.1455937623977661, acc: 0.82, recall: 0.8200000000000001, precision: 0.8284072249589491, f_beta: 0.818840579710145
train: step: 555, loss: 0.07340239733457565, acc: 0.92, recall: 0.92, precision: 0.92, f_beta: 0.92
train: step: 556, loss: 0.08675368130207062, acc: 0.88, recall: 0.88, precision: 0.8824476650563606, f_beta: 0.8798076923076923
train: step: 557, loss: 0.15528613328933716, acc: 0.82, recall: 0.8200000000000001, precision: 0.8472222222222222, f_beta: 0.8164014687882497
train: step: 558, loss: 0.08725860714912415, acc: 0.9, recall: 0.8999999999999999, precision: 0.9025764895330113, f_beta: 0.8998397435897436
train: step: 559, loss: 0.1581646054983139, acc: 0.76, recall: 0.76, precision: 0.7896613190730838, f_beta: 0.7536945812807881
train: step: 560, loss: 0.12211570888757706, acc: 0.85, recall: 0.85, precision: 0.8617197188921042, f_beta: 0.8487750781328762
train: step: 561, loss: 0.11810456216335297, acc: 0.86, recall: 0.86, precision: 0.8694581280788177, f_beta: 0.8590982286634461
train: step: 562, loss: 0.09502468258142471, acc: 0.86, recall: 0.86, precision: 0.8605769230769231, f_beta: 0.8599439775910365
train: step: 563, loss: 0.10556858777999878, acc: 0.85, recall: 0.8500000000000001, precision: 0.8535353535353536, f_beta: 0.849624060150376
train: step: 564, loss: 0.11729244887828827, acc: 0.85, recall: 0.85, precision: 0.8501400560224089, f_beta: 0.84998499849985
train: step: 565, loss: 0.1449042558670044, acc: 0.82, recall: 0.8200000000000001, precision: 0.8205128205128205, f_beta: 0.8199279711884755
train: step: 566, loss: 0.10735800117254257, acc: 0.88, recall: 0.8799999999999999, precision: 0.885551948051948, f_beta: 0.8795664391810518
train: step: 567, loss: 0.08643317222595215, acc: 0.87, recall: 0.87, precision: 0.8713368125250903, f_beta: 0.8698828946051447
train: step: 568, loss: 0.05625074729323387, acc: 0.93, recall: 0.9299999999999999, precision: 0.930172068827531, f_beta: 0.9299929992999301
train: step: 569, loss: 0.07364141941070557, acc: 0.89, recall: 0.89, precision: 0.89015606242497, f_beta: 0.88998899889989
train: step: 570, loss: 0.08894035220146179, acc: 0.9, recall: 0.9, precision: 0.9006410256410255, f_beta: 0.8999599839935974
train: step: 571, loss: 0.3970049023628235, acc: 0.82, recall: 0.8200000000000001, precision: 0.8333333333333334, f_beta: 0.8181818181818181
train: step: 572, loss: 0.08079007267951965, acc: 0.89, recall: 0.89, precision: 0.8914090726615818, f_beta: 0.8899009108197378
train: step: 573, loss: 0.1047070100903511, acc: 0.88, recall: 0.88, precision: 0.88, f_beta: 0.88
train: step: 574, loss: 0.07908038049936295, acc: 0.89, recall: 0.8899999999999999, precision: 0.8939393939393939, f_beta: 0.8897243107769424
train: step: 575, loss: 0.11474312096834183, acc: 0.82, recall: 0.8200000000000001, precision: 0.8246753246753247, f_beta: 0.8193496587715776
train: step: 576, loss: 0.3146863877773285, acc: 0.87, recall: 0.87, precision: 0.8737373737373737, f_beta: 0.8696741854636592
train: step: 577, loss: 0.10322128236293793, acc: 0.83, recall: 0.8300000000000001, precision: 0.8333333333333334, f_beta: 0.8295739348370927
train: step: 578, loss: 0.13061057031154633, acc: 0.81, recall: 0.81, precision: 0.8203803224472923, f_beta: 0.8084484323016432
train: step: 579, loss: 0.07368070632219315, acc: 0.9, recall: 0.8999999999999999, precision: 0.9025764895330113, f_beta: 0.8998397435897436
train: step: 580, loss: 0.09594188630580902, acc: 0.88, recall: 0.88, precision: 0.8899835796387521, f_beta: 0.8792270531400966
train: step: 581, loss: 0.11440858244895935, acc: 0.86, recall: 0.8600000000000001, precision: 0.8652597402597402, f_beta: 0.8594941790445605
train: step: 582, loss: 0.14808990061283112, acc: 0.78, recall: 0.78, precision: 0.78, f_beta: 0.78
train: step: 583, loss: 0.13229356706142426, acc: 0.82, recall: 0.82, precision: 0.8395585738539898, f_beta: 0.8173701298701298
train: step: 584, loss: 0.1235988587141037, acc: 0.81, recall: 0.81, precision: 0.8203803224472923, f_beta: 0.8084484323016432
train: step: 585, loss: 0.1106177344918251, acc: 0.88, recall: 0.88, precision: 0.8824476650563606, f_beta: 0.8798076923076923
train: step: 586, loss: 0.11789407581090927, acc: 0.87, recall: 0.87, precision: 0.8701480592236894, f_beta: 0.8699869986998701
train: step: 587, loss: 0.1210283637046814, acc: 0.85, recall: 0.8500000000000001, precision: 0.8535353535353536, f_beta: 0.849624060150376
train: step: 588, loss: 0.12232832610607147, acc: 0.85, recall: 0.85, precision: 0.8501400560224089, f_beta: 0.84998499849985
train: step: 589, loss: 0.30331146717071533, acc: 0.81, recall: 0.81, precision: 0.8203803224472923, f_beta: 0.8084484323016432
train: step: 590, loss: 0.1300351768732071, acc: 0.82, recall: 0.8200000000000001, precision: 0.8333333333333334, f_beta: 0.8181818181818181
train: step: 591, loss: 0.12153124809265137, acc: 0.85, recall: 0.85, precision: 0.8753753753753755, f_beta: 0.8474214220323468
train: step: 592, loss: 0.155358225107193, acc: 0.77, recall: 0.77, precision: 0.8278290432248665, f_beta: 0.7593890574327858
train: step: 593, loss: 0.13775458931922913, acc: 0.8, recall: 0.8, precision: 0.8125, f_beta: 0.797979797979798
train: step: 594, loss: 0.16438952088356018, acc: 0.8, recall: 0.8, precision: 0.8183361629881154, f_beta: 0.797077922077922
train: step: 595, loss: 0.0899457186460495, acc: 0.87, recall: 0.87, precision: 0.8737373737373737, f_beta: 0.8696741854636592
train: step: 596, loss: 0.09223698824644089, acc: 0.88, recall: 0.88, precision: 0.8806089743589745, f_beta: 0.879951980792317
train: step: 597, loss: 0.08750812709331512, acc: 0.86, recall: 0.86, precision: 0.86, f_beta: 0.8599999999999999
train: step: 598, loss: 0.15430040657520294, acc: 0.81, recall: 0.81, precision: 0.8324753324753325, f_beta: 0.8067338012409724
train: step: 599, loss: 0.1071566641330719, acc: 0.85, recall: 0.8500000000000001, precision: 0.8535353535353536, f_beta: 0.849624060150376
train: step: 600, loss: 0.09741085022687912, acc: 0.85, recall: 0.85, precision: 0.851264552388599, f_beta: 0.8498648783905516
train: step: 601, loss: 0.10967954993247986, acc: 0.85, recall: 0.8500000000000001, precision: 0.8535353535353536, f_beta: 0.849624060150376
train: step: 602, loss: 0.15290303528308868, acc: 0.79, recall: 0.79, precision: 0.7901160464185675, f_beta: 0.78997899789979
train: step: 603, loss: 0.15141785144805908, acc: 0.79, recall: 0.79, precision: 0.7997106242248864, f_beta: 0.7882851093860268
train: step: 604, loss: 0.12497539818286896, acc: 0.8, recall: 0.8, precision: 0.8125, f_beta: 0.797979797979798
train: step: 605, loss: 0.34723037481307983, acc: 0.72, recall: 0.72, precision: 0.7728174603174603, f_beta: 0.7057587221521648
train: step: 606, loss: 0.13519619405269623, acc: 0.82, recall: 0.8200000000000001, precision: 0.822061191626409, f_beta: 0.8197115384615385
train: step: 607, loss: 0.11280510574579239, acc: 0.82, recall: 0.8200000000000001, precision: 0.8246753246753247, f_beta: 0.8193496587715776
train: step: 608, loss: 0.13931158185005188, acc: 0.84, recall: 0.8400000000000001, precision: 0.8489326765188834, f_beta: 0.8389694041867956
train: step: 609, loss: 0.1957731395959854, acc: 0.66, recall: 0.66, precision: 0.6666666666666666, f_beta: 0.6565656565656566
train: step: 610, loss: 0.098625048995018, acc: 0.86, recall: 0.86, precision: 0.8820033955857385, f_beta: 0.8579545454545454
train: step: 611, loss: 0.09398327767848969, acc: 0.85, recall: 0.85, precision: 0.851264552388599, f_beta: 0.8498648783905516
train: step: 612, loss: 0.1760074943304062, acc: 0.72, recall: 0.72, precision: 0.7232142857142857, f_beta: 0.7189883580891208
train: step: 613, loss: 0.10348722338676453, acc: 0.86, recall: 0.86, precision: 0.8605769230769231, f_beta: 0.8599439775910365
train: step: 614, loss: 0.0626511350274086, acc: 0.92, recall: 0.9199999999999999, precision: 0.9261363636363636, f_beta: 0.9197109594540345
train: step: 615, loss: 0.09993059933185577, acc: 0.86, recall: 0.86, precision: 0.86, f_beta: 0.8599999999999999
train: step: 616, loss: 0.07577136904001236, acc: 0.91, recall: 0.9099999999999999, precision: 0.9181966544267646, f_beta: 0.9095568284594513
train: step: 617, loss: 0.38267308473587036, acc: 0.68, recall: 0.6799999999999999, precision: 0.8048780487804879, f_beta: 0.6434937611408199
train: step: 618, loss: 0.08615844696760178, acc: 0.85, recall: 0.8500000000000001, precision: 0.8569971440228478, f_beta: 0.8492613807657521
train: step: 619, loss: 0.0616765134036541, acc: 0.92, recall: 0.9199999999999999, precision: 0.9206730769230769, f_beta: 0.919967987194878
train: step: 620, loss: 0.10506657510995865, acc: 0.89, recall: 0.89, precision: 0.8914090726615818, f_beta: 0.8899009108197378
train: step: 621, loss: 0.062003038823604584, acc: 0.93, recall: 0.9299999999999999, precision: 0.930172068827531, f_beta: 0.9299929992999301
train: step: 622, loss: 0.07065030932426453, acc: 0.93, recall: 0.9299999999999999, precision: 0.9343434343434343, f_beta: 0.9298245614035088
train: step: 623, loss: 0.11530538648366928, acc: 0.87, recall: 0.87, precision: 0.8713368125250903, f_beta: 0.8698828946051447
train: step: 624, loss: 0.07425181567668915, acc: 0.89, recall: 0.89, precision: 0.89015606242497, f_beta: 0.88998899889989
train: step: 625, loss: 0.13637767732143402, acc: 0.81, recall: 0.81, precision: 0.810124049619848, f_beta: 0.8099809980998101
train: step: 626, loss: 0.2619374990463257, acc: 0.87, recall: 0.87, precision: 0.8737373737373737, f_beta: 0.8696741854636592
train: step: 627, loss: 0.10190952569246292, acc: 0.86, recall: 0.86, precision: 0.875, f_beta: 0.8585858585858586
train: step: 628, loss: 0.15910857915878296, acc: 0.77, recall: 0.77, precision: 0.7967032967032968, f_beta: 0.7647058823529411
train: step: 629, loss: 0.13369794189929962, acc: 0.8, recall: 0.8, precision: 0.8342245989304813, f_beta: 0.794745484400657
train: step: 630, loss: 0.07273276150226593, acc: 0.89, recall: 0.8899999999999999, precision: 0.8939393939393939, f_beta: 0.8897243107769424
train: step: 631, loss: 0.1107955351471901, acc: 0.84, recall: 0.8400000000000001, precision: 0.8421900161030595, f_beta: 0.8397435897435896
train: step: 632, loss: 0.08496488630771637, acc: 0.91, recall: 0.9099999999999999, precision: 0.9114813327980731, f_beta: 0.9099189270343309
train: step: 633, loss: 0.16815894842147827, acc: 0.78, recall: 0.78, precision: 0.8472222222222222, f_beta: 0.7688104245481295
train: step: 634, loss: 0.11413010954856873, acc: 0.85, recall: 0.85, precision: 0.8617197188921042, f_beta: 0.8487750781328762
train: step: 635, loss: 0.09011486172676086, acc: 0.88, recall: 0.88, precision: 0.8899835796387521, f_beta: 0.8792270531400966
train: step: 636, loss: 0.06743107736110687, acc: 0.91, recall: 0.9099999999999999, precision: 0.9141414141414141, f_beta: 0.9097744360902256
train: step: 637, loss: 0.10667163878679276, acc: 0.86, recall: 0.86, precision: 0.8694581280788177, f_beta: 0.8590982286634461
train: step: 638, loss: 0.13235771656036377, acc: 0.84, recall: 0.8400000000000001, precision: 0.8421900161030595, f_beta: 0.8397435897435896
train: step: 639, loss: 0.0915846899151802, acc: 0.89, recall: 0.8899999999999999, precision: 0.8939393939393939, f_beta: 0.8897243107769424
train: step: 640, loss: 0.09598388522863388, acc: 0.89, recall: 0.8899999999999999, precision: 0.8939393939393939, f_beta: 0.8897243107769424
train: step: 641, loss: 0.11394687741994858, acc: 0.85, recall: 0.8500000000000001, precision: 0.8569971440228478, f_beta: 0.8492613807657521
train: step: 642, loss: 0.12975367903709412, acc: 0.85, recall: 0.85, precision: 0.8617197188921042, f_beta: 0.8487750781328762
train: step: 643, loss: 0.1024850383400917, acc: 0.87, recall: 0.87, precision: 0.8737373737373737, f_beta: 0.8696741854636592
train: step: 644, loss: 0.12401849776506424, acc: 0.87, recall: 0.87, precision: 0.8773969808241534, f_beta: 0.8693598633303186
train: step: 645, loss: 0.08515051007270813, acc: 0.89, recall: 0.8899999999999999, precision: 0.8977968176254589, f_beta: 0.8894583458948848
train: step: 646, loss: 0.11510614305734634, acc: 0.84, recall: 0.8400000000000001, precision: 0.8421900161030595, f_beta: 0.8397435897435896
train: step: 647, loss: 0.07613737136125565, acc: 0.9, recall: 0.9, precision: 0.9006410256410255, f_beta: 0.8999599839935974
train: step: 648, loss: 0.12170368432998657, acc: 0.84, recall: 0.8400000000000001, precision: 0.8449675324675325, f_beta: 0.839421918908069
train: step: 649, loss: 0.0929584950208664, acc: 0.86, recall: 0.8600000000000001, precision: 0.8652597402597402, f_beta: 0.8594941790445605
train: step: 650, loss: 0.10258058458566666, acc: 0.85, recall: 0.8500000000000001, precision: 0.8569971440228478, f_beta: 0.8492613807657521
train: step: 651, loss: 0.08505082130432129, acc: 0.86, recall: 0.86, precision: 0.8623188405797102, f_beta: 0.859775641025641
train: step: 652, loss: 0.13220317661762238, acc: 0.84, recall: 0.84, precision: 0.8405448717948718, f_beta: 0.8399359743897559
train: step: 653, loss: 0.1329466849565506, acc: 0.82, recall: 0.8200000000000001, precision: 0.8205128205128205, f_beta: 0.8199279711884755
train: step: 654, loss: 0.07783451676368713, acc: 0.91, recall: 0.91, precision: 0.9101640656262505, f_beta: 0.90999099909991
train: step: 655, loss: 0.13253411650657654, acc: 0.81, recall: 0.81, precision: 0.8131313131313131, f_beta: 0.8095238095238095
train: step: 656, loss: 0.06866209954023361, acc: 0.91, recall: 0.91, precision: 0.9101640656262505, f_beta: 0.90999099909991
train: step: 657, loss: 0.08344997465610504, acc: 0.88, recall: 0.88, precision: 0.8824476650563606, f_beta: 0.8798076923076923
train: step: 658, loss: 0.07771828770637512, acc: 0.89, recall: 0.89, precision: 0.8914090726615818, f_beta: 0.8899009108197378
train: step: 659, loss: 0.06956658512353897, acc: 0.93, recall: 0.9299999999999999, precision: 0.9343434343434343, f_beta: 0.9298245614035088
train: step: 660, loss: 0.06918731331825256, acc: 0.9, recall: 0.9, precision: 0.9, f_beta: 0.9
train: step: 661, loss: 0.08560207486152649, acc: 0.89, recall: 0.89, precision: 0.89015606242497, f_beta: 0.88998899889989
train: step: 662, loss: 0.13056804239749908, acc: 0.79, recall: 0.79, precision: 0.7910477719791249, f_beta: 0.7898108297467722
train: step: 663, loss: 0.08879859745502472, acc: 0.9, recall: 0.9, precision: 0.9006410256410255, f_beta: 0.8999599839935974
train: step: 664, loss: 0.05111562833189964, acc: 0.96, recall: 0.96, precision: 0.96, f_beta: 0.96
train: step: 665, loss: 0.0741926059126854, acc: 0.9, recall: 0.9, precision: 0.9166666666666667, f_beta: 0.898989898989899
train: step: 666, loss: 0.11440470069646835, acc: 0.85, recall: 0.85, precision: 0.8501400560224089, f_beta: 0.84998499849985
train: step: 667, loss: 0.11727994680404663, acc: 0.86, recall: 0.86, precision: 0.8623188405797102, f_beta: 0.859775641025641
train: step: 668, loss: 0.11988703906536102, acc: 0.86, recall: 0.86, precision: 0.8623188405797102, f_beta: 0.859775641025641
train: step: 669, loss: 0.0893978700041771, acc: 0.89, recall: 0.8899999999999999, precision: 0.8939393939393939, f_beta: 0.8897243107769424
train: step: 670, loss: 0.05561468005180359, acc: 0.92, recall: 0.92, precision: 0.92, f_beta: 0.92
train: step: 671, loss: 0.04569186270236969, acc: 0.92, recall: 0.9199999999999999, precision: 0.9261363636363636, f_beta: 0.9197109594540345
train: step: 672, loss: 0.10114406794309616, acc: 0.86, recall: 0.86, precision: 0.86, f_beta: 0.8599999999999999
train: step: 673, loss: 0.1031741201877594, acc: 0.87, recall: 0.87, precision: 0.8701480592236894, f_beta: 0.8699869986998701
train: step: 674, loss: 0.08890418708324432, acc: 0.88, recall: 0.88, precision: 0.8806089743589745, f_beta: 0.879951980792317
train: step: 675, loss: 0.072597935795784, acc: 0.9, recall: 0.9, precision: 0.9, f_beta: 0.9
train: step: 676, loss: 0.057651061564683914, acc: 0.94, recall: 0.94, precision: 0.94, f_beta: 0.94
train: step: 677, loss: 0.07593759894371033, acc: 0.88, recall: 0.88, precision: 0.8806089743589745, f_beta: 0.879951980792317
train: step: 678, loss: 0.10422720015048981, acc: 0.87, recall: 0.87, precision: 0.8713368125250903, f_beta: 0.8698828946051447
train: step: 679, loss: 0.07257100939750671, acc: 0.91, recall: 0.91, precision: 0.9101640656262505, f_beta: 0.90999099909991
train: step: 680, loss: 0.08996520191431046, acc: 0.89, recall: 0.89, precision: 0.89015606242497, f_beta: 0.88998899889989
train: step: 681, loss: 0.10193023830652237, acc: 0.85, recall: 0.85, precision: 0.8501400560224089, f_beta: 0.84998499849985
train: step: 682, loss: 0.08770424127578735, acc: 0.87, recall: 0.87, precision: 0.8713368125250903, f_beta: 0.8698828946051447
train: step: 683, loss: 0.09117565304040909, acc: 0.88, recall: 0.88, precision: 0.8806089743589745, f_beta: 0.879951980792317
train: step: 684, loss: 0.0440794862806797, acc: 0.96, recall: 0.96, precision: 0.9607371794871795, f_beta: 0.959983993597439
train: step: 685, loss: 0.07814304530620575, acc: 0.9, recall: 0.8999999999999999, precision: 0.9025764895330113, f_beta: 0.8998397435897436
train: step: 686, loss: 0.055578604340553284, acc: 0.91, recall: 0.9099999999999999, precision: 0.9114813327980731, f_beta: 0.9099189270343309
train: step: 687, loss: 0.08849141746759415, acc: 0.87, recall: 0.87, precision: 0.8713368125250903, f_beta: 0.8698828946051447
train: step: 688, loss: 0.11069212108850479, acc: 0.86, recall: 0.8600000000000001, precision: 0.8652597402597402, f_beta: 0.8594941790445605
train: step: 689, loss: 0.08105887472629547, acc: 0.89, recall: 0.89, precision: 0.89015606242497, f_beta: 0.88998899889989
train: step: 690, loss: 0.08076966553926468, acc: 0.92, recall: 0.9199999999999999, precision: 0.9206730769230769, f_beta: 0.919967987194878
train: step: 691, loss: 0.1298477053642273, acc: 0.8, recall: 0.8, precision: 0.8078817733990147, f_beta: 0.7987117552334944
train: step: 692, loss: 0.11179555207490921, acc: 0.85, recall: 0.85, precision: 0.851264552388599, f_beta: 0.8498648783905516
train: step: 693, loss: 0.10283582657575607, acc: 0.84, recall: 0.8400000000000001, precision: 0.8449675324675325, f_beta: 0.839421918908069
train: step: 694, loss: 0.059147290885448456, acc: 0.92, recall: 0.9199999999999999, precision: 0.9206730769230769, f_beta: 0.919967987194878
train: step: 695, loss: 0.09639545530080795, acc: 0.87, recall: 0.87, precision: 0.8701480592236894, f_beta: 0.8699869986998701
train: step: 696, loss: 0.07117396593093872, acc: 0.91, recall: 0.91, precision: 0.9101640656262505, f_beta: 0.90999099909991
train: step: 697, loss: 0.06674133986234665, acc: 0.91, recall: 0.9099999999999999, precision: 0.9141414141414141, f_beta: 0.9097744360902256
train: step: 698, loss: 0.10299354791641235, acc: 0.87, recall: 0.87, precision: 0.8773969808241534, f_beta: 0.8693598633303186
train: step: 699, loss: 0.12141098082065582, acc: 0.86, recall: 0.86, precision: 0.8605769230769231, f_beta: 0.8599439775910365
train: step: 700, loss: 0.07063636183738708, acc: 0.88, recall: 0.88, precision: 0.8806089743589745, f_beta: 0.879951980792317
train: step: 701, loss: 0.41866952180862427, acc: 0.78, recall: 0.78, precision: 0.8216911764705883, f_beta: 0.7726333195535344
train: step: 702, loss: 0.08486693352460861, acc: 0.91, recall: 0.91, precision: 0.9101640656262505, f_beta: 0.90999099909991
train: step: 703, loss: 0.1093231588602066, acc: 0.85, recall: 0.8500000000000001, precision: 0.8535353535353536, f_beta: 0.849624060150376
train: step: 704, loss: 0.09762582927942276, acc: 0.89, recall: 0.89, precision: 0.8914090726615818, f_beta: 0.8899009108197378
train: step: 705, loss: 0.12372729182243347, acc: 0.83, recall: 0.8300000000000001, precision: 0.8311922922521076, f_beta: 0.8298468621759585
train: step: 706, loss: 0.10283905267715454, acc: 0.86, recall: 0.86, precision: 0.8623188405797102, f_beta: 0.859775641025641
train: step: 707, loss: 0.07106354087591171, acc: 0.89, recall: 0.89, precision: 0.89015606242497, f_beta: 0.88998899889989
train: step: 708, loss: 0.08282479643821716, acc: 0.9, recall: 0.9, precision: 0.9, f_beta: 0.9
train: step: 709, loss: 0.06173958629369736, acc: 0.95, recall: 0.95, precision: 0.9516258530710557, f_beta: 0.949954959463517
train: step: 710, loss: 0.12534326314926147, acc: 0.84, recall: 0.84, precision: 0.8405448717948718, f_beta: 0.8399359743897559
train: step: 711, loss: 0.09236349910497665, acc: 0.86, recall: 0.86, precision: 0.8605769230769231, f_beta: 0.8599439775910365
train: step: 712, loss: 0.062181051820516586, acc: 0.92, recall: 0.92, precision: 0.92, f_beta: 0.92
train: step: 713, loss: 0.07371770590543747, acc: 0.9, recall: 0.8999999999999999, precision: 0.9058441558441559, f_beta: 0.8996386993175431
train: step: 714, loss: 0.11075028777122498, acc: 0.84, recall: 0.84, precision: 0.8405448717948718, f_beta: 0.8399359743897559
train: step: 715, loss: 0.09906280785799026, acc: 0.87, recall: 0.87, precision: 0.8773969808241534, f_beta: 0.8693598633303186
train: step: 716, loss: 0.09591399878263474, acc: 0.88, recall: 0.88, precision: 0.88, f_beta: 0.88
train: step: 717, loss: 0.10304576903581619, acc: 0.85, recall: 0.8500000000000001, precision: 0.8535353535353536, f_beta: 0.849624060150376
train: step: 718, loss: 0.16127070784568787, acc: 0.82, recall: 0.8200000000000001, precision: 0.8284072249589491, f_beta: 0.818840579710145
train: step: 719, loss: 0.06574781239032745, acc: 0.89, recall: 0.89, precision: 0.89015606242497, f_beta: 0.88998899889989
train: step: 720, loss: 0.09257625788450241, acc: 0.86, recall: 0.86, precision: 0.8694581280788177, f_beta: 0.8590982286634461
train: step: 721, loss: 0.12542159855365753, acc: 0.85, recall: 0.85, precision: 0.8617197188921042, f_beta: 0.8487750781328762
train: step: 722, loss: 0.08057205379009247, acc: 0.88, recall: 0.88, precision: 0.8806089743589745, f_beta: 0.879951980792317
train: step: 723, loss: 0.40425217151641846, acc: 0.75, recall: 0.75, precision: 0.7627154266498528, f_beta: 0.7469379491851402
train: step: 724, loss: 0.05956060066819191, acc: 0.93, recall: 0.9299999999999999, precision: 0.9315535929345644, f_beta: 0.9299369432489241
train: step: 725, loss: 0.07572341710329056, acc: 0.92, recall: 0.9199999999999999, precision: 0.9206730769230769, f_beta: 0.919967987194878
train: step: 726, loss: 0.07780491560697556, acc: 0.87, recall: 0.87, precision: 0.8737373737373737, f_beta: 0.8696741854636592
train: step: 727, loss: 0.060794636607170105, acc: 0.93, recall: 0.9299999999999999, precision: 0.9315535929345644, f_beta: 0.9299369432489241
train: step: 728, loss: 0.11217188090085983, acc: 0.84, recall: 0.84, precision: 0.8405448717948718, f_beta: 0.8399359743897559
train: step: 729, loss: 0.07471737265586853, acc: 0.91, recall: 0.9099999999999999, precision: 0.9114813327980731, f_beta: 0.9099189270343309
train: step: 730, loss: 0.10106141865253448, acc: 0.85, recall: 0.85, precision: 0.8501400560224089, f_beta: 0.84998499849985
train: step: 731, loss: 0.06440061330795288, acc: 0.93, recall: 0.9299999999999999, precision: 0.9343434343434343, f_beta: 0.9298245614035088
train: step: 732, loss: 0.08540363609790802, acc: 0.91, recall: 0.9099999999999999, precision: 0.9181966544267646, f_beta: 0.9095568284594513
train: step: 733, loss: 0.08491628617048264, acc: 0.88, recall: 0.88, precision: 0.8958333333333333, f_beta: 0.8787878787878789
train: step: 734, loss: 0.09262976795434952, acc: 0.89, recall: 0.89, precision: 0.89015606242497, f_beta: 0.88998899889989
train: step: 735, loss: 0.10703852027654648, acc: 0.86, recall: 0.86, precision: 0.8623188405797102, f_beta: 0.859775641025641
train: step: 736, loss: 0.10632917284965515, acc: 0.84, recall: 0.84, precision: 0.8405448717948718, f_beta: 0.8399359743897559
train: step: 737, loss: 0.07315508276224136, acc: 0.91, recall: 0.91, precision: 0.9101640656262505, f_beta: 0.90999099909991
train: step: 738, loss: 0.12617991864681244, acc: 0.79, recall: 0.79, precision: 0.7901160464185675, f_beta: 0.78997899789979
train: step: 739, loss: 0.11274179816246033, acc: 0.85, recall: 0.8500000000000001, precision: 0.8535353535353536, f_beta: 0.849624060150376
train: step: 740, loss: 0.059012457728385925, acc: 0.93, recall: 0.9299999999999999, precision: 0.930172068827531, f_beta: 0.9299929992999301
train: step: 741, loss: 0.07233809679746628, acc: 0.91, recall: 0.91, precision: 0.9101640656262505, f_beta: 0.90999099909991
train: step: 742, loss: 0.08677689731121063, acc: 0.87, recall: 0.87, precision: 0.8701480592236894, f_beta: 0.8699869986998701
train: step: 743, loss: 0.09557035565376282, acc: 0.86, recall: 0.86, precision: 0.8605769230769231, f_beta: 0.8599439775910365
train: step: 744, loss: 0.12918131053447723, acc: 0.84, recall: 0.8400000000000001, precision: 0.8449675324675325, f_beta: 0.839421918908069
train: step: 745, loss: 0.0767878070473671, acc: 0.89, recall: 0.8899999999999999, precision: 0.8939393939393939, f_beta: 0.8897243107769424
train: step: 746, loss: 0.09690914303064346, acc: 0.86, recall: 0.86, precision: 0.8623188405797102, f_beta: 0.859775641025641
train: step: 747, loss: 0.056346435099840164, acc: 0.93, recall: 0.9299999999999999, precision: 0.9343434343434343, f_beta: 0.9298245614035088
train: step: 748, loss: 0.12105514854192734, acc: 0.81, recall: 0.81, precision: 0.8203803224472923, f_beta: 0.8084484323016432
train: step: 749, loss: 0.05871320888400078, acc: 0.92, recall: 0.92, precision: 0.92, f_beta: 0.92
train: step: 750, loss: 0.09486725181341171, acc: 0.87, recall: 0.87, precision: 0.8713368125250903, f_beta: 0.8698828946051447
train: step: 751, loss: 0.07685893774032593, acc: 0.89, recall: 0.89, precision: 0.89015606242497, f_beta: 0.88998899889989
train: step: 752, loss: 0.11671968549489975, acc: 0.84, recall: 0.84, precision: 0.84, f_beta: 0.8399999999999999
train: step: 753, loss: 0.08313005417585373, acc: 0.9, recall: 0.9, precision: 0.9006410256410255, f_beta: 0.8999599839935974
train: step: 754, loss: 0.09190631657838821, acc: 0.86, recall: 0.86, precision: 0.8623188405797102, f_beta: 0.859775641025641
train: step: 755, loss: 0.3616231083869934, acc: 0.86, recall: 0.86, precision: 0.86, f_beta: 0.8599999999999999
train: step: 756, loss: 0.08739298582077026, acc: 0.86, recall: 0.86, precision: 0.8605769230769231, f_beta: 0.8599439775910365
train: step: 757, loss: 0.06761130690574646, acc: 0.92, recall: 0.92, precision: 0.92, f_beta: 0.92
train: step: 758, loss: 0.06306648999452591, acc: 0.92, recall: 0.9199999999999999, precision: 0.9206730769230769, f_beta: 0.919967987194878
train: step: 759, loss: 0.08948919922113419, acc: 0.86, recall: 0.8600000000000001, precision: 0.8652597402597402, f_beta: 0.8594941790445605
train: step: 760, loss: 0.08662793040275574, acc: 0.9, recall: 0.9, precision: 0.9, f_beta: 0.9
train: step: 761, loss: 0.0774574875831604, acc: 0.92, recall: 0.9199999999999999, precision: 0.9206730769230769, f_beta: 0.919967987194878
train: step: 762, loss: 0.11193918436765671, acc: 0.88, recall: 0.88, precision: 0.8899835796387521, f_beta: 0.8792270531400966
train: step: 763, loss: 0.05955470725893974, acc: 0.93, recall: 0.9299999999999999, precision: 0.9343434343434343, f_beta: 0.9298245614035088
train: step: 764, loss: 0.12377519905567169, acc: 0.82, recall: 0.82, precision: 0.8395585738539898, f_beta: 0.8173701298701298
train: step: 765, loss: 0.11549991369247437, acc: 0.83, recall: 0.8300000000000001, precision: 0.8365973072215422, f_beta: 0.8291628982011858
train: step: 766, loss: 0.1057700663805008, acc: 0.86, recall: 0.86, precision: 0.8623188405797102, f_beta: 0.859775641025641
train: step: 767, loss: 0.0959291160106659, acc: 0.88, recall: 0.88, precision: 0.8958333333333333, f_beta: 0.8787878787878789
train: step: 768, loss: 0.09838496148586273, acc: 0.86, recall: 0.86, precision: 0.8605769230769231, f_beta: 0.8599439775910365
train: step: 769, loss: 0.09720098227262497, acc: 0.86, recall: 0.86, precision: 0.890625, f_beta: 0.8572011423908608
train: step: 770, loss: 0.09480457007884979, acc: 0.88, recall: 0.88, precision: 0.88, f_beta: 0.88
train: step: 771, loss: 0.07627218961715698, acc: 0.92, recall: 0.9199999999999999, precision: 0.9261363636363636, f_beta: 0.9197109594540345
train: step: 772, loss: 0.10077295452356339, acc: 0.84, recall: 0.8400000000000001, precision: 0.8421900161030595, f_beta: 0.8397435897435896
train: step: 773, loss: 0.12494787573814392, acc: 0.84, recall: 0.84, precision: 0.8541666666666667, f_beta: 0.8383838383838385
train: step: 774, loss: 0.08712897449731827, acc: 0.91, recall: 0.9099999999999999, precision: 0.9141414141414141, f_beta: 0.9097744360902256
train: step: 775, loss: 0.07607057690620422, acc: 0.91, recall: 0.9099999999999999, precision: 0.9114813327980731, f_beta: 0.9099189270343309
train: step: 776, loss: 0.08930429816246033, acc: 0.88, recall: 0.88, precision: 0.8806089743589745, f_beta: 0.879951980792317
train: step: 777, loss: 0.10586562752723694, acc: 0.87, recall: 0.87, precision: 0.8737373737373737, f_beta: 0.8696741854636592
train: step: 778, loss: 0.09020277112722397, acc: 0.89, recall: 0.89, precision: 0.89015606242497, f_beta: 0.88998899889989
train: step: 779, loss: 0.10979055613279343, acc: 0.87, recall: 0.87, precision: 0.8737373737373737, f_beta: 0.8696741854636592
train: step: 780, loss: 0.08912450820207596, acc: 0.87, recall: 0.87, precision: 0.8701480592236894, f_beta: 0.8699869986998701
train: step: 781, loss: 0.10016254335641861, acc: 0.87, recall: 0.87, precision: 0.8701480592236894, f_beta: 0.8699869986998701
train: step: 782, loss: 0.035739172250032425, acc: 0.96, recall: 0.96, precision: 0.962962962962963, f_beta: 0.9599358974358974
train: step: 783, loss: 0.09500027447938919, acc: 0.87, recall: 0.87, precision: 0.8701480592236894, f_beta: 0.8699869986998701
train: step: 784, loss: 0.08724000304937363, acc: 0.9, recall: 0.8999999999999999, precision: 0.9058441558441559, f_beta: 0.8996386993175431
train: step: 785, loss: 0.1076151430606842, acc: 0.86, recall: 0.8600000000000001, precision: 0.8652597402597402, f_beta: 0.8594941790445605
train: step: 786, loss: 0.09881626814603806, acc: 0.85, recall: 0.85, precision: 0.8501400560224089, f_beta: 0.84998499849985
train: step: 787, loss: 0.4001537263393402, acc: 0.85, recall: 0.85, precision: 0.867801597309794, f_beta: 0.848162769511084
train: step: 788, loss: 0.09201499819755554, acc: 0.86, recall: 0.86, precision: 0.8623188405797102, f_beta: 0.859775641025641
train: step: 789, loss: 0.133741557598114, acc: 0.83, recall: 0.8300000000000001, precision: 0.8333333333333334, f_beta: 0.8295739348370927
train: step: 790, loss: 0.08382768929004669, acc: 0.89, recall: 0.89, precision: 0.89015606242497, f_beta: 0.88998899889989
train: step: 791, loss: 0.10012402385473251, acc: 0.86, recall: 0.86, precision: 0.8623188405797102, f_beta: 0.859775641025641
train: step: 792, loss: 0.0817263051867485, acc: 0.9, recall: 0.9, precision: 0.9, f_beta: 0.9
train: step: 793, loss: 0.11466707289218903, acc: 0.84, recall: 0.84, precision: 0.8607809847198642, f_beta: 0.8376623376623376
train: step: 794, loss: 0.10759102553129196, acc: 0.85, recall: 0.85, precision: 0.867801597309794, f_beta: 0.848162769511084
train: step: 795, loss: 0.058470286428928375, acc: 0.95, recall: 0.95, precision: 0.9501800720288115, f_beta: 0.9499949994999499
train: step: 796, loss: 0.1038583368062973, acc: 0.85, recall: 0.8500000000000001, precision: 0.8535353535353536, f_beta: 0.849624060150376
train: step: 797, loss: 0.09950985014438629, acc: 0.9, recall: 0.8999999999999999, precision: 0.9105090311986863, f_beta: 0.8993558776167472
train: step: 798, loss: 0.08994171023368835, acc: 0.87, recall: 0.87, precision: 0.8888188314417822, f_beta: 0.868407733576273
train: step: 799, loss: 0.11122310906648636, acc: 0.86, recall: 0.86, precision: 0.86, f_beta: 0.8599999999999999
train: step: 800, loss: 0.11345400661230087, acc: 0.83, recall: 0.8300000000000001, precision: 0.8333333333333334, f_beta: 0.8295739348370927
train: step: 801, loss: 0.35935017466545105, acc: 0.89, recall: 0.8899999999999999, precision: 0.8977968176254589, f_beta: 0.8894583458948848
train: step: 802, loss: 0.060563117265701294, acc: 0.94, recall: 0.94, precision: 0.94, f_beta: 0.94
train: step: 803, loss: 0.07496769726276398, acc: 0.91, recall: 0.91, precision: 0.9101640656262505, f_beta: 0.90999099909991
train: step: 804, loss: 0.14017856121063232, acc: 0.84, recall: 0.8400000000000001, precision: 0.8449675324675325, f_beta: 0.839421918908069
train: step: 805, loss: 0.09405630081892014, acc: 0.86, recall: 0.8600000000000001, precision: 0.8652597402597402, f_beta: 0.8594941790445605
train: step: 806, loss: 0.08294235169887543, acc: 0.91, recall: 0.9099999999999999, precision: 0.9114813327980731, f_beta: 0.9099189270343309
train: step: 807, loss: 0.04852623865008354, acc: 0.95, recall: 0.95, precision: 0.9501800720288115, f_beta: 0.9499949994999499
train: step: 808, loss: 0.06994316726922989, acc: 0.9, recall: 0.9, precision: 0.9006410256410255, f_beta: 0.8999599839935974
train: step: 809, loss: 0.05430407449603081, acc: 0.94, recall: 0.94, precision: 0.9464285714285714, f_beta: 0.939783219590526
train: step: 810, loss: 0.07100651413202286, acc: 0.91, recall: 0.91, precision: 0.9101640656262505, f_beta: 0.90999099909991
train: step: 811, loss: 0.09015584737062454, acc: 0.89, recall: 0.89, precision: 0.89015606242497, f_beta: 0.88998899889989
train: step: 812, loss: 0.11520766466856003, acc: 0.83, recall: 0.8300000000000001, precision: 0.8410500206696983, f_beta: 0.8286117552172598
train: step: 813, loss: 0.1229449063539505, acc: 0.79, recall: 0.79, precision: 0.7929292929292929, f_beta: 0.7894736842105263
train: step: 814, loss: 0.0750458613038063, acc: 0.92, recall: 0.9199999999999999, precision: 0.9206730769230769, f_beta: 0.919967987194878
train: step: 815, loss: 0.10924938321113586, acc: 0.87, recall: 0.87, precision: 0.8713368125250903, f_beta: 0.8698828946051447
train: step: 816, loss: 0.07056569308042526, acc: 0.91, recall: 0.91, precision: 0.9101640656262505, f_beta: 0.90999099909991
train: step: 817, loss: 0.07594484090805054, acc: 0.9, recall: 0.8999999999999999, precision: 0.9025764895330113, f_beta: 0.8998397435897436
train: step: 818, loss: 0.11396797001361847, acc: 0.85, recall: 0.8500000000000001, precision: 0.8569971440228478, f_beta: 0.8492613807657521
train: step: 819, loss: 0.05426458269357681, acc: 0.94, recall: 0.94, precision: 0.9407051282051282, f_beta: 0.9399759903961584
train: step: 820, loss: 0.056544285267591476, acc: 0.94, recall: 0.94, precision: 0.9428341384863124, f_beta: 0.9399038461538463
train: step: 821, loss: 0.08733150362968445, acc: 0.87, recall: 0.87, precision: 0.8713368125250903, f_beta: 0.8698828946051447
train: step: 822, loss: 0.10224403440952301, acc: 0.86, recall: 0.86, precision: 0.86, f_beta: 0.8599999999999999
train: step: 823, loss: 0.08113708347082138, acc: 0.88, recall: 0.8799999999999999, precision: 0.885551948051948, f_beta: 0.8795664391810518
train: step: 824, loss: 0.12071746587753296, acc: 0.85, recall: 0.85, precision: 0.851264552388599, f_beta: 0.8498648783905516
train: step: 825, loss: 0.07924181967973709, acc: 0.89, recall: 0.8899999999999999, precision: 0.8977968176254589, f_beta: 0.8894583458948848
train: step: 826, loss: 0.09391839802265167, acc: 0.88, recall: 0.88, precision: 0.8824476650563606, f_beta: 0.8798076923076923
train: step: 827, loss: 0.07042151689529419, acc: 0.9, recall: 0.9, precision: 0.9006410256410255, f_beta: 0.8999599839935974
train: step: 828, loss: 0.05133095756173134, acc: 0.95, recall: 0.95, precision: 0.9516258530710557, f_beta: 0.949954959463517
train: step: 829, loss: 0.09585633873939514, acc: 0.89, recall: 0.8899999999999999, precision: 0.8977968176254589, f_beta: 0.8894583458948848
train: step: 830, loss: 0.06926287710666656, acc: 0.93, recall: 0.9299999999999999, precision: 0.9315535929345644, f_beta: 0.9299369432489241
train: step: 831, loss: 0.12433383613824844, acc: 0.84, recall: 0.8400000000000001, precision: 0.8449675324675325, f_beta: 0.839421918908069
train: step: 832, loss: 0.07399918884038925, acc: 0.9, recall: 0.9, precision: 0.9, f_beta: 0.9
train: step: 833, loss: 0.07210957258939743, acc: 0.9, recall: 0.9, precision: 0.9006410256410255, f_beta: 0.8999599839935974
train: step: 834, loss: 0.08206786215305328, acc: 0.89, recall: 0.89, precision: 0.9030591153369161, f_beta: 0.8891017239641092
train: step: 835, loss: 0.0964568555355072, acc: 0.86, recall: 0.86, precision: 0.8605769230769231, f_beta: 0.8599439775910365
train: step: 836, loss: 0.12032975256443024, acc: 0.81, recall: 0.81, precision: 0.8161974704202366, f_beta: 0.8090644156366193
train: step: 837, loss: 0.05267667770385742, acc: 0.93, recall: 0.9299999999999999, precision: 0.930172068827531, f_beta: 0.9299929992999301
train: step: 838, loss: 0.10072578489780426, acc: 0.84, recall: 0.8400000000000001, precision: 0.8421900161030595, f_beta: 0.8397435897435896
train: step: 839, loss: 0.09680686146020889, acc: 0.88, recall: 0.88, precision: 0.8806089743589745, f_beta: 0.879951980792317
train: step: 840, loss: 0.09549598395824432, acc: 0.87, recall: 0.87, precision: 0.8701480592236894, f_beta: 0.8699869986998701
train: step: 841, loss: 0.0925212875008583, acc: 0.87, recall: 0.87, precision: 0.8737373737373737, f_beta: 0.8696741854636592
train: step: 842, loss: 0.0858270451426506, acc: 0.89, recall: 0.89, precision: 0.8914090726615818, f_beta: 0.8899009108197378
train: step: 843, loss: 0.0997314527630806, acc: 0.89, recall: 0.8899999999999999, precision: 0.8977968176254589, f_beta: 0.8894583458948848
train: step: 844, loss: 0.12679612636566162, acc: 0.83, recall: 0.83, precision: 0.8301320528211285, f_beta: 0.8299829982998299
train: step: 845, loss: 0.08805043995380402, acc: 0.89, recall: 0.89, precision: 0.8914090726615818, f_beta: 0.8899009108197378
train: step: 846, loss: 0.06178152933716774, acc: 0.92, recall: 0.92, precision: 0.92, f_beta: 0.92
train: step: 847, loss: 0.09003129601478577, acc: 0.88, recall: 0.88, precision: 0.88, f_beta: 0.88
train: step: 848, loss: 0.06253845244646072, acc: 0.95, recall: 0.95, precision: 0.9545454545454546, f_beta: 0.949874686716792
train: step: 849, loss: 0.10548073053359985, acc: 0.88, recall: 0.88, precision: 0.8806089743589745, f_beta: 0.879951980792317
train: step: 850, loss: 0.09382162243127823, acc: 0.88, recall: 0.88, precision: 0.8806089743589745, f_beta: 0.879951980792317
train: step: 851, loss: 0.1074189841747284, acc: 0.9, recall: 0.8999999999999999, precision: 0.9025764895330113, f_beta: 0.8998397435897436
train: step: 852, loss: 0.06847918778657913, acc: 0.93, recall: 0.9299999999999999, precision: 0.9315535929345644, f_beta: 0.9299369432489241
train: step: 853, loss: 0.07584960758686066, acc: 0.91, recall: 0.9099999999999999, precision: 0.9141414141414141, f_beta: 0.9097744360902256
train: step: 854, loss: 0.08832474797964096, acc: 0.89, recall: 0.89, precision: 0.8914090726615818, f_beta: 0.8899009108197378
train: step: 855, loss: 0.085227832198143, acc: 0.9, recall: 0.8999999999999999, precision: 0.9025764895330113, f_beta: 0.8998397435897436
train: step: 856, loss: 0.08639505505561829, acc: 0.89, recall: 0.89, precision: 0.8914090726615818, f_beta: 0.8899009108197378
train: step: 857, loss: 0.06734445691108704, acc: 0.91, recall: 0.9099999999999999, precision: 0.9114813327980731, f_beta: 0.9099189270343309
train: step: 858, loss: 0.10132401436567307, acc: 0.86, recall: 0.86, precision: 0.8605769230769231, f_beta: 0.8599439775910365
train: step: 859, loss: 0.09293020516633987, acc: 0.86, recall: 0.86, precision: 0.8623188405797102, f_beta: 0.859775641025641
train: step: 860, loss: 0.0582321435213089, acc: 0.94, recall: 0.94, precision: 0.9428341384863124, f_beta: 0.9399038461538463
train: step: 861, loss: 0.0468251071870327, acc: 0.94, recall: 0.94, precision: 0.9407051282051282, f_beta: 0.9399759903961584
train: step: 862, loss: 0.0721416249871254, acc: 0.88, recall: 0.88, precision: 0.8806089743589745, f_beta: 0.879951980792317
train: step: 863, loss: 0.0913991704583168, acc: 0.87, recall: 0.87, precision: 0.8773969808241534, f_beta: 0.8693598633303186
train: step: 864, loss: 0.08447404950857162, acc: 0.9, recall: 0.8999999999999999, precision: 0.9025764895330113, f_beta: 0.8998397435897436
train: step: 865, loss: 0.05340828746557236, acc: 0.91, recall: 0.91, precision: 0.9101640656262505, f_beta: 0.90999099909991
train: step: 866, loss: 0.16118915379047394, acc: 0.78, recall: 0.78, precision: 0.7818035426731079, f_beta: 0.7796474358974359
train: step: 867, loss: 0.13587656617164612, acc: 0.81, recall: 0.81, precision: 0.8131313131313131, f_beta: 0.8095238095238095
train: step: 868, loss: 0.049863189458847046, acc: 0.92, recall: 0.9199999999999999, precision: 0.9261363636363636, f_beta: 0.9197109594540345
train: step: 869, loss: 0.1696128398180008, acc: 0.85, recall: 0.8500000000000001, precision: 0.8535353535353536, f_beta: 0.849624060150376
train: step: 870, loss: 0.1145862564444542, acc: 0.84, recall: 0.84, precision: 0.8541666666666667, f_beta: 0.8383838383838385
train: step: 871, loss: 0.07947944849729538, acc: 0.93, recall: 0.9299999999999999, precision: 0.930172068827531, f_beta: 0.9299929992999301
train: step: 872, loss: 0.1176745668053627, acc: 0.82, recall: 0.8200000000000001, precision: 0.822061191626409, f_beta: 0.8197115384615385
train: step: 873, loss: 0.0724184438586235, acc: 0.91, recall: 0.91, precision: 0.9101640656262505, f_beta: 0.90999099909991
train: step: 874, loss: 0.1006762683391571, acc: 0.88, recall: 0.88, precision: 0.8806089743589745, f_beta: 0.879951980792317
train: step: 875, loss: 0.07370231300592422, acc: 0.9, recall: 0.9, precision: 0.9006410256410255, f_beta: 0.8999599839935974
train: step: 876, loss: 0.10213447362184525, acc: 0.89, recall: 0.89, precision: 0.89015606242497, f_beta: 0.88998899889989
train: step: 877, loss: 0.09668970108032227, acc: 0.87, recall: 0.87, precision: 0.8823894171145101, f_beta: 0.8689384010484928
train: step: 878, loss: 0.0689987912774086, acc: 0.91, recall: 0.91, precision: 0.9101640656262505, f_beta: 0.90999099909991
train: step: 879, loss: 0.05615909397602081, acc: 0.91, recall: 0.91, precision: 0.9101640656262505, f_beta: 0.90999099909991
train: step: 880, loss: 0.39872193336486816, acc: 0.8, recall: 0.8, precision: 0.8019323671497585, f_beta: 0.7996794871794872
train: step: 881, loss: 0.07773400098085403, acc: 0.89, recall: 0.89, precision: 0.89015606242497, f_beta: 0.88998899889989
train: step: 882, loss: 0.14003390073776245, acc: 0.8, recall: 0.8, precision: 0.8004807692307692, f_beta: 0.7999199679871949
train: step: 883, loss: 0.08786047250032425, acc: 0.88, recall: 0.88, precision: 0.8824476650563606, f_beta: 0.8798076923076923
train: step: 884, loss: 0.09545860439538956, acc: 0.87, recall: 0.87, precision: 0.8823894171145101, f_beta: 0.8689384010484928
train: step: 885, loss: 0.0692734345793724, acc: 0.88, recall: 0.88, precision: 0.8824476650563606, f_beta: 0.8798076923076923
train: step: 886, loss: 0.35333579778671265, acc: 0.77, recall: 0.77, precision: 0.7895752895752897, f_beta: 0.7660461804495982
train: step: 887, loss: 0.07041538506746292, acc: 0.91, recall: 0.9099999999999999, precision: 0.9141414141414141, f_beta: 0.9097744360902256
train: step: 888, loss: 0.10029430687427521, acc: 0.84, recall: 0.8400000000000001, precision: 0.8449675324675325, f_beta: 0.839421918908069
train: step: 889, loss: 0.11588478088378906, acc: 0.85, recall: 0.8500000000000001, precision: 0.8535353535353536, f_beta: 0.849624060150376
train: step: 890, loss: 0.09453779458999634, acc: 0.87, recall: 0.87, precision: 0.8713368125250903, f_beta: 0.8698828946051447
train: step: 891, loss: 0.11019805818796158, acc: 0.89, recall: 0.89, precision: 0.9030591153369161, f_beta: 0.8891017239641092
train: step: 892, loss: 0.07567238062620163, acc: 0.9, recall: 0.8999999999999999, precision: 0.9105090311986863, f_beta: 0.8993558776167472
train: step: 893, loss: 0.17916728556156158, acc: 0.73, recall: 0.73, precision: 0.7377015295576685, f_beta: 0.7277951406391774
train: step: 894, loss: 0.35194721817970276, acc: 0.58, recall: 0.58, precision: 0.7717391304347826, f_beta: 0.4900437105390967
train: step: 895, loss: 0.09860052913427353, acc: 0.85, recall: 0.85, precision: 0.851264552388599, f_beta: 0.8498648783905516
train: step: 896, loss: 0.1410466432571411, acc: 0.81, recall: 0.81, precision: 0.8111200321156162, f_beta: 0.8098288459613652
train: step: 897, loss: 0.10114385932683945, acc: 0.88, recall: 0.8799999999999999, precision: 0.885551948051948, f_beta: 0.8795664391810518
train: step: 898, loss: 0.08443249017000198, acc: 0.9, recall: 0.8999999999999999, precision: 0.9058441558441559, f_beta: 0.8996386993175431
train: step: 899, loss: 0.08989943563938141, acc: 0.86, recall: 0.86, precision: 0.8694581280788177, f_beta: 0.8590982286634461
train: step: 900, loss: 0.10053744167089462, acc: 0.85, recall: 0.85, precision: 0.8753753753753755, f_beta: 0.8474214220323468
train: step: 901, loss: 0.11468781530857086, acc: 0.82, recall: 0.8200000000000001, precision: 0.8284072249589491, f_beta: 0.818840579710145
train: step: 902, loss: 0.09793055802583694, acc: 0.88, recall: 0.88, precision: 0.8806089743589745, f_beta: 0.879951980792317
train: step: 903, loss: 0.08240365982055664, acc: 0.9, recall: 0.9, precision: 0.9006410256410255, f_beta: 0.8999599839935974
train: step: 904, loss: 0.08286704868078232, acc: 0.88, recall: 0.88, precision: 0.8806089743589745, f_beta: 0.879951980792317
train: step: 905, loss: 0.10952648520469666, acc: 0.83, recall: 0.8300000000000001, precision: 0.8365973072215422, f_beta: 0.8291628982011858
train: step: 906, loss: 0.0781140998005867, acc: 0.89, recall: 0.89, precision: 0.8914090726615818, f_beta: 0.8899009108197378
train: step: 907, loss: 0.09063549339771271, acc: 0.86, recall: 0.86, precision: 0.8623188405797102, f_beta: 0.859775641025641
train: step: 908, loss: 0.10883381217718124, acc: 0.86, recall: 0.86, precision: 0.875, f_beta: 0.8585858585858586
train: step: 909, loss: 0.08459454774856567, acc: 0.92, recall: 0.9199999999999999, precision: 0.9206730769230769, f_beta: 0.919967987194878
train: step: 910, loss: 0.09811530262231827, acc: 0.82, recall: 0.82, precision: 0.82, f_beta: 0.82
train: step: 911, loss: 0.078010693192482, acc: 0.89, recall: 0.89, precision: 0.8914090726615818, f_beta: 0.8899009108197378
train: step: 912, loss: 0.08546046912670135, acc: 0.9, recall: 0.9, precision: 0.9006410256410255, f_beta: 0.8999599839935974
train: step: 913, loss: 0.08573450148105621, acc: 0.9, recall: 0.8999999999999999, precision: 0.9025764895330113, f_beta: 0.8998397435897436
train: step: 914, loss: 0.0996646136045456, acc: 0.87, recall: 0.87, precision: 0.8713368125250903, f_beta: 0.8698828946051447
train: step: 915, loss: 0.0967797115445137, acc: 0.84, recall: 0.84, precision: 0.84, f_beta: 0.8399999999999999
train: step: 916, loss: 0.11948656290769577, acc: 0.87, recall: 0.87, precision: 0.8737373737373737, f_beta: 0.8696741854636592
train: step: 917, loss: 0.07271462678909302, acc: 0.92, recall: 0.9199999999999999, precision: 0.9206730769230769, f_beta: 0.919967987194878
train: step: 918, loss: 0.08102907240390778, acc: 0.88, recall: 0.8799999999999999, precision: 0.885551948051948, f_beta: 0.8795664391810518
train: step: 919, loss: 0.10436104983091354, acc: 0.86, recall: 0.86, precision: 0.8820033955857385, f_beta: 0.8579545454545454
train: step: 920, loss: 0.06218314170837402, acc: 0.92, recall: 0.9199999999999999, precision: 0.9206730769230769, f_beta: 0.919967987194878
train: step: 921, loss: 0.07160145789384842, acc: 0.91, recall: 0.9099999999999999, precision: 0.9114813327980731, f_beta: 0.9099189270343309
train: step: 922, loss: 0.09165740013122559, acc: 0.88, recall: 0.8799999999999999, precision: 0.885551948051948, f_beta: 0.8795664391810518
train: step: 923, loss: 0.08591406047344208, acc: 0.87, recall: 0.87, precision: 0.8713368125250903, f_beta: 0.8698828946051447
train: step: 924, loss: 0.12114036828279495, acc: 0.83, recall: 0.83, precision: 0.853925353925354, f_beta: 0.8270776116366596
train: step: 925, loss: 0.09734424948692322, acc: 0.89, recall: 0.8899999999999999, precision: 0.8939393939393939, f_beta: 0.8897243107769424
train: step: 926, loss: 0.11635415256023407, acc: 0.84, recall: 0.8400000000000001, precision: 0.8421900161030595, f_beta: 0.8397435897435896
train: step: 927, loss: 0.06948637217283249, acc: 0.91, recall: 0.9099999999999999, precision: 0.9141414141414141, f_beta: 0.9097744360902256
train: step: 928, loss: 0.10044945776462555, acc: 0.82, recall: 0.8200000000000001, precision: 0.8246753246753247, f_beta: 0.8193496587715776
train: step: 929, loss: 0.12162912636995316, acc: 0.93, recall: 0.9299999999999999, precision: 0.930172068827531, f_beta: 0.9299929992999301
train: step: 930, loss: 0.08454795926809311, acc: 0.9, recall: 0.8999999999999999, precision: 0.9025764895330113, f_beta: 0.8998397435897436
train: step: 931, loss: 0.04268889129161835, acc: 0.95, recall: 0.95, precision: 0.9545454545454546, f_beta: 0.949874686716792
train: step: 932, loss: 0.07048830389976501, acc: 0.91, recall: 0.91, precision: 0.9101640656262505, f_beta: 0.90999099909991
train: step: 933, loss: 0.1017884761095047, acc: 0.88, recall: 0.88, precision: 0.8824476650563606, f_beta: 0.8798076923076923
train: step: 934, loss: 0.08771631121635437, acc: 0.88, recall: 0.88, precision: 0.9032258064516129, f_beta: 0.8782467532467533
train: step: 935, loss: 0.07332968711853027, acc: 0.9, recall: 0.8999999999999999, precision: 0.9058441558441559, f_beta: 0.8996386993175431
train: step: 936, loss: 0.09977395087480545, acc: 0.88, recall: 0.88, precision: 0.8899835796387521, f_beta: 0.8792270531400966
train: step: 937, loss: 0.07612190395593643, acc: 0.91, recall: 0.9099999999999999, precision: 0.9141414141414141, f_beta: 0.9097744360902256
train: step: 938, loss: 0.14228251576423645, acc: 0.84, recall: 0.8400000000000001, precision: 0.8489326765188834, f_beta: 0.8389694041867956
train: step: 939, loss: 0.14080604910850525, acc: 0.81, recall: 0.81, precision: 0.8203803224472923, f_beta: 0.8084484323016432
train: step: 940, loss: 0.06392098218202591, acc: 0.91, recall: 0.91, precision: 0.9101640656262505, f_beta: 0.90999099909991
train: step: 941, loss: 0.07694839686155319, acc: 0.89, recall: 0.8899999999999999, precision: 0.8939393939393939, f_beta: 0.8897243107769424
train: step: 942, loss: 0.0758393257856369, acc: 0.92, recall: 0.9199999999999999, precision: 0.9227053140096618, f_beta: 0.9198717948717948
train: step: 943, loss: 0.1509450376033783, acc: 0.8, recall: 0.8, precision: 0.8125, f_beta: 0.797979797979798
train: step: 944, loss: 0.08830142766237259, acc: 0.87, recall: 0.87, precision: 0.8713368125250903, f_beta: 0.8698828946051447
train: step: 945, loss: 0.10535877197980881, acc: 0.86, recall: 0.86, precision: 0.8605769230769231, f_beta: 0.8599439775910365
train: step: 946, loss: 0.060813482850790024, acc: 0.92, recall: 0.9199999999999999, precision: 0.9206730769230769, f_beta: 0.919967987194878
train: step: 947, loss: 0.08056876808404922, acc: 0.88, recall: 0.8799999999999999, precision: 0.885551948051948, f_beta: 0.8795664391810518
train: step: 948, loss: 0.0821898877620697, acc: 0.91, recall: 0.91, precision: 0.9101640656262505, f_beta: 0.90999099909991
train: step: 949, loss: 0.41277503967285156, acc: 0.87, recall: 0.87, precision: 0.8701480592236894, f_beta: 0.8699869986998701
train: step: 950, loss: 0.06859957426786423, acc: 0.89, recall: 0.89, precision: 0.8914090726615818, f_beta: 0.8899009108197378
train: step: 951, loss: 0.06440521031618118, acc: 0.92, recall: 0.9199999999999999, precision: 0.9261363636363636, f_beta: 0.9197109594540345
train: step: 952, loss: 0.08186732977628708, acc: 0.9, recall: 0.9, precision: 0.9006410256410255, f_beta: 0.8999599839935974
train: step: 953, loss: 0.10488884150981903, acc: 0.84, recall: 0.8400000000000001, precision: 0.8489326765188834, f_beta: 0.8389694041867956
train: step: 954, loss: 0.059190548956394196, acc: 0.92, recall: 0.92, precision: 0.92, f_beta: 0.92
train: step: 955, loss: 0.1266503483057022, acc: 0.83, recall: 0.8300000000000001, precision: 0.8311922922521076, f_beta: 0.8298468621759585
train: step: 956, loss: 0.11176640540361404, acc: 0.84, recall: 0.84, precision: 0.8405448717948718, f_beta: 0.8399359743897559
train: step: 957, loss: 0.05447796732187271, acc: 0.94, recall: 0.94, precision: 0.9428341384863124, f_beta: 0.9399038461538463
train: step: 958, loss: 0.0861484631896019, acc: 0.87, recall: 0.87, precision: 0.8701480592236894, f_beta: 0.8699869986998701
train: step: 959, loss: 0.08793427795171738, acc: 0.88, recall: 0.88, precision: 0.8806089743589745, f_beta: 0.879951980792317
train: step: 960, loss: 0.39794740080833435, acc: 0.9, recall: 0.9, precision: 0.9006410256410255, f_beta: 0.8999599839935974
train: step: 961, loss: 0.09996522963047028, acc: 0.86, recall: 0.86, precision: 0.8623188405797102, f_beta: 0.859775641025641
train: step: 962, loss: 0.08257732540369034, acc: 0.88, recall: 0.8799999999999999, precision: 0.885551948051948, f_beta: 0.8795664391810518
train: step: 963, loss: 0.21568937599658966, acc: 0.9, recall: 0.8999999999999999, precision: 0.9058441558441559, f_beta: 0.8996386993175431
train: step: 964, loss: 0.06273753941059113, acc: 0.9, recall: 0.9, precision: 0.9006410256410255, f_beta: 0.8999599839935974
train: step: 965, loss: 0.06857432425022125, acc: 0.9, recall: 0.9, precision: 0.9006410256410255, f_beta: 0.8999599839935974
train: step: 966, loss: 0.0771268978714943, acc: 0.9, recall: 0.9, precision: 0.9, f_beta: 0.9
train: step: 967, loss: 0.056077051907777786, acc: 0.93, recall: 0.9299999999999999, precision: 0.9315535929345644, f_beta: 0.9299369432489241
train: step: 968, loss: 0.08682584762573242, acc: 0.87, recall: 0.87, precision: 0.8773969808241534, f_beta: 0.8693598633303186
train: step: 969, loss: 0.05959206074476242, acc: 0.92, recall: 0.9199999999999999, precision: 0.9227053140096618, f_beta: 0.9198717948717948
train: step: 970, loss: 0.07300768047571182, acc: 0.92, recall: 0.9199999999999999, precision: 0.9227053140096618, f_beta: 0.9198717948717948
train: step: 971, loss: 0.09856226295232773, acc: 0.85, recall: 0.8500000000000001, precision: 0.8535353535353536, f_beta: 0.849624060150376
train: step: 972, loss: 0.07924264669418335, acc: 0.89, recall: 0.89, precision: 0.8914090726615818, f_beta: 0.8899009108197378
train: step: 973, loss: 0.05269741639494896, acc: 0.96, recall: 0.96, precision: 0.96, f_beta: 0.96
train: step: 974, loss: 0.09773316979408264, acc: 0.88, recall: 0.88, precision: 0.8824476650563606, f_beta: 0.8798076923076923
train: step: 975, loss: 0.21049930155277252, acc: 0.81, recall: 0.81, precision: 0.8203803224472923, f_beta: 0.8084484323016432
train: step: 976, loss: 0.10019750893115997, acc: 0.85, recall: 0.8500000000000001, precision: 0.8569971440228478, f_beta: 0.8492613807657521
train: step: 977, loss: 0.03282888978719711, acc: 0.98, recall: 0.98, precision: 0.98, f_beta: 0.98
train: step: 978, loss: 0.0887952446937561, acc: 0.9, recall: 0.8999999999999999, precision: 0.9058441558441559, f_beta: 0.8996386993175431
train: step: 979, loss: 0.15153604745864868, acc: 0.8, recall: 0.8, precision: 0.8255208333333333, f_beta: 0.7960016319869441
train: step: 980, loss: 0.07395875453948975, acc: 0.9, recall: 0.8999999999999999, precision: 0.9105090311986863, f_beta: 0.8993558776167472
train: step: 981, loss: 0.2305791825056076, acc: 0.69, recall: 0.69, precision: 0.7682100508187464, f_beta: 0.6656239887822242
train: step: 982, loss: 0.11970539391040802, acc: 0.82, recall: 0.8200000000000001, precision: 0.8333333333333334, f_beta: 0.8181818181818181
train: step: 983, loss: 0.1407654732465744, acc: 0.82, recall: 0.8200000000000001, precision: 0.822061191626409, f_beta: 0.8197115384615385
train: step: 984, loss: 0.11449537426233292, acc: 0.87, recall: 0.87, precision: 0.8737373737373737, f_beta: 0.8696741854636592
train: step: 985, loss: 0.09954185783863068, acc: 0.87, recall: 0.87, precision: 0.8737373737373737, f_beta: 0.8696741854636592
train: step: 986, loss: 0.08947815001010895, acc: 0.88, recall: 0.88, precision: 0.88, f_beta: 0.88
train: step: 987, loss: 0.07303443551063538, acc: 0.92, recall: 0.9199999999999999, precision: 0.9310344827586207, f_beta: 0.9194847020933978
train: step: 988, loss: 0.08951198309659958, acc: 0.88, recall: 0.8799999999999999, precision: 0.885551948051948, f_beta: 0.8795664391810518
train: step: 989, loss: 0.07389327883720398, acc: 0.89, recall: 0.8899999999999999, precision: 0.8977968176254589, f_beta: 0.8894583458948848
train: step: 990, loss: 0.08614539355039597, acc: 0.89, recall: 0.89, precision: 0.8914090726615818, f_beta: 0.8899009108197378
train: step: 991, loss: 0.10759858787059784, acc: 0.86, recall: 0.86, precision: 0.875, f_beta: 0.8585858585858586
train: step: 992, loss: 0.06525775045156479, acc: 0.91, recall: 0.91, precision: 0.9101640656262505, f_beta: 0.90999099909991
train: step: 993, loss: 0.1623363494873047, acc: 0.79, recall: 0.79, precision: 0.7910477719791249, f_beta: 0.7898108297467722
train: step: 994, loss: 0.09398937970399857, acc: 0.88, recall: 0.88, precision: 0.8824476650563606, f_beta: 0.8798076923076923
train: step: 995, loss: 0.09672852605581284, acc: 0.88, recall: 0.88, precision: 0.8899835796387521, f_beta: 0.8792270531400966
train: step: 996, loss: 0.10262401401996613, acc: 0.87, recall: 0.87, precision: 0.8713368125250903, f_beta: 0.8698828946051447
train: step: 997, loss: 0.10944898426532745, acc: 0.84, recall: 0.8400000000000001, precision: 0.8421900161030595, f_beta: 0.8397435897435896
train: step: 998, loss: 0.09526839107275009, acc: 0.84, recall: 0.8400000000000001, precision: 0.8421900161030595, f_beta: 0.8397435897435896
train: step: 999, loss: 0.3016335666179657, acc: 0.82, recall: 0.8200000000000001, precision: 0.8333333333333334, f_beta: 0.8181818181818181
train: step: 1000, loss: 0.07627896964550018, acc: 0.9, recall: 0.8999999999999999, precision: 0.9058441558441559, f_beta: 0.8996386993175431
train: step: 1001, loss: 0.07755977660417557, acc: 0.9, recall: 0.9, precision: 0.9006410256410255, f_beta: 0.8999599839935974
train: step: 1002, loss: 0.10281078517436981, acc: 0.86, recall: 0.86, precision: 0.8694581280788177, f_beta: 0.8590982286634461
train: step: 1003, loss: 0.05699237808585167, acc: 0.93, recall: 0.9299999999999999, precision: 0.9315535929345644, f_beta: 0.9299369432489241
train: step: 1004, loss: 0.07787349820137024, acc: 0.9, recall: 0.8999999999999999, precision: 0.9025764895330113, f_beta: 0.8998397435897436
train: step: 1005, loss: 0.06871180236339569, acc: 0.92, recall: 0.92, precision: 0.92, f_beta: 0.92
train: step: 1006, loss: 0.10272476077079773, acc: 0.88, recall: 0.88, precision: 0.8824476650563606, f_beta: 0.8798076923076923
train: step: 1007, loss: 0.08384303748607635, acc: 0.88, recall: 0.88, precision: 0.8806089743589745, f_beta: 0.879951980792317
train: step: 1008, loss: 0.07742965966463089, acc: 0.89, recall: 0.89, precision: 0.9030591153369161, f_beta: 0.8891017239641092
train: step: 1009, loss: 0.07139329612255096, acc: 0.91, recall: 0.91, precision: 0.9101640656262505, f_beta: 0.90999099909991
train: step: 1010, loss: 0.07215993106365204, acc: 0.9, recall: 0.9, precision: 0.9, f_beta: 0.9
train: step: 1011, loss: 0.3676975667476654, acc: 0.86, recall: 0.86, precision: 0.8820033955857385, f_beta: 0.8579545454545454
train: step: 1012, loss: 0.14292122423648834, acc: 0.78, recall: 0.78, precision: 0.7971137521222411, f_beta: 0.7767857142857144
train: step: 1013, loss: 0.06962058693170547, acc: 0.91, recall: 0.91, precision: 0.9101640656262505, f_beta: 0.90999099909991
train: step: 1014, loss: 0.06457877904176712, acc: 0.9, recall: 0.9, precision: 0.9006410256410255, f_beta: 0.8999599839935974
train: step: 1015, loss: 0.07932140678167343, acc: 0.87, recall: 0.87, precision: 0.8823894171145101, f_beta: 0.8689384010484928
train: step: 1016, loss: 0.09293205291032791, acc: 0.88, recall: 0.88, precision: 0.8824476650563606, f_beta: 0.8798076923076923
train: step: 1017, loss: 0.053366728127002716, acc: 0.94, recall: 0.94, precision: 0.9428341384863124, f_beta: 0.9399038461538463
train: step: 1018, loss: 0.0811392217874527, acc: 0.89, recall: 0.89, precision: 0.9030591153369161, f_beta: 0.8891017239641092
train: step: 1019, loss: 0.041231464594602585, acc: 0.94, recall: 0.94, precision: 0.94, f_beta: 0.94
train: step: 1020, loss: 0.09100383520126343, acc: 0.89, recall: 0.8899999999999999, precision: 0.8977968176254589, f_beta: 0.8894583458948848
train: step: 1021, loss: 0.11521799117326736, acc: 0.84, recall: 0.84, precision: 0.8607809847198642, f_beta: 0.8376623376623376
train: step: 1022, loss: 0.13208697736263275, acc: 0.83, recall: 0.83, precision: 0.8467843631778058, f_beta: 0.8279178054458953
train: step: 1023, loss: 0.09502577036619186, acc: 0.87, recall: 0.87, precision: 0.8773969808241534, f_beta: 0.8693598633303186
train: step: 1024, loss: 0.10220614075660706, acc: 0.86, recall: 0.86, precision: 0.8623188405797102, f_beta: 0.859775641025641
train: step: 1025, loss: 0.08996216952800751, acc: 0.89, recall: 0.89, precision: 0.8914090726615818, f_beta: 0.8899009108197378
train: step: 1026, loss: 0.09546571969985962, acc: 0.88, recall: 0.8799999999999999, precision: 0.885551948051948, f_beta: 0.8795664391810518
train: step: 1027, loss: 0.07226528972387314, acc: 0.91, recall: 0.91, precision: 0.9101640656262505, f_beta: 0.90999099909991
train: step: 1028, loss: 0.09301229566335678, acc: 0.86, recall: 0.86, precision: 0.8623188405797102, f_beta: 0.859775641025641
train: step: 1029, loss: 0.0441267304122448, acc: 0.96, recall: 0.96, precision: 0.9607371794871795, f_beta: 0.959983993597439
train: step: 1030, loss: 0.10157342255115509, acc: 0.85, recall: 0.85, precision: 0.851264552388599, f_beta: 0.8498648783905516
train: step: 1031, loss: 0.06451889872550964, acc: 0.9, recall: 0.8999999999999999, precision: 0.9025764895330113, f_beta: 0.8998397435897436
train: step: 1032, loss: 0.09162464737892151, acc: 0.87, recall: 0.87, precision: 0.8737373737373737, f_beta: 0.8696741854636592
train: step: 1033, loss: 0.08447334170341492, acc: 0.9, recall: 0.9, precision: 0.9, f_beta: 0.9
train: step: 1034, loss: 0.07914064824581146, acc: 0.87, recall: 0.87, precision: 0.8823894171145101, f_beta: 0.8689384010484928
train: step: 1035, loss: 0.10420367866754532, acc: 0.88, recall: 0.88, precision: 0.8806089743589745, f_beta: 0.879951980792317
train: step: 1036, loss: 0.09665194153785706, acc: 0.88, recall: 0.88, precision: 0.8806089743589745, f_beta: 0.879951980792317
train: step: 1037, loss: 0.10323262959718704, acc: 0.86, recall: 0.86, precision: 0.8605769230769231, f_beta: 0.8599439775910365
train: step: 1038, loss: 0.09024251997470856, acc: 0.89, recall: 0.89, precision: 0.89015606242497, f_beta: 0.88998899889989
train: step: 1039, loss: 0.10385429114103317, acc: 0.86, recall: 0.86, precision: 0.86, f_beta: 0.8599999999999999
train: step: 1040, loss: 0.10718158632516861, acc: 0.86, recall: 0.86, precision: 0.8605769230769231, f_beta: 0.8599439775910365
train: step: 1041, loss: 0.07858971506357193, acc: 0.91, recall: 0.91, precision: 0.9101640656262505, f_beta: 0.90999099909991
train: step: 1042, loss: 0.0892045870423317, acc: 0.87, recall: 0.87, precision: 0.8701480592236894, f_beta: 0.8699869986998701
train: step: 1043, loss: 0.05194491520524025, acc: 0.92, recall: 0.9199999999999999, precision: 0.9206730769230769, f_beta: 0.919967987194878
train: step: 1044, loss: 0.10826542228460312, acc: 0.87, recall: 0.87, precision: 0.8773969808241534, f_beta: 0.8693598633303186
train: step: 1045, loss: 0.099020816385746, acc: 0.9, recall: 0.9, precision: 0.9006410256410255, f_beta: 0.8999599839935974
train: step: 1046, loss: 0.097737617790699, acc: 0.87, recall: 0.87, precision: 0.8773969808241534, f_beta: 0.8693598633303186
train: step: 1047, loss: 0.08037697523832321, acc: 0.91, recall: 0.9099999999999999, precision: 0.9114813327980731, f_beta: 0.9099189270343309
train: step: 1048, loss: 0.09886928647756577, acc: 0.87, recall: 0.87, precision: 0.8713368125250903, f_beta: 0.8698828946051447
train: step: 1049, loss: 0.10382751375436783, acc: 0.85, recall: 0.85, precision: 0.851264552388599, f_beta: 0.8498648783905516
train: step: 1050, loss: 0.028234267607331276, acc: 0.99, recall: 0.99, precision: 0.9901960784313726, f_beta: 0.98999899989999
train: step: 1051, loss: 0.06892392784357071, acc: 0.91, recall: 0.9099999999999999, precision: 0.9114813327980731, f_beta: 0.9099189270343309
train: step: 1052, loss: 0.3768330514431, acc: 0.81, recall: 0.81, precision: 0.8324753324753325, f_beta: 0.8067338012409724
train: step: 1053, loss: 0.07210594415664673, acc: 0.9, recall: 0.8999999999999999, precision: 0.9058441558441559, f_beta: 0.8996386993175431
train: step: 1054, loss: 0.09430497884750366, acc: 0.88, recall: 0.88, precision: 0.8824476650563606, f_beta: 0.8798076923076923
train: step: 1055, loss: 0.06355823576450348, acc: 0.92, recall: 0.92, precision: 0.92, f_beta: 0.92
train: step: 1056, loss: 0.08633565902709961, acc: 0.89, recall: 0.89, precision: 0.89015606242497, f_beta: 0.88998899889989
train: step: 1057, loss: 0.09105350822210312, acc: 0.87, recall: 0.87, precision: 0.8737373737373737, f_beta: 0.8696741854636592
train: step: 1058, loss: 0.11565614491701126, acc: 0.81, recall: 0.81, precision: 0.8131313131313131, f_beta: 0.8095238095238095
train: step: 1059, loss: 0.08956152200698853, acc: 0.88, recall: 0.8799999999999999, precision: 0.885551948051948, f_beta: 0.8795664391810518
train: step: 1060, loss: 0.061831507831811905, acc: 0.92, recall: 0.92, precision: 0.92, f_beta: 0.92
train: step: 1061, loss: 0.04818095266819, acc: 0.95, recall: 0.95, precision: 0.9516258530710557, f_beta: 0.949954959463517
train: step: 1062, loss: 0.11642544716596603, acc: 0.85, recall: 0.85, precision: 0.851264552388599, f_beta: 0.8498648783905516
train: step: 1063, loss: 0.11577392369508743, acc: 0.82, recall: 0.8200000000000001, precision: 0.8284072249589491, f_beta: 0.818840579710145
train: step: 1064, loss: 0.12065400928258896, acc: 0.82, recall: 0.8200000000000001, precision: 0.8205128205128205, f_beta: 0.8199279711884755
train: step: 1065, loss: 0.05842103809118271, acc: 0.94, recall: 0.94, precision: 0.9428341384863124, f_beta: 0.9399038461538463
train: step: 1066, loss: 0.06556501239538193, acc: 0.92, recall: 0.92, precision: 0.92, f_beta: 0.92
train: step: 1067, loss: 0.04505196213722229, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 1068, loss: 0.09881994128227234, acc: 0.88, recall: 0.88, precision: 0.8958333333333333, f_beta: 0.8787878787878789
train: step: 1069, loss: 0.11124756187200546, acc: 0.85, recall: 0.85, precision: 0.851264552388599, f_beta: 0.8498648783905516
train: step: 1070, loss: 0.12418565154075623, acc: 0.83, recall: 0.8300000000000001, precision: 0.8311922922521076, f_beta: 0.8298468621759585
train: step: 1071, loss: 0.08300483971834183, acc: 0.89, recall: 0.8899999999999999, precision: 0.8939393939393939, f_beta: 0.8897243107769424
train: step: 1072, loss: 0.08556938916444778, acc: 0.87, recall: 0.87, precision: 0.8823894171145101, f_beta: 0.8689384010484928
train: step: 1073, loss: 0.0836646631360054, acc: 0.91, recall: 0.9099999999999999, precision: 0.9181966544267646, f_beta: 0.9095568284594513
train: step: 1074, loss: 0.09008724242448807, acc: 0.86, recall: 0.86, precision: 0.8623188405797102, f_beta: 0.859775641025641
train: step: 1075, loss: 0.081842802464962, acc: 0.89, recall: 0.89, precision: 0.89015606242497, f_beta: 0.88998899889989
train: step: 1076, loss: 0.09781978279352188, acc: 0.85, recall: 0.85, precision: 0.8501400560224089, f_beta: 0.84998499849985
train: step: 1077, loss: 0.10258214175701141, acc: 0.87, recall: 0.87, precision: 0.8701480592236894, f_beta: 0.8699869986998701
train: step: 1078, loss: 0.06990353763103485, acc: 0.9, recall: 0.8999999999999999, precision: 0.9105090311986863, f_beta: 0.8993558776167472
train: step: 1079, loss: 0.09871780127286911, acc: 0.87, recall: 0.87, precision: 0.8773969808241534, f_beta: 0.8693598633303186
train: step: 1080, loss: 0.047891128808259964, acc: 0.91, recall: 0.91, precision: 0.9101640656262505, f_beta: 0.90999099909991
train: step: 1081, loss: 0.12231291830539703, acc: 0.82, recall: 0.82, precision: 0.82, f_beta: 0.82
train: step: 1082, loss: 0.10302005708217621, acc: 0.88, recall: 0.88, precision: 0.8824476650563606, f_beta: 0.8798076923076923
train: step: 1083, loss: 0.061376798897981644, acc: 0.92, recall: 0.9199999999999999, precision: 0.9310344827586207, f_beta: 0.9194847020933978
train: step: 1084, loss: 0.09219890832901001, acc: 0.87, recall: 0.87, precision: 0.8713368125250903, f_beta: 0.8698828946051447
train: step: 1085, loss: 0.11986736953258514, acc: 0.86, recall: 0.86, precision: 0.86, f_beta: 0.8599999999999999
train: step: 1086, loss: 0.07182753086090088, acc: 0.9, recall: 0.8999999999999999, precision: 0.9025764895330113, f_beta: 0.8998397435897436
train: step: 1087, loss: 0.0597638376057148, acc: 0.92, recall: 0.9199999999999999, precision: 0.9206730769230769, f_beta: 0.919967987194878
train: step: 1088, loss: 0.09867696464061737, acc: 0.86, recall: 0.8600000000000001, precision: 0.8652597402597402, f_beta: 0.8594941790445605
train: step: 1089, loss: 0.07422355562448502, acc: 0.9, recall: 0.8999999999999999, precision: 0.9025764895330113, f_beta: 0.8998397435897436
train: step: 1090, loss: 0.05119045823812485, acc: 0.94, recall: 0.94, precision: 0.94, f_beta: 0.94
train: step: 1091, loss: 0.06422102451324463, acc: 0.91, recall: 0.91, precision: 0.9101640656262505, f_beta: 0.90999099909991
train: step: 1092, loss: 0.07863505184650421, acc: 0.89, recall: 0.89, precision: 0.8914090726615818, f_beta: 0.8899009108197378
train: step: 1093, loss: 0.12563352286815643, acc: 0.85, recall: 0.85, precision: 0.851264552388599, f_beta: 0.8498648783905516
train: step: 1094, loss: 0.08517133444547653, acc: 0.91, recall: 0.91, precision: 0.9101640656262505, f_beta: 0.90999099909991
train: step: 1095, loss: 0.041678763926029205, acc: 0.96, recall: 0.96, precision: 0.9607371794871795, f_beta: 0.959983993597439
train: step: 1096, loss: 0.0770871639251709, acc: 0.9, recall: 0.9, precision: 0.9006410256410255, f_beta: 0.8999599839935974
train: step: 1097, loss: 0.07758078724145889, acc: 0.9, recall: 0.8999999999999999, precision: 0.9058441558441559, f_beta: 0.8996386993175431
train: step: 1098, loss: 0.08299684524536133, acc: 0.91, recall: 0.91, precision: 0.9101640656262505, f_beta: 0.90999099909991
train: step: 1099, loss: 0.07391149550676346, acc: 0.91, recall: 0.91, precision: 0.9101640656262505, f_beta: 0.90999099909991
train: step: 1100, loss: 0.08287233114242554, acc: 0.89, recall: 0.89, precision: 0.89015606242497, f_beta: 0.88998899889989
train: step: 1101, loss: 0.08380395174026489, acc: 0.88, recall: 0.88, precision: 0.8806089743589745, f_beta: 0.879951980792317
train: step: 1102, loss: 0.06736167520284653, acc: 0.92, recall: 0.9199999999999999, precision: 0.9206730769230769, f_beta: 0.919967987194878
train: step: 1103, loss: 0.03234831988811493, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 1104, loss: 0.052103232592344284, acc: 0.93, recall: 0.9299999999999999, precision: 0.9315535929345644, f_beta: 0.9299369432489241
train: step: 1105, loss: 0.08007892966270447, acc: 0.9, recall: 0.8999999999999999, precision: 0.9058441558441559, f_beta: 0.8996386993175431
train: step: 1106, loss: 0.03824690729379654, acc: 0.95, recall: 0.95, precision: 0.9545454545454546, f_beta: 0.949874686716792
train: step: 1107, loss: 0.06715181469917297, acc: 0.9, recall: 0.9, precision: 0.9006410256410255, f_beta: 0.8999599839935974
train: step: 1108, loss: 0.10077875852584839, acc: 0.87, recall: 0.87, precision: 0.8737373737373737, f_beta: 0.8696741854636592
train: step: 1109, loss: 0.08845247328281403, acc: 0.87, recall: 0.87, precision: 0.8888188314417822, f_beta: 0.868407733576273
train: step: 1110, loss: 0.05513806268572807, acc: 0.94, recall: 0.94, precision: 0.94, f_beta: 0.94
train: step: 1111, loss: 0.0520356260240078, acc: 0.9, recall: 0.9, precision: 0.9, f_beta: 0.9
train: step: 1112, loss: 0.054902929812669754, acc: 0.94, recall: 0.94, precision: 0.94, f_beta: 0.94
train: step: 1113, loss: 0.05202696844935417, acc: 0.95, recall: 0.95, precision: 0.9516258530710557, f_beta: 0.949954959463517
train: step: 1114, loss: 0.09946253150701523, acc: 0.87, recall: 0.87, precision: 0.8737373737373737, f_beta: 0.8696741854636592
train: step: 1115, loss: 0.08958718925714493, acc: 0.87, recall: 0.87, precision: 0.8701480592236894, f_beta: 0.8699869986998701
train: step: 1116, loss: 0.09018377214670181, acc: 0.86, recall: 0.8600000000000001, precision: 0.8652597402597402, f_beta: 0.8594941790445605
train: step: 1117, loss: 0.0589398592710495, acc: 0.92, recall: 0.9199999999999999, precision: 0.9261363636363636, f_beta: 0.9197109594540345
train: step: 1118, loss: 0.07627551257610321, acc: 0.91, recall: 0.9099999999999999, precision: 0.9141414141414141, f_beta: 0.9097744360902256
train: step: 1119, loss: 0.06469447165727615, acc: 0.94, recall: 0.94, precision: 0.9428341384863124, f_beta: 0.9399038461538463
train: step: 1120, loss: 0.08274063467979431, acc: 0.91, recall: 0.91, precision: 0.9101640656262505, f_beta: 0.90999099909991
train: step: 1121, loss: 0.07996530830860138, acc: 0.88, recall: 0.88, precision: 0.8806089743589745, f_beta: 0.879951980792317
train: step: 1122, loss: 0.04046235978603363, acc: 0.94, recall: 0.94, precision: 0.9407051282051282, f_beta: 0.9399759903961584
train: step: 1123, loss: 0.08738616853952408, acc: 0.89, recall: 0.89, precision: 0.89015606242497, f_beta: 0.88998899889989
train: step: 1124, loss: 0.03830986097455025, acc: 0.94, recall: 0.94, precision: 0.94, f_beta: 0.94
train: step: 1125, loss: 0.09901346266269684, acc: 0.89, recall: 0.89, precision: 0.89015606242497, f_beta: 0.88998899889989
train: step: 1126, loss: 0.07112564891576767, acc: 0.9, recall: 0.9, precision: 0.9, f_beta: 0.9
train: step: 1127, loss: 0.1300172507762909, acc: 0.85, recall: 0.85, precision: 0.8501400560224089, f_beta: 0.84998499849985
train: step: 1128, loss: 0.09006036072969437, acc: 0.89, recall: 0.89, precision: 0.89015606242497, f_beta: 0.88998899889989
train: step: 1129, loss: 0.07515551894903183, acc: 0.91, recall: 0.91, precision: 0.9101640656262505, f_beta: 0.90999099909991
train: step: 1130, loss: 0.07372656464576721, acc: 0.91, recall: 0.9099999999999999, precision: 0.9141414141414141, f_beta: 0.9097744360902256
train: step: 1131, loss: 0.06685155630111694, acc: 0.92, recall: 0.9199999999999999, precision: 0.9227053140096618, f_beta: 0.9198717948717948
train: step: 1132, loss: 0.06028866395354271, acc: 0.91, recall: 0.91, precision: 0.9101640656262505, f_beta: 0.90999099909991
train: step: 1133, loss: 0.05352017283439636, acc: 0.94, recall: 0.94, precision: 0.9407051282051282, f_beta: 0.9399759903961584
train: step: 1134, loss: 0.1037977784872055, acc: 0.84, recall: 0.84, precision: 0.8405448717948718, f_beta: 0.8399359743897559
train: step: 1135, loss: 0.044347476214170456, acc: 0.93, recall: 0.9299999999999999, precision: 0.930172068827531, f_beta: 0.9299929992999301
train: step: 1136, loss: 0.07696697860956192, acc: 0.91, recall: 0.91, precision: 0.9101640656262505, f_beta: 0.90999099909991
train: step: 1137, loss: 0.08799754083156586, acc: 0.87, recall: 0.87, precision: 0.8713368125250903, f_beta: 0.8698828946051447
train: step: 1138, loss: 0.0538233183324337, acc: 0.92, recall: 0.9199999999999999, precision: 0.9206730769230769, f_beta: 0.919967987194878
train: step: 1139, loss: 0.05974684655666351, acc: 0.91, recall: 0.91, precision: 0.9101640656262505, f_beta: 0.90999099909991
train: step: 1140, loss: 0.09030680358409882, acc: 0.9, recall: 0.9, precision: 0.9, f_beta: 0.9
train: step: 1141, loss: 0.03965918719768524, acc: 0.94, recall: 0.94, precision: 0.9407051282051282, f_beta: 0.9399759903961584
train: step: 1142, loss: 0.09177619963884354, acc: 0.85, recall: 0.85, precision: 0.851264552388599, f_beta: 0.8498648783905516
train: step: 1143, loss: 0.052152082324028015, acc: 0.95, recall: 0.95, precision: 0.9501800720288115, f_beta: 0.9499949994999499
train: step: 1144, loss: 0.09573309123516083, acc: 0.87, recall: 0.87, precision: 0.8737373737373737, f_beta: 0.8696741854636592
train: step: 1145, loss: 0.056210245937108994, acc: 0.94, recall: 0.94, precision: 0.94, f_beta: 0.94
train: step: 1146, loss: 0.09164996445178986, acc: 0.89, recall: 0.89, precision: 0.8914090726615818, f_beta: 0.8899009108197378
train: step: 1147, loss: 0.05434035137295723, acc: 0.94, recall: 0.94, precision: 0.9407051282051282, f_beta: 0.9399759903961584
train: step: 1148, loss: 0.04766370728611946, acc: 0.95, recall: 0.95, precision: 0.9501800720288115, f_beta: 0.9499949994999499
train: step: 1149, loss: 0.08426931500434875, acc: 0.9, recall: 0.9, precision: 0.9006410256410255, f_beta: 0.8999599839935974
train: step: 1150, loss: 0.09001144766807556, acc: 0.88, recall: 0.88, precision: 0.8824476650563606, f_beta: 0.8798076923076923
train: step: 1151, loss: 0.08147358894348145, acc: 0.89, recall: 0.8899999999999999, precision: 0.8939393939393939, f_beta: 0.8897243107769424
train: step: 1152, loss: 0.08705996721982956, acc: 0.89, recall: 0.89, precision: 0.89015606242497, f_beta: 0.88998899889989
train: step: 1153, loss: 0.05486639589071274, acc: 0.95, recall: 0.95, precision: 0.9501800720288115, f_beta: 0.9499949994999499
train: step: 1154, loss: 0.05693124607205391, acc: 0.93, recall: 0.9299999999999999, precision: 0.930172068827531, f_beta: 0.9299929992999301
train: step: 1155, loss: 0.06844081729650497, acc: 0.91, recall: 0.9099999999999999, precision: 0.9114813327980731, f_beta: 0.9099189270343309
train: step: 1156, loss: 0.08369354158639908, acc: 0.88, recall: 0.88, precision: 0.8824476650563606, f_beta: 0.8798076923076923
train: step: 1157, loss: 0.09366552531719208, acc: 0.89, recall: 0.8899999999999999, precision: 0.8939393939393939, f_beta: 0.8897243107769424
train: step: 1158, loss: 0.04340364411473274, acc: 0.93, recall: 0.9299999999999999, precision: 0.9343434343434343, f_beta: 0.9298245614035088
train: step: 1159, loss: 0.10465896874666214, acc: 0.87, recall: 0.87, precision: 0.8773969808241534, f_beta: 0.8693598633303186
train: step: 1160, loss: 0.08274660259485245, acc: 0.88, recall: 0.88, precision: 0.8958333333333333, f_beta: 0.8787878787878789
train: step: 1161, loss: 0.07354062050580978, acc: 0.9, recall: 0.9, precision: 0.9, f_beta: 0.9
train: step: 1162, loss: 0.08327978104352951, acc: 0.89, recall: 0.89, precision: 0.89015606242497, f_beta: 0.88998899889989
train: step: 1163, loss: 0.06477150321006775, acc: 0.9, recall: 0.9, precision: 0.9, f_beta: 0.9
train: step: 1164, loss: 0.04263387620449066, acc: 0.95, recall: 0.95, precision: 0.9501800720288115, f_beta: 0.9499949994999499
train: step: 1165, loss: 0.05796978995203972, acc: 0.93, recall: 0.9299999999999999, precision: 0.930172068827531, f_beta: 0.9299929992999301
train: step: 1166, loss: 0.08936955779790878, acc: 0.89, recall: 0.89, precision: 0.8914090726615818, f_beta: 0.8899009108197378
train: step: 1167, loss: 0.30568015575408936, acc: 0.89, recall: 0.89, precision: 0.8914090726615818, f_beta: 0.8899009108197378
train: step: 1168, loss: 0.06772401183843613, acc: 0.92, recall: 0.9199999999999999, precision: 0.9227053140096618, f_beta: 0.9198717948717948
train: step: 1169, loss: 0.050024863332509995, acc: 0.94, recall: 0.94, precision: 0.9407051282051282, f_beta: 0.9399759903961584
train: step: 1170, loss: 0.04572240263223648, acc: 0.95, recall: 0.95, precision: 0.9516258530710557, f_beta: 0.949954959463517
train: step: 1171, loss: 0.10076411068439484, acc: 0.88, recall: 0.8799999999999999, precision: 0.885551948051948, f_beta: 0.8795664391810518
train: step: 1172, loss: 0.07748264819383621, acc: 0.89, recall: 0.8899999999999999, precision: 0.8977968176254589, f_beta: 0.8894583458948848
train: step: 1173, loss: 0.04591257870197296, acc: 0.94, recall: 0.94, precision: 0.9407051282051282, f_beta: 0.9399759903961584
train: step: 1174, loss: 0.12391004711389542, acc: 0.82, recall: 0.8200000000000001, precision: 0.8205128205128205, f_beta: 0.8199279711884755
train: step: 1175, loss: 0.09656210243701935, acc: 0.82, recall: 0.82, precision: 0.82, f_beta: 0.82
train: step: 1176, loss: 0.08635279536247253, acc: 0.89, recall: 0.89, precision: 0.8914090726615818, f_beta: 0.8899009108197378
train: step: 1177, loss: 0.041538055986166, acc: 0.94, recall: 0.94, precision: 0.9428341384863124, f_beta: 0.9399038461538463
train: step: 1178, loss: 0.06325848400592804, acc: 0.94, recall: 0.94, precision: 0.9428341384863124, f_beta: 0.9399038461538463
train: step: 1179, loss: 0.05130831152200699, acc: 0.95, recall: 0.95, precision: 0.9501800720288115, f_beta: 0.9499949994999499
train: step: 1180, loss: 0.09169687330722809, acc: 0.86, recall: 0.8600000000000001, precision: 0.8652597402597402, f_beta: 0.8594941790445605
train: step: 1181, loss: 0.05452514812350273, acc: 0.92, recall: 0.9199999999999999, precision: 0.9227053140096618, f_beta: 0.9198717948717948
train: step: 1182, loss: 0.05658664554357529, acc: 0.92, recall: 0.9199999999999999, precision: 0.9227053140096618, f_beta: 0.9198717948717948
train: step: 1183, loss: 0.0664895549416542, acc: 0.92, recall: 0.9199999999999999, precision: 0.9227053140096618, f_beta: 0.9198717948717948
train: step: 1184, loss: 0.05407433584332466, acc: 0.92, recall: 0.92, precision: 0.92, f_beta: 0.92
train: step: 1185, loss: 0.07748614996671677, acc: 0.9, recall: 0.8999999999999999, precision: 0.9025764895330113, f_beta: 0.8998397435897436
train: step: 1186, loss: 0.09124433249235153, acc: 0.88, recall: 0.88, precision: 0.8824476650563606, f_beta: 0.8798076923076923
train: step: 1187, loss: 0.12583941221237183, acc: 0.83, recall: 0.8300000000000001, precision: 0.8410500206696983, f_beta: 0.8286117552172598
train: step: 1188, loss: 0.05662744864821434, acc: 0.93, recall: 0.9299999999999999, precision: 0.9343434343434343, f_beta: 0.9298245614035088
train: step: 1189, loss: 0.07818660140037537, acc: 0.88, recall: 0.88, precision: 0.88, f_beta: 0.88
train: step: 1190, loss: 0.043229687958955765, acc: 0.92, recall: 0.9199999999999999, precision: 0.9206730769230769, f_beta: 0.919967987194878
train: step: 1191, loss: 0.13482072949409485, acc: 0.82, recall: 0.8200000000000001, precision: 0.8472222222222222, f_beta: 0.8164014687882497
train: step: 1192, loss: 0.10186630487442017, acc: 0.86, recall: 0.8600000000000001, precision: 0.8652597402597402, f_beta: 0.8594941790445605
train: step: 1193, loss: 0.07552658766508102, acc: 0.93, recall: 0.9299999999999999, precision: 0.9343434343434343, f_beta: 0.9298245614035088
train: step: 1194, loss: 0.06934251636266708, acc: 0.89, recall: 0.89, precision: 0.89015606242497, f_beta: 0.88998899889989
train: step: 1195, loss: 0.11540794372558594, acc: 0.84, recall: 0.84, precision: 0.8607809847198642, f_beta: 0.8376623376623376
train: step: 1196, loss: 0.11767463386058807, acc: 0.85, recall: 0.85, precision: 0.851264552388599, f_beta: 0.8498648783905516
train: step: 1197, loss: 0.11390205472707748, acc: 0.86, recall: 0.86, precision: 0.8623188405797102, f_beta: 0.859775641025641
train: step: 1198, loss: 0.09970995038747787, acc: 0.86, recall: 0.8600000000000001, precision: 0.8652597402597402, f_beta: 0.8594941790445605
train: step: 1199, loss: 0.07856063544750214, acc: 0.88, recall: 0.88, precision: 0.8824476650563606, f_beta: 0.8798076923076923
train: step: 1200, loss: 0.056478243321180344, acc: 0.91, recall: 0.91, precision: 0.9101640656262505, f_beta: 0.90999099909991
train: step: 1201, loss: 0.06952986121177673, acc: 0.9, recall: 0.9, precision: 0.9006410256410255, f_beta: 0.8999599839935974
train: step: 1202, loss: 0.0708298608660698, acc: 0.89, recall: 0.8899999999999999, precision: 0.8939393939393939, f_beta: 0.8897243107769424
train: step: 1203, loss: 0.12067504972219467, acc: 0.82, recall: 0.8200000000000001, precision: 0.8205128205128205, f_beta: 0.8199279711884755
train: step: 1204, loss: 0.06946119666099548, acc: 0.89, recall: 0.89, precision: 0.89015606242497, f_beta: 0.88998899889989
train: step: 1205, loss: 0.10996652394533157, acc: 0.84, recall: 0.84, precision: 0.8405448717948718, f_beta: 0.8399359743897559
train: step: 1206, loss: 0.10252292454242706, acc: 0.87, recall: 0.87, precision: 0.8701480592236894, f_beta: 0.8699869986998701
train: step: 1207, loss: 0.08381757885217667, acc: 0.88, recall: 0.88, precision: 0.8899835796387521, f_beta: 0.8792270531400966
train: step: 1208, loss: 0.07700913399457932, acc: 0.91, recall: 0.9099999999999999, precision: 0.9141414141414141, f_beta: 0.9097744360902256
train: step: 1209, loss: 0.08914738893508911, acc: 0.9, recall: 0.8999999999999999, precision: 0.9025764895330113, f_beta: 0.8998397435897436
train: step: 1210, loss: 0.09513866156339645, acc: 0.85, recall: 0.8500000000000001, precision: 0.8535353535353536, f_beta: 0.849624060150376
train: step: 1211, loss: 0.0723014697432518, acc: 0.91, recall: 0.9099999999999999, precision: 0.9141414141414141, f_beta: 0.9097744360902256
train: step: 1212, loss: 0.08250325918197632, acc: 0.88, recall: 0.88, precision: 0.8806089743589745, f_beta: 0.879951980792317
train: step: 1213, loss: 0.06685847043991089, acc: 0.9, recall: 0.8999999999999999, precision: 0.9025764895330113, f_beta: 0.8998397435897436
train: step: 1214, loss: 0.12718375027179718, acc: 0.82, recall: 0.8200000000000001, precision: 0.8333333333333334, f_beta: 0.8181818181818181
train: step: 1215, loss: 0.07559125125408173, acc: 0.89, recall: 0.89, precision: 0.8914090726615818, f_beta: 0.8899009108197378
train: step: 1216, loss: 0.09964414685964584, acc: 0.87, recall: 0.87, precision: 0.8701480592236894, f_beta: 0.8699869986998701
train: step: 1217, loss: 0.05776897445321083, acc: 0.9, recall: 0.9, precision: 0.9006410256410255, f_beta: 0.8999599839935974
train: step: 1218, loss: 0.09613359719514847, acc: 0.88, recall: 0.8799999999999999, precision: 0.885551948051948, f_beta: 0.8795664391810518
train: step: 1219, loss: 0.06953238695859909, acc: 0.9, recall: 0.9, precision: 0.9006410256410255, f_beta: 0.8999599839935974
train: step: 1220, loss: 0.1000923216342926, acc: 0.84, recall: 0.8400000000000001, precision: 0.8421900161030595, f_beta: 0.8397435897435896
train: step: 1221, loss: 0.050967518240213394, acc: 0.93, recall: 0.9299999999999999, precision: 0.9315535929345644, f_beta: 0.9299369432489241
train: step: 1222, loss: 0.08944277465343475, acc: 0.88, recall: 0.88, precision: 0.8824476650563606, f_beta: 0.8798076923076923
train: step: 1223, loss: 0.06843205541372299, acc: 0.93, recall: 0.9299999999999999, precision: 0.9315535929345644, f_beta: 0.9299369432489241
train: step: 1224, loss: 0.07172276824712753, acc: 0.92, recall: 0.9199999999999999, precision: 0.9206730769230769, f_beta: 0.919967987194878
train: step: 1225, loss: 0.04306308180093765, acc: 0.96, recall: 0.96, precision: 0.9607371794871795, f_beta: 0.959983993597439
train: step: 1226, loss: 0.0474218986928463, acc: 0.92, recall: 0.9199999999999999, precision: 0.9227053140096618, f_beta: 0.9198717948717948
train: step: 1227, loss: 0.07755711674690247, acc: 0.88, recall: 0.88, precision: 0.8824476650563606, f_beta: 0.8798076923076923
train: step: 1228, loss: 0.07679471373558044, acc: 0.89, recall: 0.89, precision: 0.8914090726615818, f_beta: 0.8899009108197378
train: step: 1229, loss: 0.06768391281366348, acc: 0.91, recall: 0.91, precision: 0.9101640656262505, f_beta: 0.90999099909991
train: step: 1230, loss: 0.10830262303352356, acc: 0.86, recall: 0.8600000000000001, precision: 0.8652597402597402, f_beta: 0.8594941790445605
train: step: 1231, loss: 0.09783468395471573, acc: 0.88, recall: 0.88, precision: 0.8824476650563606, f_beta: 0.8798076923076923
train: step: 1232, loss: 0.09325531125068665, acc: 0.87, recall: 0.87, precision: 0.8737373737373737, f_beta: 0.8696741854636592
train: step: 1233, loss: 0.048379067331552505, acc: 0.94, recall: 0.94, precision: 0.9428341384863124, f_beta: 0.9399038461538463
train: step: 1234, loss: 0.10274427384138107, acc: 0.85, recall: 0.8500000000000001, precision: 0.8535353535353536, f_beta: 0.849624060150376
train: step: 1235, loss: 0.05998130887746811, acc: 0.92, recall: 0.9199999999999999, precision: 0.9206730769230769, f_beta: 0.919967987194878
train: step: 1236, loss: 0.0909218117594719, acc: 0.88, recall: 0.88, precision: 0.88, f_beta: 0.88
train: step: 1237, loss: 0.08549684286117554, acc: 0.88, recall: 0.88, precision: 0.88, f_beta: 0.88
train: step: 1238, loss: 0.15091471374034882, acc: 0.8, recall: 0.8, precision: 0.8183361629881154, f_beta: 0.797077922077922
train: step: 1239, loss: 0.05709153041243553, acc: 0.92, recall: 0.9199999999999999, precision: 0.9227053140096618, f_beta: 0.9198717948717948
train: step: 1240, loss: 0.070421501994133, acc: 0.9, recall: 0.9, precision: 0.9, f_beta: 0.9
train: step: 1241, loss: 0.06888183951377869, acc: 0.89, recall: 0.89, precision: 0.89015606242497, f_beta: 0.88998899889989
train: step: 1242, loss: 0.03789253160357475, acc: 0.96, recall: 0.96, precision: 0.962962962962963, f_beta: 0.9599358974358974
train: step: 1243, loss: 0.057198356837034225, acc: 0.94, recall: 0.94, precision: 0.9464285714285714, f_beta: 0.939783219590526
train: step: 1244, loss: 0.1162109449505806, acc: 0.85, recall: 0.85, precision: 0.851264552388599, f_beta: 0.8498648783905516
train: step: 1245, loss: 0.07202866673469543, acc: 0.91, recall: 0.91, precision: 0.9101640656262505, f_beta: 0.90999099909991
train: step: 1246, loss: 0.053236257284879684, acc: 0.94, recall: 0.94, precision: 0.9407051282051282, f_beta: 0.9399759903961584
train: step: 1247, loss: 0.04302169010043144, acc: 0.95, recall: 0.95, precision: 0.9501800720288115, f_beta: 0.9499949994999499
train: step: 1248, loss: 0.0560692697763443, acc: 0.94, recall: 0.94, precision: 0.9407051282051282, f_beta: 0.9399759903961584
train: step: 1249, loss: 0.06115203723311424, acc: 0.9, recall: 0.9, precision: 0.9006410256410255, f_beta: 0.8999599839935974
train: step: 1250, loss: 0.06734325736761093, acc: 0.92, recall: 0.9199999999999999, precision: 0.9206730769230769, f_beta: 0.919967987194878
train: step: 1251, loss: 0.04330822825431824, acc: 0.94, recall: 0.94, precision: 0.94, f_beta: 0.94
train: step: 1252, loss: 0.04526800289750099, acc: 0.96, recall: 0.96, precision: 0.96, f_beta: 0.96
train: step: 1253, loss: 0.09422078728675842, acc: 0.87, recall: 0.87, precision: 0.8773969808241534, f_beta: 0.8693598633303186
train: step: 1254, loss: 0.06233634799718857, acc: 0.9, recall: 0.8999999999999999, precision: 0.9025764895330113, f_beta: 0.8998397435897436
train: step: 1255, loss: 0.09447114914655685, acc: 0.84, recall: 0.84, precision: 0.8405448717948718, f_beta: 0.8399359743897559
train: step: 1256, loss: 0.049802981317043304, acc: 0.94, recall: 0.94, precision: 0.9428341384863124, f_beta: 0.9399038461538463
train: step: 1257, loss: 0.03705702722072601, acc: 0.95, recall: 0.95, precision: 0.9501800720288115, f_beta: 0.9499949994999499
train: step: 1258, loss: 0.049083516001701355, acc: 0.95, recall: 0.95, precision: 0.9545454545454546, f_beta: 0.949874686716792
train: step: 1259, loss: 0.06985819339752197, acc: 0.89, recall: 0.89, precision: 0.8914090726615818, f_beta: 0.8899009108197378
train: step: 1260, loss: 0.06167081743478775, acc: 0.92, recall: 0.9199999999999999, precision: 0.9206730769230769, f_beta: 0.919967987194878
train: step: 1261, loss: 0.046234965324401855, acc: 0.93, recall: 0.9299999999999999, precision: 0.930172068827531, f_beta: 0.9299929992999301
train: step: 1262, loss: 0.11487430334091187, acc: 0.87, recall: 0.87, precision: 0.8701480592236894, f_beta: 0.8699869986998701
train: step: 1263, loss: 0.11051630228757858, acc: 0.85, recall: 0.8500000000000001, precision: 0.8535353535353536, f_beta: 0.849624060150376
train: step: 1264, loss: 0.04337277263402939, acc: 0.94, recall: 0.94, precision: 0.94, f_beta: 0.94
train: step: 1265, loss: 0.08277113735675812, acc: 0.92, recall: 0.9199999999999999, precision: 0.9206730769230769, f_beta: 0.919967987194878
train: step: 1266, loss: 0.08947010338306427, acc: 0.89, recall: 0.89, precision: 0.89015606242497, f_beta: 0.88998899889989
train: step: 1267, loss: 0.10339713096618652, acc: 0.87, recall: 0.87, precision: 0.8773969808241534, f_beta: 0.8693598633303186
train: step: 1268, loss: 0.08454965800046921, acc: 0.89, recall: 0.89, precision: 0.89015606242497, f_beta: 0.88998899889989
train: step: 1269, loss: 0.05199309438467026, acc: 0.96, recall: 0.96, precision: 0.9607371794871795, f_beta: 0.959983993597439
train: step: 1270, loss: 0.07587956637144089, acc: 0.91, recall: 0.9099999999999999, precision: 0.9114813327980731, f_beta: 0.9099189270343309
train: step: 1271, loss: 0.09106441587209702, acc: 0.88, recall: 0.88, precision: 0.88, f_beta: 0.88
train: step: 1272, loss: 0.05520954728126526, acc: 0.94, recall: 0.94, precision: 0.94, f_beta: 0.94
train: step: 1273, loss: 0.07197883725166321, acc: 0.89, recall: 0.89, precision: 0.89015606242497, f_beta: 0.88998899889989
train: step: 1274, loss: 0.10417284071445465, acc: 0.85, recall: 0.85, precision: 0.851264552388599, f_beta: 0.8498648783905516
train: step: 1275, loss: 0.05353023111820221, acc: 0.93, recall: 0.9299999999999999, precision: 0.930172068827531, f_beta: 0.9299929992999301
train: step: 1276, loss: 0.05927914008498192, acc: 0.93, recall: 0.9299999999999999, precision: 0.930172068827531, f_beta: 0.9299929992999301
train: step: 1277, loss: 0.0680043026804924, acc: 0.92, recall: 0.92, precision: 0.92, f_beta: 0.92
train: step: 1278, loss: 0.0593746080994606, acc: 0.91, recall: 0.9099999999999999, precision: 0.9114813327980731, f_beta: 0.9099189270343309
train: step: 1279, loss: 0.05878753215074539, acc: 0.93, recall: 0.9299999999999999, precision: 0.930172068827531, f_beta: 0.9299929992999301
train: step: 1280, loss: 0.07008963078260422, acc: 0.89, recall: 0.89, precision: 0.89015606242497, f_beta: 0.88998899889989
train: step: 1281, loss: 0.079926498234272, acc: 0.88, recall: 0.88, precision: 0.8958333333333333, f_beta: 0.8787878787878789
train: step: 1282, loss: 0.062342673540115356, acc: 0.91, recall: 0.91, precision: 0.9101640656262505, f_beta: 0.90999099909991
train: step: 1283, loss: 0.06586678326129913, acc: 0.93, recall: 0.9299999999999999, precision: 0.9315535929345644, f_beta: 0.9299369432489241
train: step: 1284, loss: 0.03966744244098663, acc: 0.96, recall: 0.96, precision: 0.96, f_beta: 0.96
train: step: 1285, loss: 0.06825888901948929, acc: 0.92, recall: 0.9199999999999999, precision: 0.9227053140096618, f_beta: 0.9198717948717948
train: step: 1286, loss: 0.05371547117829323, acc: 0.94, recall: 0.94, precision: 0.9428341384863124, f_beta: 0.9399038461538463
train: step: 1287, loss: 0.06182919442653656, acc: 0.92, recall: 0.9199999999999999, precision: 0.9206730769230769, f_beta: 0.919967987194878
train: step: 1288, loss: 0.0700988695025444, acc: 0.92, recall: 0.92, precision: 0.92, f_beta: 0.92
train: step: 1289, loss: 0.09003793448209763, acc: 0.88, recall: 0.88, precision: 0.8824476650563606, f_beta: 0.8798076923076923
train: step: 1290, loss: 0.08055543899536133, acc: 0.9, recall: 0.9, precision: 0.9, f_beta: 0.9
train: step: 1291, loss: 0.09375900030136108, acc: 0.85, recall: 0.85, precision: 0.851264552388599, f_beta: 0.8498648783905516
train: step: 1292, loss: 0.0642973855137825, acc: 0.94, recall: 0.94, precision: 0.9407051282051282, f_beta: 0.9399759903961584
train: step: 1293, loss: 0.05459165945649147, acc: 0.92, recall: 0.9199999999999999, precision: 0.9206730769230769, f_beta: 0.919967987194878
train: step: 1294, loss: 0.04886332526803017, acc: 0.94, recall: 0.94, precision: 0.9407051282051282, f_beta: 0.9399759903961584
train: step: 1295, loss: 0.026258103549480438, acc: 0.98, recall: 0.98, precision: 0.9807692307692308, f_beta: 0.9799919967987194
train: step: 1296, loss: 0.059176910668611526, acc: 0.93, recall: 0.9299999999999999, precision: 0.9343434343434343, f_beta: 0.9298245614035088
train: step: 1297, loss: 0.07134703546762466, acc: 0.91, recall: 0.9099999999999999, precision: 0.9114813327980731, f_beta: 0.9099189270343309
train: step: 1298, loss: 0.08411948382854462, acc: 0.88, recall: 0.88, precision: 0.88, f_beta: 0.88
train: step: 1299, loss: 0.10830183327198029, acc: 0.87, recall: 0.87, precision: 0.8773969808241534, f_beta: 0.8693598633303186
train: step: 1300, loss: 0.03923224285244942, acc: 0.95, recall: 0.95, precision: 0.9545454545454546, f_beta: 0.949874686716792
train: step: 1301, loss: 0.044468823820352554, acc: 0.93, recall: 0.9299999999999999, precision: 0.930172068827531, f_beta: 0.9299929992999301
train: step: 1302, loss: 0.0869835913181305, acc: 0.89, recall: 0.89, precision: 0.89015606242497, f_beta: 0.88998899889989
train: step: 1303, loss: 0.11046907305717468, acc: 0.85, recall: 0.85, precision: 0.851264552388599, f_beta: 0.8498648783905516
train: step: 1304, loss: 0.06729784607887268, acc: 0.9, recall: 0.9, precision: 0.9006410256410255, f_beta: 0.8999599839935974
train: step: 1305, loss: 0.10118422657251358, acc: 0.86, recall: 0.86, precision: 0.8623188405797102, f_beta: 0.859775641025641
train: step: 1306, loss: 0.06536485999822617, acc: 0.91, recall: 0.91, precision: 0.9101640656262505, f_beta: 0.90999099909991
train: step: 1307, loss: 0.06285479664802551, acc: 0.93, recall: 0.9299999999999999, precision: 0.930172068827531, f_beta: 0.9299929992999301
train: step: 1308, loss: 0.07787064462900162, acc: 0.88, recall: 0.88, precision: 0.8824476650563606, f_beta: 0.8798076923076923
train: step: 1309, loss: 0.060399044305086136, acc: 0.92, recall: 0.92, precision: 0.92, f_beta: 0.92
train: step: 1310, loss: 0.0864918977022171, acc: 0.88, recall: 0.88, precision: 0.8899835796387521, f_beta: 0.8792270531400966
train: step: 1311, loss: 0.07806040346622467, acc: 0.92, recall: 0.92, precision: 0.92, f_beta: 0.92
train: step: 1312, loss: 0.06304647773504257, acc: 0.89, recall: 0.89, precision: 0.89015606242497, f_beta: 0.88998899889989
train: step: 1313, loss: 0.03626399487257004, acc: 0.95, recall: 0.95, precision: 0.9501800720288115, f_beta: 0.9499949994999499
train: step: 1314, loss: 0.05618533492088318, acc: 0.94, recall: 0.94, precision: 0.9464285714285714, f_beta: 0.939783219590526
train: step: 1315, loss: 0.07262373715639114, acc: 0.9, recall: 0.9, precision: 0.9006410256410255, f_beta: 0.8999599839935974
train: step: 1316, loss: 0.08133594691753387, acc: 0.9, recall: 0.9, precision: 0.9, f_beta: 0.9
train: step: 1317, loss: 0.10072211176156998, acc: 0.88, recall: 0.88, precision: 0.8824476650563606, f_beta: 0.8798076923076923
train: step: 1318, loss: 0.08954334259033203, acc: 0.86, recall: 0.86, precision: 0.86, f_beta: 0.8599999999999999
train: step: 1319, loss: 0.11838950216770172, acc: 0.86, recall: 0.8600000000000001, precision: 0.8652597402597402, f_beta: 0.8594941790445605
train: step: 1320, loss: 0.09135866165161133, acc: 0.86, recall: 0.86, precision: 0.8694581280788177, f_beta: 0.8590982286634461
train: step: 1321, loss: 0.05461232736706734, acc: 0.92, recall: 0.92, precision: 0.92, f_beta: 0.92
train: step: 1322, loss: 0.08724548667669296, acc: 0.89, recall: 0.89, precision: 0.89015606242497, f_beta: 0.88998899889989
train: step: 1323, loss: 0.06931193172931671, acc: 0.92, recall: 0.9199999999999999, precision: 0.9261363636363636, f_beta: 0.9197109594540345
train: step: 1324, loss: 0.06977313756942749, acc: 0.9, recall: 0.8999999999999999, precision: 0.9025764895330113, f_beta: 0.8998397435897436
train: step: 1325, loss: 0.04608294367790222, acc: 0.94, recall: 0.94, precision: 0.9464285714285714, f_beta: 0.939783219590526
train: step: 1326, loss: 0.403400719165802, acc: 0.71, recall: 0.71, precision: 0.7964426877470356, f_beta: 0.687196634667242
train: step: 1327, loss: 0.07424283772706985, acc: 0.9, recall: 0.9, precision: 0.9006410256410255, f_beta: 0.8999599839935974
train: step: 1328, loss: 0.08490565419197083, acc: 0.88, recall: 0.88, precision: 0.8806089743589745, f_beta: 0.879951980792317
train: step: 1329, loss: 0.055533379316329956, acc: 0.9, recall: 0.9, precision: 0.9006410256410255, f_beta: 0.8999599839935974
train: step: 1330, loss: 0.10187689960002899, acc: 0.87, recall: 0.87, precision: 0.8737373737373737, f_beta: 0.8696741854636592
train: step: 1331, loss: 0.09869203716516495, acc: 0.87, recall: 0.87, precision: 0.8701480592236894, f_beta: 0.8699869986998701
train: step: 1332, loss: 0.03298356011509895, acc: 0.96, recall: 0.96, precision: 0.96, f_beta: 0.96
train: step: 1333, loss: 0.03717745840549469, acc: 0.95, recall: 0.95, precision: 0.9516258530710557, f_beta: 0.949954959463517
train: step: 1334, loss: 0.04683380201458931, acc: 0.95, recall: 0.95, precision: 0.9501800720288115, f_beta: 0.9499949994999499
train: step: 1335, loss: 0.10786058753728867, acc: 0.83, recall: 0.8300000000000001, precision: 0.8365973072215422, f_beta: 0.8291628982011858
train: step: 1336, loss: 0.054053839296102524, acc: 0.92, recall: 0.9199999999999999, precision: 0.9227053140096618, f_beta: 0.9198717948717948
train: step: 1337, loss: 0.050849925726652145, acc: 0.94, recall: 0.94, precision: 0.9407051282051282, f_beta: 0.9399759903961584
train: step: 1338, loss: 0.09331972897052765, acc: 0.87, recall: 0.87, precision: 0.8713368125250903, f_beta: 0.8698828946051447
train: step: 1339, loss: 0.10655651986598969, acc: 0.82, recall: 0.8200000000000001, precision: 0.8284072249589491, f_beta: 0.818840579710145
train: step: 1340, loss: 0.09080648422241211, acc: 0.86, recall: 0.86, precision: 0.8623188405797102, f_beta: 0.859775641025641
train: step: 1341, loss: 0.09713548421859741, acc: 0.88, recall: 0.8799999999999999, precision: 0.885551948051948, f_beta: 0.8795664391810518
train: step: 1342, loss: 0.05564350262284279, acc: 0.92, recall: 0.9199999999999999, precision: 0.9227053140096618, f_beta: 0.9198717948717948
train: step: 1343, loss: 0.10272939503192902, acc: 0.89, recall: 0.89, precision: 0.8914090726615818, f_beta: 0.8899009108197378
train: step: 1344, loss: 0.0417848639190197, acc: 0.94, recall: 0.94, precision: 0.9464285714285714, f_beta: 0.939783219590526
train: step: 1345, loss: 0.10645119845867157, acc: 0.84, recall: 0.84, precision: 0.8689236111111112, f_beta: 0.8368013055895552
train: step: 1346, loss: 0.0934184268116951, acc: 0.84, recall: 0.84, precision: 0.8541666666666667, f_beta: 0.8383838383838385
train: step: 1347, loss: 0.09628814458847046, acc: 0.87, recall: 0.87, precision: 0.8888188314417822, f_beta: 0.868407733576273
train: step: 1348, loss: 0.10366950929164886, acc: 0.86, recall: 0.86, precision: 0.875, f_beta: 0.8585858585858586
train: step: 1349, loss: 0.05150917172431946, acc: 0.95, recall: 0.95, precision: 0.9501800720288115, f_beta: 0.9499949994999499
train: step: 1350, loss: 0.046591952443122864, acc: 0.94, recall: 0.94, precision: 0.9407051282051282, f_beta: 0.9399759903961584
train: step: 1351, loss: 0.0559476837515831, acc: 0.94, recall: 0.94, precision: 0.9407051282051282, f_beta: 0.9399759903961584
train: step: 1352, loss: 0.06786227226257324, acc: 0.92, recall: 0.9199999999999999, precision: 0.9261363636363636, f_beta: 0.9197109594540345
train: step: 1353, loss: 0.4020368158817291, acc: 0.81, recall: 0.81, precision: 0.8505201266395297, f_beta: 0.8043455874781176
train: step: 1354, loss: 0.08545977622270584, acc: 0.9, recall: 0.8999999999999999, precision: 0.9105090311986863, f_beta: 0.8993558776167472
train: step: 1355, loss: 0.04812053591012955, acc: 0.96, recall: 0.96, precision: 0.9607371794871795, f_beta: 0.959983993597439
train: step: 1356, loss: 0.04579782858490944, acc: 0.96, recall: 0.96, precision: 0.962962962962963, f_beta: 0.9599358974358974
train: step: 1357, loss: 0.14324048161506653, acc: 0.8, recall: 0.8, precision: 0.8255208333333333, f_beta: 0.7960016319869441
train: step: 1358, loss: 0.10983549803495407, acc: 0.85, recall: 0.8500000000000001, precision: 0.8569971440228478, f_beta: 0.8492613807657521
train: step: 1359, loss: 0.03269866108894348, acc: 0.96, recall: 0.96, precision: 0.96, f_beta: 0.96
train: step: 1360, loss: 0.05755090340971947, acc: 0.92, recall: 0.9199999999999999, precision: 0.9206730769230769, f_beta: 0.919967987194878
train: step: 1361, loss: 0.10630228370428085, acc: 0.86, recall: 0.86, precision: 0.8623188405797102, f_beta: 0.859775641025641
train: step: 1362, loss: 0.05386674776673317, acc: 0.93, recall: 0.9299999999999999, precision: 0.9315535929345644, f_beta: 0.9299369432489241
train: step: 1363, loss: 0.06778733432292938, acc: 0.91, recall: 0.91, precision: 0.9101640656262505, f_beta: 0.90999099909991
train: step: 1364, loss: 0.07496527582406998, acc: 0.87, recall: 0.87, precision: 0.8701480592236894, f_beta: 0.8699869986998701
train: step: 1365, loss: 0.07743075489997864, acc: 0.87, recall: 0.87, precision: 0.8823894171145101, f_beta: 0.8689384010484928
train: step: 1366, loss: 0.07187935709953308, acc: 0.9, recall: 0.8999999999999999, precision: 0.9058441558441559, f_beta: 0.8996386993175431
train: step: 1367, loss: 0.0678766667842865, acc: 0.92, recall: 0.9199999999999999, precision: 0.9261363636363636, f_beta: 0.9197109594540345
train: step: 1368, loss: 0.13078773021697998, acc: 0.82, recall: 0.8200000000000001, precision: 0.822061191626409, f_beta: 0.8197115384615385
train: step: 1369, loss: 0.0753251239657402, acc: 0.91, recall: 0.9099999999999999, precision: 0.9181966544267646, f_beta: 0.9095568284594513
train: step: 1370, loss: 0.053016889840364456, acc: 0.93, recall: 0.9299999999999999, precision: 0.9315535929345644, f_beta: 0.9299369432489241
train: step: 1371, loss: 0.428699254989624, acc: 0.15, recall: 0.15000000000000002, precision: 0.14985994397759103, f_beta: 0.14991499149914989
train: step: 1372, loss: 0.07573864609003067, acc: 0.87, recall: 0.87, precision: 0.8737373737373737, f_beta: 0.8696741854636592
train: step: 1373, loss: 0.04059204086661339, acc: 0.94, recall: 0.94, precision: 0.9407051282051282, f_beta: 0.9399759903961584
train: step: 1374, loss: 0.07431821525096893, acc: 0.89, recall: 0.89, precision: 0.89015606242497, f_beta: 0.88998899889989
train: step: 1375, loss: 0.052978288382291794, acc: 0.92, recall: 0.92, precision: 0.92, f_beta: 0.92
train: step: 1376, loss: 0.1056598648428917, acc: 0.86, recall: 0.86, precision: 0.8605769230769231, f_beta: 0.8599439775910365
train: step: 1377, loss: 0.07041960209608078, acc: 0.91, recall: 0.91, precision: 0.9101640656262505, f_beta: 0.90999099909991
train: step: 1378, loss: 0.10376477986574173, acc: 0.86, recall: 0.8600000000000001, precision: 0.8652597402597402, f_beta: 0.8594941790445605
train: step: 1379, loss: 0.08692976087331772, acc: 0.9, recall: 0.8999999999999999, precision: 0.9058441558441559, f_beta: 0.8996386993175431
train: step: 1380, loss: 0.09688843041658401, acc: 0.85, recall: 0.85, precision: 0.8617197188921042, f_beta: 0.8487750781328762
train: step: 1381, loss: 0.08309034258127213, acc: 0.9, recall: 0.8999999999999999, precision: 0.9105090311986863, f_beta: 0.8993558776167472
train: step: 1382, loss: 0.10611377656459808, acc: 0.83, recall: 0.83, precision: 0.8467843631778058, f_beta: 0.8279178054458953
train: step: 1383, loss: 0.09244666248559952, acc: 0.88, recall: 0.88, precision: 0.8899835796387521, f_beta: 0.8792270531400966
train: step: 1384, loss: 0.04249192774295807, acc: 0.95, recall: 0.95, precision: 0.9501800720288115, f_beta: 0.9499949994999499
train: step: 1385, loss: 0.04467147961258888, acc: 0.93, recall: 0.9299999999999999, precision: 0.930172068827531, f_beta: 0.9299929992999301
train: step: 1386, loss: 0.05010392144322395, acc: 0.95, recall: 0.95, precision: 0.9516258530710557, f_beta: 0.949954959463517
train: step: 1387, loss: 0.03767000138759613, acc: 0.95, recall: 0.95, precision: 0.9501800720288115, f_beta: 0.9499949994999499
train: step: 1388, loss: 0.04518885537981987, acc: 0.95, recall: 0.95, precision: 0.9516258530710557, f_beta: 0.949954959463517
train: step: 1389, loss: 0.06831209361553192, acc: 0.92, recall: 0.92, precision: 0.92, f_beta: 0.92
train: step: 1390, loss: 0.05980302765965462, acc: 0.94, recall: 0.94, precision: 0.9407051282051282, f_beta: 0.9399759903961584
train: step: 1391, loss: 0.08744114637374878, acc: 0.9, recall: 0.8999999999999999, precision: 0.9058441558441559, f_beta: 0.8996386993175431
train: step: 1392, loss: 0.09221474826335907, acc: 0.89, recall: 0.8899999999999999, precision: 0.8939393939393939, f_beta: 0.8897243107769424
train: step: 1393, loss: 0.08197498321533203, acc: 0.9, recall: 0.8999999999999999, precision: 0.9058441558441559, f_beta: 0.8996386993175431
train: step: 1394, loss: 0.0764392539858818, acc: 0.86, recall: 0.86, precision: 0.8694581280788177, f_beta: 0.8590982286634461
train: step: 1395, loss: 0.037747811526060104, acc: 0.96, recall: 0.96, precision: 0.9607371794871795, f_beta: 0.959983993597439
train: step: 1396, loss: 0.052751678973436356, acc: 0.93, recall: 0.9299999999999999, precision: 0.9315535929345644, f_beta: 0.9299369432489241
train: step: 1397, loss: 0.05810047313570976, acc: 0.94, recall: 0.94, precision: 0.9407051282051282, f_beta: 0.9399759903961584
train: step: 1398, loss: 0.06610328704118729, acc: 0.91, recall: 0.9099999999999999, precision: 0.9141414141414141, f_beta: 0.9097744360902256
train: step: 1399, loss: 0.03419652208685875, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 1400, loss: 0.06061267852783203, acc: 0.94, recall: 0.94, precision: 0.94, f_beta: 0.94
train: step: 1401, loss: 0.0373605340719223, acc: 0.96, recall: 0.96, precision: 0.9607371794871795, f_beta: 0.959983993597439
train: step: 1402, loss: 0.07869775593280792, acc: 0.89, recall: 0.89, precision: 0.89015606242497, f_beta: 0.88998899889989
train: step: 1403, loss: 0.052830614149570465, acc: 0.92, recall: 0.9199999999999999, precision: 0.9206730769230769, f_beta: 0.919967987194878
train: step: 1404, loss: 0.047561340034008026, acc: 0.95, recall: 0.95, precision: 0.9516258530710557, f_beta: 0.949954959463517
train: step: 1405, loss: 0.028833026066422462, acc: 0.98, recall: 0.98, precision: 0.9807692307692308, f_beta: 0.9799919967987194
train: step: 1406, loss: 0.039584580808877945, acc: 0.95, recall: 0.95, precision: 0.9516258530710557, f_beta: 0.949954959463517
train: step: 1407, loss: 0.08742696791887283, acc: 0.88, recall: 0.88, precision: 0.8824476650563606, f_beta: 0.8798076923076923
train: step: 1408, loss: 0.10701639205217361, acc: 0.87, recall: 0.87, precision: 0.8823894171145101, f_beta: 0.8689384010484928
train: step: 1409, loss: 0.06907562166452408, acc: 0.92, recall: 0.9199999999999999, precision: 0.9206730769230769, f_beta: 0.919967987194878
train: step: 1410, loss: 0.07931067794561386, acc: 0.89, recall: 0.89, precision: 0.89015606242497, f_beta: 0.88998899889989
train: step: 1411, loss: 0.06389600038528442, acc: 0.92, recall: 0.9199999999999999, precision: 0.9206730769230769, f_beta: 0.919967987194878
train: step: 1412, loss: 0.07010620832443237, acc: 0.89, recall: 0.89, precision: 0.9030591153369161, f_beta: 0.8891017239641092
train: step: 1413, loss: 0.05847105011343956, acc: 0.92, recall: 0.92, precision: 0.92, f_beta: 0.92
train: step: 1414, loss: 0.059343937784433365, acc: 0.92, recall: 0.9199999999999999, precision: 0.9227053140096618, f_beta: 0.9198717948717948
train: step: 1415, loss: 0.029939807951450348, acc: 0.96, recall: 0.96, precision: 0.962962962962963, f_beta: 0.9599358974358974
train: step: 1416, loss: 0.07624969631433487, acc: 0.91, recall: 0.91, precision: 0.9101640656262505, f_beta: 0.90999099909991
train: step: 1417, loss: 0.056693077087402344, acc: 0.93, recall: 0.9299999999999999, precision: 0.930172068827531, f_beta: 0.9299929992999301
train: step: 1418, loss: 0.06829480826854706, acc: 0.91, recall: 0.91, precision: 0.9101640656262505, f_beta: 0.90999099909991
train: step: 1419, loss: 0.03169042244553566, acc: 0.96, recall: 0.96, precision: 0.9607371794871795, f_beta: 0.959983993597439
train: step: 1420, loss: 0.0688139945268631, acc: 0.9, recall: 0.8999999999999999, precision: 0.9058441558441559, f_beta: 0.8996386993175431
train: step: 1421, loss: 0.07130873948335648, acc: 0.89, recall: 0.8899999999999999, precision: 0.8939393939393939, f_beta: 0.8897243107769424
train: step: 1422, loss: 0.0772036463022232, acc: 0.9, recall: 0.9, precision: 0.9006410256410255, f_beta: 0.8999599839935974
train: step: 1423, loss: 0.08331979811191559, acc: 0.9, recall: 0.9, precision: 0.9, f_beta: 0.9
train: step: 1424, loss: 0.06131705641746521, acc: 0.91, recall: 0.91, precision: 0.9101640656262505, f_beta: 0.90999099909991
train: step: 1425, loss: 0.06792838871479034, acc: 0.91, recall: 0.9099999999999999, precision: 0.9114813327980731, f_beta: 0.9099189270343309
train: step: 1426, loss: 0.053213682025671005, acc: 0.91, recall: 0.91, precision: 0.9101640656262505, f_beta: 0.90999099909991
train: step: 1427, loss: 0.41191723942756653, acc: 0.82, recall: 0.8200000000000001, precision: 0.8333333333333334, f_beta: 0.8181818181818181
train: step: 1428, loss: 0.05153163895010948, acc: 0.94, recall: 0.94, precision: 0.9407051282051282, f_beta: 0.9399759903961584
train: step: 1429, loss: 0.05296212062239647, acc: 0.94, recall: 0.94, precision: 0.9428341384863124, f_beta: 0.9399038461538463
train: step: 1430, loss: 0.06486480683088303, acc: 0.9, recall: 0.9, precision: 0.9006410256410255, f_beta: 0.8999599839935974
train: step: 1431, loss: 0.08105125278234482, acc: 0.92, recall: 0.9199999999999999, precision: 0.9206730769230769, f_beta: 0.919967987194878
train: step: 1432, loss: 0.13015200197696686, acc: 0.84, recall: 0.84, precision: 0.84, f_beta: 0.8399999999999999
train: step: 1433, loss: 0.03733231872320175, acc: 0.95, recall: 0.95, precision: 0.9501800720288115, f_beta: 0.9499949994999499
train: step: 1434, loss: 0.05277680233120918, acc: 0.92, recall: 0.92, precision: 0.92, f_beta: 0.92
train: step: 1435, loss: 0.09150567650794983, acc: 0.88, recall: 0.88, precision: 0.8824476650563606, f_beta: 0.8798076923076923
train: step: 1436, loss: 0.053155917674303055, acc: 0.94, recall: 0.94, precision: 0.9407051282051282, f_beta: 0.9399759903961584
train: step: 1437, loss: 0.08345510810613632, acc: 0.89, recall: 0.8899999999999999, precision: 0.8977968176254589, f_beta: 0.8894583458948848
train: step: 1438, loss: 0.0390813983976841, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 1439, loss: 0.0744750052690506, acc: 0.92, recall: 0.9199999999999999, precision: 0.9227053140096618, f_beta: 0.9198717948717948
train: step: 1440, loss: 0.06092531979084015, acc: 0.91, recall: 0.9099999999999999, precision: 0.9114813327980731, f_beta: 0.9099189270343309
train: step: 1441, loss: 0.07821030914783478, acc: 0.91, recall: 0.9099999999999999, precision: 0.9114813327980731, f_beta: 0.9099189270343309
train: step: 1442, loss: 0.06847976893186569, acc: 0.95, recall: 0.95, precision: 0.9501800720288115, f_beta: 0.9499949994999499
train: step: 1443, loss: 0.07555300742387772, acc: 0.9, recall: 0.9, precision: 0.9166666666666667, f_beta: 0.898989898989899
train: step: 1444, loss: 0.06471680849790573, acc: 0.92, recall: 0.9199999999999999, precision: 0.9206730769230769, f_beta: 0.919967987194878
train: step: 1445, loss: 0.08470472693443298, acc: 0.91, recall: 0.91, precision: 0.9101640656262505, f_beta: 0.90999099909991
train: step: 1446, loss: 0.10043670982122421, acc: 0.87, recall: 0.87, precision: 0.8713368125250903, f_beta: 0.8698828946051447
train: step: 1447, loss: 0.08217129111289978, acc: 0.9, recall: 0.8999999999999999, precision: 0.9025764895330113, f_beta: 0.8998397435897436
train: step: 1448, loss: 0.09659485518932343, acc: 0.87, recall: 0.87, precision: 0.8773969808241534, f_beta: 0.8693598633303186
train: step: 1449, loss: 0.025890018790960312, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 1450, loss: 0.08914913237094879, acc: 0.89, recall: 0.89, precision: 0.8914090726615818, f_beta: 0.8899009108197378
train: step: 1451, loss: 0.11275558173656464, acc: 0.86, recall: 0.86, precision: 0.8605769230769231, f_beta: 0.8599439775910365
train: step: 1452, loss: 0.07323896139860153, acc: 0.9, recall: 0.9, precision: 0.9006410256410255, f_beta: 0.8999599839935974
train: step: 1453, loss: 0.055255141109228134, acc: 0.92, recall: 0.9199999999999999, precision: 0.9227053140096618, f_beta: 0.9198717948717948
train: step: 1454, loss: 0.10109060257673264, acc: 0.87, recall: 0.87, precision: 0.8823894171145101, f_beta: 0.8689384010484928
train: step: 1455, loss: 0.033790506422519684, acc: 0.96, recall: 0.96, precision: 0.9607371794871795, f_beta: 0.959983993597439
train: step: 1456, loss: 0.0876782238483429, acc: 0.88, recall: 0.88, precision: 0.8806089743589745, f_beta: 0.879951980792317
train: step: 1457, loss: 0.04077647626399994, acc: 0.95, recall: 0.95, precision: 0.9501800720288115, f_beta: 0.9499949994999499
train: step: 1458, loss: 0.04710303246974945, acc: 0.95, recall: 0.95, precision: 0.9501800720288115, f_beta: 0.9499949994999499
train: step: 1459, loss: 0.07172076404094696, acc: 0.9, recall: 0.9, precision: 0.9006410256410255, f_beta: 0.8999599839935974
train: step: 1460, loss: 0.059720925986766815, acc: 0.94, recall: 0.94, precision: 0.9407051282051282, f_beta: 0.9399759903961584
train: step: 1461, loss: 0.043185509741306305, acc: 0.94, recall: 0.94, precision: 0.9428341384863124, f_beta: 0.9399038461538463
train: step: 1462, loss: 0.043043650686740875, acc: 0.94, recall: 0.94, precision: 0.9464285714285714, f_beta: 0.939783219590526
train: step: 1463, loss: 0.04838308319449425, acc: 0.94, recall: 0.94, precision: 0.9428341384863124, f_beta: 0.9399038461538463
train: step: 1464, loss: 0.08033078163862228, acc: 0.91, recall: 0.9099999999999999, precision: 0.9114813327980731, f_beta: 0.9099189270343309
train: step: 1465, loss: 0.04342927783727646, acc: 0.93, recall: 0.9299999999999999, precision: 0.9315535929345644, f_beta: 0.9299369432489241
train: step: 1466, loss: 0.0825803205370903, acc: 0.88, recall: 0.88, precision: 0.8824476650563606, f_beta: 0.8798076923076923
train: step: 1467, loss: 0.07392770051956177, acc: 0.91, recall: 0.91, precision: 0.9101640656262505, f_beta: 0.90999099909991
train: step: 1468, loss: 0.07485204190015793, acc: 0.87, recall: 0.87, precision: 0.8737373737373737, f_beta: 0.8696741854636592
train: step: 1469, loss: 0.04599354416131973, acc: 0.95, recall: 0.95, precision: 0.9545454545454546, f_beta: 0.949874686716792
train: step: 1470, loss: 0.070896215736866, acc: 0.91, recall: 0.9099999999999999, precision: 0.9141414141414141, f_beta: 0.9097744360902256
train: step: 1471, loss: 0.031741272658109665, acc: 0.96, recall: 0.96, precision: 0.962962962962963, f_beta: 0.9599358974358974
train: step: 1472, loss: 0.08044235408306122, acc: 0.9, recall: 0.9, precision: 0.9, f_beta: 0.9
train: step: 1473, loss: 0.03750348463654518, acc: 0.94, recall: 0.94, precision: 0.94, f_beta: 0.94
train: step: 1474, loss: 0.0746007040143013, acc: 0.89, recall: 0.89, precision: 0.89015606242497, f_beta: 0.88998899889989
train: step: 1475, loss: 0.08320770412683487, acc: 0.88, recall: 0.88, precision: 0.8824476650563606, f_beta: 0.8798076923076923
train: step: 1476, loss: 0.09150686115026474, acc: 0.86, recall: 0.86, precision: 0.8623188405797102, f_beta: 0.859775641025641
train: step: 1477, loss: 0.11585922539234161, acc: 0.84, recall: 0.84, precision: 0.84, f_beta: 0.8399999999999999
train: step: 1478, loss: 0.09026898443698883, acc: 0.86, recall: 0.86, precision: 0.8605769230769231, f_beta: 0.8599439775910365
train: step: 1479, loss: 0.029076484963297844, acc: 0.98, recall: 0.98, precision: 0.9807692307692308, f_beta: 0.9799919967987194
train: step: 1480, loss: 0.07236874103546143, acc: 0.93, recall: 0.9299999999999999, precision: 0.930172068827531, f_beta: 0.9299929992999301
train: step: 1481, loss: 0.06507828831672668, acc: 0.91, recall: 0.91, precision: 0.9101640656262505, f_beta: 0.90999099909991
train: step: 1482, loss: 0.048126135021448135, acc: 0.95, recall: 0.95, precision: 0.9501800720288115, f_beta: 0.9499949994999499
train: step: 1483, loss: 0.07318880409002304, acc: 0.91, recall: 0.9099999999999999, precision: 0.9114813327980731, f_beta: 0.9099189270343309
train: step: 1484, loss: 0.07009654492139816, acc: 0.9, recall: 0.8999999999999999, precision: 0.9105090311986863, f_beta: 0.8993558776167472
train: step: 1485, loss: 0.08022055774927139, acc: 0.89, recall: 0.89, precision: 0.8914090726615818, f_beta: 0.8899009108197378
train: step: 1486, loss: 0.036388494074344635, acc: 0.98, recall: 0.98, precision: 0.9807692307692308, f_beta: 0.9799919967987194
train: step: 1487, loss: 0.04813817888498306, acc: 0.93, recall: 0.9299999999999999, precision: 0.930172068827531, f_beta: 0.9299929992999301
train: step: 1488, loss: 0.03643108904361725, acc: 0.96, recall: 0.96, precision: 0.9607371794871795, f_beta: 0.959983993597439
train: step: 1489, loss: 0.06275089830160141, acc: 0.92, recall: 0.9199999999999999, precision: 0.9206730769230769, f_beta: 0.919967987194878
train: step: 1490, loss: 0.06127004325389862, acc: 0.92, recall: 0.9199999999999999, precision: 0.9227053140096618, f_beta: 0.9198717948717948
train: step: 1491, loss: 0.03330739587545395, acc: 0.96, recall: 0.96, precision: 0.962962962962963, f_beta: 0.9599358974358974
train: step: 1492, loss: 0.08403917402029037, acc: 0.91, recall: 0.91, precision: 0.9101640656262505, f_beta: 0.90999099909991
train: step: 1493, loss: 0.05222506448626518, acc: 0.93, recall: 0.9299999999999999, precision: 0.930172068827531, f_beta: 0.9299929992999301
train: step: 1494, loss: 0.0421685129404068, acc: 0.93, recall: 0.9299999999999999, precision: 0.9343434343434343, f_beta: 0.9298245614035088
train: step: 1495, loss: 0.07787621021270752, acc: 0.88, recall: 0.88, precision: 0.8899835796387521, f_beta: 0.8792270531400966
train: step: 1496, loss: 0.06981518119573593, acc: 0.92, recall: 0.9199999999999999, precision: 0.9206730769230769, f_beta: 0.919967987194878
train: step: 1497, loss: 0.14588920772075653, acc: 0.8, recall: 0.8, precision: 0.8004807692307692, f_beta: 0.7999199679871949
train: step: 1498, loss: 0.06916730105876923, acc: 0.94, recall: 0.94, precision: 0.9407051282051282, f_beta: 0.9399759903961584
train: step: 1499, loss: 0.05632951855659485, acc: 0.93, recall: 0.9299999999999999, precision: 0.930172068827531, f_beta: 0.9299929992999301
train: step: 1500, loss: 0.037750791758298874, acc: 0.96, recall: 0.96, precision: 0.96, f_beta: 0.96
train: step: 1501, loss: 0.0719839483499527, acc: 0.9, recall: 0.8999999999999999, precision: 0.9058441558441559, f_beta: 0.8996386993175431
train: step: 1502, loss: 0.08115339279174805, acc: 0.88, recall: 0.88, precision: 0.8806089743589745, f_beta: 0.879951980792317
train: step: 1503, loss: 0.04072271287441254, acc: 0.96, recall: 0.96, precision: 0.9607371794871795, f_beta: 0.959983993597439
train: step: 1504, loss: 0.052390456199645996, acc: 0.94, recall: 0.94, precision: 0.9428341384863124, f_beta: 0.9399038461538463
train: step: 1505, loss: 0.0402342714369297, acc: 0.94, recall: 0.94, precision: 0.94, f_beta: 0.94
train: step: 1506, loss: 0.09120794385671616, acc: 0.9, recall: 0.9, precision: 0.9006410256410255, f_beta: 0.8999599839935974
train: step: 1507, loss: 0.08213292807340622, acc: 0.9, recall: 0.9, precision: 0.9, f_beta: 0.9
train: step: 1508, loss: 0.07568199187517166, acc: 0.92, recall: 0.92, precision: 0.92, f_beta: 0.92
train: step: 1509, loss: 0.052364226430654526, acc: 0.94, recall: 0.94, precision: 0.94, f_beta: 0.94
train: step: 1510, loss: 0.045633044093847275, acc: 0.94, recall: 0.94, precision: 0.9407051282051282, f_beta: 0.9399759903961584
train: step: 1511, loss: 0.06211978942155838, acc: 0.92, recall: 0.9199999999999999, precision: 0.9206730769230769, f_beta: 0.919967987194878
train: step: 1512, loss: 0.04737669974565506, acc: 0.94, recall: 0.94, precision: 0.9407051282051282, f_beta: 0.9399759903961584
train: step: 1513, loss: 0.03160030022263527, acc: 0.97, recall: 0.97, precision: 0.9716981132075472, f_beta: 0.9699729756781104
train: step: 1514, loss: 0.04076504707336426, acc: 0.95, recall: 0.95, precision: 0.9545454545454546, f_beta: 0.949874686716792
train: step: 1515, loss: 0.033983513712882996, acc: 0.95, recall: 0.95, precision: 0.9501800720288115, f_beta: 0.9499949994999499
train: step: 1516, loss: 0.0376293808221817, acc: 0.95, recall: 0.95, precision: 0.9501800720288115, f_beta: 0.9499949994999499
train: step: 1517, loss: 0.10446035116910934, acc: 0.86, recall: 0.86, precision: 0.8605769230769231, f_beta: 0.8599439775910365
train: step: 1518, loss: 0.05698246881365776, acc: 0.93, recall: 0.9299999999999999, precision: 0.9315535929345644, f_beta: 0.9299369432489241
train: step: 1519, loss: 0.06246769055724144, acc: 0.91, recall: 0.91, precision: 0.9101640656262505, f_beta: 0.90999099909991
train: step: 1520, loss: 0.08540800213813782, acc: 0.9, recall: 0.8999999999999999, precision: 0.9058441558441559, f_beta: 0.8996386993175431
train: step: 1521, loss: 0.12342511117458344, acc: 0.82, recall: 0.8200000000000001, precision: 0.822061191626409, f_beta: 0.8197115384615385
train: step: 1522, loss: 0.07378949224948883, acc: 0.91, recall: 0.9099999999999999, precision: 0.9114813327980731, f_beta: 0.9099189270343309
train: step: 1523, loss: 0.04416523873806, acc: 0.94, recall: 0.94, precision: 0.9407051282051282, f_beta: 0.9399759903961584
train: step: 1524, loss: 0.06754705309867859, acc: 0.91, recall: 0.9099999999999999, precision: 0.9114813327980731, f_beta: 0.9099189270343309
train: step: 1525, loss: 0.09077759087085724, acc: 0.85, recall: 0.8500000000000001, precision: 0.8535353535353536, f_beta: 0.849624060150376
train: step: 1526, loss: 0.0639064833521843, acc: 0.89, recall: 0.8899999999999999, precision: 0.8939393939393939, f_beta: 0.8897243107769424
train: step: 1527, loss: 0.04442664235830307, acc: 0.96, recall: 0.96, precision: 0.96, f_beta: 0.96
train: step: 1528, loss: 0.06513985991477966, acc: 0.91, recall: 0.9099999999999999, precision: 0.9114813327980731, f_beta: 0.9099189270343309
train: step: 1529, loss: 0.07124484330415726, acc: 0.91, recall: 0.91, precision: 0.9101640656262505, f_beta: 0.90999099909991
train: step: 1530, loss: 0.05519640073180199, acc: 0.93, recall: 0.9299999999999999, precision: 0.9315535929345644, f_beta: 0.9299369432489241
train: step: 1531, loss: 0.05667869746685028, acc: 0.95, recall: 0.95, precision: 0.9516258530710557, f_beta: 0.949954959463517
train: step: 1532, loss: 0.04435717687010765, acc: 0.94, recall: 0.94, precision: 0.9407051282051282, f_beta: 0.9399759903961584
train: step: 1533, loss: 0.0543997585773468, acc: 0.92, recall: 0.9199999999999999, precision: 0.9206730769230769, f_beta: 0.919967987194878
train: step: 1534, loss: 0.05359572544693947, acc: 0.92, recall: 0.9199999999999999, precision: 0.9227053140096618, f_beta: 0.9198717948717948
train: step: 1535, loss: 0.042064227163791656, acc: 0.95, recall: 0.95, precision: 0.9501800720288115, f_beta: 0.9499949994999499
train: step: 1536, loss: 0.08230989426374435, acc: 0.9, recall: 0.9, precision: 0.9006410256410255, f_beta: 0.8999599839935974
train: step: 1537, loss: 0.06316197663545609, acc: 0.91, recall: 0.9099999999999999, precision: 0.9114813327980731, f_beta: 0.9099189270343309
train: step: 1538, loss: 0.11294691264629364, acc: 0.84, recall: 0.8400000000000001, precision: 0.8489326765188834, f_beta: 0.8389694041867956
train: step: 1539, loss: 0.06303694099187851, acc: 0.94, recall: 0.94, precision: 0.9407051282051282, f_beta: 0.9399759903961584
train: step: 1540, loss: 0.07222630828619003, acc: 0.91, recall: 0.9099999999999999, precision: 0.9114813327980731, f_beta: 0.9099189270343309
train: step: 1541, loss: 0.022226035594940186, acc: 0.99, recall: 0.99, precision: 0.9901960784313726, f_beta: 0.98999899989999
train: step: 1542, loss: 0.08553602546453476, acc: 0.89, recall: 0.89, precision: 0.89015606242497, f_beta: 0.88998899889989
train: step: 1543, loss: 0.05103443190455437, acc: 0.94, recall: 0.94, precision: 0.9464285714285714, f_beta: 0.939783219590526
train: step: 1544, loss: 0.11436575651168823, acc: 0.86, recall: 0.86, precision: 0.8605769230769231, f_beta: 0.8599439775910365
train: step: 1545, loss: 0.032654814422130585, acc: 0.96, recall: 0.96, precision: 0.9607371794871795, f_beta: 0.959983993597439
train: step: 1546, loss: 0.1382010132074356, acc: 0.81, recall: 0.81, precision: 0.8131313131313131, f_beta: 0.8095238095238095
train: step: 1547, loss: 0.07079020142555237, acc: 0.9, recall: 0.8999999999999999, precision: 0.9058441558441559, f_beta: 0.8996386993175431
train: step: 1548, loss: 0.0638720691204071, acc: 0.92, recall: 0.92, precision: 0.92, f_beta: 0.92
train: step: 1549, loss: 0.053727854043245316, acc: 0.92, recall: 0.9199999999999999, precision: 0.9206730769230769, f_beta: 0.919967987194878
train: step: 1550, loss: 0.0454346239566803, acc: 0.93, recall: 0.9299999999999999, precision: 0.930172068827531, f_beta: 0.9299929992999301
train: step: 1551, loss: 0.04477599263191223, acc: 0.92, recall: 0.9199999999999999, precision: 0.9206730769230769, f_beta: 0.919967987194878
train: step: 1552, loss: 0.06279534101486206, acc: 0.91, recall: 0.91, precision: 0.9101640656262505, f_beta: 0.90999099909991
train: step: 1553, loss: 0.11690422147512436, acc: 0.86, recall: 0.86, precision: 0.8623188405797102, f_beta: 0.859775641025641
train: step: 1554, loss: 0.05335834622383118, acc: 0.93, recall: 0.9299999999999999, precision: 0.930172068827531, f_beta: 0.9299929992999301
train: step: 1555, loss: 0.01461793389171362, acc: 0.99, recall: 0.99, precision: 0.9901960784313726, f_beta: 0.98999899989999
train: step: 1556, loss: 0.0619194358587265, acc: 0.9, recall: 0.8999999999999999, precision: 0.9105090311986863, f_beta: 0.8993558776167472
train: step: 1557, loss: 0.029210209846496582, acc: 0.98, recall: 0.98, precision: 0.9807692307692308, f_beta: 0.9799919967987194
train: step: 1558, loss: 0.03682417422533035, acc: 0.96, recall: 0.96, precision: 0.96, f_beta: 0.96
train: step: 1559, loss: 0.030193395912647247, acc: 0.96, recall: 0.96, precision: 0.96, f_beta: 0.96
train: step: 1560, loss: 0.06510219722986221, acc: 0.88, recall: 0.88, precision: 0.8824476650563606, f_beta: 0.8798076923076923
train: step: 1561, loss: 0.055871810764074326, acc: 0.93, recall: 0.9299999999999999, precision: 0.930172068827531, f_beta: 0.9299929992999301
train: step: 1562, loss: 0.034674786031246185, acc: 0.95, recall: 0.95, precision: 0.9545454545454546, f_beta: 0.949874686716792
train: step: 1563, loss: 0.047650717198848724, acc: 0.95, recall: 0.95, precision: 0.9501800720288115, f_beta: 0.9499949994999499
train: step: 1564, loss: 0.06158023700118065, acc: 0.92, recall: 0.9199999999999999, precision: 0.9261363636363636, f_beta: 0.9197109594540345
train: step: 1565, loss: 0.053631287068128586, acc: 0.96, recall: 0.96, precision: 0.96, f_beta: 0.96
train: step: 1566, loss: 0.05815918371081352, acc: 0.92, recall: 0.9199999999999999, precision: 0.9261363636363636, f_beta: 0.9197109594540345
train: step: 1567, loss: 0.04960639774799347, acc: 0.93, recall: 0.9299999999999999, precision: 0.930172068827531, f_beta: 0.9299929992999301
train: step: 1568, loss: 0.0605146661400795, acc: 0.93, recall: 0.9299999999999999, precision: 0.9343434343434343, f_beta: 0.9298245614035088
train: step: 1569, loss: 0.054340094327926636, acc: 0.94, recall: 0.94, precision: 0.9407051282051282, f_beta: 0.9399759903961584
train: step: 1570, loss: 0.045981597155332565, acc: 0.92, recall: 0.9199999999999999, precision: 0.9206730769230769, f_beta: 0.919967987194878
train: step: 1571, loss: 0.07004652917385101, acc: 0.91, recall: 0.9099999999999999, precision: 0.9114813327980731, f_beta: 0.9099189270343309
train: step: 1572, loss: 0.05419569090008736, acc: 0.94, recall: 0.94, precision: 0.94, f_beta: 0.94
train: step: 1573, loss: 0.08297291398048401, acc: 0.89, recall: 0.89, precision: 0.89015606242497, f_beta: 0.88998899889989
train: step: 1574, loss: 0.03614804893732071, acc: 0.96, recall: 0.96, precision: 0.9607371794871795, f_beta: 0.959983993597439
train: step: 1575, loss: 0.07168807089328766, acc: 0.91, recall: 0.9099999999999999, precision: 0.9141414141414141, f_beta: 0.9097744360902256
train: step: 1576, loss: 0.12586888670921326, acc: 0.84, recall: 0.84, precision: 0.8405448717948718, f_beta: 0.8399359743897559
train: step: 1577, loss: 0.10252820700407028, acc: 0.88, recall: 0.8799999999999999, precision: 0.885551948051948, f_beta: 0.8795664391810518
train: step: 1578, loss: 0.057331543415784836, acc: 0.95, recall: 0.95, precision: 0.9516258530710557, f_beta: 0.949954959463517
train: step: 1579, loss: 0.05192015692591667, acc: 0.91, recall: 0.9099999999999999, precision: 0.9114813327980731, f_beta: 0.9099189270343309
train: step: 1580, loss: 0.07082167267799377, acc: 0.9, recall: 0.9, precision: 0.9006410256410255, f_beta: 0.8999599839935974
train: step: 1581, loss: 0.05014617741107941, acc: 0.95, recall: 0.95, precision: 0.9501800720288115, f_beta: 0.9499949994999499
train: step: 1582, loss: 0.07148320972919464, acc: 0.9, recall: 0.9, precision: 0.9006410256410255, f_beta: 0.8999599839935974
train: step: 1583, loss: 0.05168473348021507, acc: 0.93, recall: 0.9299999999999999, precision: 0.9343434343434343, f_beta: 0.9298245614035088
train: step: 1584, loss: 0.017787829041481018, acc: 0.98, recall: 0.98, precision: 0.9807692307692308, f_beta: 0.9799919967987194
train: step: 1585, loss: 0.05722286179661751, acc: 0.94, recall: 0.94, precision: 0.94, f_beta: 0.94
train: step: 1586, loss: 0.0217289999127388, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 1587, loss: 0.054969578981399536, acc: 0.92, recall: 0.9199999999999999, precision: 0.9206730769230769, f_beta: 0.919967987194878
train: step: 1588, loss: 0.08178558945655823, acc: 0.89, recall: 0.89, precision: 0.8914090726615818, f_beta: 0.8899009108197378
train: step: 1589, loss: 0.04299497604370117, acc: 0.96, recall: 0.96, precision: 0.96, f_beta: 0.96
train: step: 1590, loss: 0.05527033656835556, acc: 0.93, recall: 0.9299999999999999, precision: 0.9315535929345644, f_beta: 0.9299369432489241
train: step: 1591, loss: 0.07354368269443512, acc: 0.9, recall: 0.9, precision: 0.9, f_beta: 0.9
train: step: 1592, loss: 0.07290015369653702, acc: 0.91, recall: 0.91, precision: 0.9101640656262505, f_beta: 0.90999099909991
train: step: 1593, loss: 0.060239456593990326, acc: 0.92, recall: 0.9199999999999999, precision: 0.9206730769230769, f_beta: 0.919967987194878
train: step: 1594, loss: 0.05673559010028839, acc: 0.93, recall: 0.9299999999999999, precision: 0.9315535929345644, f_beta: 0.9299369432489241
train: step: 1595, loss: 0.06003805249929428, acc: 0.93, recall: 0.9299999999999999, precision: 0.930172068827531, f_beta: 0.9299929992999301
train: step: 1596, loss: 0.04552144929766655, acc: 0.95, recall: 0.95, precision: 0.9501800720288115, f_beta: 0.9499949994999499
train: step: 1597, loss: 0.03995456546545029, acc: 0.96, recall: 0.96, precision: 0.96, f_beta: 0.96
train: step: 1598, loss: 0.060424890369176865, acc: 0.92, recall: 0.9199999999999999, precision: 0.9206730769230769, f_beta: 0.919967987194878
train: step: 1599, loss: 0.058065153658390045, acc: 0.91, recall: 0.9099999999999999, precision: 0.9114813327980731, f_beta: 0.9099189270343309
train: step: 1600, loss: 0.046431221067905426, acc: 0.95, recall: 0.95, precision: 0.9516258530710557, f_beta: 0.949954959463517
train: step: 1601, loss: 0.045113906264305115, acc: 0.93, recall: 0.9299999999999999, precision: 0.930172068827531, f_beta: 0.9299929992999301
train: step: 1602, loss: 0.0929255485534668, acc: 0.89, recall: 0.89, precision: 0.89015606242497, f_beta: 0.88998899889989
train: step: 1603, loss: 0.07791502773761749, acc: 0.91, recall: 0.91, precision: 0.9101640656262505, f_beta: 0.90999099909991
train: step: 1604, loss: 0.0811438262462616, acc: 0.9, recall: 0.8999999999999999, precision: 0.9025764895330113, f_beta: 0.8998397435897436
train: step: 1605, loss: 0.05467137321829796, acc: 0.94, recall: 0.94, precision: 0.94, f_beta: 0.94
train: step: 1606, loss: 0.05188959464430809, acc: 0.95, recall: 0.95, precision: 0.9516258530710557, f_beta: 0.949954959463517
train: step: 1607, loss: 0.05936184898018837, acc: 0.91, recall: 0.91, precision: 0.9101640656262505, f_beta: 0.90999099909991
train: step: 1608, loss: 0.04196400195360184, acc: 0.94, recall: 0.94, precision: 0.9407051282051282, f_beta: 0.9399759903961584
train: step: 1609, loss: 0.043558068573474884, acc: 0.94, recall: 0.94, precision: 0.9407051282051282, f_beta: 0.9399759903961584
train: step: 1610, loss: 0.05315599590539932, acc: 0.93, recall: 0.9299999999999999, precision: 0.9315535929345644, f_beta: 0.9299369432489241
train: step: 1611, loss: 0.03977162390947342, acc: 0.96, recall: 0.96, precision: 0.9607371794871795, f_beta: 0.959983993597439
train: step: 1612, loss: 0.10014358162879944, acc: 0.87, recall: 0.87, precision: 0.8701480592236894, f_beta: 0.8699869986998701
train: step: 1613, loss: 0.06067541241645813, acc: 0.91, recall: 0.91, precision: 0.9101640656262505, f_beta: 0.90999099909991
train: step: 1614, loss: 0.06505932658910751, acc: 0.91, recall: 0.91, precision: 0.9101640656262505, f_beta: 0.90999099909991
train: step: 1615, loss: 0.030286913737654686, acc: 0.96, recall: 0.96, precision: 0.9607371794871795, f_beta: 0.959983993597439
train: step: 1616, loss: 0.024593299254775047, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 1617, loss: 0.10451460629701614, acc: 0.85, recall: 0.8500000000000001, precision: 0.8535353535353536, f_beta: 0.849624060150376
train: step: 1618, loss: 0.06827586889266968, acc: 0.92, recall: 0.92, precision: 0.92, f_beta: 0.92
train: step: 1619, loss: 0.07496120780706406, acc: 0.91, recall: 0.9099999999999999, precision: 0.9114813327980731, f_beta: 0.9099189270343309
train: step: 1620, loss: 0.05614779144525528, acc: 0.93, recall: 0.9299999999999999, precision: 0.9315535929345644, f_beta: 0.9299369432489241
train: step: 1621, loss: 0.049638912081718445, acc: 0.94, recall: 0.94, precision: 0.9407051282051282, f_beta: 0.9399759903961584
train: step: 1622, loss: 0.0352633073925972, acc: 0.95, recall: 0.95, precision: 0.9516258530710557, f_beta: 0.949954959463517
train: step: 1623, loss: 0.0830615684390068, acc: 0.9, recall: 0.9, precision: 0.9006410256410255, f_beta: 0.8999599839935974
train: step: 1624, loss: 0.043947648257017136, acc: 0.96, recall: 0.96, precision: 0.9607371794871795, f_beta: 0.959983993597439
train: step: 1625, loss: 0.026961054652929306, acc: 0.96, recall: 0.96, precision: 0.96, f_beta: 0.96
train: step: 1626, loss: 0.05546105280518532, acc: 0.93, recall: 0.9299999999999999, precision: 0.9315535929345644, f_beta: 0.9299369432489241
train: step: 1627, loss: 0.08676580339670181, acc: 0.88, recall: 0.8799999999999999, precision: 0.885551948051948, f_beta: 0.8795664391810518
train: step: 1628, loss: 0.030418405309319496, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 1629, loss: 0.07631624490022659, acc: 0.9, recall: 0.9, precision: 0.9006410256410255, f_beta: 0.8999599839935974
train: step: 1630, loss: 0.06315410882234573, acc: 0.9, recall: 0.9, precision: 0.9006410256410255, f_beta: 0.8999599839935974
train: step: 1631, loss: 0.02867198921740055, acc: 0.98, recall: 0.98, precision: 0.98, f_beta: 0.98
train: step: 1632, loss: 0.06976589560508728, acc: 0.92, recall: 0.9199999999999999, precision: 0.9206730769230769, f_beta: 0.919967987194878
train: step: 1633, loss: 0.05466555431485176, acc: 0.91, recall: 0.9099999999999999, precision: 0.9114813327980731, f_beta: 0.9099189270343309
train: step: 1634, loss: 0.08159825950860977, acc: 0.89, recall: 0.8899999999999999, precision: 0.8977968176254589, f_beta: 0.8894583458948848
train: step: 1635, loss: 0.028431186452507973, acc: 0.96, recall: 0.96, precision: 0.96, f_beta: 0.96
train: step: 1636, loss: 0.04295120760798454, acc: 0.95, recall: 0.95, precision: 0.9516258530710557, f_beta: 0.949954959463517
train: step: 1637, loss: 0.038405731320381165, acc: 0.93, recall: 0.9299999999999999, precision: 0.9343434343434343, f_beta: 0.9298245614035088
train: step: 1638, loss: 0.0580269917845726, acc: 0.92, recall: 0.9199999999999999, precision: 0.9206730769230769, f_beta: 0.919967987194878
train: step: 1639, loss: 0.0792250856757164, acc: 0.91, recall: 0.91, precision: 0.9101640656262505, f_beta: 0.90999099909991
train: step: 1640, loss: 0.04362860321998596, acc: 0.94, recall: 0.94, precision: 0.94, f_beta: 0.94
train: step: 1641, loss: 0.05565023422241211, acc: 0.93, recall: 0.9299999999999999, precision: 0.9315535929345644, f_beta: 0.9299369432489241
train: step: 1642, loss: 0.05683206394314766, acc: 0.94, recall: 0.94, precision: 0.9407051282051282, f_beta: 0.9399759903961584
train: step: 1643, loss: 0.05842743068933487, acc: 0.93, recall: 0.9299999999999999, precision: 0.9315535929345644, f_beta: 0.9299369432489241
train: step: 1644, loss: 0.045869216322898865, acc: 0.92, recall: 0.9199999999999999, precision: 0.9227053140096618, f_beta: 0.9198717948717948
train: step: 1645, loss: 0.029645586386322975, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 1646, loss: 0.0457686148583889, acc: 0.94, recall: 0.94, precision: 0.9407051282051282, f_beta: 0.9399759903961584
train: step: 1647, loss: 0.07637377083301544, acc: 0.9, recall: 0.9, precision: 0.9006410256410255, f_beta: 0.8999599839935974
train: step: 1648, loss: 0.04545595496892929, acc: 0.94, recall: 0.94, precision: 0.9407051282051282, f_beta: 0.9399759903961584
train: step: 1649, loss: 0.05842646211385727, acc: 0.93, recall: 0.9299999999999999, precision: 0.930172068827531, f_beta: 0.9299929992999301
train: step: 1650, loss: 0.04948373883962631, acc: 0.95, recall: 0.95, precision: 0.9516258530710557, f_beta: 0.949954959463517
train: step: 1651, loss: 0.04135306179523468, acc: 0.95, recall: 0.95, precision: 0.9501800720288115, f_beta: 0.9499949994999499
train: step: 1652, loss: 0.06792532652616501, acc: 0.91, recall: 0.91, precision: 0.9101640656262505, f_beta: 0.90999099909991
train: step: 1653, loss: 0.04440937936306, acc: 0.95, recall: 0.95, precision: 0.9501800720288115, f_beta: 0.9499949994999499
train: step: 1654, loss: 0.04712054878473282, acc: 0.93, recall: 0.9299999999999999, precision: 0.9343434343434343, f_beta: 0.9298245614035088
train: step: 1655, loss: 0.06607311964035034, acc: 0.94, recall: 0.94, precision: 0.94, f_beta: 0.94
train: step: 1656, loss: 0.05124163627624512, acc: 0.94, recall: 0.94, precision: 0.9428341384863124, f_beta: 0.9399038461538463
train: step: 1657, loss: 0.0727071762084961, acc: 0.92, recall: 0.92, precision: 0.92, f_beta: 0.92
train: step: 1658, loss: 0.0636226013302803, acc: 0.91, recall: 0.91, precision: 0.9101640656262505, f_beta: 0.90999099909991
train: step: 1659, loss: 0.045264359563589096, acc: 0.94, recall: 0.94, precision: 0.94, f_beta: 0.94
train: step: 1660, loss: 0.04424256458878517, acc: 0.93, recall: 0.9299999999999999, precision: 0.930172068827531, f_beta: 0.9299929992999301
train: step: 1661, loss: 0.043611593544483185, acc: 0.95, recall: 0.95, precision: 0.9501800720288115, f_beta: 0.9499949994999499
train: step: 1662, loss: 0.06567501276731491, acc: 0.92, recall: 0.9199999999999999, precision: 0.9206730769230769, f_beta: 0.919967987194878
train: step: 1663, loss: 0.0797194391489029, acc: 0.89, recall: 0.8899999999999999, precision: 0.8939393939393939, f_beta: 0.8897243107769424
train: step: 1664, loss: 0.05898411571979523, acc: 0.93, recall: 0.9299999999999999, precision: 0.9315535929345644, f_beta: 0.9299369432489241
train: step: 1665, loss: 0.06167861074209213, acc: 0.92, recall: 0.92, precision: 0.92, f_beta: 0.92
train: step: 1666, loss: 0.06496835500001907, acc: 0.91, recall: 0.91, precision: 0.9101640656262505, f_beta: 0.90999099909991
train: step: 1667, loss: 0.08939491957426071, acc: 0.86, recall: 0.86, precision: 0.86, f_beta: 0.8599999999999999
train: step: 1668, loss: 0.04274589568376541, acc: 0.94, recall: 0.94, precision: 0.9407051282051282, f_beta: 0.9399759903961584
train: step: 1669, loss: 0.06304577738046646, acc: 0.93, recall: 0.9299999999999999, precision: 0.930172068827531, f_beta: 0.9299929992999301
train: step: 1670, loss: 0.06963015347719193, acc: 0.91, recall: 0.91, precision: 0.9101640656262505, f_beta: 0.90999099909991
train: step: 1671, loss: 0.0351085290312767, acc: 0.94, recall: 0.94, precision: 0.9428341384863124, f_beta: 0.9399038461538463
train: step: 1672, loss: 0.04427100718021393, acc: 0.96, recall: 0.96, precision: 0.96, f_beta: 0.96
train: step: 1673, loss: 0.03241153061389923, acc: 0.94, recall: 0.94, precision: 0.9428341384863124, f_beta: 0.9399038461538463
train: step: 1674, loss: 0.025752611458301544, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 1675, loss: 0.06604108959436417, acc: 0.91, recall: 0.91, precision: 0.9101640656262505, f_beta: 0.90999099909991
train: step: 1676, loss: 0.026522332802414894, acc: 0.98, recall: 0.98, precision: 0.98, f_beta: 0.98
train: step: 1677, loss: 0.017627976834774017, acc: 0.99, recall: 0.99, precision: 0.9901960784313726, f_beta: 0.98999899989999
train: step: 1678, loss: 0.02267536148428917, acc: 0.98, recall: 0.98, precision: 0.98, f_beta: 0.98
train: step: 1679, loss: 0.07188057899475098, acc: 0.92, recall: 0.92, precision: 0.92, f_beta: 0.92
train: step: 1680, loss: 0.09640530496835709, acc: 0.87, recall: 0.87, precision: 0.8823894171145101, f_beta: 0.8689384010484928
train: step: 1681, loss: 0.03883202373981476, acc: 0.96, recall: 0.96, precision: 0.9607371794871795, f_beta: 0.959983993597439
train: step: 1682, loss: 0.0937943235039711, acc: 0.87, recall: 0.87, precision: 0.8713368125250903, f_beta: 0.8698828946051447
train: step: 1683, loss: 0.08902566879987717, acc: 0.88, recall: 0.88, precision: 0.8899835796387521, f_beta: 0.8792270531400966
train: step: 1684, loss: 0.03126337006688118, acc: 0.95, recall: 0.95, precision: 0.9501800720288115, f_beta: 0.9499949994999499
train: step: 1685, loss: 0.042060062289237976, acc: 0.93, recall: 0.9299999999999999, precision: 0.9315535929345644, f_beta: 0.9299369432489241
train: step: 1686, loss: 0.07512379437685013, acc: 0.9, recall: 0.9, precision: 0.9, f_beta: 0.9
train: step: 1687, loss: 0.07742618769407272, acc: 0.9, recall: 0.9, precision: 0.9006410256410255, f_beta: 0.8999599839935974
train: step: 1688, loss: 0.06968392431735992, acc: 0.92, recall: 0.92, precision: 0.92, f_beta: 0.92
train: step: 1689, loss: 0.060278572142124176, acc: 0.93, recall: 0.9299999999999999, precision: 0.9315535929345644, f_beta: 0.9299369432489241
train: step: 1690, loss: 0.08347011357545853, acc: 0.9, recall: 0.8999999999999999, precision: 0.9025764895330113, f_beta: 0.8998397435897436
train: step: 1691, loss: 0.068956159055233, acc: 0.9, recall: 0.9, precision: 0.9, f_beta: 0.9
train: step: 1692, loss: 0.03717965632677078, acc: 0.96, recall: 0.96, precision: 0.96, f_beta: 0.96
train: step: 1693, loss: 0.059008531272411346, acc: 0.92, recall: 0.9199999999999999, precision: 0.9261363636363636, f_beta: 0.9197109594540345
train: step: 1694, loss: 0.08145353198051453, acc: 0.89, recall: 0.89, precision: 0.89015606242497, f_beta: 0.88998899889989
train: step: 1695, loss: 0.05483673885464668, acc: 0.94, recall: 0.94, precision: 0.9428341384863124, f_beta: 0.9399038461538463
train: step: 1696, loss: 0.041066598147153854, acc: 0.94, recall: 0.94, precision: 0.9407051282051282, f_beta: 0.9399759903961584
train: step: 1697, loss: 0.06814254075288773, acc: 0.89, recall: 0.8899999999999999, precision: 0.8977968176254589, f_beta: 0.8894583458948848
train: step: 1698, loss: 0.04050501808524132, acc: 0.95, recall: 0.95, precision: 0.9501800720288115, f_beta: 0.9499949994999499
train: step: 1699, loss: 0.06671230494976044, acc: 0.93, recall: 0.9299999999999999, precision: 0.9315535929345644, f_beta: 0.9299369432489241
train: step: 1700, loss: 0.0717402920126915, acc: 0.91, recall: 0.91, precision: 0.9101640656262505, f_beta: 0.90999099909991
train: step: 1701, loss: 0.05777447670698166, acc: 0.93, recall: 0.9299999999999999, precision: 0.9315535929345644, f_beta: 0.9299369432489241
train: step: 1702, loss: 0.06925980746746063, acc: 0.91, recall: 0.91, precision: 0.9101640656262505, f_beta: 0.90999099909991
train: step: 1703, loss: 0.038075245916843414, acc: 0.95, recall: 0.95, precision: 0.9516258530710557, f_beta: 0.949954959463517
train: step: 1704, loss: 0.06085686385631561, acc: 0.93, recall: 0.9299999999999999, precision: 0.930172068827531, f_beta: 0.9299929992999301
train: step: 1705, loss: 0.07545432448387146, acc: 0.89, recall: 0.89, precision: 0.8914090726615818, f_beta: 0.8899009108197378
train: step: 1706, loss: 0.06954380869865417, acc: 0.91, recall: 0.91, precision: 0.9101640656262505, f_beta: 0.90999099909991
train: step: 1707, loss: 0.04952423274517059, acc: 0.96, recall: 0.96, precision: 0.9607371794871795, f_beta: 0.959983993597439
train: step: 1708, loss: 0.05192895233631134, acc: 0.94, recall: 0.94, precision: 0.94, f_beta: 0.94
train: step: 1709, loss: 0.029835060238838196, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 1710, loss: 0.04466509819030762, acc: 0.95, recall: 0.95, precision: 0.9516258530710557, f_beta: 0.949954959463517
train: step: 1711, loss: 0.05929320678114891, acc: 0.92, recall: 0.9199999999999999, precision: 0.9227053140096618, f_beta: 0.9198717948717948
train: step: 1712, loss: 0.07367753237485886, acc: 0.91, recall: 0.9099999999999999, precision: 0.9114813327980731, f_beta: 0.9099189270343309
train: step: 1713, loss: 0.07629131525754929, acc: 0.9, recall: 0.9, precision: 0.9006410256410255, f_beta: 0.8999599839935974
train: step: 1714, loss: 0.039694566279649734, acc: 0.95, recall: 0.95, precision: 0.9516258530710557, f_beta: 0.949954959463517
train: step: 1715, loss: 0.030355745926499367, acc: 0.98, recall: 0.98, precision: 0.9807692307692308, f_beta: 0.9799919967987194
train: step: 1716, loss: 0.03121059015393257, acc: 0.97, recall: 0.97, precision: 0.9716981132075472, f_beta: 0.9699729756781104
train: step: 1717, loss: 0.04374530166387558, acc: 0.94, recall: 0.94, precision: 0.9407051282051282, f_beta: 0.9399759903961584
train: step: 1718, loss: 0.029686450958251953, acc: 0.96, recall: 0.96, precision: 0.9607371794871795, f_beta: 0.959983993597439
train: step: 1719, loss: 0.09366808831691742, acc: 0.88, recall: 0.88, precision: 0.8806089743589745, f_beta: 0.879951980792317
train: step: 1720, loss: 0.12009444087743759, acc: 0.81, recall: 0.81, precision: 0.810124049619848, f_beta: 0.8099809980998101
train: step: 1721, loss: 0.039830781519412994, acc: 0.95, recall: 0.95, precision: 0.9501800720288115, f_beta: 0.9499949994999499
train: step: 1722, loss: 0.07786174863576889, acc: 0.87, recall: 0.87, precision: 0.8701480592236894, f_beta: 0.8699869986998701
train: step: 1723, loss: 0.07928047329187393, acc: 0.9, recall: 0.9, precision: 0.9006410256410255, f_beta: 0.8999599839935974
train: step: 1724, loss: 0.06820373237133026, acc: 0.91, recall: 0.9099999999999999, precision: 0.9114813327980731, f_beta: 0.9099189270343309
train: step: 1725, loss: 0.03375465050339699, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 1726, loss: 0.029793161898851395, acc: 0.97, recall: 0.97, precision: 0.9716981132075472, f_beta: 0.9699729756781104
train: step: 1727, loss: 0.09599412977695465, acc: 0.89, recall: 0.8899999999999999, precision: 0.8939393939393939, f_beta: 0.8897243107769424
train: step: 1728, loss: 0.08128472417593002, acc: 0.88, recall: 0.8799999999999999, precision: 0.885551948051948, f_beta: 0.8795664391810518
train: step: 1729, loss: 0.032282426953315735, acc: 0.95, recall: 0.95, precision: 0.9501800720288115, f_beta: 0.9499949994999499
train: step: 1730, loss: 0.4519846439361572, acc: 0.94, recall: 0.94, precision: 0.9428341384863124, f_beta: 0.9399038461538463
train: step: 1731, loss: 0.025924086570739746, acc: 0.95, recall: 0.95, precision: 0.9501800720288115, f_beta: 0.9499949994999499
train: step: 1732, loss: 0.059451308101415634, acc: 0.91, recall: 0.91, precision: 0.9101640656262505, f_beta: 0.90999099909991
train: step: 1733, loss: 0.019359668716788292, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 1734, loss: 0.040591415017843246, acc: 0.95, recall: 0.95, precision: 0.9501800720288115, f_beta: 0.9499949994999499
train: step: 1735, loss: 0.0763736367225647, acc: 0.91, recall: 0.91, precision: 0.9101640656262505, f_beta: 0.90999099909991
train: step: 1736, loss: 0.04156442731618881, acc: 0.95, recall: 0.95, precision: 0.9516258530710557, f_beta: 0.949954959463517
train: step: 1737, loss: 0.050646983087062836, acc: 0.92, recall: 0.9199999999999999, precision: 0.9227053140096618, f_beta: 0.9198717948717948
train: step: 1738, loss: 0.07441341131925583, acc: 0.91, recall: 0.9099999999999999, precision: 0.9114813327980731, f_beta: 0.9099189270343309
train: step: 1739, loss: 0.03203272446990013, acc: 0.96, recall: 0.96, precision: 0.96, f_beta: 0.96
train: step: 1740, loss: 0.07337243109941483, acc: 0.91, recall: 0.9099999999999999, precision: 0.9114813327980731, f_beta: 0.9099189270343309
train: step: 1741, loss: 0.0805744156241417, acc: 0.91, recall: 0.91, precision: 0.9101640656262505, f_beta: 0.90999099909991
train: step: 1742, loss: 0.06201843172311783, acc: 0.92, recall: 0.92, precision: 0.92, f_beta: 0.92
train: step: 1743, loss: 0.07207714766263962, acc: 0.9, recall: 0.9, precision: 0.9, f_beta: 0.9
train: step: 1744, loss: 0.03528153523802757, acc: 0.96, recall: 0.96, precision: 0.9607371794871795, f_beta: 0.959983993597439
train: step: 1745, loss: 0.04422102868556976, acc: 0.95, recall: 0.95, precision: 0.9516258530710557, f_beta: 0.949954959463517
train: step: 1746, loss: 0.06272108852863312, acc: 0.92, recall: 0.9199999999999999, precision: 0.9261363636363636, f_beta: 0.9197109594540345
train: step: 1747, loss: 0.07065077126026154, acc: 0.9, recall: 0.9, precision: 0.9166666666666667, f_beta: 0.898989898989899
train: step: 1748, loss: 0.0628686472773552, acc: 0.93, recall: 0.9299999999999999, precision: 0.9315535929345644, f_beta: 0.9299369432489241
train: step: 1749, loss: 0.028711214661598206, acc: 0.97, recall: 0.97, precision: 0.9716981132075472, f_beta: 0.9699729756781104
train: step: 1750, loss: 0.032395828515291214, acc: 0.96, recall: 0.96, precision: 0.9607371794871795, f_beta: 0.959983993597439
train: step: 1751, loss: 0.037081923335790634, acc: 0.94, recall: 0.94, precision: 0.9428341384863124, f_beta: 0.9399038461538463
train: step: 1752, loss: 0.05337272584438324, acc: 0.92, recall: 0.9199999999999999, precision: 0.9206730769230769, f_beta: 0.919967987194878
train: step: 1753, loss: 0.059775687754154205, acc: 0.92, recall: 0.9199999999999999, precision: 0.9310344827586207, f_beta: 0.9194847020933978
train: step: 1754, loss: 0.04581775516271591, acc: 0.95, recall: 0.95, precision: 0.9516258530710557, f_beta: 0.949954959463517
train: step: 1755, loss: 0.03218860551714897, acc: 0.96, recall: 0.96, precision: 0.96, f_beta: 0.96
train: step: 1756, loss: 0.04932079464197159, acc: 0.95, recall: 0.95, precision: 0.9501800720288115, f_beta: 0.9499949994999499
train: step: 1757, loss: 0.01426376961171627, acc: 0.99, recall: 0.99, precision: 0.9901960784313726, f_beta: 0.98999899989999
train: step: 1758, loss: 0.09068205207586288, acc: 0.86, recall: 0.86, precision: 0.86, f_beta: 0.8599999999999999
train: step: 1759, loss: 0.034279584884643555, acc: 0.95, recall: 0.95, precision: 0.9516258530710557, f_beta: 0.949954959463517
train: step: 1760, loss: 0.08454891294240952, acc: 0.9, recall: 0.8999999999999999, precision: 0.9058441558441559, f_beta: 0.8996386993175431
train: step: 1761, loss: 0.09464100748300552, acc: 0.89, recall: 0.89, precision: 0.8914090726615818, f_beta: 0.8899009108197378
train: step: 1762, loss: 0.4518354535102844, acc: 0.09, recall: 0.09000000000000001, precision: 0.08585858585858586, f_beta: 0.08771929824561403
train: step: 1763, loss: 0.055265530943870544, acc: 0.93, recall: 0.9299999999999999, precision: 0.930172068827531, f_beta: 0.9299929992999301
train: step: 1764, loss: 0.06216023117303848, acc: 0.92, recall: 0.9199999999999999, precision: 0.9206730769230769, f_beta: 0.919967987194878
train: step: 1765, loss: 0.05629292502999306, acc: 0.93, recall: 0.9299999999999999, precision: 0.930172068827531, f_beta: 0.9299929992999301
train: step: 1766, loss: 0.08856109529733658, acc: 0.88, recall: 0.88, precision: 0.8824476650563606, f_beta: 0.8798076923076923
train: step: 1767, loss: 0.07038555294275284, acc: 0.93, recall: 0.9299999999999999, precision: 0.9315535929345644, f_beta: 0.9299369432489241
train: step: 1768, loss: 0.048813603818416595, acc: 0.95, recall: 0.95, precision: 0.9501800720288115, f_beta: 0.9499949994999499
train: step: 1769, loss: 0.04918941482901573, acc: 0.92, recall: 0.9199999999999999, precision: 0.9206730769230769, f_beta: 0.919967987194878
train: step: 1770, loss: 0.03139113634824753, acc: 0.96, recall: 0.96, precision: 0.96, f_beta: 0.96
train: step: 1771, loss: 0.03670984134078026, acc: 0.95, recall: 0.95, precision: 0.9516258530710557, f_beta: 0.949954959463517
train: step: 1772, loss: 0.05565560236573219, acc: 0.93, recall: 0.9299999999999999, precision: 0.930172068827531, f_beta: 0.9299929992999301
train: step: 1773, loss: 0.0635579526424408, acc: 0.92, recall: 0.92, precision: 0.92, f_beta: 0.92
train: step: 1774, loss: 0.05932438746094704, acc: 0.91, recall: 0.9099999999999999, precision: 0.9141414141414141, f_beta: 0.9097744360902256
train: step: 1775, loss: 0.05161753296852112, acc: 0.91, recall: 0.91, precision: 0.9101640656262505, f_beta: 0.90999099909991
train: step: 1776, loss: 0.07526975870132446, acc: 0.89, recall: 0.8899999999999999, precision: 0.8939393939393939, f_beta: 0.8897243107769424
train: step: 1777, loss: 0.0710807517170906, acc: 0.91, recall: 0.9099999999999999, precision: 0.9141414141414141, f_beta: 0.9097744360902256
train: step: 1778, loss: 0.06309225410223007, acc: 0.91, recall: 0.9099999999999999, precision: 0.9181966544267646, f_beta: 0.9095568284594513
train: step: 1779, loss: 0.023536888882517815, acc: 0.97, recall: 0.97, precision: 0.9716981132075472, f_beta: 0.9699729756781104
train: step: 1780, loss: 0.03959202393889427, acc: 0.95, recall: 0.95, precision: 0.9501800720288115, f_beta: 0.9499949994999499
train: step: 1781, loss: 0.04686256870627403, acc: 0.95, recall: 0.95, precision: 0.9501800720288115, f_beta: 0.9499949994999499
train: step: 1782, loss: 0.06741246581077576, acc: 0.9, recall: 0.9, precision: 0.9006410256410255, f_beta: 0.8999599839935974
train: step: 1783, loss: 0.04962586238980293, acc: 0.94, recall: 0.94, precision: 0.9407051282051282, f_beta: 0.9399759903961584
train: step: 1784, loss: 0.04710497707128525, acc: 0.93, recall: 0.9299999999999999, precision: 0.930172068827531, f_beta: 0.9299929992999301
train: step: 1785, loss: 0.07252190262079239, acc: 0.93, recall: 0.9299999999999999, precision: 0.930172068827531, f_beta: 0.9299929992999301
train: step: 1786, loss: 0.01865476556122303, acc: 0.99, recall: 0.99, precision: 0.9901960784313726, f_beta: 0.98999899989999
train: step: 1787, loss: 0.04457319155335426, acc: 0.95, recall: 0.95, precision: 0.9516258530710557, f_beta: 0.949954959463517
train: step: 1788, loss: 0.09183679521083832, acc: 0.89, recall: 0.8899999999999999, precision: 0.8939393939393939, f_beta: 0.8897243107769424
train: step: 1789, loss: 0.06805543601512909, acc: 0.93, recall: 0.9299999999999999, precision: 0.9315535929345644, f_beta: 0.9299369432489241
train: step: 1790, loss: 0.047752637416124344, acc: 0.92, recall: 0.9199999999999999, precision: 0.9206730769230769, f_beta: 0.919967987194878
train: step: 1791, loss: 0.039023470133543015, acc: 0.96, recall: 0.96, precision: 0.9607371794871795, f_beta: 0.959983993597439
train: step: 1792, loss: 0.05606066808104515, acc: 0.92, recall: 0.9199999999999999, precision: 0.9206730769230769, f_beta: 0.919967987194878
train: step: 1793, loss: 0.0506778210401535, acc: 0.94, recall: 0.94, precision: 0.94, f_beta: 0.94
train: step: 1794, loss: 0.08407947421073914, acc: 0.89, recall: 0.89, precision: 0.89015606242497, f_beta: 0.88998899889989
train: step: 1795, loss: 0.03553671017289162, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 1796, loss: 0.06738580018281937, acc: 0.91, recall: 0.91, precision: 0.9101640656262505, f_beta: 0.90999099909991
train: step: 1797, loss: 0.06978146731853485, acc: 0.9, recall: 0.8999999999999999, precision: 0.9025764895330113, f_beta: 0.8998397435897436
train: step: 1798, loss: 0.06041568145155907, acc: 0.92, recall: 0.9199999999999999, precision: 0.9206730769230769, f_beta: 0.919967987194878
train: step: 1799, loss: 0.04731883853673935, acc: 0.94, recall: 0.94, precision: 0.94, f_beta: 0.94
train: step: 1800, loss: 0.06371314823627472, acc: 0.92, recall: 0.9199999999999999, precision: 0.9206730769230769, f_beta: 0.919967987194878
train: step: 1801, loss: 0.03733653947710991, acc: 0.95, recall: 0.95, precision: 0.9516258530710557, f_beta: 0.949954959463517
train: step: 1802, loss: 0.0399283803999424, acc: 0.94, recall: 0.94, precision: 0.9407051282051282, f_beta: 0.9399759903961584
train: step: 1803, loss: 0.03659040480852127, acc: 0.95, recall: 0.95, precision: 0.9516258530710557, f_beta: 0.949954959463517
train: step: 1804, loss: 0.05799821764230728, acc: 0.93, recall: 0.9299999999999999, precision: 0.9315535929345644, f_beta: 0.9299369432489241
train: step: 1805, loss: 0.08537652343511581, acc: 0.88, recall: 0.88, precision: 0.8806089743589745, f_beta: 0.879951980792317
train: step: 1806, loss: 0.028477555140852928, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 1807, loss: 0.12197773158550262, acc: 0.84, recall: 0.84, precision: 0.8405448717948718, f_beta: 0.8399359743897559
train: step: 1808, loss: 0.049159031361341476, acc: 0.93, recall: 0.9299999999999999, precision: 0.930172068827531, f_beta: 0.9299929992999301
train: step: 1809, loss: 0.09601517021656036, acc: 0.88, recall: 0.88, precision: 0.88, f_beta: 0.88
train: step: 1810, loss: 0.09181598573923111, acc: 0.88, recall: 0.88, precision: 0.8824476650563606, f_beta: 0.8798076923076923
train: step: 1811, loss: 0.11129703372716904, acc: 0.86, recall: 0.8600000000000001, precision: 0.8652597402597402, f_beta: 0.8594941790445605
train: step: 1812, loss: 0.052857656031847, acc: 0.91, recall: 0.91, precision: 0.9101640656262505, f_beta: 0.90999099909991
train: step: 1813, loss: 0.11136844009160995, acc: 0.85, recall: 0.85, precision: 0.851264552388599, f_beta: 0.8498648783905516
train: step: 1814, loss: 0.059737518429756165, acc: 0.92, recall: 0.92, precision: 0.92, f_beta: 0.92
train: step: 1815, loss: 0.05626222491264343, acc: 0.93, recall: 0.9299999999999999, precision: 0.930172068827531, f_beta: 0.9299929992999301
train: step: 1816, loss: 0.021325360983610153, acc: 0.98, recall: 0.98, precision: 0.98, f_beta: 0.98
train: step: 1817, loss: 0.027165411040186882, acc: 0.96, recall: 0.96, precision: 0.96, f_beta: 0.96
train: step: 1818, loss: 0.10387087613344193, acc: 0.87, recall: 0.87, precision: 0.8773969808241534, f_beta: 0.8693598633303186
train: step: 1819, loss: 0.06610136479139328, acc: 0.92, recall: 0.9199999999999999, precision: 0.9206730769230769, f_beta: 0.919967987194878
train: step: 1820, loss: 0.058633655309677124, acc: 0.91, recall: 0.9099999999999999, precision: 0.9141414141414141, f_beta: 0.9097744360902256
train: step: 1821, loss: 0.0462808832526207, acc: 0.93, recall: 0.9299999999999999, precision: 0.9343434343434343, f_beta: 0.9298245614035088
train: step: 1822, loss: 0.06608261913061142, acc: 0.93, recall: 0.9299999999999999, precision: 0.9385964912280702, f_beta: 0.9296553110240177
train: step: 1823, loss: 0.042574621737003326, acc: 0.94, recall: 0.94, precision: 0.9407051282051282, f_beta: 0.9399759903961584
train: step: 1824, loss: 0.024537205696105957, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 1825, loss: 0.03160766139626503, acc: 0.96, recall: 0.96, precision: 0.96, f_beta: 0.96
train: step: 1826, loss: 0.06096753478050232, acc: 0.92, recall: 0.9199999999999999, precision: 0.9206730769230769, f_beta: 0.919967987194878
train: step: 1827, loss: 0.041914332658052444, acc: 0.94, recall: 0.94, precision: 0.94, f_beta: 0.94
train: step: 1828, loss: 0.04073690250515938, acc: 0.95, recall: 0.95, precision: 0.9501800720288115, f_beta: 0.9499949994999499
train: step: 1829, loss: 0.04758293181657791, acc: 0.95, recall: 0.95, precision: 0.9501800720288115, f_beta: 0.9499949994999499
train: step: 1830, loss: 0.02830216847360134, acc: 0.97, recall: 0.97, precision: 0.9716981132075472, f_beta: 0.9699729756781104
train: step: 1831, loss: 0.062345217913389206, acc: 0.92, recall: 0.9199999999999999, precision: 0.9310344827586207, f_beta: 0.9194847020933978
train: step: 1832, loss: 0.059815481305122375, acc: 0.91, recall: 0.91, precision: 0.9101640656262505, f_beta: 0.90999099909991
train: step: 1833, loss: 0.040931083261966705, acc: 0.94, recall: 0.94, precision: 0.9428341384863124, f_beta: 0.9399038461538463
train: step: 1834, loss: 0.0339558981359005, acc: 0.96, recall: 0.96, precision: 0.962962962962963, f_beta: 0.9599358974358974
train: step: 1835, loss: 0.0635559931397438, acc: 0.92, recall: 0.9199999999999999, precision: 0.9206730769230769, f_beta: 0.919967987194878
train: step: 1836, loss: 0.041828908026218414, acc: 0.94, recall: 0.94, precision: 0.9428341384863124, f_beta: 0.9399038461538463
train: step: 1837, loss: 0.033562902361154556, acc: 0.95, recall: 0.95, precision: 0.9516258530710557, f_beta: 0.949954959463517
train: step: 1838, loss: 0.07804469019174576, acc: 0.89, recall: 0.89, precision: 0.89015606242497, f_beta: 0.88998899889989
train: step: 1839, loss: 0.04241587594151497, acc: 0.96, recall: 0.96, precision: 0.962962962962963, f_beta: 0.9599358974358974
train: step: 1840, loss: 0.07997456938028336, acc: 0.9, recall: 0.9, precision: 0.9006410256410255, f_beta: 0.8999599839935974
train: step: 1841, loss: 0.039993446320295334, acc: 0.95, recall: 0.95, precision: 0.9516258530710557, f_beta: 0.949954959463517
train: step: 1842, loss: 0.03325426205992699, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 1843, loss: 0.04080922529101372, acc: 0.94, recall: 0.94, precision: 0.9407051282051282, f_beta: 0.9399759903961584
train: step: 1844, loss: 0.06510306149721146, acc: 0.91, recall: 0.91, precision: 0.9101640656262505, f_beta: 0.90999099909991
train: step: 1845, loss: 0.029614252969622612, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 1846, loss: 0.048210591077804565, acc: 0.94, recall: 0.94, precision: 0.9407051282051282, f_beta: 0.9399759903961584
train: step: 1847, loss: 0.042168766260147095, acc: 0.95, recall: 0.95, precision: 0.9501800720288115, f_beta: 0.9499949994999499
train: step: 1848, loss: 0.02667449228465557, acc: 0.96, recall: 0.96, precision: 0.96, f_beta: 0.96
train: step: 1849, loss: 0.046345971524715424, acc: 0.94, recall: 0.94, precision: 0.9407051282051282, f_beta: 0.9399759903961584
train: step: 1850, loss: 0.03099089302122593, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 1851, loss: 0.07152973860502243, acc: 0.92, recall: 0.9199999999999999, precision: 0.9227053140096618, f_beta: 0.9198717948717948
train: step: 1852, loss: 0.06665440648794174, acc: 0.9, recall: 0.8999999999999999, precision: 0.9025764895330113, f_beta: 0.8998397435897436
train: step: 1853, loss: 0.05179766193032265, acc: 0.93, recall: 0.9299999999999999, precision: 0.930172068827531, f_beta: 0.9299929992999301
train: step: 1854, loss: 0.03955540060997009, acc: 0.96, recall: 0.96, precision: 0.962962962962963, f_beta: 0.9599358974358974
train: step: 1855, loss: 0.02574920654296875, acc: 0.95, recall: 0.95, precision: 0.9501800720288115, f_beta: 0.9499949994999499
train: step: 1856, loss: 0.10124585777521133, acc: 0.87, recall: 0.87, precision: 0.8713368125250903, f_beta: 0.8698828946051447
train: step: 1857, loss: 0.055337093770504, acc: 0.93, recall: 0.9299999999999999, precision: 0.9343434343434343, f_beta: 0.9298245614035088
train: step: 1858, loss: 0.039466481655836105, acc: 0.95, recall: 0.95, precision: 0.9501800720288115, f_beta: 0.9499949994999499
train: step: 1859, loss: 0.045461904257535934, acc: 0.96, recall: 0.96, precision: 0.9607371794871795, f_beta: 0.959983993597439
train: step: 1860, loss: 0.027422988787293434, acc: 0.96, recall: 0.96, precision: 0.9607371794871795, f_beta: 0.959983993597439
train: step: 1861, loss: 0.0634165108203888, acc: 0.92, recall: 0.9199999999999999, precision: 0.9206730769230769, f_beta: 0.919967987194878
train: step: 1862, loss: 0.045308470726013184, acc: 0.95, recall: 0.95, precision: 0.9501800720288115, f_beta: 0.9499949994999499
train: step: 1863, loss: 0.06156283989548683, acc: 0.93, recall: 0.9299999999999999, precision: 0.9343434343434343, f_beta: 0.9298245614035088
train: step: 1864, loss: 0.04095444828271866, acc: 0.95, recall: 0.95, precision: 0.9545454545454546, f_beta: 0.949874686716792
train: step: 1865, loss: 0.05220562592148781, acc: 0.94, recall: 0.94, precision: 0.9428341384863124, f_beta: 0.9399038461538463
train: step: 1866, loss: 0.06069789454340935, acc: 0.9, recall: 0.9, precision: 0.9006410256410255, f_beta: 0.8999599839935974
train: step: 1867, loss: 0.05912415683269501, acc: 0.92, recall: 0.9199999999999999, precision: 0.9227053140096618, f_beta: 0.9198717948717948
train: step: 1868, loss: 0.05713457986712456, acc: 0.92, recall: 0.92, precision: 0.92, f_beta: 0.92
train: step: 1869, loss: 0.05210016667842865, acc: 0.94, recall: 0.94, precision: 0.94, f_beta: 0.94
train: step: 1870, loss: 0.07882136106491089, acc: 0.9, recall: 0.8999999999999999, precision: 0.9025764895330113, f_beta: 0.8998397435897436
train: step: 1871, loss: 0.03893589600920677, acc: 0.96, recall: 0.96, precision: 0.96, f_beta: 0.96
train: step: 1872, loss: 0.06849101930856705, acc: 0.91, recall: 0.91, precision: 0.9101640656262505, f_beta: 0.90999099909991
train: step: 1873, loss: 0.07189497351646423, acc: 0.91, recall: 0.91, precision: 0.9101640656262505, f_beta: 0.90999099909991
train: step: 1874, loss: 0.056861571967601776, acc: 0.93, recall: 0.9299999999999999, precision: 0.930172068827531, f_beta: 0.9299929992999301
train: step: 1875, loss: 0.09023325145244598, acc: 0.89, recall: 0.89, precision: 0.89015606242497, f_beta: 0.88998899889989
train: step: 1876, loss: 0.08028113096952438, acc: 0.91, recall: 0.9099999999999999, precision: 0.9141414141414141, f_beta: 0.9097744360902256
train: step: 1877, loss: 0.09113678336143494, acc: 0.88, recall: 0.88, precision: 0.88, f_beta: 0.88
train: step: 1878, loss: 0.056046951562166214, acc: 0.94, recall: 0.94, precision: 0.9407051282051282, f_beta: 0.9399759903961584
train: step: 1879, loss: 0.03263504430651665, acc: 0.96, recall: 0.96, precision: 0.962962962962963, f_beta: 0.9599358974358974
train: step: 1880, loss: 0.03896448761224747, acc: 0.95, recall: 0.95, precision: 0.9501800720288115, f_beta: 0.9499949994999499
train: step: 1881, loss: 0.09230463206768036, acc: 0.89, recall: 0.8899999999999999, precision: 0.8939393939393939, f_beta: 0.8897243107769424
train: step: 1882, loss: 0.03255016729235649, acc: 0.95, recall: 0.95, precision: 0.9516258530710557, f_beta: 0.949954959463517
train: step: 1883, loss: 0.08949531614780426, acc: 0.89, recall: 0.89, precision: 0.89015606242497, f_beta: 0.88998899889989
train: step: 1884, loss: 0.05750635266304016, acc: 0.92, recall: 0.9199999999999999, precision: 0.9261363636363636, f_beta: 0.9197109594540345
train: step: 1885, loss: 0.06790761649608612, acc: 0.91, recall: 0.91, precision: 0.9101640656262505, f_beta: 0.90999099909991
train: step: 1886, loss: 0.08144010603427887, acc: 0.9, recall: 0.8999999999999999, precision: 0.9025764895330113, f_beta: 0.8998397435897436
train: step: 1887, loss: 0.0491819754242897, acc: 0.95, recall: 0.95, precision: 0.9501800720288115, f_beta: 0.9499949994999499
train: step: 1888, loss: 0.03352995961904526, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 1889, loss: 0.09133770316839218, acc: 0.88, recall: 0.88, precision: 0.8806089743589745, f_beta: 0.879951980792317
train: step: 1890, loss: 0.044568758457899094, acc: 0.94, recall: 0.94, precision: 0.94, f_beta: 0.94
train: step: 1891, loss: 0.031324390321969986, acc: 0.96, recall: 0.96, precision: 0.9607371794871795, f_beta: 0.959983993597439
train: step: 1892, loss: 0.037994612008333206, acc: 0.96, recall: 0.96, precision: 0.96, f_beta: 0.96
train: step: 1893, loss: 0.06952318549156189, acc: 0.9, recall: 0.9, precision: 0.9006410256410255, f_beta: 0.8999599839935974
train: step: 1894, loss: 0.08460476994514465, acc: 0.87, recall: 0.87, precision: 0.8701480592236894, f_beta: 0.8699869986998701
train: step: 1895, loss: 0.04357288405299187, acc: 0.93, recall: 0.9299999999999999, precision: 0.9343434343434343, f_beta: 0.9298245614035088
train: step: 1896, loss: 0.04800776392221451, acc: 0.95, recall: 0.95, precision: 0.9516258530710557, f_beta: 0.949954959463517
train: step: 1897, loss: 0.0692078098654747, acc: 0.89, recall: 0.89, precision: 0.9030591153369161, f_beta: 0.8891017239641092
train: step: 1898, loss: 0.04673682153224945, acc: 0.95, recall: 0.95, precision: 0.9501800720288115, f_beta: 0.9499949994999499
train: step: 1899, loss: 0.04255756363272667, acc: 0.95, recall: 0.95, precision: 0.9501800720288115, f_beta: 0.9499949994999499
train: step: 1900, loss: 0.09479614347219467, acc: 0.86, recall: 0.86, precision: 0.86, f_beta: 0.8599999999999999
train: step: 1901, loss: 0.05533017963171005, acc: 0.93, recall: 0.9299999999999999, precision: 0.930172068827531, f_beta: 0.9299929992999301
train: step: 1902, loss: 0.052547749131917953, acc: 0.91, recall: 0.9099999999999999, precision: 0.9181966544267646, f_beta: 0.9095568284594513
train: step: 1903, loss: 0.06541193276643753, acc: 0.91, recall: 0.91, precision: 0.9101640656262505, f_beta: 0.90999099909991
train: step: 1904, loss: 0.06494686752557755, acc: 0.9, recall: 0.9, precision: 0.9, f_beta: 0.9
train: step: 1905, loss: 0.07481003552675247, acc: 0.9, recall: 0.8999999999999999, precision: 0.9058441558441559, f_beta: 0.8996386993175431
train: step: 1906, loss: 0.09635905921459198, acc: 0.87, recall: 0.87, precision: 0.8701480592236894, f_beta: 0.8699869986998701
train: step: 1907, loss: 0.051516350358724594, acc: 0.93, recall: 0.9299999999999999, precision: 0.9343434343434343, f_beta: 0.9298245614035088
train: step: 1908, loss: 0.016893625259399414, acc: 0.98, recall: 0.98, precision: 0.9807692307692308, f_beta: 0.9799919967987194
train: step: 1909, loss: 0.02832828462123871, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 1910, loss: 0.050360213965177536, acc: 0.93, recall: 0.9299999999999999, precision: 0.930172068827531, f_beta: 0.9299929992999301
train: step: 1911, loss: 0.0336310975253582, acc: 0.97, recall: 0.97, precision: 0.9716981132075472, f_beta: 0.9699729756781104
train: step: 1912, loss: 0.07553629577159882, acc: 0.88, recall: 0.88, precision: 0.8806089743589745, f_beta: 0.879951980792317
train: step: 1913, loss: 0.059987954795360565, acc: 0.93, recall: 0.9299999999999999, precision: 0.9343434343434343, f_beta: 0.9298245614035088
train: step: 1914, loss: 0.04603467881679535, acc: 0.96, recall: 0.96, precision: 0.9607371794871795, f_beta: 0.959983993597439
train: step: 1915, loss: 0.07114061713218689, acc: 0.91, recall: 0.9099999999999999, precision: 0.9181966544267646, f_beta: 0.9095568284594513
train: step: 1916, loss: 0.040634721517562866, acc: 0.96, recall: 0.96, precision: 0.9607371794871795, f_beta: 0.959983993597439
train: step: 1917, loss: 0.0975794792175293, acc: 0.88, recall: 0.8799999999999999, precision: 0.885551948051948, f_beta: 0.8795664391810518
train: step: 1918, loss: 0.04739541932940483, acc: 0.93, recall: 0.9299999999999999, precision: 0.9343434343434343, f_beta: 0.9298245614035088
train: step: 1919, loss: 0.050593990832567215, acc: 0.95, recall: 0.95, precision: 0.9501800720288115, f_beta: 0.9499949994999499
train: step: 1920, loss: 0.09923069179058075, acc: 0.88, recall: 0.88, precision: 0.8824476650563606, f_beta: 0.8798076923076923
train: step: 1921, loss: 0.10304007679224014, acc: 0.87, recall: 0.87, precision: 0.8737373737373737, f_beta: 0.8696741854636592
train: step: 1922, loss: 0.07647807896137238, acc: 0.91, recall: 0.91, precision: 0.9101640656262505, f_beta: 0.90999099909991
train: step: 1923, loss: 0.04991189017891884, acc: 0.93, recall: 0.9299999999999999, precision: 0.9315535929345644, f_beta: 0.9299369432489241
train: step: 1924, loss: 0.0491960346698761, acc: 0.94, recall: 0.94, precision: 0.9464285714285714, f_beta: 0.939783219590526
train: step: 1925, loss: 0.016970457509160042, acc: 0.98, recall: 0.98, precision: 0.9807692307692308, f_beta: 0.9799919967987194
train: step: 1926, loss: 0.03886621817946434, acc: 0.96, recall: 0.96, precision: 0.96, f_beta: 0.96
train: step: 1927, loss: 0.044403064996004105, acc: 0.96, recall: 0.96, precision: 0.9607371794871795, f_beta: 0.959983993597439
train: step: 1928, loss: 0.04079340770840645, acc: 0.97, recall: 0.97, precision: 0.9716981132075472, f_beta: 0.9699729756781104
train: step: 1929, loss: 0.0588788241147995, acc: 0.94, recall: 0.94, precision: 0.9428341384863124, f_beta: 0.9399038461538463
train: step: 1930, loss: 0.035731807351112366, acc: 0.96, recall: 0.96, precision: 0.962962962962963, f_beta: 0.9599358974358974
train: step: 1931, loss: 0.05479242280125618, acc: 0.95, recall: 0.95, precision: 0.9501800720288115, f_beta: 0.9499949994999499
train: step: 1932, loss: 0.05708013474941254, acc: 0.93, recall: 0.9299999999999999, precision: 0.9315535929345644, f_beta: 0.9299369432489241
train: step: 1933, loss: 0.10233838111162186, acc: 0.87, recall: 0.87, precision: 0.8823894171145101, f_beta: 0.8689384010484928
train: step: 1934, loss: 0.07170625030994415, acc: 0.9, recall: 0.9, precision: 0.9006410256410255, f_beta: 0.8999599839935974
train: step: 1935, loss: 0.06668305397033691, acc: 0.91, recall: 0.91, precision: 0.9101640656262505, f_beta: 0.90999099909991
train: step: 1936, loss: 0.029453271999955177, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 1937, loss: 0.020920146256685257, acc: 0.98, recall: 0.98, precision: 0.9807692307692308, f_beta: 0.9799919967987194
train: step: 1938, loss: 0.06356437504291534, acc: 0.92, recall: 0.92, precision: 0.92, f_beta: 0.92
train: step: 1939, loss: 0.04894646629691124, acc: 0.93, recall: 0.9299999999999999, precision: 0.9315535929345644, f_beta: 0.9299369432489241
train: step: 1940, loss: 0.4536767303943634, acc: 0.93, recall: 0.9299999999999999, precision: 0.9343434343434343, f_beta: 0.9298245614035088
train: step: 1941, loss: 0.056392595171928406, acc: 0.93, recall: 0.9299999999999999, precision: 0.930172068827531, f_beta: 0.9299929992999301
train: step: 1942, loss: 0.05469440296292305, acc: 0.92, recall: 0.9199999999999999, precision: 0.9206730769230769, f_beta: 0.919967987194878
train: step: 1943, loss: 0.03380860388278961, acc: 0.96, recall: 0.96, precision: 0.962962962962963, f_beta: 0.9599358974358974
train: step: 1944, loss: 0.07362546026706696, acc: 0.91, recall: 0.9099999999999999, precision: 0.9114813327980731, f_beta: 0.9099189270343309
train: step: 1945, loss: 0.04263971000909805, acc: 0.95, recall: 0.95, precision: 0.9516258530710557, f_beta: 0.949954959463517
train: step: 1946, loss: 0.04964442178606987, acc: 0.93, recall: 0.9299999999999999, precision: 0.930172068827531, f_beta: 0.9299929992999301
train: step: 1947, loss: 0.019120370969176292, acc: 0.98, recall: 0.98, precision: 0.98, f_beta: 0.98
train: step: 1948, loss: 0.08822976797819138, acc: 0.9, recall: 0.8999999999999999, precision: 0.9025764895330113, f_beta: 0.8998397435897436
train: step: 1949, loss: 0.014732131734490395, acc: 0.98, recall: 0.98, precision: 0.98, f_beta: 0.98
train: step: 1950, loss: 0.04009372368454933, acc: 0.96, recall: 0.96, precision: 0.962962962962963, f_beta: 0.9599358974358974
train: step: 1951, loss: 0.03792108595371246, acc: 0.95, recall: 0.95, precision: 0.9501800720288115, f_beta: 0.9499949994999499
train: step: 1952, loss: 0.04394175484776497, acc: 0.93, recall: 0.9299999999999999, precision: 0.9343434343434343, f_beta: 0.9298245614035088
train: step: 1953, loss: 0.03655530512332916, acc: 0.96, recall: 0.96, precision: 0.9607371794871795, f_beta: 0.959983993597439
train: step: 1954, loss: 0.09274336695671082, acc: 0.87, recall: 0.87, precision: 0.8713368125250903, f_beta: 0.8698828946051447
train: step: 1955, loss: 0.030766351148486137, acc: 0.95, recall: 0.95, precision: 0.9501800720288115, f_beta: 0.9499949994999499
train: step: 1956, loss: 0.03764323145151138, acc: 0.96, recall: 0.96, precision: 0.9607371794871795, f_beta: 0.959983993597439
train: step: 1957, loss: 0.03757047280669212, acc: 0.95, recall: 0.95, precision: 0.9516258530710557, f_beta: 0.949954959463517
train: step: 1958, loss: 0.05662176012992859, acc: 0.94, recall: 0.94, precision: 0.9428341384863124, f_beta: 0.9399038461538463
train: step: 1959, loss: 0.02078968472778797, acc: 0.98, recall: 0.98, precision: 0.9807692307692308, f_beta: 0.9799919967987194
train: step: 1960, loss: 0.033016908913850784, acc: 0.96, recall: 0.96, precision: 0.9607371794871795, f_beta: 0.959983993597439
train: step: 1961, loss: 0.058364029973745346, acc: 0.93, recall: 0.9299999999999999, precision: 0.930172068827531, f_beta: 0.9299929992999301
train: step: 1962, loss: 0.05186622589826584, acc: 0.94, recall: 0.94, precision: 0.94, f_beta: 0.94
train: step: 1963, loss: 0.06311456859111786, acc: 0.94, recall: 0.94, precision: 0.94, f_beta: 0.94
train: step: 1964, loss: 0.07084213942289352, acc: 0.91, recall: 0.9099999999999999, precision: 0.9114813327980731, f_beta: 0.9099189270343309
train: step: 1965, loss: 0.04513523727655411, acc: 0.94, recall: 0.94, precision: 0.9464285714285714, f_beta: 0.939783219590526
train: step: 1966, loss: 0.025350036099553108, acc: 0.98, recall: 0.98, precision: 0.9807692307692308, f_beta: 0.9799919967987194
train: step: 1967, loss: 0.05505216121673584, acc: 0.93, recall: 0.9299999999999999, precision: 0.930172068827531, f_beta: 0.9299929992999301
train: step: 1968, loss: 0.08274546265602112, acc: 0.88, recall: 0.8799999999999999, precision: 0.885551948051948, f_beta: 0.8795664391810518
train: step: 1969, loss: 0.04370373860001564, acc: 0.96, recall: 0.96, precision: 0.9607371794871795, f_beta: 0.959983993597439
train: step: 1970, loss: 0.02995368465781212, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 1971, loss: 0.05650423839688301, acc: 0.93, recall: 0.9299999999999999, precision: 0.9315535929345644, f_beta: 0.9299369432489241
train: step: 1972, loss: 0.04084248095750809, acc: 0.96, recall: 0.96, precision: 0.9607371794871795, f_beta: 0.959983993597439
train: step: 1973, loss: 0.0366254597902298, acc: 0.95, recall: 0.95, precision: 0.9516258530710557, f_beta: 0.949954959463517
train: step: 1974, loss: 0.007369486149400473, acc: 0.99, recall: 0.99, precision: 0.9901960784313726, f_beta: 0.98999899989999
train: step: 1975, loss: 0.026635393500328064, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 1976, loss: 0.04435227811336517, acc: 0.94, recall: 0.94, precision: 0.9428341384863124, f_beta: 0.9399038461538463
train: step: 1977, loss: 0.058464813977479935, acc: 0.92, recall: 0.9199999999999999, precision: 0.9227053140096618, f_beta: 0.9198717948717948
train: step: 1978, loss: 0.047415591776371, acc: 0.94, recall: 0.94, precision: 0.9407051282051282, f_beta: 0.9399759903961584
train: step: 1979, loss: 0.051370181143283844, acc: 0.93, recall: 0.9299999999999999, precision: 0.930172068827531, f_beta: 0.9299929992999301
train: step: 1980, loss: 0.049909982830286026, acc: 0.95, recall: 0.95, precision: 0.9501800720288115, f_beta: 0.9499949994999499
train: step: 1981, loss: 0.039086032658815384, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 1982, loss: 0.04785603657364845, acc: 0.95, recall: 0.95, precision: 0.9516258530710557, f_beta: 0.949954959463517
train: step: 1983, loss: 0.033824432641267776, acc: 0.96, recall: 0.96, precision: 0.9607371794871795, f_beta: 0.959983993597439
train: step: 1984, loss: 0.020038997754454613, acc: 0.96, recall: 0.96, precision: 0.96, f_beta: 0.96
train: step: 1985, loss: 0.04094703122973442, acc: 0.94, recall: 0.94, precision: 0.9428341384863124, f_beta: 0.9399038461538463
train: step: 1986, loss: 0.06118886545300484, acc: 0.91, recall: 0.9099999999999999, precision: 0.9181966544267646, f_beta: 0.9095568284594513
train: step: 1987, loss: 0.054917823523283005, acc: 0.92, recall: 0.9199999999999999, precision: 0.9227053140096618, f_beta: 0.9198717948717948
train: step: 1988, loss: 0.06064451113343239, acc: 0.92, recall: 0.9199999999999999, precision: 0.9206730769230769, f_beta: 0.919967987194878
train: step: 1989, loss: 0.06861584633588791, acc: 0.91, recall: 0.9099999999999999, precision: 0.9114813327980731, f_beta: 0.9099189270343309
train: step: 1990, loss: 0.04931457340717316, acc: 0.94, recall: 0.94, precision: 0.94, f_beta: 0.94
train: step: 1991, loss: 0.03005821444094181, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 1992, loss: 0.06016336753964424, acc: 0.93, recall: 0.9299999999999999, precision: 0.9315535929345644, f_beta: 0.9299369432489241
train: step: 1993, loss: 0.03810858726501465, acc: 0.95, recall: 0.95, precision: 0.9545454545454546, f_beta: 0.949874686716792
train: step: 1994, loss: 0.0504131019115448, acc: 0.94, recall: 0.94, precision: 0.9464285714285714, f_beta: 0.939783219590526
train: step: 1995, loss: 0.0806124284863472, acc: 0.91, recall: 0.9099999999999999, precision: 0.9181966544267646, f_beta: 0.9095568284594513
train: step: 1996, loss: 0.022909507155418396, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 1997, loss: 0.07025539129972458, acc: 0.91, recall: 0.91, precision: 0.9101640656262505, f_beta: 0.90999099909991
train: step: 1998, loss: 0.08153380453586578, acc: 0.87, recall: 0.87, precision: 0.8701480592236894, f_beta: 0.8699869986998701
train: step: 1999, loss: 0.06353233009576797, acc: 0.92, recall: 0.9199999999999999, precision: 0.9261363636363636, f_beta: 0.9197109594540345
train: step: 2000, loss: 0.009822212159633636, acc: 0.99, recall: 0.99, precision: 0.9901960784313726, f_beta: 0.98999899989999
train: step: 2001, loss: 0.058946315199136734, acc: 0.92, recall: 0.92, precision: 0.92, f_beta: 0.92
train: step: 2002, loss: 0.0464901365339756, acc: 0.96, recall: 0.96, precision: 0.9607371794871795, f_beta: 0.959983993597439
train: step: 2003, loss: 0.06027090921998024, acc: 0.93, recall: 0.9299999999999999, precision: 0.9315535929345644, f_beta: 0.9299369432489241
train: step: 2004, loss: 0.050683096051216125, acc: 0.93, recall: 0.9299999999999999, precision: 0.9343434343434343, f_beta: 0.9298245614035088
train: step: 2005, loss: 0.066606804728508, acc: 0.91, recall: 0.9099999999999999, precision: 0.9114813327980731, f_beta: 0.9099189270343309
train: step: 2006, loss: 0.029990920796990395, acc: 0.96, recall: 0.96, precision: 0.96, f_beta: 0.96
train: step: 2007, loss: 0.03850647062063217, acc: 0.95, recall: 0.95, precision: 0.9501800720288115, f_beta: 0.9499949994999499
train: step: 2008, loss: 0.063815638422966, acc: 0.93, recall: 0.9299999999999999, precision: 0.9315535929345644, f_beta: 0.9299369432489241
train: step: 2009, loss: 0.031218409538269043, acc: 0.96, recall: 0.96, precision: 0.96, f_beta: 0.96
train: step: 2010, loss: 0.0666281208395958, acc: 0.91, recall: 0.9099999999999999, precision: 0.9141414141414141, f_beta: 0.9097744360902256
train: step: 2011, loss: 0.0343765914440155, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 2012, loss: 0.042895812541246414, acc: 0.94, recall: 0.94, precision: 0.9407051282051282, f_beta: 0.9399759903961584
train: step: 2013, loss: 0.019765641540288925, acc: 0.98, recall: 0.98, precision: 0.98, f_beta: 0.98
train: step: 2014, loss: 0.04531878978013992, acc: 0.94, recall: 0.94, precision: 0.94, f_beta: 0.94
train: step: 2015, loss: 0.02962084487080574, acc: 0.97, recall: 0.97, precision: 0.9716981132075472, f_beta: 0.9699729756781104
train: step: 2016, loss: 0.05964066460728645, acc: 0.92, recall: 0.9199999999999999, precision: 0.9206730769230769, f_beta: 0.919967987194878
train: step: 2017, loss: 0.07561921328306198, acc: 0.91, recall: 0.9099999999999999, precision: 0.923728813559322, f_beta: 0.9092650468797258
train: step: 2018, loss: 0.031331002712249756, acc: 0.94, recall: 0.94, precision: 0.9464285714285714, f_beta: 0.939783219590526
train: step: 2019, loss: 0.05205557867884636, acc: 0.94, recall: 0.94, precision: 0.94, f_beta: 0.94
train: step: 2020, loss: 0.009870119392871857, acc: 0.99, recall: 0.99, precision: 0.9901960784313726, f_beta: 0.98999899989999
train: step: 2021, loss: 0.06929373741149902, acc: 0.91, recall: 0.9099999999999999, precision: 0.9114813327980731, f_beta: 0.9099189270343309
train: step: 2022, loss: 0.054400183260440826, acc: 0.91, recall: 0.9099999999999999, precision: 0.9141414141414141, f_beta: 0.9097744360902256
train: step: 2023, loss: 0.06559143960475922, acc: 0.89, recall: 0.89, precision: 0.89015606242497, f_beta: 0.88998899889989
train: step: 2024, loss: 0.041689109057188034, acc: 0.94, recall: 0.94, precision: 0.94, f_beta: 0.94
train: step: 2025, loss: 0.043334148824214935, acc: 0.95, recall: 0.95, precision: 0.9501800720288115, f_beta: 0.9499949994999499
train: step: 2026, loss: 0.03382235765457153, acc: 0.96, recall: 0.96, precision: 0.9607371794871795, f_beta: 0.959983993597439
train: step: 2027, loss: 0.03803245723247528, acc: 0.94, recall: 0.94, precision: 0.9407051282051282, f_beta: 0.9399759903961584
train: step: 2028, loss: 0.0487341433763504, acc: 0.96, recall: 0.96, precision: 0.962962962962963, f_beta: 0.9599358974358974
train: step: 2029, loss: 0.01957530714571476, acc: 0.98, recall: 0.98, precision: 0.98, f_beta: 0.98
train: step: 2030, loss: 0.02279093489050865, acc: 0.97, recall: 0.97, precision: 0.9716981132075472, f_beta: 0.9699729756781104
train: step: 2031, loss: 0.0833498015999794, acc: 0.9, recall: 0.9, precision: 0.9006410256410255, f_beta: 0.8999599839935974
train: step: 2032, loss: 0.036266766488552094, acc: 0.95, recall: 0.95, precision: 0.9501800720288115, f_beta: 0.9499949994999499
train: step: 2033, loss: 0.0369749590754509, acc: 0.96, recall: 0.96, precision: 0.962962962962963, f_beta: 0.9599358974358974
train: step: 2034, loss: 0.060219716280698776, acc: 0.93, recall: 0.9299999999999999, precision: 0.9315535929345644, f_beta: 0.9299369432489241
train: step: 2035, loss: 0.058642178773880005, acc: 0.93, recall: 0.9299999999999999, precision: 0.9343434343434343, f_beta: 0.9298245614035088
train: step: 2036, loss: 0.03450596705079079, acc: 0.94, recall: 0.94, precision: 0.9428341384863124, f_beta: 0.9399038461538463
train: step: 2037, loss: 0.0823289304971695, acc: 0.89, recall: 0.8899999999999999, precision: 0.8939393939393939, f_beta: 0.8897243107769424
train: step: 2038, loss: 0.04929046705365181, acc: 0.94, recall: 0.94, precision: 0.9407051282051282, f_beta: 0.9399759903961584
train: step: 2039, loss: 0.04126114398241043, acc: 0.95, recall: 0.95, precision: 0.9501800720288115, f_beta: 0.9499949994999499
train: step: 2040, loss: 0.03945394605398178, acc: 0.93, recall: 0.9299999999999999, precision: 0.9315535929345644, f_beta: 0.9299369432489241
train: step: 2041, loss: 0.009236455895006657, acc: 0.99, recall: 0.99, precision: 0.9901960784313726, f_beta: 0.98999899989999
train: step: 2042, loss: 0.10589185357093811, acc: 0.84, recall: 0.84, precision: 0.8405448717948718, f_beta: 0.8399359743897559
train: step: 2043, loss: 0.045769304037094116, acc: 0.95, recall: 0.95, precision: 0.9516258530710557, f_beta: 0.949954959463517
train: step: 2044, loss: 0.055762797594070435, acc: 0.95, recall: 0.95, precision: 0.9501800720288115, f_beta: 0.9499949994999499
train: step: 2045, loss: 0.011876028962433338, acc: 0.98, recall: 0.98, precision: 0.98, f_beta: 0.98
train: step: 2046, loss: 0.07869002968072891, acc: 0.89, recall: 0.89, precision: 0.89015606242497, f_beta: 0.88998899889989
train: step: 2047, loss: 0.06468060612678528, acc: 0.93, recall: 0.9299999999999999, precision: 0.930172068827531, f_beta: 0.9299929992999301
train: step: 2048, loss: 0.08209957182407379, acc: 0.9, recall: 0.8999999999999999, precision: 0.9025764895330113, f_beta: 0.8998397435897436
train: step: 2049, loss: 0.047560349106788635, acc: 0.94, recall: 0.94, precision: 0.94, f_beta: 0.94
train: step: 2050, loss: 0.03424271196126938, acc: 0.96, recall: 0.96, precision: 0.962962962962963, f_beta: 0.9599358974358974
train: step: 2051, loss: 0.027419526129961014, acc: 0.98, recall: 0.98, precision: 0.98, f_beta: 0.98
train: step: 2052, loss: 0.06010095775127411, acc: 0.92, recall: 0.9199999999999999, precision: 0.9227053140096618, f_beta: 0.9198717948717948
train: step: 2053, loss: 0.04819805920124054, acc: 0.92, recall: 0.9199999999999999, precision: 0.9261363636363636, f_beta: 0.9197109594540345
train: step: 2054, loss: 0.04780593514442444, acc: 0.95, recall: 0.95, precision: 0.9501800720288115, f_beta: 0.9499949994999499
train: step: 2055, loss: 0.07088848948478699, acc: 0.93, recall: 0.9299999999999999, precision: 0.930172068827531, f_beta: 0.9299929992999301
train: step: 2056, loss: 0.0714457631111145, acc: 0.9, recall: 0.8999999999999999, precision: 0.9105090311986863, f_beta: 0.8993558776167472
train: step: 2057, loss: 0.03727619722485542, acc: 0.94, recall: 0.94, precision: 0.9428341384863124, f_beta: 0.9399038461538463
train: step: 2058, loss: 0.05691666156053543, acc: 0.94, recall: 0.94, precision: 0.9428341384863124, f_beta: 0.9399038461538463
train: step: 2059, loss: 0.027494488283991814, acc: 0.98, recall: 0.98, precision: 0.9807692307692308, f_beta: 0.9799919967987194
train: step: 2060, loss: 0.04706767946481705, acc: 0.94, recall: 0.94, precision: 0.9464285714285714, f_beta: 0.939783219590526
train: step: 2061, loss: 0.06262761354446411, acc: 0.93, recall: 0.9299999999999999, precision: 0.930172068827531, f_beta: 0.9299929992999301
train: step: 2062, loss: 0.05487038567662239, acc: 0.92, recall: 0.92, precision: 0.92, f_beta: 0.92
train: step: 2063, loss: 0.055589210242033005, acc: 0.91, recall: 0.9099999999999999, precision: 0.9114813327980731, f_beta: 0.9099189270343309
train: step: 2064, loss: 0.08652199059724808, acc: 0.9, recall: 0.9, precision: 0.9006410256410255, f_beta: 0.8999599839935974
train: step: 2065, loss: 0.0710272565484047, acc: 0.9, recall: 0.8999999999999999, precision: 0.9025764895330113, f_beta: 0.8998397435897436
train: step: 2066, loss: 0.05499548092484474, acc: 0.92, recall: 0.9199999999999999, precision: 0.9206730769230769, f_beta: 0.919967987194878
train: step: 2067, loss: 0.02829541265964508, acc: 0.97, recall: 0.97, precision: 0.9716981132075472, f_beta: 0.9699729756781104
train: step: 2068, loss: 0.07424130290746689, acc: 0.89, recall: 0.89, precision: 0.89015606242497, f_beta: 0.88998899889989
train: step: 2069, loss: 0.038144711405038834, acc: 0.95, recall: 0.95, precision: 0.9516258530710557, f_beta: 0.949954959463517
train: step: 2070, loss: 0.037462346255779266, acc: 0.95, recall: 0.95, precision: 0.9516258530710557, f_beta: 0.949954959463517
train: step: 2071, loss: 0.03541428968310356, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 2072, loss: 0.04650029167532921, acc: 0.94, recall: 0.94, precision: 0.9428341384863124, f_beta: 0.9399038461538463
train: step: 2073, loss: 0.017205219715833664, acc: 0.98, recall: 0.98, precision: 0.9807692307692308, f_beta: 0.9799919967987194
train: step: 2074, loss: 0.056935109198093414, acc: 0.92, recall: 0.92, precision: 0.92, f_beta: 0.92
train: step: 2075, loss: 0.022505411878228188, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 2076, loss: 0.08957979083061218, acc: 0.88, recall: 0.88, precision: 0.8806089743589745, f_beta: 0.879951980792317
train: step: 2077, loss: 0.04351210221648216, acc: 0.95, recall: 0.95, precision: 0.9501800720288115, f_beta: 0.9499949994999499
train: step: 2078, loss: 0.04633503034710884, acc: 0.95, recall: 0.95, precision: 0.9501800720288115, f_beta: 0.9499949994999499
train: step: 2079, loss: 0.026247205212712288, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 2080, loss: 0.030838079750537872, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 2081, loss: 0.035170067101716995, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 2082, loss: 0.054103318601846695, acc: 0.93, recall: 0.9299999999999999, precision: 0.930172068827531, f_beta: 0.9299929992999301
train: step: 2083, loss: 0.08345550298690796, acc: 0.9, recall: 0.9, precision: 0.9, f_beta: 0.9
train: step: 2084, loss: 0.06448190659284592, acc: 0.92, recall: 0.9199999999999999, precision: 0.9261363636363636, f_beta: 0.9197109594540345
train: step: 2085, loss: 0.06464648991823196, acc: 0.92, recall: 0.9199999999999999, precision: 0.9227053140096618, f_beta: 0.9198717948717948
train: step: 2086, loss: 0.053034283220767975, acc: 0.92, recall: 0.9199999999999999, precision: 0.9227053140096618, f_beta: 0.9198717948717948
train: step: 2087, loss: 0.056661199778318405, acc: 0.92, recall: 0.9199999999999999, precision: 0.9227053140096618, f_beta: 0.9198717948717948
train: step: 2088, loss: 0.015742037445306778, acc: 0.99, recall: 0.99, precision: 0.9901960784313726, f_beta: 0.98999899989999
train: step: 2089, loss: 0.03952202945947647, acc: 0.95, recall: 0.95, precision: 0.9501800720288115, f_beta: 0.9499949994999499
train: step: 2090, loss: 0.05890996754169464, acc: 0.92, recall: 0.92, precision: 0.92, f_beta: 0.92
train: step: 2091, loss: 0.040098968893289566, acc: 0.96, recall: 0.96, precision: 0.96, f_beta: 0.96
train: step: 2092, loss: 0.03484829515218735, acc: 0.94, recall: 0.94, precision: 0.94, f_beta: 0.94
train: step: 2093, loss: 0.06771255284547806, acc: 0.92, recall: 0.9199999999999999, precision: 0.9261363636363636, f_beta: 0.9197109594540345
train: step: 2094, loss: 0.03368866443634033, acc: 0.97, recall: 0.97, precision: 0.9716981132075472, f_beta: 0.9699729756781104
train: step: 2095, loss: 0.05406522750854492, acc: 0.94, recall: 0.94, precision: 0.94, f_beta: 0.94
train: step: 2096, loss: 0.10064277797937393, acc: 0.87, recall: 0.87, precision: 0.8713368125250903, f_beta: 0.8698828946051447
train: step: 2097, loss: 0.06527957320213318, acc: 0.91, recall: 0.91, precision: 0.9101640656262505, f_beta: 0.90999099909991
train: step: 2098, loss: 0.034710124135017395, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 2099, loss: 0.044074077159166336, acc: 0.95, recall: 0.95, precision: 0.9501800720288115, f_beta: 0.9499949994999499
train: step: 2100, loss: 0.025254778563976288, acc: 0.98, recall: 0.98, precision: 0.98, f_beta: 0.98
train: step: 2101, loss: 0.059979476034641266, acc: 0.93, recall: 0.9299999999999999, precision: 0.9315535929345644, f_beta: 0.9299369432489241
train: step: 2102, loss: 0.07456980645656586, acc: 0.91, recall: 0.9099999999999999, precision: 0.9114813327980731, f_beta: 0.9099189270343309
train: step: 2103, loss: 0.0655045360326767, acc: 0.91, recall: 0.9099999999999999, precision: 0.9114813327980731, f_beta: 0.9099189270343309
train: step: 2104, loss: 0.05937515199184418, acc: 0.93, recall: 0.9299999999999999, precision: 0.9343434343434343, f_beta: 0.9298245614035088
train: step: 2105, loss: 0.05876261740922928, acc: 0.91, recall: 0.91, precision: 0.9101640656262505, f_beta: 0.90999099909991
train: step: 2106, loss: 0.06226788088679314, acc: 0.92, recall: 0.9199999999999999, precision: 0.9227053140096618, f_beta: 0.9198717948717948
train: step: 2107, loss: 0.04390861093997955, acc: 0.93, recall: 0.9299999999999999, precision: 0.9315535929345644, f_beta: 0.9299369432489241
train: step: 2108, loss: 0.04967932775616646, acc: 0.94, recall: 0.94, precision: 0.9428341384863124, f_beta: 0.9399038461538463
train: step: 2109, loss: 0.005747717805206776, acc: 0.99, recall: 0.99, precision: 0.9901960784313726, f_beta: 0.98999899989999
train: step: 2110, loss: 0.03300324082374573, acc: 0.95, recall: 0.95, precision: 0.9501800720288115, f_beta: 0.9499949994999499
train: step: 2111, loss: 0.04771152511239052, acc: 0.95, recall: 0.95, precision: 0.9501800720288115, f_beta: 0.9499949994999499
train: step: 2112, loss: 0.07429513335227966, acc: 0.91, recall: 0.9099999999999999, precision: 0.9141414141414141, f_beta: 0.9097744360902256
train: step: 2113, loss: 0.060166824609041214, acc: 0.92, recall: 0.9199999999999999, precision: 0.9206730769230769, f_beta: 0.919967987194878
train: step: 2114, loss: 0.07263221591711044, acc: 0.92, recall: 0.9199999999999999, precision: 0.9206730769230769, f_beta: 0.919967987194878
train: step: 2115, loss: 0.030360352247953415, acc: 0.95, recall: 0.95, precision: 0.9516258530710557, f_beta: 0.949954959463517
train: step: 2116, loss: 0.05357122793793678, acc: 0.93, recall: 0.9299999999999999, precision: 0.9315535929345644, f_beta: 0.9299369432489241
train: step: 2117, loss: 0.04641478508710861, acc: 0.94, recall: 0.94, precision: 0.9428341384863124, f_beta: 0.9399038461538463
train: step: 2118, loss: 0.05729547515511513, acc: 0.91, recall: 0.9099999999999999, precision: 0.923728813559322, f_beta: 0.9092650468797258
train: step: 2119, loss: 0.08822144567966461, acc: 0.88, recall: 0.8799999999999999, precision: 0.885551948051948, f_beta: 0.8795664391810518
train: step: 2120, loss: 0.08027534186840057, acc: 0.89, recall: 0.89, precision: 0.8914090726615818, f_beta: 0.8899009108197378
train: step: 2121, loss: 0.057447634637355804, acc: 0.92, recall: 0.9199999999999999, precision: 0.9261363636363636, f_beta: 0.9197109594540345
train: step: 2122, loss: 0.08872263133525848, acc: 0.9, recall: 0.9, precision: 0.9006410256410255, f_beta: 0.8999599839935974
train: step: 2123, loss: 0.04778199642896652, acc: 0.93, recall: 0.9299999999999999, precision: 0.9315535929345644, f_beta: 0.9299369432489241
train: step: 2124, loss: 0.051610831171274185, acc: 0.95, recall: 0.95, precision: 0.9516258530710557, f_beta: 0.949954959463517
train: step: 2125, loss: 0.06286049634218216, acc: 0.93, recall: 0.9299999999999999, precision: 0.930172068827531, f_beta: 0.9299929992999301
train: step: 2126, loss: 0.04105177894234657, acc: 0.95, recall: 0.95, precision: 0.9516258530710557, f_beta: 0.949954959463517
train: step: 2127, loss: 0.04242054000496864, acc: 0.95, recall: 0.95, precision: 0.9501800720288115, f_beta: 0.9499949994999499
train: step: 2128, loss: 0.056159280240535736, acc: 0.94, recall: 0.94, precision: 0.9407051282051282, f_beta: 0.9399759903961584
train: step: 2129, loss: 0.04086309298872948, acc: 0.95, recall: 0.95, precision: 0.9501800720288115, f_beta: 0.9499949994999499
train: step: 2130, loss: 0.07967022061347961, acc: 0.91, recall: 0.91, precision: 0.9101640656262505, f_beta: 0.90999099909991
train: step: 2131, loss: 0.05671445280313492, acc: 0.92, recall: 0.9199999999999999, precision: 0.9206730769230769, f_beta: 0.919967987194878
train: step: 2132, loss: 0.0234238151460886, acc: 0.99, recall: 0.99, precision: 0.9901960784313726, f_beta: 0.98999899989999
train: step: 2133, loss: 0.02924327552318573, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 2134, loss: 0.07287896424531937, acc: 0.91, recall: 0.9099999999999999, precision: 0.9181966544267646, f_beta: 0.9095568284594513
train: step: 2135, loss: 0.04165216535329819, acc: 0.94, recall: 0.94, precision: 0.9428341384863124, f_beta: 0.9399038461538463
train: step: 2136, loss: 0.0736377164721489, acc: 0.91, recall: 0.9099999999999999, precision: 0.9141414141414141, f_beta: 0.9097744360902256
train: step: 2137, loss: 0.043349795043468475, acc: 0.94, recall: 0.94, precision: 0.9407051282051282, f_beta: 0.9399759903961584
train: step: 2138, loss: 0.05453024059534073, acc: 0.94, recall: 0.94, precision: 0.9407051282051282, f_beta: 0.9399759903961584
train: step: 2139, loss: 0.04757482558488846, acc: 0.93, recall: 0.9299999999999999, precision: 0.9385964912280702, f_beta: 0.9296553110240177
train: step: 2140, loss: 0.05612138658761978, acc: 0.92, recall: 0.9199999999999999, precision: 0.9310344827586207, f_beta: 0.9194847020933978
train: step: 2141, loss: 0.0596146434545517, acc: 0.91, recall: 0.9099999999999999, precision: 0.923728813559322, f_beta: 0.9092650468797258
train: step: 2142, loss: 0.03273792564868927, acc: 0.96, recall: 0.96, precision: 0.9607371794871795, f_beta: 0.959983993597439
train: step: 2143, loss: 0.04545815289020538, acc: 0.94, recall: 0.94, precision: 0.9407051282051282, f_beta: 0.9399759903961584
train: step: 2144, loss: 0.0896737277507782, acc: 0.88, recall: 0.88, precision: 0.8806089743589745, f_beta: 0.879951980792317
train: step: 2145, loss: 0.07968496531248093, acc: 0.9, recall: 0.9, precision: 0.9006410256410255, f_beta: 0.8999599839935974
train: step: 2146, loss: 0.04784121364355087, acc: 0.93, recall: 0.9299999999999999, precision: 0.930172068827531, f_beta: 0.9299929992999301
train: step: 2147, loss: 0.03356735780835152, acc: 0.97, recall: 0.97, precision: 0.9716981132075472, f_beta: 0.9699729756781104
train: step: 2148, loss: 0.05888821929693222, acc: 0.92, recall: 0.9199999999999999, precision: 0.9206730769230769, f_beta: 0.919967987194878
train: step: 2149, loss: 0.07846596091985703, acc: 0.91, recall: 0.91, precision: 0.9101640656262505, f_beta: 0.90999099909991
train: step: 2150, loss: 0.05445657670497894, acc: 0.94, recall: 0.94, precision: 0.9407051282051282, f_beta: 0.9399759903961584
train: step: 2151, loss: 0.0404619425535202, acc: 0.95, recall: 0.95, precision: 0.9516258530710557, f_beta: 0.949954959463517
train: step: 2152, loss: 0.05188751220703125, acc: 0.92, recall: 0.9199999999999999, precision: 0.9206730769230769, f_beta: 0.919967987194878
train: step: 2153, loss: 0.07549412548542023, acc: 0.9, recall: 0.9, precision: 0.9006410256410255, f_beta: 0.8999599839935974
train: step: 2154, loss: 0.07903971523046494, acc: 0.91, recall: 0.91, precision: 0.9101640656262505, f_beta: 0.90999099909991
train: step: 2155, loss: 0.028879832476377487, acc: 0.96, recall: 0.96, precision: 0.962962962962963, f_beta: 0.9599358974358974
train: step: 2156, loss: 0.038013044744729996, acc: 0.95, recall: 0.95, precision: 0.9516258530710557, f_beta: 0.949954959463517
train: step: 2157, loss: 0.0536290742456913, acc: 0.92, recall: 0.9199999999999999, precision: 0.9227053140096618, f_beta: 0.9198717948717948
train: step: 2158, loss: 0.040825095027685165, acc: 0.96, recall: 0.96, precision: 0.9607371794871795, f_beta: 0.959983993597439
train: step: 2159, loss: 0.08298776298761368, acc: 0.89, recall: 0.89, precision: 0.89015606242497, f_beta: 0.88998899889989
train: step: 2160, loss: 0.04252442345023155, acc: 0.94, recall: 0.94, precision: 0.94, f_beta: 0.94
train: step: 2161, loss: 0.04962516948580742, acc: 0.94, recall: 0.94, precision: 0.9407051282051282, f_beta: 0.9399759903961584
train: step: 2162, loss: 0.050136443227529526, acc: 0.92, recall: 0.92, precision: 0.92, f_beta: 0.92
train: step: 2163, loss: 0.042917195707559586, acc: 0.96, recall: 0.96, precision: 0.96, f_beta: 0.96
train: step: 2164, loss: 0.026332102715969086, acc: 0.95, recall: 0.95, precision: 0.9516258530710557, f_beta: 0.949954959463517
train: step: 2165, loss: 0.051464181393384933, acc: 0.94, recall: 0.94, precision: 0.94, f_beta: 0.94
train: step: 2166, loss: 0.06590709835290909, acc: 0.91, recall: 0.91, precision: 0.9101640656262505, f_beta: 0.90999099909991
train: step: 2167, loss: 0.02659042365849018, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 2168, loss: 0.05448437109589577, acc: 0.93, recall: 0.9299999999999999, precision: 0.9315535929345644, f_beta: 0.9299369432489241
train: step: 2169, loss: 0.030861444771289825, acc: 0.96, recall: 0.96, precision: 0.9607371794871795, f_beta: 0.959983993597439
train: step: 2170, loss: 0.13352133333683014, acc: 0.86, recall: 0.86, precision: 0.8623188405797102, f_beta: 0.859775641025641
train: step: 2171, loss: 0.025197576731443405, acc: 0.98, recall: 0.98, precision: 0.98, f_beta: 0.98
train: step: 2172, loss: 0.007192034740000963, acc: 1.0, recall: 1.0, precision: 1.0, f_beta: 1.0
train: step: 2173, loss: 0.05724048614501953, acc: 0.94, recall: 0.94, precision: 0.9407051282051282, f_beta: 0.9399759903961584
train: step: 2174, loss: 0.033769670873880386, acc: 0.95, recall: 0.95, precision: 0.9516258530710557, f_beta: 0.949954959463517
train: step: 2175, loss: 0.053491994738578796, acc: 0.94, recall: 0.94, precision: 0.94, f_beta: 0.94
train: step: 2176, loss: 0.039549268782138824, acc: 0.95, recall: 0.95, precision: 0.9501800720288115, f_beta: 0.9499949994999499
train: step: 2177, loss: 0.05454934760928154, acc: 0.93, recall: 0.9299999999999999, precision: 0.9343434343434343, f_beta: 0.9298245614035088
train: step: 2178, loss: 0.03460688889026642, acc: 0.95, recall: 0.95, precision: 0.9545454545454546, f_beta: 0.949874686716792
train: step: 2179, loss: 0.04166284576058388, acc: 0.95, recall: 0.95, precision: 0.9501800720288115, f_beta: 0.9499949994999499
train: step: 2180, loss: 0.08018980175256729, acc: 0.91, recall: 0.91, precision: 0.9101640656262505, f_beta: 0.90999099909991
train: step: 2181, loss: 0.0429362952709198, acc: 0.96, recall: 0.96, precision: 0.9607371794871795, f_beta: 0.959983993597439
train: step: 2182, loss: 0.051246996968984604, acc: 0.94, recall: 0.94, precision: 0.9428341384863124, f_beta: 0.9399038461538463
train: step: 2183, loss: 0.09237857908010483, acc: 0.9, recall: 0.9, precision: 0.9, f_beta: 0.9
train: step: 2184, loss: 0.01960381492972374, acc: 0.98, recall: 0.98, precision: 0.98, f_beta: 0.98
train: step: 2185, loss: 0.07646039128303528, acc: 0.91, recall: 0.91, precision: 0.9101640656262505, f_beta: 0.90999099909991
train: step: 2186, loss: 0.03822022303938866, acc: 0.96, recall: 0.96, precision: 0.96, f_beta: 0.96
train: step: 2187, loss: 0.06663309037685394, acc: 0.9, recall: 0.8999999999999999, precision: 0.9025764895330113, f_beta: 0.8998397435897436
train: step: 2188, loss: 0.023687701672315598, acc: 0.96, recall: 0.96, precision: 0.962962962962963, f_beta: 0.9599358974358974
train: step: 2189, loss: 0.032581936568021774, acc: 0.95, recall: 0.95, precision: 0.9501800720288115, f_beta: 0.9499949994999499
train: step: 2190, loss: 0.06264064460992813, acc: 0.94, recall: 0.94, precision: 0.9428341384863124, f_beta: 0.9399038461538463
train: step: 2191, loss: 0.0742899626493454, acc: 0.91, recall: 0.91, precision: 0.9101640656262505, f_beta: 0.90999099909991
train: step: 2192, loss: 0.06604699790477753, acc: 0.9, recall: 0.8999999999999999, precision: 0.9058441558441559, f_beta: 0.8996386993175431
train: step: 2193, loss: 0.037990231066942215, acc: 0.95, recall: 0.95, precision: 0.9501800720288115, f_beta: 0.9499949994999499
train: step: 2194, loss: 0.04725503548979759, acc: 0.93, recall: 0.9299999999999999, precision: 0.9315535929345644, f_beta: 0.9299369432489241
train: step: 2195, loss: 0.06855472922325134, acc: 0.89, recall: 0.89, precision: 0.8914090726615818, f_beta: 0.8899009108197378
train: step: 2196, loss: 0.041402921080589294, acc: 0.95, recall: 0.95, precision: 0.9501800720288115, f_beta: 0.9499949994999499
train: step: 2197, loss: 0.03182598948478699, acc: 0.94, recall: 0.94, precision: 0.94, f_beta: 0.94
train: step: 2198, loss: 0.03972902148962021, acc: 0.94, recall: 0.94, precision: 0.9407051282051282, f_beta: 0.9399759903961584
train: step: 2199, loss: 0.02811829000711441, acc: 0.94, recall: 0.94, precision: 0.94, f_beta: 0.94
train: step: 2200, loss: 0.06257934868335724, acc: 0.92, recall: 0.9199999999999999, precision: 0.9206730769230769, f_beta: 0.919967987194878
train: step: 2201, loss: 0.0446476936340332, acc: 0.95, recall: 0.95, precision: 0.9501800720288115, f_beta: 0.9499949994999499
train: step: 2202, loss: 0.05257179215550423, acc: 0.94, recall: 0.94, precision: 0.9428341384863124, f_beta: 0.9399038461538463
train: step: 2203, loss: 0.056870438158512115, acc: 0.92, recall: 0.92, precision: 0.92, f_beta: 0.92
train: step: 2204, loss: 0.05332724004983902, acc: 0.92, recall: 0.9199999999999999, precision: 0.9206730769230769, f_beta: 0.919967987194878
train: step: 2205, loss: 0.07661940157413483, acc: 0.91, recall: 0.9099999999999999, precision: 0.9114813327980731, f_beta: 0.9099189270343309
train: step: 2206, loss: 0.07848060876131058, acc: 0.91, recall: 0.91, precision: 0.9101640656262505, f_beta: 0.90999099909991
train: step: 2207, loss: 0.030157461762428284, acc: 0.95, recall: 0.95, precision: 0.9516258530710557, f_beta: 0.949954959463517
train: step: 2208, loss: 0.06168841943144798, acc: 0.93, recall: 0.9299999999999999, precision: 0.930172068827531, f_beta: 0.9299929992999301
train: step: 2209, loss: 0.05099610239267349, acc: 0.92, recall: 0.9199999999999999, precision: 0.9227053140096618, f_beta: 0.9198717948717948
train: step: 2210, loss: 0.03666862100362778, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 2211, loss: 0.037942275404930115, acc: 0.95, recall: 0.95, precision: 0.9501800720288115, f_beta: 0.9499949994999499
train: step: 2212, loss: 0.08178254216909409, acc: 0.88, recall: 0.88, precision: 0.8958333333333333, f_beta: 0.8787878787878789
train: step: 2213, loss: 0.04416707158088684, acc: 0.94, recall: 0.94, precision: 0.9428341384863124, f_beta: 0.9399038461538463
train: step: 2214, loss: 0.06809453666210175, acc: 0.9, recall: 0.8999999999999999, precision: 0.9058441558441559, f_beta: 0.8996386993175431
train: step: 2215, loss: 0.04894917830824852, acc: 0.94, recall: 0.94, precision: 0.9428341384863124, f_beta: 0.9399038461538463
train: step: 2216, loss: 0.055794693529605865, acc: 0.93, recall: 0.9299999999999999, precision: 0.930172068827531, f_beta: 0.9299929992999301
train: step: 2217, loss: 0.040620479732751846, acc: 0.95, recall: 0.95, precision: 0.9501800720288115, f_beta: 0.9499949994999499
train: step: 2218, loss: 0.0500422865152359, acc: 0.94, recall: 0.94, precision: 0.9407051282051282, f_beta: 0.9399759903961584
train: step: 2219, loss: 0.06109217554330826, acc: 0.92, recall: 0.9199999999999999, precision: 0.9227053140096618, f_beta: 0.9198717948717948
train: step: 2220, loss: 0.0277330931276083, acc: 0.97, recall: 0.97, precision: 0.9716981132075472, f_beta: 0.9699729756781104
train: step: 2221, loss: 0.040481146425008774, acc: 0.95, recall: 0.95, precision: 0.9501800720288115, f_beta: 0.9499949994999499
train: step: 2222, loss: 0.049206849187612534, acc: 0.93, recall: 0.9299999999999999, precision: 0.930172068827531, f_beta: 0.9299929992999301
train: step: 2223, loss: 0.0342971533536911, acc: 0.96, recall: 0.96, precision: 0.9607371794871795, f_beta: 0.959983993597439
train: step: 2224, loss: 0.04293571412563324, acc: 0.95, recall: 0.95, precision: 0.9516258530710557, f_beta: 0.949954959463517
train: step: 2225, loss: 0.08944570273160934, acc: 0.87, recall: 0.87, precision: 0.8713368125250903, f_beta: 0.8698828946051447
train: step: 2226, loss: 0.025866884738206863, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 2227, loss: 0.04966031014919281, acc: 0.95, recall: 0.95, precision: 0.9501800720288115, f_beta: 0.9499949994999499
train: step: 2228, loss: 0.07247557491064072, acc: 0.9, recall: 0.9, precision: 0.9, f_beta: 0.9
train: step: 2229, loss: 0.07195781916379929, acc: 0.89, recall: 0.89, precision: 0.8914090726615818, f_beta: 0.8899009108197378
train: step: 2230, loss: 0.09231672435998917, acc: 0.87, recall: 0.87, precision: 0.8737373737373737, f_beta: 0.8696741854636592
train: step: 2231, loss: 0.05547120049595833, acc: 0.92, recall: 0.9199999999999999, precision: 0.9206730769230769, f_beta: 0.919967987194878
train: step: 2232, loss: 0.0464889258146286, acc: 0.92, recall: 0.9199999999999999, precision: 0.9227053140096618, f_beta: 0.9198717948717948
train: step: 2233, loss: 0.06009164825081825, acc: 0.93, recall: 0.9299999999999999, precision: 0.930172068827531, f_beta: 0.9299929992999301
train: step: 2234, loss: 0.02259131707251072, acc: 0.96, recall: 0.96, precision: 0.9607371794871795, f_beta: 0.959983993597439
train: step: 2235, loss: 0.04356676712632179, acc: 0.94, recall: 0.94, precision: 0.94, f_beta: 0.94
train: step: 2236, loss: 0.04836025834083557, acc: 0.93, recall: 0.9299999999999999, precision: 0.930172068827531, f_beta: 0.9299929992999301
train: step: 2237, loss: 0.03440573811531067, acc: 0.95, recall: 0.95, precision: 0.9501800720288115, f_beta: 0.9499949994999499
train: step: 2238, loss: 0.03216749057173729, acc: 0.96, recall: 0.96, precision: 0.96, f_beta: 0.96
train: step: 2239, loss: 0.06375858187675476, acc: 0.93, recall: 0.9299999999999999, precision: 0.930172068827531, f_beta: 0.9299929992999301
train: step: 2240, loss: 0.03653676435351372, acc: 0.95, recall: 0.95, precision: 0.9545454545454546, f_beta: 0.949874686716792
train: step: 2241, loss: 0.04566003754734993, acc: 0.94, recall: 0.94, precision: 0.94, f_beta: 0.94
train: step: 2242, loss: 0.02749057300388813, acc: 0.98, recall: 0.98, precision: 0.9807692307692308, f_beta: 0.9799919967987194
train: step: 2243, loss: 0.06456253677606583, acc: 0.93, recall: 0.9299999999999999, precision: 0.9315535929345644, f_beta: 0.9299369432489241
train: step: 2244, loss: 0.04785027354955673, acc: 0.94, recall: 0.94, precision: 0.9407051282051282, f_beta: 0.9399759903961584
train: step: 2245, loss: 0.03859798610210419, acc: 0.96, recall: 0.96, precision: 0.962962962962963, f_beta: 0.9599358974358974
train: step: 2246, loss: 0.033666059374809265, acc: 0.96, recall: 0.96, precision: 0.96, f_beta: 0.96
train: step: 2247, loss: 0.07003616541624069, acc: 0.92, recall: 0.92, precision: 0.92, f_beta: 0.92
train: step: 2248, loss: 0.03851206228137016, acc: 0.95, recall: 0.95, precision: 0.9501800720288115, f_beta: 0.9499949994999499
train: step: 2249, loss: 0.03211512416601181, acc: 0.95, recall: 0.95, precision: 0.9501800720288115, f_beta: 0.9499949994999499
train: step: 2250, loss: 0.052603911608457565, acc: 0.93, recall: 0.9299999999999999, precision: 0.9315535929345644, f_beta: 0.9299369432489241
train: step: 2251, loss: 0.053440406918525696, acc: 0.93, recall: 0.9299999999999999, precision: 0.9315535929345644, f_beta: 0.9299369432489241
train: step: 2252, loss: 0.04451052099466324, acc: 0.94, recall: 0.94, precision: 0.9407051282051282, f_beta: 0.9399759903961584
train: step: 2253, loss: 0.027752790600061417, acc: 0.98, recall: 0.98, precision: 0.98, f_beta: 0.98
train: step: 2254, loss: 0.07657219469547272, acc: 0.9, recall: 0.9, precision: 0.9006410256410255, f_beta: 0.8999599839935974
train: step: 2255, loss: 0.06301791220903397, acc: 0.91, recall: 0.9099999999999999, precision: 0.9114813327980731, f_beta: 0.9099189270343309
train: step: 2256, loss: 0.02282268926501274, acc: 0.98, recall: 0.98, precision: 0.98, f_beta: 0.98
train: step: 2257, loss: 0.013434508815407753, acc: 0.99, recall: 0.99, precision: 0.9901960784313726, f_beta: 0.98999899989999
train: step: 2258, loss: 0.006540573667734861, acc: 0.99, recall: 0.99, precision: 0.9901960784313726, f_beta: 0.98999899989999
train: step: 2259, loss: 0.04970498010516167, acc: 0.93, recall: 0.9299999999999999, precision: 0.9315535929345644, f_beta: 0.9299369432489241
train: step: 2260, loss: 0.014575972221791744, acc: 0.98, recall: 0.98, precision: 0.9807692307692308, f_beta: 0.9799919967987194
train: step: 2261, loss: 0.03695016726851463, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 2262, loss: 0.07302121073007584, acc: 0.9, recall: 0.8999999999999999, precision: 0.9025764895330113, f_beta: 0.8998397435897436
train: step: 2263, loss: 0.07280833274126053, acc: 0.91, recall: 0.9099999999999999, precision: 0.9114813327980731, f_beta: 0.9099189270343309
train: step: 2264, loss: 0.05589749664068222, acc: 0.91, recall: 0.91, precision: 0.9101640656262505, f_beta: 0.90999099909991
train: step: 2265, loss: 0.02615651674568653, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 2266, loss: 0.05585115775465965, acc: 0.91, recall: 0.9099999999999999, precision: 0.9114813327980731, f_beta: 0.9099189270343309
train: step: 2267, loss: 0.07526698708534241, acc: 0.91, recall: 0.9099999999999999, precision: 0.9114813327980731, f_beta: 0.9099189270343309
train: step: 2268, loss: 0.0330621711909771, acc: 0.96, recall: 0.96, precision: 0.9607371794871795, f_beta: 0.959983993597439
train: step: 2269, loss: 0.0647590160369873, acc: 0.91, recall: 0.91, precision: 0.9101640656262505, f_beta: 0.90999099909991
train: step: 2270, loss: 0.04996946454048157, acc: 0.95, recall: 0.95, precision: 0.9501800720288115, f_beta: 0.9499949994999499
train: step: 2271, loss: 0.03514431416988373, acc: 0.96, recall: 0.96, precision: 0.9607371794871795, f_beta: 0.959983993597439
train: step: 2272, loss: 0.055631931871175766, acc: 0.91, recall: 0.9099999999999999, precision: 0.9114813327980731, f_beta: 0.9099189270343309
train: step: 2273, loss: 0.030004030093550682, acc: 0.96, recall: 0.96, precision: 0.9607371794871795, f_beta: 0.959983993597439
train: step: 2274, loss: 0.012929100543260574, acc: 0.98, recall: 0.98, precision: 0.9807692307692308, f_beta: 0.9799919967987194
train: step: 2275, loss: 0.03861553966999054, acc: 0.95, recall: 0.95, precision: 0.9501800720288115, f_beta: 0.9499949994999499
train: step: 2276, loss: 0.03618599474430084, acc: 0.96, recall: 0.96, precision: 0.962962962962963, f_beta: 0.9599358974358974
train: step: 2277, loss: 0.036389149725437164, acc: 0.96, recall: 0.96, precision: 0.96, f_beta: 0.96
train: step: 2278, loss: 0.10975660383701324, acc: 0.84, recall: 0.84, precision: 0.8405448717948718, f_beta: 0.8399359743897559
train: step: 2279, loss: 0.037374719977378845, acc: 0.95, recall: 0.95, precision: 0.9501800720288115, f_beta: 0.9499949994999499
train: step: 2280, loss: 0.0522143617272377, acc: 0.93, recall: 0.9299999999999999, precision: 0.930172068827531, f_beta: 0.9299929992999301
train: step: 2281, loss: 0.05012985318899155, acc: 0.94, recall: 0.94, precision: 0.9407051282051282, f_beta: 0.9399759903961584
train: step: 2282, loss: 0.0124332495033741, acc: 0.98, recall: 0.98, precision: 0.98, f_beta: 0.98
train: step: 2283, loss: 0.03688483312726021, acc: 0.96, recall: 0.96, precision: 0.962962962962963, f_beta: 0.9599358974358974
train: step: 2284, loss: 0.058594923466444016, acc: 0.93, recall: 0.9299999999999999, precision: 0.930172068827531, f_beta: 0.9299929992999301
train: step: 2285, loss: 0.02227591909468174, acc: 0.97, recall: 0.97, precision: 0.9716981132075472, f_beta: 0.9699729756781104
train: step: 2286, loss: 0.05473276972770691, acc: 0.94, recall: 0.94, precision: 0.94, f_beta: 0.94
train: step: 2287, loss: 0.03286285698413849, acc: 0.94, recall: 0.94, precision: 0.9464285714285714, f_beta: 0.939783219590526
train: step: 2288, loss: 0.03489149734377861, acc: 0.96, recall: 0.96, precision: 0.962962962962963, f_beta: 0.9599358974358974
train: step: 2289, loss: 0.021296406164765358, acc: 0.98, recall: 0.98, precision: 0.9807692307692308, f_beta: 0.9799919967987194
train: step: 2290, loss: 0.05196908488869667, acc: 0.93, recall: 0.9299999999999999, precision: 0.930172068827531, f_beta: 0.9299929992999301
train: step: 2291, loss: 0.020703041926026344, acc: 0.98, recall: 0.98, precision: 0.98, f_beta: 0.98
train: step: 2292, loss: 0.03254828229546547, acc: 0.96, recall: 0.96, precision: 0.96, f_beta: 0.96
train: step: 2293, loss: 0.08120860159397125, acc: 0.92, recall: 0.9199999999999999, precision: 0.9206730769230769, f_beta: 0.919967987194878
train: step: 2294, loss: 0.04364600032567978, acc: 0.93, recall: 0.9299999999999999, precision: 0.930172068827531, f_beta: 0.9299929992999301
train: step: 2295, loss: 0.016832878813147545, acc: 0.99, recall: 0.99, precision: 0.9901960784313726, f_beta: 0.98999899989999
train: step: 2296, loss: 0.030668634921312332, acc: 0.96, recall: 0.96, precision: 0.96, f_beta: 0.96
train: step: 2297, loss: 0.05460458621382713, acc: 0.94, recall: 0.94, precision: 0.9407051282051282, f_beta: 0.9399759903961584
train: step: 2298, loss: 0.017918076366186142, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 2299, loss: 0.04736844077706337, acc: 0.94, recall: 0.94, precision: 0.9407051282051282, f_beta: 0.9399759903961584
train: step: 2300, loss: 0.038460321724414825, acc: 0.95, recall: 0.95, precision: 0.9516258530710557, f_beta: 0.949954959463517
train: step: 2301, loss: 0.0028176880441606045, acc: 1.0, recall: 1.0, precision: 1.0, f_beta: 1.0
train: step: 2302, loss: 0.029318390414118767, acc: 0.96, recall: 0.96, precision: 0.9607371794871795, f_beta: 0.959983993597439
train: step: 2303, loss: 0.055274203419685364, acc: 0.91, recall: 0.9099999999999999, precision: 0.9141414141414141, f_beta: 0.9097744360902256
train: step: 2304, loss: 0.05201409384608269, acc: 0.93, recall: 0.9299999999999999, precision: 0.930172068827531, f_beta: 0.9299929992999301
train: step: 2305, loss: 0.07659543305635452, acc: 0.89, recall: 0.89, precision: 0.8914090726615818, f_beta: 0.8899009108197378
train: step: 2306, loss: 0.041583672165870667, acc: 0.96, recall: 0.96, precision: 0.9607371794871795, f_beta: 0.959983993597439
train: step: 2307, loss: 0.02173508331179619, acc: 0.98, recall: 0.98, precision: 0.9807692307692308, f_beta: 0.9799919967987194
train: step: 2308, loss: 0.02898947149515152, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 2309, loss: 0.007060835137963295, acc: 1.0, recall: 1.0, precision: 1.0, f_beta: 1.0
train: step: 2310, loss: 0.031063757836818695, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 2311, loss: 0.03173735737800598, acc: 0.96, recall: 0.96, precision: 0.96, f_beta: 0.96
train: step: 2312, loss: 0.01406319160014391, acc: 0.99, recall: 0.99, precision: 0.9901960784313726, f_beta: 0.98999899989999
train: step: 2313, loss: 0.028237003833055496, acc: 0.96, recall: 0.96, precision: 0.9607371794871795, f_beta: 0.959983993597439
train: step: 2314, loss: 0.061119575053453445, acc: 0.93, recall: 0.9299999999999999, precision: 0.9343434343434343, f_beta: 0.9298245614035088
train: step: 2315, loss: 0.020669525489211082, acc: 0.98, recall: 0.98, precision: 0.98, f_beta: 0.98
train: step: 2316, loss: 0.0424322783946991, acc: 0.94, recall: 0.94, precision: 0.94, f_beta: 0.94
train: step: 2317, loss: 0.009659201838076115, acc: 0.99, recall: 0.99, precision: 0.9901960784313726, f_beta: 0.98999899989999
train: step: 2318, loss: 0.02125137858092785, acc: 0.99, recall: 0.99, precision: 0.9901960784313726, f_beta: 0.98999899989999
train: step: 2319, loss: 0.07561168819665909, acc: 0.89, recall: 0.89, precision: 0.8914090726615818, f_beta: 0.8899009108197378
train: step: 2320, loss: 0.050804268568754196, acc: 0.95, recall: 0.95, precision: 0.9516258530710557, f_beta: 0.949954959463517
train: step: 2321, loss: 0.05352504551410675, acc: 0.96, recall: 0.96, precision: 0.96, f_beta: 0.96
train: step: 2322, loss: 0.06077934429049492, acc: 0.91, recall: 0.9099999999999999, precision: 0.9141414141414141, f_beta: 0.9097744360902256
train: step: 2323, loss: 0.03984507545828819, acc: 0.94, recall: 0.94, precision: 0.94, f_beta: 0.94
train: step: 2324, loss: 0.04910244047641754, acc: 0.94, recall: 0.94, precision: 0.9407051282051282, f_beta: 0.9399759903961584
train: step: 2325, loss: 0.031102051958441734, acc: 0.96, recall: 0.96, precision: 0.962962962962963, f_beta: 0.9599358974358974
train: step: 2326, loss: 0.04014923423528671, acc: 0.96, recall: 0.96, precision: 0.962962962962963, f_beta: 0.9599358974358974
train: step: 2327, loss: 0.042298413813114166, acc: 0.96, recall: 0.96, precision: 0.96, f_beta: 0.96
train: step: 2328, loss: 0.055325184017419815, acc: 0.94, recall: 0.94, precision: 0.9407051282051282, f_beta: 0.9399759903961584
train: step: 2329, loss: 0.06116068735718727, acc: 0.92, recall: 0.9199999999999999, precision: 0.9227053140096618, f_beta: 0.9198717948717948
train: step: 2330, loss: 0.04550791159272194, acc: 0.96, recall: 0.96, precision: 0.96, f_beta: 0.96
train: step: 2331, loss: 0.022395310923457146, acc: 0.97, recall: 0.97, precision: 0.9716981132075472, f_beta: 0.9699729756781104
train: step: 2332, loss: 0.06829165667295456, acc: 0.92, recall: 0.92, precision: 0.92, f_beta: 0.92
train: step: 2333, loss: 0.043024007230997086, acc: 0.94, recall: 0.94, precision: 0.9407051282051282, f_beta: 0.9399759903961584
train: step: 2334, loss: 0.03842746838927269, acc: 0.94, recall: 0.94, precision: 0.9428341384863124, f_beta: 0.9399038461538463
train: step: 2335, loss: 0.03576185926795006, acc: 0.96, recall: 0.96, precision: 0.96, f_beta: 0.96
train: step: 2336, loss: 0.017598135396838188, acc: 0.98, recall: 0.98, precision: 0.9807692307692308, f_beta: 0.9799919967987194
train: step: 2337, loss: 0.05622905120253563, acc: 0.93, recall: 0.9299999999999999, precision: 0.9315535929345644, f_beta: 0.9299369432489241
train: step: 2338, loss: 0.03338012471795082, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 2339, loss: 0.03904030844569206, acc: 0.96, recall: 0.96, precision: 0.9607371794871795, f_beta: 0.959983993597439
train: step: 2340, loss: 0.04114460572600365, acc: 0.95, recall: 0.95, precision: 0.9501800720288115, f_beta: 0.9499949994999499
train: step: 2341, loss: 0.039424408227205276, acc: 0.94, recall: 0.94, precision: 0.94, f_beta: 0.94
train: step: 2342, loss: 0.08649440109729767, acc: 0.88, recall: 0.8799999999999999, precision: 0.885551948051948, f_beta: 0.8795664391810518
train: step: 2343, loss: 0.07084403187036514, acc: 0.92, recall: 0.9199999999999999, precision: 0.9227053140096618, f_beta: 0.9198717948717948
train: step: 2344, loss: 0.0722740963101387, acc: 0.91, recall: 0.91, precision: 0.9101640656262505, f_beta: 0.90999099909991
train: step: 2345, loss: 0.027260974049568176, acc: 0.96, recall: 0.96, precision: 0.9607371794871795, f_beta: 0.959983993597439
train: step: 2346, loss: 0.0551273487508297, acc: 0.93, recall: 0.9299999999999999, precision: 0.9315535929345644, f_beta: 0.9299369432489241
train: step: 2347, loss: 0.09521399438381195, acc: 0.87, recall: 0.87, precision: 0.8773969808241534, f_beta: 0.8693598633303186
train: step: 2348, loss: 0.015415091067552567, acc: 0.98, recall: 0.98, precision: 0.9807692307692308, f_beta: 0.9799919967987194
train: step: 2349, loss: 0.08300818502902985, acc: 0.87, recall: 0.87, precision: 0.8773969808241534, f_beta: 0.8693598633303186
train: step: 2350, loss: 0.039565373212099075, acc: 0.96, recall: 0.96, precision: 0.96, f_beta: 0.96
train: step: 2351, loss: 0.019111692905426025, acc: 0.96, recall: 0.96, precision: 0.9607371794871795, f_beta: 0.959983993597439
train: step: 2352, loss: 0.022203819826245308, acc: 0.97, recall: 0.97, precision: 0.9716981132075472, f_beta: 0.9699729756781104
train: step: 2353, loss: 0.03473060205578804, acc: 0.94, recall: 0.94, precision: 0.9407051282051282, f_beta: 0.9399759903961584
train: step: 2354, loss: 0.06164184585213661, acc: 0.91, recall: 0.91, precision: 0.9101640656262505, f_beta: 0.90999099909991
train: step: 2355, loss: 0.05774186551570892, acc: 0.92, recall: 0.92, precision: 0.92, f_beta: 0.92
train: step: 2356, loss: 0.03273142874240875, acc: 0.95, recall: 0.95, precision: 0.9516258530710557, f_beta: 0.949954959463517
train: step: 2357, loss: 0.022287780418992043, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 2358, loss: 0.027323350310325623, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 2359, loss: 0.027581384405493736, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 2360, loss: 0.0751001387834549, acc: 0.91, recall: 0.9099999999999999, precision: 0.9114813327980731, f_beta: 0.9099189270343309
train: step: 2361, loss: 0.050340380519628525, acc: 0.93, recall: 0.9299999999999999, precision: 0.9315535929345644, f_beta: 0.9299369432489241
train: step: 2362, loss: 0.09894485771656036, acc: 0.87, recall: 0.87, precision: 0.8713368125250903, f_beta: 0.8698828946051447
train: step: 2363, loss: 0.004241624381393194, acc: 0.99, recall: 0.99, precision: 0.9901960784313726, f_beta: 0.98999899989999
train: step: 2364, loss: 0.059858646243810654, acc: 0.92, recall: 0.9199999999999999, precision: 0.9206730769230769, f_beta: 0.919967987194878
train: step: 2365, loss: 0.03743445500731468, acc: 0.94, recall: 0.94, precision: 0.9407051282051282, f_beta: 0.9399759903961584
train: step: 2366, loss: 0.0676877424120903, acc: 0.89, recall: 0.8899999999999999, precision: 0.8977968176254589, f_beta: 0.8894583458948848
train: step: 2367, loss: 0.05844695493578911, acc: 0.92, recall: 0.9199999999999999, precision: 0.9206730769230769, f_beta: 0.919967987194878
train: step: 2368, loss: 0.029208622872829437, acc: 0.97, recall: 0.97, precision: 0.9716981132075472, f_beta: 0.9699729756781104
train: step: 2369, loss: 0.03655789792537689, acc: 0.95, recall: 0.95, precision: 0.9501800720288115, f_beta: 0.9499949994999499
train: step: 2370, loss: 0.05404050275683403, acc: 0.93, recall: 0.9299999999999999, precision: 0.930172068827531, f_beta: 0.9299929992999301
train: step: 2371, loss: 0.03926316276192665, acc: 0.96, recall: 0.96, precision: 0.9607371794871795, f_beta: 0.959983993597439
train: step: 2372, loss: 0.03945297375321388, acc: 0.94, recall: 0.94, precision: 0.94, f_beta: 0.94
train: step: 2373, loss: 0.03355305269360542, acc: 0.96, recall: 0.96, precision: 0.9607371794871795, f_beta: 0.959983993597439
train: step: 2374, loss: 0.096243716776371, acc: 0.88, recall: 0.88, precision: 0.9032258064516129, f_beta: 0.8782467532467533
train: step: 2375, loss: 0.11177996546030045, acc: 0.87, recall: 0.87, precision: 0.8701480592236894, f_beta: 0.8699869986998701
train: step: 2376, loss: 0.04841185733675957, acc: 0.94, recall: 0.94, precision: 0.94, f_beta: 0.94
train: step: 2377, loss: 0.055630218237638474, acc: 0.93, recall: 0.9299999999999999, precision: 0.9343434343434343, f_beta: 0.9298245614035088
train: step: 2378, loss: 0.048047393560409546, acc: 0.93, recall: 0.9299999999999999, precision: 0.930172068827531, f_beta: 0.9299929992999301
train: step: 2379, loss: 0.05320493131875992, acc: 0.94, recall: 0.94, precision: 0.9407051282051282, f_beta: 0.9399759903961584
train: step: 2380, loss: 0.01663031429052353, acc: 0.98, recall: 0.98, precision: 0.98, f_beta: 0.98
train: step: 2381, loss: 0.04686911404132843, acc: 0.94, recall: 0.94, precision: 0.9407051282051282, f_beta: 0.9399759903961584
train: step: 2382, loss: 0.048936933279037476, acc: 0.92, recall: 0.9199999999999999, precision: 0.9206730769230769, f_beta: 0.919967987194878
train: step: 2383, loss: 0.05282807722687721, acc: 0.94, recall: 0.94, precision: 0.9428341384863124, f_beta: 0.9399038461538463
train: step: 2384, loss: 0.06420363485813141, acc: 0.92, recall: 0.9199999999999999, precision: 0.9227053140096618, f_beta: 0.9198717948717948
train: step: 2385, loss: 0.06174948811531067, acc: 0.93, recall: 0.9299999999999999, precision: 0.930172068827531, f_beta: 0.9299929992999301
train: step: 2386, loss: 0.08740581572055817, acc: 0.87, recall: 0.87, precision: 0.8773969808241534, f_beta: 0.8693598633303186
train: step: 2387, loss: 0.036138057708740234, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 2388, loss: 0.031343862414360046, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 2389, loss: 0.056590594351291656, acc: 0.93, recall: 0.9299999999999999, precision: 0.9315535929345644, f_beta: 0.9299369432489241
train: step: 2390, loss: 0.055726565420627594, acc: 0.92, recall: 0.9199999999999999, precision: 0.9261363636363636, f_beta: 0.9197109594540345
train: step: 2391, loss: 0.02495640330016613, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 2392, loss: 0.11508399993181229, acc: 0.82, recall: 0.8200000000000001, precision: 0.8284072249589491, f_beta: 0.818840579710145
train: step: 2393, loss: 0.06958199292421341, acc: 0.91, recall: 0.9099999999999999, precision: 0.9181966544267646, f_beta: 0.9095568284594513
train: step: 2394, loss: 0.056063130497932434, acc: 0.91, recall: 0.9099999999999999, precision: 0.923728813559322, f_beta: 0.9092650468797258
train: step: 2395, loss: 0.02017912082374096, acc: 0.98, recall: 0.98, precision: 0.9807692307692308, f_beta: 0.9799919967987194
train: step: 2396, loss: 0.01347783301025629, acc: 0.99, recall: 0.99, precision: 0.9901960784313726, f_beta: 0.98999899989999
train: step: 2397, loss: 0.038911547511816025, acc: 0.95, recall: 0.95, precision: 0.9516258530710557, f_beta: 0.949954959463517
train: step: 2398, loss: 0.03400889039039612, acc: 0.96, recall: 0.96, precision: 0.96, f_beta: 0.96
train: step: 2399, loss: 0.03891013562679291, acc: 0.95, recall: 0.95, precision: 0.9516258530710557, f_beta: 0.949954959463517
train: step: 2400, loss: 0.030632052570581436, acc: 0.96, recall: 0.96, precision: 0.9607371794871795, f_beta: 0.959983993597439
train: step: 2401, loss: 0.034710053354501724, acc: 0.95, recall: 0.95, precision: 0.9516258530710557, f_beta: 0.949954959463517
train: step: 2402, loss: 0.048245545476675034, acc: 0.95, recall: 0.95, precision: 0.9545454545454546, f_beta: 0.949874686716792
train: step: 2403, loss: 0.027430595830082893, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 2404, loss: 0.025881249457597733, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 2405, loss: 0.04087863117456436, acc: 0.92, recall: 0.9199999999999999, precision: 0.9206730769230769, f_beta: 0.919967987194878
train: step: 2406, loss: 0.03915474936366081, acc: 0.93, recall: 0.9299999999999999, precision: 0.9385964912280702, f_beta: 0.9296553110240177
train: step: 2407, loss: 0.09165647625923157, acc: 0.89, recall: 0.89, precision: 0.89015606242497, f_beta: 0.88998899889989
train: step: 2408, loss: 0.0439734160900116, acc: 0.95, recall: 0.95, precision: 0.9545454545454546, f_beta: 0.949874686716792
train: step: 2409, loss: 0.04146895930171013, acc: 0.96, recall: 0.96, precision: 0.9607371794871795, f_beta: 0.959983993597439
train: step: 2410, loss: 0.03207062929868698, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 2411, loss: 0.10679963231086731, acc: 0.88, recall: 0.88, precision: 0.8899835796387521, f_beta: 0.8792270531400966
train: step: 2412, loss: 0.08606262505054474, acc: 0.89, recall: 0.8899999999999999, precision: 0.8977968176254589, f_beta: 0.8894583458948848
train: step: 2413, loss: 0.0270506851375103, acc: 0.96, recall: 0.96, precision: 0.962962962962963, f_beta: 0.9599358974358974
train: step: 2414, loss: 0.1294427216053009, acc: 0.85, recall: 0.85, precision: 0.8501400560224089, f_beta: 0.84998499849985
train: step: 2415, loss: 0.039023563265800476, acc: 0.94, recall: 0.94, precision: 0.9428341384863124, f_beta: 0.9399038461538463
train: step: 2416, loss: 0.0263547133654356, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 2417, loss: 0.022249288856983185, acc: 0.98, recall: 0.98, precision: 0.9807692307692308, f_beta: 0.9799919967987194
train: step: 2418, loss: 0.02576659619808197, acc: 0.96, recall: 0.96, precision: 0.96, f_beta: 0.96
train: step: 2419, loss: 0.04794866591691971, acc: 0.94, recall: 0.94, precision: 0.94, f_beta: 0.94
train: step: 2420, loss: 0.017114028334617615, acc: 0.99, recall: 0.99, precision: 0.9901960784313726, f_beta: 0.98999899989999
train: step: 2421, loss: 0.0865091010928154, acc: 0.9, recall: 0.9, precision: 0.9006410256410255, f_beta: 0.8999599839935974
train: step: 2422, loss: 0.045862339437007904, acc: 0.95, recall: 0.95, precision: 0.9516258530710557, f_beta: 0.949954959463517
train: step: 2423, loss: 0.07126820087432861, acc: 0.9, recall: 0.8999999999999999, precision: 0.9058441558441559, f_beta: 0.8996386993175431
train: step: 2424, loss: 0.07094179838895798, acc: 0.91, recall: 0.91, precision: 0.9101640656262505, f_beta: 0.90999099909991
train: step: 2425, loss: 0.04888458549976349, acc: 0.94, recall: 0.94, precision: 0.9407051282051282, f_beta: 0.9399759903961584
train: step: 2426, loss: 0.017767272889614105, acc: 0.99, recall: 0.99, precision: 0.9901960784313726, f_beta: 0.98999899989999
train: step: 2427, loss: 0.03617452085018158, acc: 0.96, recall: 0.96, precision: 0.96, f_beta: 0.96
train: step: 2428, loss: 0.04054378345608711, acc: 0.95, recall: 0.95, precision: 0.9501800720288115, f_beta: 0.9499949994999499
train: step: 2429, loss: 0.049844950437545776, acc: 0.94, recall: 0.94, precision: 0.94, f_beta: 0.94
train: step: 2430, loss: 0.03844250366091728, acc: 0.96, recall: 0.96, precision: 0.9607371794871795, f_beta: 0.959983993597439
train: step: 2431, loss: 0.04976315423846245, acc: 0.92, recall: 0.9199999999999999, precision: 0.9227053140096618, f_beta: 0.9198717948717948
train: step: 2432, loss: 0.0720658004283905, acc: 0.9, recall: 0.8999999999999999, precision: 0.9025764895330113, f_beta: 0.8998397435897436
train: step: 2433, loss: 0.07184849679470062, acc: 0.9, recall: 0.8999999999999999, precision: 0.9025764895330113, f_beta: 0.8998397435897436
train: step: 2434, loss: 0.0626320093870163, acc: 0.93, recall: 0.9299999999999999, precision: 0.9315535929345644, f_beta: 0.9299369432489241
train: step: 2435, loss: 0.039093345403671265, acc: 0.96, recall: 0.96, precision: 0.9607371794871795, f_beta: 0.959983993597439
train: step: 2436, loss: 0.04763168469071388, acc: 0.94, recall: 0.94, precision: 0.9407051282051282, f_beta: 0.9399759903961584
train: step: 2437, loss: 0.03948204964399338, acc: 0.95, recall: 0.95, precision: 0.9501800720288115, f_beta: 0.9499949994999499
train: step: 2438, loss: 0.03521616756916046, acc: 0.96, recall: 0.96, precision: 0.96, f_beta: 0.96
train: step: 2439, loss: 0.026363039389252663, acc: 0.96, recall: 0.96, precision: 0.9607371794871795, f_beta: 0.959983993597439
train: step: 2440, loss: 0.031457409262657166, acc: 0.96, recall: 0.96, precision: 0.96, f_beta: 0.96
train: step: 2441, loss: 0.047665949910879135, acc: 0.94, recall: 0.94, precision: 0.9428341384863124, f_beta: 0.9399038461538463
train: step: 2442, loss: 0.07181667536497116, acc: 0.91, recall: 0.9099999999999999, precision: 0.9114813327980731, f_beta: 0.9099189270343309
train: step: 2443, loss: 0.0447542667388916, acc: 0.93, recall: 0.9299999999999999, precision: 0.9315535929345644, f_beta: 0.9299369432489241
train: step: 2444, loss: 0.036381784826517105, acc: 0.96, recall: 0.96, precision: 0.96, f_beta: 0.96
train: step: 2445, loss: 0.03674671798944473, acc: 0.96, recall: 0.96, precision: 0.962962962962963, f_beta: 0.9599358974358974
train: step: 2446, loss: 0.030147094279527664, acc: 0.96, recall: 0.96, precision: 0.9607371794871795, f_beta: 0.959983993597439
train: step: 2447, loss: 0.08934900164604187, acc: 0.88, recall: 0.88, precision: 0.8899835796387521, f_beta: 0.8792270531400966
train: step: 2448, loss: 0.05492561310529709, acc: 0.93, recall: 0.9299999999999999, precision: 0.930172068827531, f_beta: 0.9299929992999301
train: step: 2449, loss: 0.022315043956041336, acc: 0.98, recall: 0.98, precision: 0.98, f_beta: 0.98
train: step: 2450, loss: 0.09056340903043747, acc: 0.89, recall: 0.89, precision: 0.89015606242497, f_beta: 0.88998899889989
train: step: 2451, loss: 0.0526338629424572, acc: 0.93, recall: 0.9299999999999999, precision: 0.930172068827531, f_beta: 0.9299929992999301
train: step: 2452, loss: 0.06230169162154198, acc: 0.92, recall: 0.9199999999999999, precision: 0.9227053140096618, f_beta: 0.9198717948717948
train: step: 2453, loss: 0.04990389943122864, acc: 0.95, recall: 0.95, precision: 0.9516258530710557, f_beta: 0.949954959463517
train: step: 2454, loss: 0.05909120664000511, acc: 0.91, recall: 0.91, precision: 0.9101640656262505, f_beta: 0.90999099909991
train: step: 2455, loss: 0.05157490819692612, acc: 0.93, recall: 0.9299999999999999, precision: 0.9343434343434343, f_beta: 0.9298245614035088
train: step: 2456, loss: 0.03491340950131416, acc: 0.96, recall: 0.96, precision: 0.9607371794871795, f_beta: 0.959983993597439
train: step: 2457, loss: 0.031966887414455414, acc: 0.96, recall: 0.96, precision: 0.96, f_beta: 0.96
train: step: 2458, loss: 0.06056264042854309, acc: 0.91, recall: 0.91, precision: 0.9101640656262505, f_beta: 0.90999099909991
train: step: 2459, loss: 0.03959769755601883, acc: 0.94, recall: 0.94, precision: 0.9428341384863124, f_beta: 0.9399038461538463
train: step: 2460, loss: 0.07872673869132996, acc: 0.88, recall: 0.88, precision: 0.8824476650563606, f_beta: 0.8798076923076923
train: step: 2461, loss: 0.0980900377035141, acc: 0.89, recall: 0.8899999999999999, precision: 0.8977968176254589, f_beta: 0.8894583458948848
train: step: 2462, loss: 0.04315643757581711, acc: 0.94, recall: 0.94, precision: 0.9407051282051282, f_beta: 0.9399759903961584
train: step: 2463, loss: 0.044036902487277985, acc: 0.95, recall: 0.95, precision: 0.9501800720288115, f_beta: 0.9499949994999499
train: step: 2464, loss: 0.035780034959316254, acc: 0.94, recall: 0.94, precision: 0.94, f_beta: 0.94
train: step: 2465, loss: 0.02603290043771267, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 2466, loss: 0.04605861008167267, acc: 0.94, recall: 0.94, precision: 0.94, f_beta: 0.94
train: step: 2467, loss: 0.06414936482906342, acc: 0.93, recall: 0.9299999999999999, precision: 0.930172068827531, f_beta: 0.9299929992999301
train: step: 2468, loss: 0.08526197075843811, acc: 0.9, recall: 0.8999999999999999, precision: 0.9025764895330113, f_beta: 0.8998397435897436
train: step: 2469, loss: 0.030443701893091202, acc: 0.94, recall: 0.94, precision: 0.9407051282051282, f_beta: 0.9399759903961584
train: step: 2470, loss: 0.05707671120762825, acc: 0.93, recall: 0.9299999999999999, precision: 0.9315535929345644, f_beta: 0.9299369432489241
train: step: 2471, loss: 0.027366433292627335, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 2472, loss: 0.028571929782629013, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 2473, loss: 0.0449041947722435, acc: 0.94, recall: 0.94, precision: 0.9407051282051282, f_beta: 0.9399759903961584
train: step: 2474, loss: 0.07293333858251572, acc: 0.91, recall: 0.91, precision: 0.9101640656262505, f_beta: 0.90999099909991
train: step: 2475, loss: 0.027921786531805992, acc: 0.96, recall: 0.96, precision: 0.9607371794871795, f_beta: 0.959983993597439
train: step: 2476, loss: 0.03305654600262642, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 2477, loss: 0.04016460105776787, acc: 0.95, recall: 0.95, precision: 0.9501800720288115, f_beta: 0.9499949994999499
train: step: 2478, loss: 0.034332700073719025, acc: 0.94, recall: 0.94, precision: 0.9428341384863124, f_beta: 0.9399038461538463
train: step: 2479, loss: 0.06578122824430466, acc: 0.91, recall: 0.9099999999999999, precision: 0.9141414141414141, f_beta: 0.9097744360902256
train: step: 2480, loss: 0.028371140360832214, acc: 0.95, recall: 0.95, precision: 0.9501800720288115, f_beta: 0.9499949994999499
train: step: 2481, loss: 0.041567668318748474, acc: 0.95, recall: 0.95, precision: 0.9516258530710557, f_beta: 0.949954959463517
train: step: 2482, loss: 0.02666804939508438, acc: 0.96, recall: 0.96, precision: 0.9607371794871795, f_beta: 0.959983993597439
train: step: 2483, loss: 0.047874003648757935, acc: 0.93, recall: 0.9299999999999999, precision: 0.9315535929345644, f_beta: 0.9299369432489241
train: step: 2484, loss: 0.03276541084051132, acc: 0.95, recall: 0.95, precision: 0.9516258530710557, f_beta: 0.949954959463517
train: step: 2485, loss: 0.060662221163511276, acc: 0.92, recall: 0.9199999999999999, precision: 0.9206730769230769, f_beta: 0.919967987194878
train: step: 2486, loss: 0.028748225420713425, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 2487, loss: 0.038215719163417816, acc: 0.95, recall: 0.95, precision: 0.9501800720288115, f_beta: 0.9499949994999499
train: step: 2488, loss: 0.08098825812339783, acc: 0.89, recall: 0.89, precision: 0.8914090726615818, f_beta: 0.8899009108197378
train: step: 2489, loss: 0.049392472952604294, acc: 0.93, recall: 0.9299999999999999, precision: 0.9315535929345644, f_beta: 0.9299369432489241
train: step: 2490, loss: 0.032914355397224426, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 2491, loss: 0.05665978416800499, acc: 0.94, recall: 0.94, precision: 0.9407051282051282, f_beta: 0.9399759903961584
train: step: 2492, loss: 0.051627323031425476, acc: 0.94, recall: 0.94, precision: 0.9407051282051282, f_beta: 0.9399759903961584
train: step: 2493, loss: 0.04994414001703262, acc: 0.93, recall: 0.9299999999999999, precision: 0.9315535929345644, f_beta: 0.9299369432489241
train: step: 2494, loss: 0.037483830004930496, acc: 0.95, recall: 0.95, precision: 0.9501800720288115, f_beta: 0.9499949994999499
train: step: 2495, loss: 0.0344700813293457, acc: 0.96, recall: 0.96, precision: 0.9607371794871795, f_beta: 0.959983993597439
train: step: 2496, loss: 0.029945457354187965, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 2497, loss: 0.04286694526672363, acc: 0.96, recall: 0.96, precision: 0.96, f_beta: 0.96
train: step: 2498, loss: 0.020999876782298088, acc: 0.98, recall: 0.98, precision: 0.9807692307692308, f_beta: 0.9799919967987194
train: step: 2499, loss: 0.07413078099489212, acc: 0.92, recall: 0.9199999999999999, precision: 0.9261363636363636, f_beta: 0.9197109594540345
train: step: 2500, loss: 0.04320661723613739, acc: 0.94, recall: 0.94, precision: 0.9407051282051282, f_beta: 0.9399759903961584
train: step: 2501, loss: 0.052322521805763245, acc: 0.94, recall: 0.94, precision: 0.94, f_beta: 0.94
train: step: 2502, loss: 0.06954774260520935, acc: 0.9, recall: 0.8999999999999999, precision: 0.9058441558441559, f_beta: 0.8996386993175431
train: step: 2503, loss: 0.014025688171386719, acc: 0.99, recall: 0.99, precision: 0.9901960784313726, f_beta: 0.98999899989999
train: step: 2504, loss: 0.08129402250051498, acc: 0.91, recall: 0.9099999999999999, precision: 0.9114813327980731, f_beta: 0.9099189270343309
train: step: 2505, loss: 0.040283069014549255, acc: 0.96, recall: 0.96, precision: 0.9607371794871795, f_beta: 0.959983993597439
train: step: 2506, loss: 0.04092969745397568, acc: 0.96, recall: 0.96, precision: 0.9607371794871795, f_beta: 0.959983993597439
train: step: 2507, loss: 0.021774621680378914, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 2508, loss: 0.02514122985303402, acc: 0.97, recall: 0.97, precision: 0.9716981132075472, f_beta: 0.9699729756781104
train: step: 2509, loss: 0.07870812714099884, acc: 0.9, recall: 0.8999999999999999, precision: 0.9025764895330113, f_beta: 0.8998397435897436
train: step: 2510, loss: 0.035383690148591995, acc: 0.95, recall: 0.95, precision: 0.9501800720288115, f_beta: 0.9499949994999499
train: step: 2511, loss: 0.030194919556379318, acc: 0.97, recall: 0.97, precision: 0.9716981132075472, f_beta: 0.9699729756781104
train: step: 2512, loss: 0.0884452611207962, acc: 0.87, recall: 0.87, precision: 0.8713368125250903, f_beta: 0.8698828946051447
train: step: 2513, loss: 0.06467237323522568, acc: 0.93, recall: 0.9299999999999999, precision: 0.9315535929345644, f_beta: 0.9299369432489241
train: step: 2514, loss: 0.030234914273023605, acc: 0.96, recall: 0.96, precision: 0.9607371794871795, f_beta: 0.959983993597439
train: step: 2515, loss: 0.06538768857717514, acc: 0.91, recall: 0.91, precision: 0.9101640656262505, f_beta: 0.90999099909991
train: step: 2516, loss: 0.03535657748579979, acc: 0.96, recall: 0.96, precision: 0.9607371794871795, f_beta: 0.959983993597439
train: step: 2517, loss: 0.03999871388077736, acc: 0.95, recall: 0.95, precision: 0.9516258530710557, f_beta: 0.949954959463517
train: step: 2518, loss: 0.05866727977991104, acc: 0.92, recall: 0.9199999999999999, precision: 0.9227053140096618, f_beta: 0.9198717948717948
train: step: 2519, loss: 0.03691621124744415, acc: 0.96, recall: 0.96, precision: 0.96, f_beta: 0.96
train: step: 2520, loss: 0.05869900807738304, acc: 0.94, recall: 0.94, precision: 0.9428341384863124, f_beta: 0.9399038461538463
train: step: 2521, loss: 0.00840771198272705, acc: 0.98, recall: 0.98, precision: 0.9807692307692308, f_beta: 0.9799919967987194
train: step: 2522, loss: 0.09386099129915237, acc: 0.86, recall: 0.8600000000000001, precision: 0.8652597402597402, f_beta: 0.8594941790445605
train: step: 2523, loss: 0.029253629967570305, acc: 0.98, recall: 0.98, precision: 0.98, f_beta: 0.98
train: step: 2524, loss: 0.061274319887161255, acc: 0.91, recall: 0.91, precision: 0.9101640656262505, f_beta: 0.90999099909991
train: step: 2525, loss: 0.039947062730789185, acc: 0.96, recall: 0.96, precision: 0.96, f_beta: 0.96
train: step: 2526, loss: 0.04222622886300087, acc: 0.93, recall: 0.9299999999999999, precision: 0.930172068827531, f_beta: 0.9299929992999301
train: step: 2527, loss: 0.028442025184631348, acc: 0.96, recall: 0.96, precision: 0.96, f_beta: 0.96
train: step: 2528, loss: 0.049979761242866516, acc: 0.94, recall: 0.94, precision: 0.94, f_beta: 0.94
train: step: 2529, loss: 0.03807732090353966, acc: 0.94, recall: 0.94, precision: 0.9428341384863124, f_beta: 0.9399038461538463
train: step: 2530, loss: 0.040983252227306366, acc: 0.96, recall: 0.96, precision: 0.9607371794871795, f_beta: 0.959983993597439
train: step: 2531, loss: 0.060615476220846176, acc: 0.92, recall: 0.92, precision: 0.92, f_beta: 0.92
train: step: 2532, loss: 0.03431569039821625, acc: 0.96, recall: 0.96, precision: 0.9607371794871795, f_beta: 0.959983993597439
train: step: 2533, loss: 0.051137905567884445, acc: 0.94, recall: 0.94, precision: 0.9464285714285714, f_beta: 0.939783219590526
train: step: 2534, loss: 0.051740940660238266, acc: 0.94, recall: 0.94, precision: 0.9407051282051282, f_beta: 0.9399759903961584
train: step: 2535, loss: 0.022792406380176544, acc: 0.98, recall: 0.98, precision: 0.9807692307692308, f_beta: 0.9799919967987194
train: step: 2536, loss: 0.02993907406926155, acc: 0.96, recall: 0.96, precision: 0.96, f_beta: 0.96
train: step: 2537, loss: 0.03389980271458626, acc: 0.96, recall: 0.96, precision: 0.96, f_beta: 0.96
train: step: 2538, loss: 0.04209749773144722, acc: 0.96, recall: 0.96, precision: 0.962962962962963, f_beta: 0.9599358974358974
train: step: 2539, loss: 0.032032400369644165, acc: 0.96, recall: 0.96, precision: 0.962962962962963, f_beta: 0.9599358974358974
train: step: 2540, loss: 0.055011823773384094, acc: 0.92, recall: 0.9199999999999999, precision: 0.9206730769230769, f_beta: 0.919967987194878
train: step: 2541, loss: 0.04768083617091179, acc: 0.94, recall: 0.94, precision: 0.9407051282051282, f_beta: 0.9399759903961584
train: step: 2542, loss: 0.04630928486585617, acc: 0.96, recall: 0.96, precision: 0.9607371794871795, f_beta: 0.959983993597439
train: step: 2543, loss: 0.0623500794172287, acc: 0.91, recall: 0.91, precision: 0.9101640656262505, f_beta: 0.90999099909991
train: step: 2544, loss: 0.07788621634244919, acc: 0.89, recall: 0.8899999999999999, precision: 0.8977968176254589, f_beta: 0.8894583458948848
train: step: 2545, loss: 0.05408259481191635, acc: 0.92, recall: 0.9199999999999999, precision: 0.9227053140096618, f_beta: 0.9198717948717948
train: step: 2546, loss: 0.01886800490319729, acc: 0.98, recall: 0.98, precision: 0.98, f_beta: 0.98
train: step: 2547, loss: 0.055912259966135025, acc: 0.92, recall: 0.9199999999999999, precision: 0.9206730769230769, f_beta: 0.919967987194878
train: step: 2548, loss: 0.03911479562520981, acc: 0.95, recall: 0.95, precision: 0.9545454545454546, f_beta: 0.949874686716792
train: step: 2549, loss: 0.01562783680856228, acc: 0.98, recall: 0.98, precision: 0.9807692307692308, f_beta: 0.9799919967987194
train: step: 2550, loss: 0.056504860520362854, acc: 0.91, recall: 0.9099999999999999, precision: 0.9114813327980731, f_beta: 0.9099189270343309
train: step: 2551, loss: 0.06259176880121231, acc: 0.9, recall: 0.8999999999999999, precision: 0.9025764895330113, f_beta: 0.8998397435897436
train: step: 2552, loss: 0.0368884839117527, acc: 0.95, recall: 0.95, precision: 0.9516258530710557, f_beta: 0.949954959463517
train: step: 2553, loss: 0.01572575978934765, acc: 0.98, recall: 0.98, precision: 0.98, f_beta: 0.98
train: step: 2554, loss: 0.01398954913020134, acc: 0.99, recall: 0.99, precision: 0.9901960784313726, f_beta: 0.98999899989999
train: step: 2555, loss: 0.045045364648103714, acc: 0.93, recall: 0.9299999999999999, precision: 0.930172068827531, f_beta: 0.9299929992999301
train: step: 2556, loss: 0.04895935207605362, acc: 0.93, recall: 0.9299999999999999, precision: 0.9315535929345644, f_beta: 0.9299369432489241
train: step: 2557, loss: 0.036239661276340485, acc: 0.97, recall: 0.97, precision: 0.9716981132075472, f_beta: 0.9699729756781104
train: step: 2558, loss: 0.03329043090343475, acc: 0.95, recall: 0.95, precision: 0.9516258530710557, f_beta: 0.949954959463517
train: step: 2559, loss: 0.047453366219997406, acc: 0.94, recall: 0.94, precision: 0.9407051282051282, f_beta: 0.9399759903961584
train: step: 2560, loss: 0.024779167026281357, acc: 0.97, recall: 0.97, precision: 0.9716981132075472, f_beta: 0.9699729756781104
train: step: 2561, loss: 0.029946517199277878, acc: 0.95, recall: 0.95, precision: 0.9516258530710557, f_beta: 0.949954959463517
train: step: 2562, loss: 0.06081291288137436, acc: 0.92, recall: 0.9199999999999999, precision: 0.9206730769230769, f_beta: 0.919967987194878
train: step: 2563, loss: 0.017371350899338722, acc: 0.97, recall: 0.97, precision: 0.9716981132075472, f_beta: 0.9699729756781104
train: step: 2564, loss: 0.058307189494371414, acc: 0.91, recall: 0.91, precision: 0.9101640656262505, f_beta: 0.90999099909991
train: step: 2565, loss: 0.02812042646110058, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 2566, loss: 0.033826738595962524, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 2567, loss: 0.06607417017221451, acc: 0.93, recall: 0.9299999999999999, precision: 0.9343434343434343, f_beta: 0.9298245614035088
train: step: 2568, loss: 0.02136615291237831, acc: 0.98, recall: 0.98, precision: 0.9807692307692308, f_beta: 0.9799919967987194
train: step: 2569, loss: 0.03185711428523064, acc: 0.96, recall: 0.96, precision: 0.96, f_beta: 0.96
train: step: 2570, loss: 0.0634702667593956, acc: 0.92, recall: 0.9199999999999999, precision: 0.9206730769230769, f_beta: 0.919967987194878
train: step: 2571, loss: 0.028821278363466263, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 2572, loss: 0.048501815646886826, acc: 0.94, recall: 0.94, precision: 0.9407051282051282, f_beta: 0.9399759903961584
train: step: 2573, loss: 0.07972349226474762, acc: 0.9, recall: 0.9, precision: 0.9006410256410255, f_beta: 0.8999599839935974
train: step: 2574, loss: 0.04068388044834137, acc: 0.94, recall: 0.94, precision: 0.94, f_beta: 0.94
train: step: 2575, loss: 0.08036696165800095, acc: 0.89, recall: 0.89, precision: 0.89015606242497, f_beta: 0.88998899889989
train: step: 2576, loss: 0.030657660216093063, acc: 0.96, recall: 0.96, precision: 0.96, f_beta: 0.96
train: step: 2577, loss: 0.04459933191537857, acc: 0.94, recall: 0.94, precision: 0.9428341384863124, f_beta: 0.9399038461538463
train: step: 2578, loss: 0.03532511740922928, acc: 0.94, recall: 0.94, precision: 0.9407051282051282, f_beta: 0.9399759903961584
train: step: 2579, loss: 0.03820205479860306, acc: 0.96, recall: 0.96, precision: 0.96, f_beta: 0.96
train: step: 2580, loss: 0.02552235685288906, acc: 0.96, recall: 0.96, precision: 0.962962962962963, f_beta: 0.9599358974358974
train: step: 2581, loss: 0.01427136454731226, acc: 0.98, recall: 0.98, precision: 0.98, f_beta: 0.98
train: step: 2582, loss: 0.05519353970885277, acc: 0.93, recall: 0.9299999999999999, precision: 0.9385964912280702, f_beta: 0.9296553110240177
train: step: 2583, loss: 0.004251094534993172, acc: 1.0, recall: 1.0, precision: 1.0, f_beta: 1.0
train: step: 2584, loss: 0.02755505032837391, acc: 0.97, recall: 0.97, precision: 0.9716981132075472, f_beta: 0.9699729756781104
train: step: 2585, loss: 0.040259670466184616, acc: 0.94, recall: 0.94, precision: 0.94, f_beta: 0.94
train: step: 2586, loss: 0.05224810540676117, acc: 0.93, recall: 0.9299999999999999, precision: 0.9343434343434343, f_beta: 0.9298245614035088
train: step: 2587, loss: 0.06960977613925934, acc: 0.92, recall: 0.9199999999999999, precision: 0.9206730769230769, f_beta: 0.919967987194878
train: step: 2588, loss: 0.07867519557476044, acc: 0.9, recall: 0.9, precision: 0.9, f_beta: 0.9
train: step: 2589, loss: 0.06138002499938011, acc: 0.91, recall: 0.9099999999999999, precision: 0.9141414141414141, f_beta: 0.9097744360902256
train: step: 2590, loss: 0.04856785386800766, acc: 0.94, recall: 0.94, precision: 0.9428341384863124, f_beta: 0.9399038461538463
train: step: 2591, loss: 0.031000971794128418, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 2592, loss: 0.03187904506921768, acc: 0.96, recall: 0.96, precision: 0.9607371794871795, f_beta: 0.959983993597439
train: step: 2593, loss: 0.039354823529720306, acc: 0.93, recall: 0.9299999999999999, precision: 0.930172068827531, f_beta: 0.9299929992999301
train: step: 2594, loss: 0.04620834439992905, acc: 0.95, recall: 0.95, precision: 0.9501800720288115, f_beta: 0.9499949994999499
train: step: 2595, loss: 0.042415160685777664, acc: 0.96, recall: 0.96, precision: 0.96, f_beta: 0.96
train: step: 2596, loss: 0.03861749917268753, acc: 0.95, recall: 0.95, precision: 0.9516258530710557, f_beta: 0.949954959463517
train: step: 2597, loss: 0.011776864528656006, acc: 0.99, recall: 0.99, precision: 0.9901960784313726, f_beta: 0.98999899989999
train: step: 2598, loss: 0.04027300700545311, acc: 0.95, recall: 0.95, precision: 0.9516258530710557, f_beta: 0.949954959463517
train: step: 2599, loss: 0.016044816002249718, acc: 0.98, recall: 0.98, precision: 0.9807692307692308, f_beta: 0.9799919967987194
train: step: 2600, loss: 0.06939896941184998, acc: 0.89, recall: 0.89, precision: 0.8914090726615818, f_beta: 0.8899009108197378
train: step: 2601, loss: 0.027695350348949432, acc: 0.98, recall: 0.98, precision: 0.98, f_beta: 0.98
train: step: 2602, loss: 0.025421306490898132, acc: 0.96, recall: 0.96, precision: 0.962962962962963, f_beta: 0.9599358974358974
train: step: 2603, loss: 0.023389797657728195, acc: 0.97, recall: 0.97, precision: 0.9716981132075472, f_beta: 0.9699729756781104
train: step: 2604, loss: 0.06011023372411728, acc: 0.91, recall: 0.9099999999999999, precision: 0.9141414141414141, f_beta: 0.9097744360902256
train: step: 2605, loss: 0.036367688328027725, acc: 0.96, recall: 0.96, precision: 0.96, f_beta: 0.96
train: step: 2606, loss: 0.01368069276213646, acc: 0.98, recall: 0.98, precision: 0.98, f_beta: 0.98
train: step: 2607, loss: 0.05187216401100159, acc: 0.94, recall: 0.94, precision: 0.9407051282051282, f_beta: 0.9399759903961584
train: step: 2608, loss: 0.03457445278763771, acc: 0.96, recall: 0.96, precision: 0.96, f_beta: 0.96
train: step: 2609, loss: 0.011873706243932247, acc: 0.99, recall: 0.99, precision: 0.9901960784313726, f_beta: 0.98999899989999
train: step: 2610, loss: 0.02335957996547222, acc: 0.97, recall: 0.97, precision: 0.9716981132075472, f_beta: 0.9699729756781104
train: step: 2611, loss: 0.021166104823350906, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 2612, loss: 0.09354434907436371, acc: 0.9, recall: 0.8999999999999999, precision: 0.9025764895330113, f_beta: 0.8998397435897436
train: step: 2613, loss: 0.038511838763952255, acc: 0.95, recall: 0.95, precision: 0.9501800720288115, f_beta: 0.9499949994999499
train: step: 2614, loss: 0.057450953871011734, acc: 0.95, recall: 0.95, precision: 0.9516258530710557, f_beta: 0.949954959463517
train: step: 2615, loss: 0.03250603750348091, acc: 0.95, recall: 0.95, precision: 0.9501800720288115, f_beta: 0.9499949994999499
train: step: 2616, loss: 0.031854063272476196, acc: 0.97, recall: 0.97, precision: 0.9716981132075472, f_beta: 0.9699729756781104
train: step: 2617, loss: 0.040964070707559586, acc: 0.94, recall: 0.94, precision: 0.9407051282051282, f_beta: 0.9399759903961584
train: step: 2618, loss: 0.029112262651324272, acc: 0.96, recall: 0.96, precision: 0.9607371794871795, f_beta: 0.959983993597439
train: step: 2619, loss: 0.0316598080098629, acc: 0.96, recall: 0.96, precision: 0.96, f_beta: 0.96
train: step: 2620, loss: 0.023535633459687233, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 2621, loss: 0.05786224454641342, acc: 0.93, recall: 0.9299999999999999, precision: 0.930172068827531, f_beta: 0.9299929992999301
train: step: 2622, loss: 0.03890706226229668, acc: 0.96, recall: 0.96, precision: 0.962962962962963, f_beta: 0.9599358974358974
train: step: 2623, loss: 0.03926757723093033, acc: 0.94, recall: 0.94, precision: 0.94, f_beta: 0.94
train: step: 2624, loss: 0.09068308025598526, acc: 0.89, recall: 0.89, precision: 0.8914090726615818, f_beta: 0.8899009108197378
train: step: 2625, loss: 0.02873591147363186, acc: 0.98, recall: 0.98, precision: 0.98, f_beta: 0.98
train: step: 2626, loss: 0.05657307058572769, acc: 0.92, recall: 0.9199999999999999, precision: 0.9206730769230769, f_beta: 0.919967987194878
train: step: 2627, loss: 0.05344545841217041, acc: 0.94, recall: 0.94, precision: 0.9428341384863124, f_beta: 0.9399038461538463
train: step: 2628, loss: 0.019608035683631897, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 2629, loss: 0.04947802051901817, acc: 0.93, recall: 0.9299999999999999, precision: 0.930172068827531, f_beta: 0.9299929992999301
train: step: 2630, loss: 0.06086614727973938, acc: 0.94, recall: 0.94, precision: 0.9407051282051282, f_beta: 0.9399759903961584
train: step: 2631, loss: 0.04412034526467323, acc: 0.96, recall: 0.96, precision: 0.962962962962963, f_beta: 0.9599358974358974
train: step: 2632, loss: 0.03271283954381943, acc: 0.96, recall: 0.96, precision: 0.9607371794871795, f_beta: 0.959983993597439
train: step: 2633, loss: 0.08504471927881241, acc: 0.89, recall: 0.89, precision: 0.8914090726615818, f_beta: 0.8899009108197378
train: step: 2634, loss: 0.02765755169093609, acc: 0.96, recall: 0.96, precision: 0.962962962962963, f_beta: 0.9599358974358974
train: step: 2635, loss: 0.05555048584938049, acc: 0.93, recall: 0.9299999999999999, precision: 0.9315535929345644, f_beta: 0.9299369432489241
train: step: 2636, loss: 0.028463440015912056, acc: 0.97, recall: 0.97, precision: 0.9716981132075472, f_beta: 0.9699729756781104
train: step: 2637, loss: 0.020678119733929634, acc: 0.98, recall: 0.98, precision: 0.98, f_beta: 0.98
train: step: 2638, loss: 0.0508611835539341, acc: 0.93, recall: 0.9299999999999999, precision: 0.930172068827531, f_beta: 0.9299929992999301
train: step: 2639, loss: 0.026174306869506836, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 2640, loss: 0.026874447241425514, acc: 0.96, recall: 0.96, precision: 0.96, f_beta: 0.96
train: step: 2641, loss: 0.06980810314416885, acc: 0.93, recall: 0.9299999999999999, precision: 0.930172068827531, f_beta: 0.9299929992999301
train: step: 2642, loss: 0.0586056262254715, acc: 0.93, recall: 0.9299999999999999, precision: 0.930172068827531, f_beta: 0.9299929992999301
train: step: 2643, loss: 0.02895946055650711, acc: 0.97, recall: 0.97, precision: 0.9716981132075472, f_beta: 0.9699729756781104
train: step: 2644, loss: 0.07573240250349045, acc: 0.91, recall: 0.91, precision: 0.9101640656262505, f_beta: 0.90999099909991
train: step: 2645, loss: 0.058489494025707245, acc: 0.94, recall: 0.94, precision: 0.9407051282051282, f_beta: 0.9399759903961584
train: step: 2646, loss: 0.028225421905517578, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 2647, loss: 0.04642321541905403, acc: 0.92, recall: 0.9199999999999999, precision: 0.9206730769230769, f_beta: 0.919967987194878
train: step: 2648, loss: 0.07177197188138962, acc: 0.91, recall: 0.9099999999999999, precision: 0.9181966544267646, f_beta: 0.9095568284594513
train: step: 2649, loss: 0.017659569159150124, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 2650, loss: 0.03723554313182831, acc: 0.93, recall: 0.9299999999999999, precision: 0.9315535929345644, f_beta: 0.9299369432489241
train: step: 2651, loss: 0.021183466538786888, acc: 0.98, recall: 0.98, precision: 0.98, f_beta: 0.98
train: step: 2652, loss: 0.033357784152030945, acc: 0.95, recall: 0.95, precision: 0.9501800720288115, f_beta: 0.9499949994999499
train: step: 2653, loss: 0.04636060819029808, acc: 0.93, recall: 0.9299999999999999, precision: 0.9315535929345644, f_beta: 0.9299369432489241
train: step: 2654, loss: 0.04543284326791763, acc: 0.94, recall: 0.94, precision: 0.9428341384863124, f_beta: 0.9399038461538463
train: step: 2655, loss: 0.013641283847391605, acc: 0.99, recall: 0.99, precision: 0.9901960784313726, f_beta: 0.98999899989999
train: step: 2656, loss: 0.029888972640037537, acc: 0.97, recall: 0.97, precision: 0.9716981132075472, f_beta: 0.9699729756781104
train: step: 2657, loss: 0.041229452937841415, acc: 0.95, recall: 0.95, precision: 0.9501800720288115, f_beta: 0.9499949994999499
train: step: 2658, loss: 0.042468663305044174, acc: 0.94, recall: 0.94, precision: 0.9407051282051282, f_beta: 0.9399759903961584
train: step: 2659, loss: 0.03471635654568672, acc: 0.95, recall: 0.95, precision: 0.9501800720288115, f_beta: 0.9499949994999499
train: step: 2660, loss: 0.024522552266716957, acc: 0.96, recall: 0.96, precision: 0.962962962962963, f_beta: 0.9599358974358974
train: step: 2661, loss: 0.03116961568593979, acc: 0.96, recall: 0.96, precision: 0.9607371794871795, f_beta: 0.959983993597439
train: step: 2662, loss: 0.02561437152326107, acc: 0.97, recall: 0.97, precision: 0.9716981132075472, f_beta: 0.9699729756781104
train: step: 2663, loss: 0.012773538008332253, acc: 0.98, recall: 0.98, precision: 0.9807692307692308, f_beta: 0.9799919967987194
train: step: 2664, loss: 0.04880571365356445, acc: 0.94, recall: 0.94, precision: 0.94, f_beta: 0.94
train: step: 2665, loss: 0.010593943297863007, acc: 0.99, recall: 0.99, precision: 0.9901960784313726, f_beta: 0.98999899989999
train: step: 2666, loss: 0.06928960978984833, acc: 0.91, recall: 0.91, precision: 0.9101640656262505, f_beta: 0.90999099909991
train: step: 2667, loss: 0.03315316513180733, acc: 0.95, recall: 0.95, precision: 0.9501800720288115, f_beta: 0.9499949994999499
train: step: 2668, loss: 0.02551330253481865, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 2669, loss: 0.05882745608687401, acc: 0.91, recall: 0.91, precision: 0.9101640656262505, f_beta: 0.90999099909991
train: step: 2670, loss: 0.06503048539161682, acc: 0.92, recall: 0.9199999999999999, precision: 0.9227053140096618, f_beta: 0.9198717948717948
train: step: 2671, loss: 0.019989587366580963, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 2672, loss: 0.02736811898648739, acc: 0.96, recall: 0.96, precision: 0.9607371794871795, f_beta: 0.959983993597439
train: step: 2673, loss: 0.057447079569101334, acc: 0.93, recall: 0.9299999999999999, precision: 0.930172068827531, f_beta: 0.9299929992999301
train: step: 2674, loss: 0.02179465815424919, acc: 0.96, recall: 0.96, precision: 0.96, f_beta: 0.96
train: step: 2675, loss: 0.08118942379951477, acc: 0.91, recall: 0.9099999999999999, precision: 0.9114813327980731, f_beta: 0.9099189270343309
train: step: 2676, loss: 0.06433935463428497, acc: 0.92, recall: 0.92, precision: 0.92, f_beta: 0.92
train: step: 2677, loss: 0.04703202098608017, acc: 0.95, recall: 0.95, precision: 0.9501800720288115, f_beta: 0.9499949994999499
train: step: 2678, loss: 0.015745442360639572, acc: 0.98, recall: 0.98, precision: 0.98, f_beta: 0.98
train: step: 2679, loss: 0.06325441598892212, acc: 0.92, recall: 0.9199999999999999, precision: 0.9227053140096618, f_beta: 0.9198717948717948
train: step: 2680, loss: 0.0504346564412117, acc: 0.94, recall: 0.94, precision: 0.9407051282051282, f_beta: 0.9399759903961584
train: step: 2681, loss: 0.030045801773667336, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 2682, loss: 0.014307880774140358, acc: 0.99, recall: 0.99, precision: 0.9901960784313726, f_beta: 0.98999899989999
train: step: 2683, loss: 0.06592284888029099, acc: 0.93, recall: 0.9299999999999999, precision: 0.930172068827531, f_beta: 0.9299929992999301
train: step: 2684, loss: 0.03475959599018097, acc: 0.95, recall: 0.95, precision: 0.9501800720288115, f_beta: 0.9499949994999499
train: step: 2685, loss: 0.035672642290592194, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 2686, loss: 0.06260102987289429, acc: 0.93, recall: 0.9299999999999999, precision: 0.9315535929345644, f_beta: 0.9299369432489241
train: step: 2687, loss: 0.04404573515057564, acc: 0.95, recall: 0.95, precision: 0.9501800720288115, f_beta: 0.9499949994999499
train: step: 2688, loss: 0.03277474269270897, acc: 0.95, recall: 0.95, precision: 0.9516258530710557, f_beta: 0.949954959463517
train: step: 2689, loss: 0.038313232362270355, acc: 0.95, recall: 0.95, precision: 0.9501800720288115, f_beta: 0.9499949994999499
train: step: 2690, loss: 0.014217612333595753, acc: 0.98, recall: 0.98, precision: 0.98, f_beta: 0.98
train: step: 2691, loss: 0.04900430515408516, acc: 0.94, recall: 0.94, precision: 0.9428341384863124, f_beta: 0.9399038461538463
train: step: 2692, loss: 0.021716579794883728, acc: 0.97, recall: 0.97, precision: 0.9716981132075472, f_beta: 0.9699729756781104
train: step: 2693, loss: 0.046202994883060455, acc: 0.94, recall: 0.94, precision: 0.9428341384863124, f_beta: 0.9399038461538463
train: step: 2694, loss: 0.019177155569195747, acc: 0.98, recall: 0.98, precision: 0.9807692307692308, f_beta: 0.9799919967987194
train: step: 2695, loss: 0.04353460296988487, acc: 0.95, recall: 0.95, precision: 0.9501800720288115, f_beta: 0.9499949994999499
train: step: 2696, loss: 0.006116781383752823, acc: 0.99, recall: 0.99, precision: 0.9901960784313726, f_beta: 0.98999899989999
train: step: 2697, loss: 0.018457628786563873, acc: 0.97, recall: 0.97, precision: 0.9716981132075472, f_beta: 0.9699729756781104
train: step: 2698, loss: 0.04498878866434097, acc: 0.94, recall: 0.94, precision: 0.94, f_beta: 0.94
train: step: 2699, loss: 0.01882942207157612, acc: 0.99, recall: 0.99, precision: 0.9901960784313726, f_beta: 0.98999899989999
train: step: 2700, loss: 0.032710038125514984, acc: 0.95, recall: 0.95, precision: 0.9501800720288115, f_beta: 0.9499949994999499
train: step: 2701, loss: 0.03891787678003311, acc: 0.94, recall: 0.94, precision: 0.9407051282051282, f_beta: 0.9399759903961584
train: step: 2702, loss: 0.03416533023118973, acc: 0.96, recall: 0.96, precision: 0.962962962962963, f_beta: 0.9599358974358974
train: step: 2703, loss: 0.03818245232105255, acc: 0.96, recall: 0.96, precision: 0.9607371794871795, f_beta: 0.959983993597439
train: step: 2704, loss: 0.028593488037586212, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 2705, loss: 0.01751837134361267, acc: 0.97, recall: 0.97, precision: 0.9716981132075472, f_beta: 0.9699729756781104
train: step: 2706, loss: 0.03851211816072464, acc: 0.95, recall: 0.95, precision: 0.9516258530710557, f_beta: 0.949954959463517
train: step: 2707, loss: 0.014506323263049126, acc: 0.99, recall: 0.99, precision: 0.9901960784313726, f_beta: 0.98999899989999
train: step: 2708, loss: 0.028878798708319664, acc: 0.95, recall: 0.95, precision: 0.9501800720288115, f_beta: 0.9499949994999499
train: step: 2709, loss: 0.019009524956345558, acc: 0.98, recall: 0.98, precision: 0.98, f_beta: 0.98
train: step: 2710, loss: 0.024611586704850197, acc: 0.98, recall: 0.98, precision: 0.9807692307692308, f_beta: 0.9799919967987194
train: step: 2711, loss: 0.042709674686193466, acc: 0.93, recall: 0.9299999999999999, precision: 0.9315535929345644, f_beta: 0.9299369432489241
train: step: 2712, loss: 0.04211507737636566, acc: 0.96, recall: 0.96, precision: 0.9607371794871795, f_beta: 0.959983993597439
train: step: 2713, loss: 0.016017626971006393, acc: 0.98, recall: 0.98, precision: 0.98, f_beta: 0.98
train: step: 2714, loss: 0.031707316637039185, acc: 0.97, recall: 0.97, precision: 0.9716981132075472, f_beta: 0.9699729756781104
train: step: 2715, loss: 0.015071282163262367, acc: 0.99, recall: 0.99, precision: 0.9901960784313726, f_beta: 0.98999899989999
train: step: 2716, loss: 0.0659124106168747, acc: 0.93, recall: 0.9299999999999999, precision: 0.930172068827531, f_beta: 0.9299929992999301
train: step: 2717, loss: 0.03322141245007515, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 2718, loss: 0.024059772491455078, acc: 0.97, recall: 0.97, precision: 0.9716981132075472, f_beta: 0.9699729756781104
train: step: 2719, loss: 0.05799363926053047, acc: 0.92, recall: 0.9199999999999999, precision: 0.9206730769230769, f_beta: 0.919967987194878
train: step: 2720, loss: 0.06527586281299591, acc: 0.91, recall: 0.91, precision: 0.9101640656262505, f_beta: 0.90999099909991
train: step: 2721, loss: 0.021150579676032066, acc: 0.98, recall: 0.98, precision: 0.9807692307692308, f_beta: 0.9799919967987194
train: step: 2722, loss: 0.04460718110203743, acc: 0.96, recall: 0.96, precision: 0.96, f_beta: 0.96
train: step: 2723, loss: 0.050112999975681305, acc: 0.92, recall: 0.9199999999999999, precision: 0.9261363636363636, f_beta: 0.9197109594540345
train: step: 2724, loss: 0.06742432713508606, acc: 0.93, recall: 0.9299999999999999, precision: 0.9343434343434343, f_beta: 0.9298245614035088
train: step: 2725, loss: 0.059263505041599274, acc: 0.93, recall: 0.9299999999999999, precision: 0.930172068827531, f_beta: 0.9299929992999301
train: step: 2726, loss: 0.02942858450114727, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 2727, loss: 0.03825587406754494, acc: 0.95, recall: 0.95, precision: 0.9501800720288115, f_beta: 0.9499949994999499
train: step: 2728, loss: 0.02998708188533783, acc: 0.96, recall: 0.96, precision: 0.9607371794871795, f_beta: 0.959983993597439
train: step: 2729, loss: 0.0493340864777565, acc: 0.94, recall: 0.94, precision: 0.9428341384863124, f_beta: 0.9399038461538463
train: step: 2730, loss: 0.053523626178503036, acc: 0.94, recall: 0.94, precision: 0.9428341384863124, f_beta: 0.9399038461538463
train: step: 2731, loss: 0.053309258073568344, acc: 0.94, recall: 0.94, precision: 0.9428341384863124, f_beta: 0.9399038461538463
train: step: 2732, loss: 0.01305045560002327, acc: 0.99, recall: 0.99, precision: 0.9901960784313726, f_beta: 0.98999899989999
train: step: 2733, loss: 0.042934808880090714, acc: 0.95, recall: 0.95, precision: 0.9501800720288115, f_beta: 0.9499949994999499
train: step: 2734, loss: 0.032210372388362885, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 2735, loss: 0.04100517928600311, acc: 0.95, recall: 0.95, precision: 0.9501800720288115, f_beta: 0.9499949994999499
train: step: 2736, loss: 0.02324492856860161, acc: 0.98, recall: 0.98, precision: 0.98, f_beta: 0.98
train: step: 2737, loss: 0.08498863875865936, acc: 0.9, recall: 0.8999999999999999, precision: 0.9025764895330113, f_beta: 0.8998397435897436
train: step: 2738, loss: 0.019063521176576614, acc: 0.98, recall: 0.98, precision: 0.9807692307692308, f_beta: 0.9799919967987194
train: step: 2739, loss: 0.023236118257045746, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 2740, loss: 0.026365069672465324, acc: 0.97, recall: 0.97, precision: 0.9716981132075472, f_beta: 0.9699729756781104
train: step: 2741, loss: 0.04918147251009941, acc: 0.95, recall: 0.95, precision: 0.9501800720288115, f_beta: 0.9499949994999499
train: step: 2742, loss: 0.04429822787642479, acc: 0.94, recall: 0.94, precision: 0.9428341384863124, f_beta: 0.9399038461538463
train: step: 2743, loss: 0.05329672992229462, acc: 0.94, recall: 0.94, precision: 0.9407051282051282, f_beta: 0.9399759903961584
train: step: 2744, loss: 0.09649400413036346, acc: 0.87, recall: 0.87, precision: 0.8773969808241534, f_beta: 0.8693598633303186
train: step: 2745, loss: 0.050832442939281464, acc: 0.93, recall: 0.9299999999999999, precision: 0.930172068827531, f_beta: 0.9299929992999301
train: step: 2746, loss: 0.03622424229979515, acc: 0.96, recall: 0.96, precision: 0.96, f_beta: 0.96
train: step: 2747, loss: 0.10366258770227432, acc: 0.85, recall: 0.85, precision: 0.8617197188921042, f_beta: 0.8487750781328762
train: step: 2748, loss: 0.027005134150385857, acc: 0.98, recall: 0.98, precision: 0.98, f_beta: 0.98
train: step: 2749, loss: 0.03810235857963562, acc: 0.96, recall: 0.96, precision: 0.96, f_beta: 0.96
train: step: 2750, loss: 0.026721687987446785, acc: 0.96, recall: 0.96, precision: 0.96, f_beta: 0.96
train: step: 2751, loss: 0.025632673874497414, acc: 0.97, recall: 0.97, precision: 0.9716981132075472, f_beta: 0.9699729756781104
train: step: 2752, loss: 0.06669668853282928, acc: 0.91, recall: 0.9099999999999999, precision: 0.9141414141414141, f_beta: 0.9097744360902256
train: step: 2753, loss: 0.04392896592617035, acc: 0.94, recall: 0.94, precision: 0.9428341384863124, f_beta: 0.9399038461538463
train: step: 2754, loss: 0.03333936259150505, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 2755, loss: 0.08764190971851349, acc: 0.91, recall: 0.91, precision: 0.9101640656262505, f_beta: 0.90999099909991
train: step: 2756, loss: 0.029344450682401657, acc: 0.97, recall: 0.97, precision: 0.9716981132075472, f_beta: 0.9699729756781104
train: step: 2757, loss: 0.0682574212551117, acc: 0.91, recall: 0.9099999999999999, precision: 0.9181966544267646, f_beta: 0.9095568284594513
train: step: 2758, loss: 0.05240223929286003, acc: 0.93, recall: 0.9299999999999999, precision: 0.930172068827531, f_beta: 0.9299929992999301
train: step: 2759, loss: 0.010675804689526558, acc: 0.98, recall: 0.98, precision: 0.98, f_beta: 0.98
train: step: 2760, loss: 0.03524083271622658, acc: 0.96, recall: 0.96, precision: 0.9607371794871795, f_beta: 0.959983993597439
train: step: 2761, loss: 0.04686596989631653, acc: 0.92, recall: 0.9199999999999999, precision: 0.9261363636363636, f_beta: 0.9197109594540345
train: step: 2762, loss: 0.0783623680472374, acc: 0.91, recall: 0.9099999999999999, precision: 0.9141414141414141, f_beta: 0.9097744360902256
train: step: 2763, loss: 0.03731595352292061, acc: 0.95, recall: 0.95, precision: 0.9501800720288115, f_beta: 0.9499949994999499
train: step: 2764, loss: 0.032988280057907104, acc: 0.96, recall: 0.96, precision: 0.96, f_beta: 0.96
train: step: 2765, loss: 0.018332654610276222, acc: 0.98, recall: 0.98, precision: 0.98, f_beta: 0.98
train: step: 2766, loss: 0.0308011956512928, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 2767, loss: 0.015514888800680637, acc: 0.99, recall: 0.99, precision: 0.9901960784313726, f_beta: 0.98999899989999
train: step: 2768, loss: 0.055654898285865784, acc: 0.94, recall: 0.94, precision: 0.9464285714285714, f_beta: 0.939783219590526
train: step: 2769, loss: 0.0126589210703969, acc: 0.98, recall: 0.98, precision: 0.98, f_beta: 0.98
train: step: 2770, loss: 0.020351219922304153, acc: 0.98, recall: 0.98, precision: 0.98, f_beta: 0.98
train: step: 2771, loss: 0.044118572026491165, acc: 0.96, recall: 0.96, precision: 0.9607371794871795, f_beta: 0.959983993597439
train: step: 2772, loss: 0.044356171041727066, acc: 0.95, recall: 0.95, precision: 0.9501800720288115, f_beta: 0.9499949994999499
train: step: 2773, loss: 0.06326428055763245, acc: 0.91, recall: 0.9099999999999999, precision: 0.9114813327980731, f_beta: 0.9099189270343309
train: step: 2774, loss: 0.046621594578027725, acc: 0.93, recall: 0.9299999999999999, precision: 0.930172068827531, f_beta: 0.9299929992999301
train: step: 2775, loss: 0.021813254803419113, acc: 0.96, recall: 0.96, precision: 0.9607371794871795, f_beta: 0.959983993597439
train: step: 2776, loss: 0.06319249421358109, acc: 0.93, recall: 0.9299999999999999, precision: 0.930172068827531, f_beta: 0.9299929992999301
train: step: 2777, loss: 0.038457952439785004, acc: 0.96, recall: 0.96, precision: 0.96, f_beta: 0.96
train: step: 2778, loss: 0.055883556604385376, acc: 0.93, recall: 0.9299999999999999, precision: 0.9315535929345644, f_beta: 0.9299369432489241
train: step: 2779, loss: 0.04935810714960098, acc: 0.94, recall: 0.94, precision: 0.94, f_beta: 0.94
train: step: 2780, loss: 0.04528357461094856, acc: 0.94, recall: 0.94, precision: 0.94, f_beta: 0.94
train: step: 2781, loss: 0.029965700581669807, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 2782, loss: 0.013203997164964676, acc: 0.99, recall: 0.99, precision: 0.9901960784313726, f_beta: 0.98999899989999
train: step: 2783, loss: 0.015390502288937569, acc: 0.99, recall: 0.99, precision: 0.9901960784313726, f_beta: 0.98999899989999
train: step: 2784, loss: 0.05024115741252899, acc: 0.93, recall: 0.9299999999999999, precision: 0.930172068827531, f_beta: 0.9299929992999301
train: step: 2785, loss: 0.033194392919540405, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 2786, loss: 0.03841041401028633, acc: 0.94, recall: 0.94, precision: 0.9428341384863124, f_beta: 0.9399038461538463
train: step: 2787, loss: 0.05469145625829697, acc: 0.92, recall: 0.92, precision: 0.92, f_beta: 0.92
train: step: 2788, loss: 0.041925687342882156, acc: 0.95, recall: 0.95, precision: 0.9501800720288115, f_beta: 0.9499949994999499
train: step: 2789, loss: 0.0238933227956295, acc: 0.96, recall: 0.96, precision: 0.96, f_beta: 0.96
train: step: 2790, loss: 0.047195807099342346, acc: 0.93, recall: 0.9299999999999999, precision: 0.930172068827531, f_beta: 0.9299929992999301
train: step: 2791, loss: 0.05408486723899841, acc: 0.94, recall: 0.94, precision: 0.9428341384863124, f_beta: 0.9399038461538463
train: step: 2792, loss: 0.03203115612268448, acc: 0.95, recall: 0.95, precision: 0.9516258530710557, f_beta: 0.949954959463517
train: step: 2793, loss: 0.014764425344765186, acc: 0.99, recall: 0.99, precision: 0.9901960784313726, f_beta: 0.98999899989999
train: step: 2794, loss: 0.02555031329393387, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 2795, loss: 0.037284597754478455, acc: 0.94, recall: 0.94, precision: 0.9407051282051282, f_beta: 0.9399759903961584
train: step: 2796, loss: 0.0663515254855156, acc: 0.9, recall: 0.9, precision: 0.9006410256410255, f_beta: 0.8999599839935974
train: step: 2797, loss: 0.023855295032262802, acc: 0.98, recall: 0.98, precision: 0.9807692307692308, f_beta: 0.9799919967987194
train: step: 2798, loss: 0.048745736479759216, acc: 0.95, recall: 0.95, precision: 0.9516258530710557, f_beta: 0.949954959463517
train: step: 2799, loss: 0.026940321549773216, acc: 0.95, recall: 0.95, precision: 0.9516258530710557, f_beta: 0.949954959463517
train: step: 2800, loss: 0.044273585081100464, acc: 0.94, recall: 0.94, precision: 0.9407051282051282, f_beta: 0.9399759903961584
train: step: 2801, loss: 0.023152051493525505, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 2802, loss: 0.017580289393663406, acc: 0.98, recall: 0.98, precision: 0.9807692307692308, f_beta: 0.9799919967987194
train: step: 2803, loss: 0.021317258477211, acc: 0.96, recall: 0.96, precision: 0.9607371794871795, f_beta: 0.959983993597439
train: step: 2804, loss: 0.037837494164705276, acc: 0.95, recall: 0.95, precision: 0.9501800720288115, f_beta: 0.9499949994999499
train: step: 2805, loss: 0.042425736784935, acc: 0.94, recall: 0.94, precision: 0.9428341384863124, f_beta: 0.9399038461538463
train: step: 2806, loss: 0.04192837327718735, acc: 0.92, recall: 0.9199999999999999, precision: 0.9261363636363636, f_beta: 0.9197109594540345
train: step: 2807, loss: 0.022816957905888557, acc: 0.98, recall: 0.98, precision: 0.9807692307692308, f_beta: 0.9799919967987194
train: step: 2808, loss: 0.014023084193468094, acc: 0.99, recall: 0.99, precision: 0.9901960784313726, f_beta: 0.98999899989999
train: step: 2809, loss: 0.019153006374835968, acc: 0.98, recall: 0.98, precision: 0.98, f_beta: 0.98
train: step: 2810, loss: 0.03799523785710335, acc: 0.96, recall: 0.96, precision: 0.9607371794871795, f_beta: 0.959983993597439
train: step: 2811, loss: 0.03015224263072014, acc: 0.96, recall: 0.96, precision: 0.9607371794871795, f_beta: 0.959983993597439
train: step: 2812, loss: 0.025886449962854385, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 2813, loss: 0.018445009365677834, acc: 0.98, recall: 0.98, precision: 0.9807692307692308, f_beta: 0.9799919967987194
train: step: 2814, loss: 0.0289698988199234, acc: 0.96, recall: 0.96, precision: 0.9607371794871795, f_beta: 0.959983993597439
train: step: 2815, loss: 0.0879879742860794, acc: 0.89, recall: 0.89, precision: 0.89015606242497, f_beta: 0.88998899889989
train: step: 2816, loss: 0.02483431063592434, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 2817, loss: 0.029198458418250084, acc: 0.96, recall: 0.96, precision: 0.96, f_beta: 0.96
train: step: 2818, loss: 0.037982452660799026, acc: 0.93, recall: 0.9299999999999999, precision: 0.9343434343434343, f_beta: 0.9298245614035088
train: step: 2819, loss: 0.043142542243003845, acc: 0.95, recall: 0.95, precision: 0.9501800720288115, f_beta: 0.9499949994999499
train: step: 2820, loss: 0.06215044483542442, acc: 0.93, recall: 0.9299999999999999, precision: 0.9343434343434343, f_beta: 0.9298245614035088
train: step: 2821, loss: 0.017613189294934273, acc: 0.98, recall: 0.98, precision: 0.98, f_beta: 0.98
train: step: 2822, loss: 0.008460752665996552, acc: 0.98, recall: 0.98, precision: 0.98, f_beta: 0.98
train: step: 2823, loss: 0.03836715593934059, acc: 0.95, recall: 0.95, precision: 0.9516258530710557, f_beta: 0.949954959463517
train: step: 2824, loss: 0.05049572139978409, acc: 0.94, recall: 0.94, precision: 0.9407051282051282, f_beta: 0.9399759903961584
train: step: 2825, loss: 0.04339761659502983, acc: 0.95, recall: 0.95, precision: 0.9516258530710557, f_beta: 0.949954959463517
train: step: 2826, loss: 0.04017793759703636, acc: 0.96, recall: 0.96, precision: 0.9607371794871795, f_beta: 0.959983993597439
train: step: 2827, loss: 0.024558085948228836, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 2828, loss: 0.051481522619724274, acc: 0.95, recall: 0.95, precision: 0.9516258530710557, f_beta: 0.949954959463517
train: step: 2829, loss: 0.032017529010772705, acc: 0.96, recall: 0.96, precision: 0.96, f_beta: 0.96
train: step: 2830, loss: 0.05457913875579834, acc: 0.93, recall: 0.9299999999999999, precision: 0.9315535929345644, f_beta: 0.9299369432489241
train: step: 2831, loss: 0.02396572194993496, acc: 0.98, recall: 0.98, precision: 0.98, f_beta: 0.98
train: step: 2832, loss: 0.06009214371442795, acc: 0.91, recall: 0.9099999999999999, precision: 0.9141414141414141, f_beta: 0.9097744360902256
train: step: 2833, loss: 0.06400913000106812, acc: 0.92, recall: 0.9199999999999999, precision: 0.9227053140096618, f_beta: 0.9198717948717948
train: step: 2834, loss: 0.04829978570342064, acc: 0.95, recall: 0.95, precision: 0.9501800720288115, f_beta: 0.9499949994999499
train: step: 2835, loss: 0.022915923967957497, acc: 0.97, recall: 0.97, precision: 0.9716981132075472, f_beta: 0.9699729756781104
train: step: 2836, loss: 0.055322226136922836, acc: 0.93, recall: 0.9299999999999999, precision: 0.9315535929345644, f_beta: 0.9299369432489241
train: step: 2837, loss: 0.032503966242074966, acc: 0.96, recall: 0.96, precision: 0.9607371794871795, f_beta: 0.959983993597439
train: step: 2838, loss: 0.05247758701443672, acc: 0.94, recall: 0.94, precision: 0.9407051282051282, f_beta: 0.9399759903961584
train: step: 2839, loss: 0.0836082398891449, acc: 0.9, recall: 0.9, precision: 0.9006410256410255, f_beta: 0.8999599839935974
train: step: 2840, loss: 0.03191855177283287, acc: 0.96, recall: 0.96, precision: 0.96, f_beta: 0.96
train: step: 2841, loss: 0.04730965569615364, acc: 0.95, recall: 0.95, precision: 0.9501800720288115, f_beta: 0.9499949994999499
train: step: 2842, loss: 0.026265781372785568, acc: 0.96, recall: 0.96, precision: 0.9607371794871795, f_beta: 0.959983993597439
train: step: 2843, loss: 0.02203480713069439, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 2844, loss: 0.004254980478435755, acc: 1.0, recall: 1.0, precision: 1.0, f_beta: 1.0
train: step: 2845, loss: 0.02771555446088314, acc: 0.94, recall: 0.94, precision: 0.94, f_beta: 0.94
train: step: 2846, loss: 0.02017446607351303, acc: 0.98, recall: 0.98, precision: 0.9807692307692308, f_beta: 0.9799919967987194
train: step: 2847, loss: 0.026743149384856224, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 2848, loss: 0.06803906708955765, acc: 0.92, recall: 0.9199999999999999, precision: 0.9227053140096618, f_beta: 0.9198717948717948
train: step: 2849, loss: 0.028954099863767624, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 2850, loss: 0.05617080628871918, acc: 0.94, recall: 0.94, precision: 0.94, f_beta: 0.94
train: step: 2851, loss: 0.03446190059185028, acc: 0.96, recall: 0.96, precision: 0.9607371794871795, f_beta: 0.959983993597439
train: step: 2852, loss: 0.0265826229006052, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 2853, loss: 0.02971557341516018, acc: 0.96, recall: 0.96, precision: 0.9607371794871795, f_beta: 0.959983993597439
train: step: 2854, loss: 0.03285709023475647, acc: 0.97, recall: 0.97, precision: 0.9716981132075472, f_beta: 0.9699729756781104
train: step: 2855, loss: 0.011100186966359615, acc: 0.99, recall: 0.99, precision: 0.9901960784313726, f_beta: 0.98999899989999
train: step: 2856, loss: 0.011949577368795872, acc: 0.99, recall: 0.99, precision: 0.9901960784313726, f_beta: 0.98999899989999
train: step: 2857, loss: 0.028060033917427063, acc: 0.96, recall: 0.96, precision: 0.962962962962963, f_beta: 0.9599358974358974
train: step: 2858, loss: 0.07147815078496933, acc: 0.91, recall: 0.9099999999999999, precision: 0.9114813327980731, f_beta: 0.9099189270343309
train: step: 2859, loss: 0.02865569293498993, acc: 0.96, recall: 0.96, precision: 0.9607371794871795, f_beta: 0.959983993597439
train: step: 2860, loss: 0.05194936320185661, acc: 0.95, recall: 0.95, precision: 0.9501800720288115, f_beta: 0.9499949994999499
train: step: 2861, loss: 0.027618827298283577, acc: 0.96, recall: 0.96, precision: 0.9607371794871795, f_beta: 0.959983993597439
train: step: 2862, loss: 0.019896147772669792, acc: 0.98, recall: 0.98, precision: 0.98, f_beta: 0.98
train: step: 2863, loss: 0.06127949804067612, acc: 0.92, recall: 0.92, precision: 0.92, f_beta: 0.92
train: step: 2864, loss: 0.03527860715985298, acc: 0.95, recall: 0.95, precision: 0.9501800720288115, f_beta: 0.9499949994999499
train: step: 2865, loss: 0.035797182470560074, acc: 0.95, recall: 0.95, precision: 0.9516258530710557, f_beta: 0.949954959463517
train: step: 2866, loss: 0.023747164756059647, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 2867, loss: 0.037484731525182724, acc: 0.96, recall: 0.96, precision: 0.96, f_beta: 0.96
train: step: 2868, loss: 0.057617854326963425, acc: 0.94, recall: 0.94, precision: 0.94, f_beta: 0.94
train: step: 2869, loss: 0.08161269873380661, acc: 0.91, recall: 0.9099999999999999, precision: 0.9114813327980731, f_beta: 0.9099189270343309
train: step: 2870, loss: 0.0370045006275177, acc: 0.96, recall: 0.96, precision: 0.9607371794871795, f_beta: 0.959983993597439
train: step: 2871, loss: 0.02278991974890232, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 2872, loss: 0.02982587367296219, acc: 0.97, recall: 0.97, precision: 0.9716981132075472, f_beta: 0.9699729756781104
train: step: 2873, loss: 0.03430980071425438, acc: 0.96, recall: 0.96, precision: 0.96, f_beta: 0.96
train: step: 2874, loss: 0.04173002392053604, acc: 0.95, recall: 0.95, precision: 0.9501800720288115, f_beta: 0.9499949994999499
train: step: 2875, loss: 0.04333112761378288, acc: 0.95, recall: 0.95, precision: 0.9501800720288115, f_beta: 0.9499949994999499
train: step: 2876, loss: 0.06665652245283127, acc: 0.92, recall: 0.9199999999999999, precision: 0.9227053140096618, f_beta: 0.9198717948717948
train: step: 2877, loss: 0.02270396798849106, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 2878, loss: 0.05537169426679611, acc: 0.92, recall: 0.9199999999999999, precision: 0.9206730769230769, f_beta: 0.919967987194878
train: step: 2879, loss: 0.03342043235898018, acc: 0.96, recall: 0.96, precision: 0.96, f_beta: 0.96
train: step: 2880, loss: 0.01999208889901638, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 2881, loss: 0.030281569808721542, acc: 0.96, recall: 0.96, precision: 0.9607371794871795, f_beta: 0.959983993597439
train: step: 2882, loss: 0.026967382058501244, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 2883, loss: 0.058353424072265625, acc: 0.92, recall: 0.9199999999999999, precision: 0.9227053140096618, f_beta: 0.9198717948717948
train: step: 2884, loss: 0.02478281781077385, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 2885, loss: 0.03583775460720062, acc: 0.95, recall: 0.95, precision: 0.9501800720288115, f_beta: 0.9499949994999499
train: step: 2886, loss: 0.042097948491573334, acc: 0.95, recall: 0.95, precision: 0.9501800720288115, f_beta: 0.9499949994999499
train: step: 2887, loss: 0.02335987240076065, acc: 0.97, recall: 0.97, precision: 0.9716981132075472, f_beta: 0.9699729756781104
train: step: 2888, loss: 0.03564032167196274, acc: 0.95, recall: 0.95, precision: 0.9516258530710557, f_beta: 0.949954959463517
train: step: 2889, loss: 0.014174871146678925, acc: 0.99, recall: 0.99, precision: 0.9901960784313726, f_beta: 0.98999899989999
train: step: 2890, loss: 0.032863568514585495, acc: 0.96, recall: 0.96, precision: 0.96, f_beta: 0.96
train: step: 2891, loss: 0.045369215309619904, acc: 0.94, recall: 0.94, precision: 0.9464285714285714, f_beta: 0.939783219590526
train: step: 2892, loss: 0.028594670817255974, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 2893, loss: 0.05217498913407326, acc: 0.93, recall: 0.9299999999999999, precision: 0.930172068827531, f_beta: 0.9299929992999301
train: step: 2894, loss: 0.0338127501308918, acc: 0.96, recall: 0.96, precision: 0.96, f_beta: 0.96
train: step: 2895, loss: 0.019421951845288277, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 2896, loss: 0.03519069030880928, acc: 0.95, recall: 0.95, precision: 0.9545454545454546, f_beta: 0.949874686716792
train: step: 2897, loss: 0.03370999917387962, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 2898, loss: 0.05993767827749252, acc: 0.93, recall: 0.9299999999999999, precision: 0.9315535929345644, f_beta: 0.9299369432489241
train: step: 2899, loss: 0.010266881436109543, acc: 0.99, recall: 0.99, precision: 0.9901960784313726, f_beta: 0.98999899989999
train: step: 2900, loss: 0.03283993899822235, acc: 0.96, recall: 0.96, precision: 0.96, f_beta: 0.96
train: step: 2901, loss: 0.04517577216029167, acc: 0.95, recall: 0.95, precision: 0.9501800720288115, f_beta: 0.9499949994999499
train: step: 2902, loss: 0.04910586401820183, acc: 0.94, recall: 0.94, precision: 0.94, f_beta: 0.94
train: step: 2903, loss: 0.0478779599070549, acc: 0.96, recall: 0.96, precision: 0.9607371794871795, f_beta: 0.959983993597439
train: step: 2904, loss: 0.05867007374763489, acc: 0.93, recall: 0.9299999999999999, precision: 0.930172068827531, f_beta: 0.9299929992999301
train: step: 2905, loss: 0.07355373352766037, acc: 0.91, recall: 0.9099999999999999, precision: 0.9141414141414141, f_beta: 0.9097744360902256
train: step: 2906, loss: 0.0481221042573452, acc: 0.95, recall: 0.95, precision: 0.9516258530710557, f_beta: 0.949954959463517
train: step: 2907, loss: 0.024052299559116364, acc: 0.96, recall: 0.96, precision: 0.962962962962963, f_beta: 0.9599358974358974
train: step: 2908, loss: 0.05918489396572113, acc: 0.93, recall: 0.9299999999999999, precision: 0.9315535929345644, f_beta: 0.9299369432489241
train: step: 2909, loss: 0.0406014546751976, acc: 0.95, recall: 0.95, precision: 0.9516258530710557, f_beta: 0.949954959463517
train: step: 2910, loss: 0.05395303666591644, acc: 0.93, recall: 0.9299999999999999, precision: 0.9315535929345644, f_beta: 0.9299369432489241
train: step: 2911, loss: 0.049803245812654495, acc: 0.94, recall: 0.94, precision: 0.9428341384863124, f_beta: 0.9399038461538463
train: step: 2912, loss: 0.0428779311478138, acc: 0.96, recall: 0.96, precision: 0.96, f_beta: 0.96
train: step: 2913, loss: 0.02835068665444851, acc: 0.96, recall: 0.96, precision: 0.96, f_beta: 0.96
train: step: 2914, loss: 0.009411667473614216, acc: 0.99, recall: 0.99, precision: 0.9901960784313726, f_beta: 0.98999899989999
train: step: 2915, loss: 0.030882375314831734, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 2916, loss: 0.046887826174497604, acc: 0.95, recall: 0.95, precision: 0.9501800720288115, f_beta: 0.9499949994999499
train: step: 2917, loss: 0.017217539250850677, acc: 0.98, recall: 0.98, precision: 0.9807692307692308, f_beta: 0.9799919967987194
train: step: 2918, loss: 0.04755840823054314, acc: 0.93, recall: 0.9299999999999999, precision: 0.9315535929345644, f_beta: 0.9299369432489241
train: step: 2919, loss: 0.03622782602906227, acc: 0.95, recall: 0.95, precision: 0.9501800720288115, f_beta: 0.9499949994999499
train: step: 2920, loss: 0.041039906442165375, acc: 0.94, recall: 0.94, precision: 0.9407051282051282, f_beta: 0.9399759903961584
train: step: 2921, loss: 0.025929706171154976, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 2922, loss: 0.029293565079569817, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 2923, loss: 0.0558595135807991, acc: 0.93, recall: 0.9299999999999999, precision: 0.930172068827531, f_beta: 0.9299929992999301
train: step: 2924, loss: 0.03206184506416321, acc: 0.96, recall: 0.96, precision: 0.96, f_beta: 0.96
train: step: 2925, loss: 0.04656476899981499, acc: 0.94, recall: 0.94, precision: 0.9428341384863124, f_beta: 0.9399038461538463
train: step: 2926, loss: 0.040263690054416656, acc: 0.95, recall: 0.95, precision: 0.9501800720288115, f_beta: 0.9499949994999499
train: step: 2927, loss: 0.03641239181160927, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 2928, loss: 0.048822108656167984, acc: 0.95, recall: 0.95, precision: 0.9501800720288115, f_beta: 0.9499949994999499
train: step: 2929, loss: 0.022968536242842674, acc: 0.98, recall: 0.98, precision: 0.98, f_beta: 0.98
train: step: 2930, loss: 0.07016909867525101, acc: 0.91, recall: 0.9099999999999999, precision: 0.9114813327980731, f_beta: 0.9099189270343309
train: step: 2931, loss: 0.02232249267399311, acc: 0.98, recall: 0.98, precision: 0.98, f_beta: 0.98
train: step: 2932, loss: 0.041695643216371536, acc: 0.94, recall: 0.94, precision: 0.9407051282051282, f_beta: 0.9399759903961584
train: step: 2933, loss: 0.020928170531988144, acc: 0.97, recall: 0.97, precision: 0.9716981132075472, f_beta: 0.9699729756781104
train: step: 2934, loss: 0.018051354214549065, acc: 0.98, recall: 0.98, precision: 0.9807692307692308, f_beta: 0.9799919967987194
train: step: 2935, loss: 0.05390186235308647, acc: 0.93, recall: 0.9299999999999999, precision: 0.9315535929345644, f_beta: 0.9299369432489241
train: step: 2936, loss: 0.05910566449165344, acc: 0.94, recall: 0.94, precision: 0.9407051282051282, f_beta: 0.9399759903961584
train: step: 2937, loss: 0.03470984846353531, acc: 0.96, recall: 0.96, precision: 0.96, f_beta: 0.96
train: step: 2938, loss: 0.03093952126801014, acc: 0.96, recall: 0.96, precision: 0.9607371794871795, f_beta: 0.959983993597439
train: step: 2939, loss: 0.006471060682088137, acc: 0.99, recall: 0.99, precision: 0.9901960784313726, f_beta: 0.98999899989999
train: step: 2940, loss: 0.030284104868769646, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 2941, loss: 0.038340263068675995, acc: 0.96, recall: 0.96, precision: 0.96, f_beta: 0.96
train: step: 2942, loss: 0.04807185009121895, acc: 0.94, recall: 0.94, precision: 0.94, f_beta: 0.94
train: step: 2943, loss: 0.04776635766029358, acc: 0.93, recall: 0.9299999999999999, precision: 0.9343434343434343, f_beta: 0.9298245614035088
train: step: 2944, loss: 0.047519583255052567, acc: 0.94, recall: 0.94, precision: 0.9407051282051282, f_beta: 0.9399759903961584
train: step: 2945, loss: 0.05608415603637695, acc: 0.92, recall: 0.92, precision: 0.92, f_beta: 0.92
train: step: 2946, loss: 0.05656880885362625, acc: 0.94, recall: 0.94, precision: 0.9407051282051282, f_beta: 0.9399759903961584
train: step: 2947, loss: 0.04736291244626045, acc: 0.95, recall: 0.95, precision: 0.9501800720288115, f_beta: 0.9499949994999499
train: step: 2948, loss: 0.04877735301852226, acc: 0.95, recall: 0.95, precision: 0.9501800720288115, f_beta: 0.9499949994999499
train: step: 2949, loss: 0.00950175803154707, acc: 0.99, recall: 0.99, precision: 0.9901960784313726, f_beta: 0.98999899989999
train: step: 2950, loss: 0.018316470086574554, acc: 0.98, recall: 0.98, precision: 0.9807692307692308, f_beta: 0.9799919967987194
train: step: 2951, loss: 0.06370379775762558, acc: 0.93, recall: 0.9299999999999999, precision: 0.9315535929345644, f_beta: 0.9299369432489241
train: step: 2952, loss: 0.013725681230425835, acc: 0.99, recall: 0.99, precision: 0.9901960784313726, f_beta: 0.98999899989999
train: step: 2953, loss: 0.04274522885680199, acc: 0.95, recall: 0.95, precision: 0.9501800720288115, f_beta: 0.9499949994999499
train: step: 2954, loss: 0.053505491465330124, acc: 0.94, recall: 0.94, precision: 0.9464285714285714, f_beta: 0.939783219590526
train: step: 2955, loss: 0.024759430438280106, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 2956, loss: 0.03549567237496376, acc: 0.96, recall: 0.96, precision: 0.9607371794871795, f_beta: 0.959983993597439
train: step: 2957, loss: 0.026858370751142502, acc: 0.98, recall: 0.98, precision: 0.98, f_beta: 0.98
train: step: 2958, loss: 0.005963859613984823, acc: 1.0, recall: 1.0, precision: 1.0, f_beta: 1.0
train: step: 2959, loss: 0.006242233328521252, acc: 0.99, recall: 0.99, precision: 0.9901960784313726, f_beta: 0.98999899989999
train: step: 2960, loss: 0.06958895176649094, acc: 0.92, recall: 0.9199999999999999, precision: 0.9261363636363636, f_beta: 0.9197109594540345
train: step: 2961, loss: 0.05050814151763916, acc: 0.95, recall: 0.95, precision: 0.9501800720288115, f_beta: 0.9499949994999499
train: step: 2962, loss: 0.03036765567958355, acc: 0.96, recall: 0.96, precision: 0.96, f_beta: 0.96
train: step: 2963, loss: 0.025150423869490623, acc: 0.96, recall: 0.96, precision: 0.9607371794871795, f_beta: 0.959983993597439
train: step: 2964, loss: 0.0550491139292717, acc: 0.94, recall: 0.94, precision: 0.94, f_beta: 0.94
train: step: 2965, loss: 0.047460392117500305, acc: 0.95, recall: 0.95, precision: 0.9516258530710557, f_beta: 0.949954959463517
train: step: 2966, loss: 0.021904462948441505, acc: 0.97, recall: 0.97, precision: 0.9716981132075472, f_beta: 0.9699729756781104
train: step: 2967, loss: 0.014446147717535496, acc: 0.99, recall: 0.99, precision: 0.9901960784313726, f_beta: 0.98999899989999
train: step: 2968, loss: 0.018447477370500565, acc: 0.98, recall: 0.98, precision: 0.98, f_beta: 0.98
train: step: 2969, loss: 0.020697874948382378, acc: 0.96, recall: 0.96, precision: 0.9607371794871795, f_beta: 0.959983993597439
train: step: 2970, loss: 0.05003893002867699, acc: 0.95, recall: 0.95, precision: 0.9545454545454546, f_beta: 0.949874686716792
train: step: 2971, loss: 0.02024991437792778, acc: 0.98, recall: 0.98, precision: 0.9807692307692308, f_beta: 0.9799919967987194
train: step: 2972, loss: 0.03211000934243202, acc: 0.97, recall: 0.97, precision: 0.9716981132075472, f_beta: 0.9699729756781104
train: step: 2973, loss: 0.028054898604750633, acc: 0.96, recall: 0.96, precision: 0.9607371794871795, f_beta: 0.959983993597439
train: step: 2974, loss: 0.05146414786577225, acc: 0.95, recall: 0.95, precision: 0.9501800720288115, f_beta: 0.9499949994999499
train: step: 2975, loss: 0.03591929003596306, acc: 0.96, recall: 0.96, precision: 0.96, f_beta: 0.96
train: step: 2976, loss: 0.03764128312468529, acc: 0.96, recall: 0.96, precision: 0.96, f_beta: 0.96
train: step: 2977, loss: 0.046287957578897476, acc: 0.94, recall: 0.94, precision: 0.94, f_beta: 0.94
train: step: 2978, loss: 0.011877484619617462, acc: 0.99, recall: 0.99, precision: 0.9901960784313726, f_beta: 0.98999899989999
train: step: 2979, loss: 0.03612838685512543, acc: 0.95, recall: 0.95, precision: 0.9501800720288115, f_beta: 0.9499949994999499
train: step: 2980, loss: 0.013731113635003567, acc: 0.99, recall: 0.99, precision: 0.9901960784313726, f_beta: 0.98999899989999
train: step: 2981, loss: 0.006596989464014769, acc: 0.99, recall: 0.99, precision: 0.9901960784313726, f_beta: 0.98999899989999
train: step: 2982, loss: 0.03451002016663551, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 2983, loss: 0.03688402101397514, acc: 0.96, recall: 0.96, precision: 0.9607371794871795, f_beta: 0.959983993597439
train: step: 2984, loss: 0.02496293932199478, acc: 0.96, recall: 0.96, precision: 0.9607371794871795, f_beta: 0.959983993597439
train: step: 2985, loss: 0.031980931758880615, acc: 0.95, recall: 0.95, precision: 0.9501800720288115, f_beta: 0.9499949994999499
train: step: 2986, loss: 0.04410175234079361, acc: 0.94, recall: 0.94, precision: 0.9428341384863124, f_beta: 0.9399038461538463
train: step: 2987, loss: 0.01849181391298771, acc: 0.98, recall: 0.98, precision: 0.9807692307692308, f_beta: 0.9799919967987194
train: step: 2988, loss: 0.04781872779130936, acc: 0.95, recall: 0.95, precision: 0.9501800720288115, f_beta: 0.9499949994999499
train: step: 2989, loss: 0.03523305058479309, acc: 0.95, recall: 0.95, precision: 0.9501800720288115, f_beta: 0.9499949994999499
train: step: 2990, loss: 0.03878552466630936, acc: 0.95, recall: 0.95, precision: 0.9501800720288115, f_beta: 0.9499949994999499
train: step: 2991, loss: 0.030966544523835182, acc: 0.95, recall: 0.95, precision: 0.9501800720288115, f_beta: 0.9499949994999499
train: step: 2992, loss: 0.06790843605995178, acc: 0.91, recall: 0.9099999999999999, precision: 0.9181966544267646, f_beta: 0.9095568284594513
train: step: 2993, loss: 0.014510792680084705, acc: 0.96, recall: 0.96, precision: 0.96, f_beta: 0.96
train: step: 2994, loss: 0.05852602794766426, acc: 0.9, recall: 0.8999999999999999, precision: 0.9025764895330113, f_beta: 0.8998397435897436
train: step: 2995, loss: 0.07184067368507385, acc: 0.91, recall: 0.9099999999999999, precision: 0.9114813327980731, f_beta: 0.9099189270343309
train: step: 2996, loss: 0.016924511641263962, acc: 0.98, recall: 0.98, precision: 0.98, f_beta: 0.98
train: step: 2997, loss: 0.06490675359964371, acc: 0.92, recall: 0.9199999999999999, precision: 0.9206730769230769, f_beta: 0.919967987194878
train: step: 2998, loss: 0.03102799504995346, acc: 0.96, recall: 0.96, precision: 0.9607371794871795, f_beta: 0.959983993597439
train: step: 2999, loss: 0.023416517302393913, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 3000, loss: 0.037642333656549454, acc: 0.96, recall: 0.96, precision: 0.9607371794871795, f_beta: 0.959983993597439
train: step: 3001, loss: 0.033909156918525696, acc: 0.96, recall: 0.96, precision: 0.96, f_beta: 0.96
train: step: 3002, loss: 0.01794634386897087, acc: 0.98, recall: 0.98, precision: 0.9807692307692308, f_beta: 0.9799919967987194
train: step: 3003, loss: 0.04090080410242081, acc: 0.95, recall: 0.95, precision: 0.9501800720288115, f_beta: 0.9499949994999499
train: step: 3004, loss: 0.023662365972995758, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 3005, loss: 0.07040110975503922, acc: 0.9, recall: 0.9, precision: 0.9006410256410255, f_beta: 0.8999599839935974
train: step: 3006, loss: 0.08902614563703537, acc: 0.89, recall: 0.89, precision: 0.8914090726615818, f_beta: 0.8899009108197378
train: step: 3007, loss: 0.023420285433530807, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 3008, loss: 0.05688624456524849, acc: 0.93, recall: 0.9299999999999999, precision: 0.9343434343434343, f_beta: 0.9298245614035088
train: step: 3009, loss: 0.05259837955236435, acc: 0.94, recall: 0.94, precision: 0.9428341384863124, f_beta: 0.9399038461538463
train: step: 3010, loss: 0.039237264543771744, acc: 0.94, recall: 0.94, precision: 0.9428341384863124, f_beta: 0.9399038461538463
train: step: 3011, loss: 0.026290306821465492, acc: 0.98, recall: 0.98, precision: 0.98, f_beta: 0.98
train: step: 3012, loss: 0.056027792394161224, acc: 0.94, recall: 0.94, precision: 0.9407051282051282, f_beta: 0.9399759903961584
train: step: 3013, loss: 0.0774284154176712, acc: 0.92, recall: 0.9199999999999999, precision: 0.9227053140096618, f_beta: 0.9198717948717948
train: step: 3014, loss: 0.020358016714453697, acc: 0.98, recall: 0.98, precision: 0.98, f_beta: 0.98
train: step: 3015, loss: 0.040209587663412094, acc: 0.94, recall: 0.94, precision: 0.9464285714285714, f_beta: 0.939783219590526
train: step: 3016, loss: 0.04708753153681755, acc: 0.94, recall: 0.94, precision: 0.9407051282051282, f_beta: 0.9399759903961584
train: step: 3017, loss: 0.040559425950050354, acc: 0.96, recall: 0.96, precision: 0.9607371794871795, f_beta: 0.959983993597439
train: step: 3018, loss: 0.028385119512677193, acc: 0.96, recall: 0.96, precision: 0.9607371794871795, f_beta: 0.959983993597439
train: step: 3019, loss: 0.015711765736341476, acc: 0.99, recall: 0.99, precision: 0.9901960784313726, f_beta: 0.98999899989999
train: step: 3020, loss: 0.023830845952033997, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 3021, loss: 0.042836882174015045, acc: 0.94, recall: 0.94, precision: 0.94, f_beta: 0.94
train: step: 3022, loss: 0.01698770746588707, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 3023, loss: 0.07138305902481079, acc: 0.9, recall: 0.9, precision: 0.9, f_beta: 0.9
train: step: 3024, loss: 0.03992846608161926, acc: 0.96, recall: 0.96, precision: 0.9607371794871795, f_beta: 0.959983993597439
train: step: 3025, loss: 0.022421352565288544, acc: 0.98, recall: 0.98, precision: 0.98, f_beta: 0.98
train: step: 3026, loss: 0.02174922451376915, acc: 0.98, recall: 0.98, precision: 0.9807692307692308, f_beta: 0.9799919967987194
train: step: 3027, loss: 0.06284131854772568, acc: 0.93, recall: 0.9299999999999999, precision: 0.930172068827531, f_beta: 0.9299929992999301
train: step: 3028, loss: 0.04402121528983116, acc: 0.94, recall: 0.94, precision: 0.9428341384863124, f_beta: 0.9399038461538463
train: step: 3029, loss: 0.024761633947491646, acc: 0.97, recall: 0.97, precision: 0.9716981132075472, f_beta: 0.9699729756781104
train: step: 3030, loss: 0.0508020780980587, acc: 0.93, recall: 0.9299999999999999, precision: 0.9315535929345644, f_beta: 0.9299369432489241
train: step: 3031, loss: 0.03969763219356537, acc: 0.95, recall: 0.95, precision: 0.9501800720288115, f_beta: 0.9499949994999499
train: step: 3032, loss: 0.001862686243839562, acc: 1.0, recall: 1.0, precision: 1.0, f_beta: 1.0
train: step: 3033, loss: 0.07504791021347046, acc: 0.91, recall: 0.91, precision: 0.9101640656262505, f_beta: 0.90999099909991
train: step: 3034, loss: 0.027091866359114647, acc: 0.95, recall: 0.95, precision: 0.9516258530710557, f_beta: 0.949954959463517
train: step: 3035, loss: 0.029612183570861816, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 3036, loss: 0.040063027292490005, acc: 0.95, recall: 0.95, precision: 0.9501800720288115, f_beta: 0.9499949994999499
train: step: 3037, loss: 0.03691290318965912, acc: 0.94, recall: 0.94, precision: 0.9428341384863124, f_beta: 0.9399038461538463
train: step: 3038, loss: 0.061491984874010086, acc: 0.9, recall: 0.9, precision: 0.9, f_beta: 0.9
train: step: 3039, loss: 0.05670392885804176, acc: 0.92, recall: 0.92, precision: 0.92, f_beta: 0.92
train: step: 3040, loss: 0.026123223826289177, acc: 0.98, recall: 0.98, precision: 0.98, f_beta: 0.98
train: step: 3041, loss: 0.05548543110489845, acc: 0.94, recall: 0.94, precision: 0.9428341384863124, f_beta: 0.9399038461538463
train: step: 3042, loss: 0.06076471507549286, acc: 0.92, recall: 0.9199999999999999, precision: 0.9206730769230769, f_beta: 0.919967987194878
train: step: 3043, loss: 0.06542913615703583, acc: 0.91, recall: 0.91, precision: 0.9101640656262505, f_beta: 0.90999099909991
train: step: 3044, loss: 0.00646574841812253, acc: 0.99, recall: 0.99, precision: 0.9901960784313726, f_beta: 0.98999899989999
train: step: 3045, loss: 0.02685621753334999, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 3046, loss: 0.03099723905324936, acc: 0.96, recall: 0.96, precision: 0.962962962962963, f_beta: 0.9599358974358974
train: step: 3047, loss: 0.05948496237397194, acc: 0.93, recall: 0.9299999999999999, precision: 0.930172068827531, f_beta: 0.9299929992999301
train: step: 3048, loss: 0.043925896286964417, acc: 0.94, recall: 0.94, precision: 0.9428341384863124, f_beta: 0.9399038461538463
train: step: 3049, loss: 0.018196403980255127, acc: 0.98, recall: 0.98, precision: 0.9807692307692308, f_beta: 0.9799919967987194
train: step: 3050, loss: 0.03520144522190094, acc: 0.96, recall: 0.96, precision: 0.9607371794871795, f_beta: 0.959983993597439
train: step: 3051, loss: 0.02802366204559803, acc: 0.95, recall: 0.95, precision: 0.9501800720288115, f_beta: 0.9499949994999499
train: step: 3052, loss: 0.029325079172849655, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 3053, loss: 0.04520536959171295, acc: 0.94, recall: 0.94, precision: 0.9407051282051282, f_beta: 0.9399759903961584
train: step: 3054, loss: 0.03112136386334896, acc: 0.96, recall: 0.96, precision: 0.96, f_beta: 0.96
train: step: 3055, loss: 0.0378328412771225, acc: 0.96, recall: 0.96, precision: 0.9607371794871795, f_beta: 0.959983993597439
train: step: 3056, loss: 0.08718658238649368, acc: 0.9, recall: 0.9, precision: 0.9006410256410255, f_beta: 0.8999599839935974
train: step: 3057, loss: 0.05763063579797745, acc: 0.94, recall: 0.94, precision: 0.9407051282051282, f_beta: 0.9399759903961584
train: step: 3058, loss: 0.04849599674344063, acc: 0.94, recall: 0.94, precision: 0.9407051282051282, f_beta: 0.9399759903961584
train: step: 3059, loss: 0.0043652597814798355, acc: 1.0, recall: 1.0, precision: 1.0, f_beta: 1.0
train: step: 3060, loss: 0.05526160076260567, acc: 0.94, recall: 0.94, precision: 0.9407051282051282, f_beta: 0.9399759903961584
train: step: 3061, loss: 0.023569855839014053, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 3062, loss: 0.028023287653923035, acc: 0.96, recall: 0.96, precision: 0.962962962962963, f_beta: 0.9599358974358974
train: step: 3063, loss: 0.02214951068162918, acc: 0.98, recall: 0.98, precision: 0.98, f_beta: 0.98
train: step: 3064, loss: 0.05188672989606857, acc: 0.93, recall: 0.9299999999999999, precision: 0.9315535929345644, f_beta: 0.9299369432489241
train: step: 3065, loss: 0.027493109926581383, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 3066, loss: 0.02858641929924488, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 3067, loss: 0.031076714396476746, acc: 0.97, recall: 0.97, precision: 0.9716981132075472, f_beta: 0.9699729756781104
train: step: 3068, loss: 0.04168398678302765, acc: 0.95, recall: 0.95, precision: 0.9516258530710557, f_beta: 0.949954959463517
train: step: 3069, loss: 0.033394649624824524, acc: 0.96, recall: 0.96, precision: 0.962962962962963, f_beta: 0.9599358974358974
train: step: 3070, loss: 0.043967172503471375, acc: 0.95, recall: 0.95, precision: 0.9501800720288115, f_beta: 0.9499949994999499
train: step: 3071, loss: 0.03943854942917824, acc: 0.94, recall: 0.94, precision: 0.9407051282051282, f_beta: 0.9399759903961584
train: step: 3072, loss: 0.03200091794133186, acc: 0.96, recall: 0.96, precision: 0.962962962962963, f_beta: 0.9599358974358974
train: step: 3073, loss: 0.045831259340047836, acc: 0.94, recall: 0.94, precision: 0.9407051282051282, f_beta: 0.9399759903961584
train: step: 3074, loss: 0.04171193763613701, acc: 0.92, recall: 0.9199999999999999, precision: 0.9227053140096618, f_beta: 0.9198717948717948
train: step: 3075, loss: 0.036835480481386185, acc: 0.96, recall: 0.96, precision: 0.9607371794871795, f_beta: 0.959983993597439
train: step: 3076, loss: 0.04153677448630333, acc: 0.95, recall: 0.95, precision: 0.9501800720288115, f_beta: 0.9499949994999499
train: step: 3077, loss: 0.06559352576732635, acc: 0.91, recall: 0.9099999999999999, precision: 0.9181966544267646, f_beta: 0.9095568284594513
train: step: 3078, loss: 0.033787891268730164, acc: 0.96, recall: 0.96, precision: 0.96, f_beta: 0.96
train: step: 3079, loss: 0.046166352927684784, acc: 0.94, recall: 0.94, precision: 0.9464285714285714, f_beta: 0.939783219590526
train: step: 3080, loss: 0.04819441959261894, acc: 0.95, recall: 0.95, precision: 0.9516258530710557, f_beta: 0.949954959463517
train: step: 3081, loss: 0.050660181790590286, acc: 0.94, recall: 0.94, precision: 0.94, f_beta: 0.94
train: step: 3082, loss: 0.035157281905412674, acc: 0.96, recall: 0.96, precision: 0.96, f_beta: 0.96
train: step: 3083, loss: 0.017754212021827698, acc: 0.98, recall: 0.98, precision: 0.98, f_beta: 0.98
train: step: 3084, loss: 0.04904314875602722, acc: 0.93, recall: 0.9299999999999999, precision: 0.930172068827531, f_beta: 0.9299929992999301
train: step: 3085, loss: 0.08201579749584198, acc: 0.9, recall: 0.9, precision: 0.9006410256410255, f_beta: 0.8999599839935974
train: step: 3086, loss: 0.04055267944931984, acc: 0.94, recall: 0.94, precision: 0.9407051282051282, f_beta: 0.9399759903961584
train: step: 3087, loss: 0.04435067996382713, acc: 0.94, recall: 0.94, precision: 0.9407051282051282, f_beta: 0.9399759903961584
train: step: 3088, loss: 0.01890963315963745, acc: 0.98, recall: 0.98, precision: 0.98, f_beta: 0.98
train: step: 3089, loss: 0.01466401107609272, acc: 0.99, recall: 0.99, precision: 0.9901960784313726, f_beta: 0.98999899989999
train: step: 3090, loss: 0.04594920948147774, acc: 0.94, recall: 0.94, precision: 0.9428341384863124, f_beta: 0.9399038461538463
train: step: 3091, loss: 0.04963693022727966, acc: 0.93, recall: 0.9299999999999999, precision: 0.930172068827531, f_beta: 0.9299929992999301
train: step: 3092, loss: 0.039766620844602585, acc: 0.95, recall: 0.95, precision: 0.9545454545454546, f_beta: 0.949874686716792
train: step: 3093, loss: 0.0666123777627945, acc: 0.92, recall: 0.9199999999999999, precision: 0.9261363636363636, f_beta: 0.9197109594540345
train: step: 3094, loss: 0.026567725464701653, acc: 0.97, recall: 0.97, precision: 0.9716981132075472, f_beta: 0.9699729756781104
train: step: 3095, loss: 0.030049825087189674, acc: 0.96, recall: 0.96, precision: 0.9607371794871795, f_beta: 0.959983993597439
train: step: 3096, loss: 0.07379724085330963, acc: 0.91, recall: 0.9099999999999999, precision: 0.9181966544267646, f_beta: 0.9095568284594513
train: step: 3097, loss: 0.00598355894908309, acc: 1.0, recall: 1.0, precision: 1.0, f_beta: 1.0
train: step: 3098, loss: 0.02215190976858139, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 3099, loss: 0.07807372510433197, acc: 0.91, recall: 0.91, precision: 0.9101640656262505, f_beta: 0.90999099909991
train: step: 3100, loss: 0.04838700219988823, acc: 0.94, recall: 0.94, precision: 0.9407051282051282, f_beta: 0.9399759903961584
train: step: 3101, loss: 0.0373736247420311, acc: 0.95, recall: 0.95, precision: 0.9516258530710557, f_beta: 0.949954959463517
train: step: 3102, loss: 0.054109249264001846, acc: 0.95, recall: 0.95, precision: 0.9516258530710557, f_beta: 0.949954959463517
train: step: 3103, loss: 0.03092680498957634, acc: 0.96, recall: 0.96, precision: 0.9607371794871795, f_beta: 0.959983993597439
train: step: 3104, loss: 0.02887049689888954, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 3105, loss: 0.026847628876566887, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 3106, loss: 0.027024593204259872, acc: 0.98, recall: 0.98, precision: 0.9807692307692308, f_beta: 0.9799919967987194
train: step: 3107, loss: 0.0643262267112732, acc: 0.92, recall: 0.9199999999999999, precision: 0.9261363636363636, f_beta: 0.9197109594540345
train: step: 3108, loss: 0.024699941277503967, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 3109, loss: 0.03270077332854271, acc: 0.95, recall: 0.95, precision: 0.9516258530710557, f_beta: 0.949954959463517
train: step: 3110, loss: 0.02223740518093109, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 3111, loss: 0.021465513855218887, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 3112, loss: 0.04504336044192314, acc: 0.94, recall: 0.94, precision: 0.9407051282051282, f_beta: 0.9399759903961584
train: step: 3113, loss: 0.017864082008600235, acc: 0.98, recall: 0.98, precision: 0.98, f_beta: 0.98
train: step: 3114, loss: 0.02287299558520317, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 3115, loss: 0.043868616223335266, acc: 0.95, recall: 0.95, precision: 0.9501800720288115, f_beta: 0.9499949994999499
train: step: 3116, loss: 0.05489705502986908, acc: 0.94, recall: 0.94, precision: 0.9428341384863124, f_beta: 0.9399038461538463
train: step: 3117, loss: 0.04986139386892319, acc: 0.94, recall: 0.94, precision: 0.94, f_beta: 0.94
train: step: 3118, loss: 0.024291835725307465, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 3119, loss: 0.039859265089035034, acc: 0.96, recall: 0.96, precision: 0.96, f_beta: 0.96
train: step: 3120, loss: 0.03030412644147873, acc: 0.97, recall: 0.97, precision: 0.9716981132075472, f_beta: 0.9699729756781104
train: step: 3121, loss: 0.023677168413996696, acc: 0.96, recall: 0.96, precision: 0.962962962962963, f_beta: 0.9599358974358974
train: step: 3122, loss: 0.03745262324810028, acc: 0.94, recall: 0.94, precision: 0.9428341384863124, f_beta: 0.9399038461538463
train: step: 3123, loss: 0.047562818974256516, acc: 0.94, recall: 0.94, precision: 0.94, f_beta: 0.94
train: step: 3124, loss: 0.02628410793840885, acc: 0.97, recall: 0.97, precision: 0.9716981132075472, f_beta: 0.9699729756781104
train: step: 3125, loss: 0.031275443732738495, acc: 0.96, recall: 0.96, precision: 0.962962962962963, f_beta: 0.9599358974358974
train: step: 3126, loss: 0.02191072516143322, acc: 0.98, recall: 0.98, precision: 0.98, f_beta: 0.98
train: step: 3127, loss: 0.04507513344287872, acc: 0.94, recall: 0.94, precision: 0.94, f_beta: 0.94
train: step: 3128, loss: 0.010351210832595825, acc: 0.99, recall: 0.99, precision: 0.9901960784313726, f_beta: 0.98999899989999
train: step: 3129, loss: 0.053773440420627594, acc: 0.92, recall: 0.92, precision: 0.92, f_beta: 0.92
train: step: 3130, loss: 0.01828833296895027, acc: 0.98, recall: 0.98, precision: 0.98, f_beta: 0.98
train: step: 3131, loss: 0.03423076122999191, acc: 0.96, recall: 0.96, precision: 0.96, f_beta: 0.96
train: step: 3132, loss: 0.05713602900505066, acc: 0.93, recall: 0.9299999999999999, precision: 0.9385964912280702, f_beta: 0.9296553110240177
train: step: 3133, loss: 0.010528714396059513, acc: 0.99, recall: 0.99, precision: 0.9901960784313726, f_beta: 0.98999899989999
train: step: 3134, loss: 0.06874006241559982, acc: 0.92, recall: 0.92, precision: 0.92, f_beta: 0.92
train: step: 3135, loss: 0.01938301883637905, acc: 0.98, recall: 0.98, precision: 0.98, f_beta: 0.98
train: step: 3136, loss: 0.03940174728631973, acc: 0.96, recall: 0.96, precision: 0.9607371794871795, f_beta: 0.959983993597439
train: step: 3137, loss: 0.040734853595495224, acc: 0.95, recall: 0.95, precision: 0.9501800720288115, f_beta: 0.9499949994999499
train: step: 3138, loss: 0.03473171964287758, acc: 0.95, recall: 0.95, precision: 0.9516258530710557, f_beta: 0.949954959463517
train: step: 3139, loss: 0.03723559528589249, acc: 0.96, recall: 0.96, precision: 0.9607371794871795, f_beta: 0.959983993597439
train: step: 3140, loss: 0.010020268149673939, acc: 0.99, recall: 0.99, precision: 0.9901960784313726, f_beta: 0.98999899989999
train: step: 3141, loss: 0.06025678664445877, acc: 0.9, recall: 0.9, precision: 0.9166666666666667, f_beta: 0.898989898989899
train: step: 3142, loss: 0.011200733482837677, acc: 0.99, recall: 0.99, precision: 0.9901960784313726, f_beta: 0.98999899989999
train: step: 3143, loss: 0.031247220933437347, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 3144, loss: 0.03232055529952049, acc: 0.97, recall: 0.97, precision: 0.9716981132075472, f_beta: 0.9699729756781104
train: step: 3145, loss: 0.05480986461043358, acc: 0.93, recall: 0.9299999999999999, precision: 0.9315535929345644, f_beta: 0.9299369432489241
train: step: 3146, loss: 0.047451525926589966, acc: 0.94, recall: 0.94, precision: 0.9407051282051282, f_beta: 0.9399759903961584
train: step: 3147, loss: 0.010267691686749458, acc: 0.99, recall: 0.99, precision: 0.9901960784313726, f_beta: 0.98999899989999
train: step: 3148, loss: 0.02544253133237362, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 3149, loss: 0.03747387230396271, acc: 0.96, recall: 0.96, precision: 0.9607371794871795, f_beta: 0.959983993597439
train: step: 3150, loss: 0.015002825297415257, acc: 0.98, recall: 0.98, precision: 0.9807692307692308, f_beta: 0.9799919967987194
train: step: 3151, loss: 0.05609625205397606, acc: 0.94, recall: 0.94, precision: 0.9407051282051282, f_beta: 0.9399759903961584
train: step: 3152, loss: 0.08657380938529968, acc: 0.89, recall: 0.8899999999999999, precision: 0.8939393939393939, f_beta: 0.8897243107769424
train: step: 3153, loss: 0.020124292001128197, acc: 0.98, recall: 0.98, precision: 0.9807692307692308, f_beta: 0.9799919967987194
train: step: 3154, loss: 0.06892916560173035, acc: 0.92, recall: 0.9199999999999999, precision: 0.9206730769230769, f_beta: 0.919967987194878
train: step: 3155, loss: 0.041078634560108185, acc: 0.96, recall: 0.96, precision: 0.96, f_beta: 0.96
train: step: 3156, loss: 0.04123470187187195, acc: 0.95, recall: 0.95, precision: 0.9501800720288115, f_beta: 0.9499949994999499
train: step: 3157, loss: 0.03867495432496071, acc: 0.91, recall: 0.9099999999999999, precision: 0.9141414141414141, f_beta: 0.9097744360902256
train: step: 3158, loss: 0.06200168654322624, acc: 0.93, recall: 0.9299999999999999, precision: 0.9315535929345644, f_beta: 0.9299369432489241
train: step: 3159, loss: 0.003204246051609516, acc: 1.0, recall: 1.0, precision: 1.0, f_beta: 1.0
train: step: 3160, loss: 0.014312774874269962, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 3161, loss: 0.031464606523513794, acc: 0.97, recall: 0.97, precision: 0.9716981132075472, f_beta: 0.9699729756781104
train: step: 3162, loss: 0.0719979777932167, acc: 0.91, recall: 0.9099999999999999, precision: 0.9181966544267646, f_beta: 0.9095568284594513
train: step: 3163, loss: 0.02940819226205349, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 3164, loss: 0.02860753983259201, acc: 0.96, recall: 0.96, precision: 0.9607371794871795, f_beta: 0.959983993597439
train: step: 3165, loss: 0.03932453319430351, acc: 0.96, recall: 0.96, precision: 0.962962962962963, f_beta: 0.9599358974358974
train: step: 3166, loss: 0.026037361472845078, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 3167, loss: 0.03909330070018768, acc: 0.94, recall: 0.94, precision: 0.9428341384863124, f_beta: 0.9399038461538463
train: step: 3168, loss: 0.011992753483355045, acc: 0.99, recall: 0.99, precision: 0.9901960784313726, f_beta: 0.98999899989999
train: step: 3169, loss: 0.013998616486787796, acc: 0.99, recall: 0.99, precision: 0.9901960784313726, f_beta: 0.98999899989999
train: step: 3170, loss: 0.03385822847485542, acc: 0.97, recall: 0.97, precision: 0.9716981132075472, f_beta: 0.9699729756781104
train: step: 3171, loss: 0.04707455262541771, acc: 0.94, recall: 0.94, precision: 0.9407051282051282, f_beta: 0.9399759903961584
train: step: 3172, loss: 0.04762221872806549, acc: 0.93, recall: 0.9299999999999999, precision: 0.930172068827531, f_beta: 0.9299929992999301
train: step: 3173, loss: 0.0353005975484848, acc: 0.95, recall: 0.95, precision: 0.9545454545454546, f_beta: 0.949874686716792
train: step: 3174, loss: 0.021120920777320862, acc: 0.98, recall: 0.98, precision: 0.9807692307692308, f_beta: 0.9799919967987194
train: step: 3175, loss: 0.050474248826503754, acc: 0.93, recall: 0.9299999999999999, precision: 0.9343434343434343, f_beta: 0.9298245614035088
train: step: 3176, loss: 0.023332681506872177, acc: 0.97, recall: 0.97, precision: 0.9716981132075472, f_beta: 0.9699729756781104
train: step: 3177, loss: 0.038209497928619385, acc: 0.95, recall: 0.95, precision: 0.9501800720288115, f_beta: 0.9499949994999499
train: step: 3178, loss: 0.05302588641643524, acc: 0.93, recall: 0.9299999999999999, precision: 0.930172068827531, f_beta: 0.9299929992999301
train: step: 3179, loss: 0.04225564002990723, acc: 0.93, recall: 0.9299999999999999, precision: 0.9315535929345644, f_beta: 0.9299369432489241
train: step: 3180, loss: 0.023692592978477478, acc: 0.98, recall: 0.98, precision: 0.9807692307692308, f_beta: 0.9799919967987194
train: step: 3181, loss: 0.038554850965738297, acc: 0.95, recall: 0.95, precision: 0.9501800720288115, f_beta: 0.9499949994999499
train: step: 3182, loss: 0.019689148291945457, acc: 0.98, recall: 0.98, precision: 0.9807692307692308, f_beta: 0.9799919967987194
train: step: 3183, loss: 0.03570606932044029, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 3184, loss: 0.0060074543580412865, acc: 0.99, recall: 0.99, precision: 0.9901960784313726, f_beta: 0.98999899989999
train: step: 3185, loss: 0.0224835816770792, acc: 0.96, recall: 0.96, precision: 0.9607371794871795, f_beta: 0.959983993597439
train: step: 3186, loss: 0.0796624943614006, acc: 0.91, recall: 0.9099999999999999, precision: 0.9114813327980731, f_beta: 0.9099189270343309
train: step: 3187, loss: 0.02668512798845768, acc: 0.97, recall: 0.97, precision: 0.9716981132075472, f_beta: 0.9699729756781104
train: step: 3188, loss: 0.05807458981871605, acc: 0.93, recall: 0.9299999999999999, precision: 0.9343434343434343, f_beta: 0.9298245614035088
train: step: 3189, loss: 0.01433403417468071, acc: 0.99, recall: 0.99, precision: 0.9901960784313726, f_beta: 0.98999899989999
train: step: 3190, loss: 0.05896981060504913, acc: 0.93, recall: 0.9299999999999999, precision: 0.930172068827531, f_beta: 0.9299929992999301
train: step: 3191, loss: 0.05185936018824577, acc: 0.94, recall: 0.94, precision: 0.9407051282051282, f_beta: 0.9399759903961584
train: step: 3192, loss: 0.011150059290230274, acc: 0.99, recall: 0.99, precision: 0.9901960784313726, f_beta: 0.98999899989999
train: step: 3193, loss: 0.06917348504066467, acc: 0.92, recall: 0.9199999999999999, precision: 0.9206730769230769, f_beta: 0.919967987194878
train: step: 3194, loss: 0.05044114962220192, acc: 0.93, recall: 0.9299999999999999, precision: 0.9315535929345644, f_beta: 0.9299369432489241
train: step: 3195, loss: 0.04721519351005554, acc: 0.95, recall: 0.95, precision: 0.9516258530710557, f_beta: 0.949954959463517
train: step: 3196, loss: 0.04522203281521797, acc: 0.93, recall: 0.9299999999999999, precision: 0.9385964912280702, f_beta: 0.9296553110240177
train: step: 3197, loss: 0.023780927062034607, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 3198, loss: 0.02244146354496479, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 3199, loss: 0.004699619021266699, acc: 1.0, recall: 1.0, precision: 1.0, f_beta: 1.0
train: step: 3200, loss: 0.05008268356323242, acc: 0.94, recall: 0.94, precision: 0.9407051282051282, f_beta: 0.9399759903961584
train: step: 3201, loss: 0.058154501020908356, acc: 0.94, recall: 0.94, precision: 0.94, f_beta: 0.94
train: step: 3202, loss: 0.026402300223708153, acc: 0.97, recall: 0.97, precision: 0.9716981132075472, f_beta: 0.9699729756781104
train: step: 3203, loss: 0.04602067172527313, acc: 0.94, recall: 0.94, precision: 0.94, f_beta: 0.94
train: step: 3204, loss: 0.049413684755563736, acc: 0.95, recall: 0.95, precision: 0.9516258530710557, f_beta: 0.949954959463517
train: step: 3205, loss: 0.045510221272706985, acc: 0.94, recall: 0.94, precision: 0.94, f_beta: 0.94
train: step: 3206, loss: 0.017667625099420547, acc: 0.98, recall: 0.98, precision: 0.98, f_beta: 0.98
train: step: 3207, loss: 0.018955672159790993, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 3208, loss: 0.012352228164672852, acc: 0.98, recall: 0.98, precision: 0.98, f_beta: 0.98
train: step: 3209, loss: 0.015051054768264294, acc: 0.98, recall: 0.98, precision: 0.98, f_beta: 0.98
train: step: 3210, loss: 0.027939854189753532, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 3211, loss: 0.05027128383517265, acc: 0.94, recall: 0.94, precision: 0.9407051282051282, f_beta: 0.9399759903961584
train: step: 3212, loss: 0.03162546455860138, acc: 0.96, recall: 0.96, precision: 0.962962962962963, f_beta: 0.9599358974358974
train: step: 3213, loss: 0.04630393907427788, acc: 0.95, recall: 0.95, precision: 0.9516258530710557, f_beta: 0.949954959463517
train: step: 3214, loss: 0.03552383556962013, acc: 0.95, recall: 0.95, precision: 0.9501800720288115, f_beta: 0.9499949994999499
train: step: 3215, loss: 0.015594899654388428, acc: 0.98, recall: 0.98, precision: 0.98, f_beta: 0.98
train: step: 3216, loss: 0.02538161724805832, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 3217, loss: 0.030550111085176468, acc: 0.97, recall: 0.97, precision: 0.9716981132075472, f_beta: 0.9699729756781104
train: step: 3218, loss: 0.004671385511755943, acc: 1.0, recall: 1.0, precision: 1.0, f_beta: 1.0
train: step: 3219, loss: 0.019336802884936333, acc: 0.98, recall: 0.98, precision: 0.9807692307692308, f_beta: 0.9799919967987194
train: step: 3220, loss: 0.03710874170064926, acc: 0.95, recall: 0.95, precision: 0.9516258530710557, f_beta: 0.949954959463517
train: step: 3221, loss: 0.027381649240851402, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 3222, loss: 0.04932279512286186, acc: 0.93, recall: 0.9299999999999999, precision: 0.9343434343434343, f_beta: 0.9298245614035088
train: step: 3223, loss: 0.03295111283659935, acc: 0.96, recall: 0.96, precision: 0.9607371794871795, f_beta: 0.959983993597439
train: step: 3224, loss: 0.009973137639462948, acc: 0.99, recall: 0.99, precision: 0.9901960784313726, f_beta: 0.98999899989999
train: step: 3225, loss: 0.022741105407476425, acc: 0.96, recall: 0.96, precision: 0.9607371794871795, f_beta: 0.959983993597439
train: step: 3226, loss: 0.036109425127506256, acc: 0.95, recall: 0.95, precision: 0.9501800720288115, f_beta: 0.9499949994999499
train: step: 3227, loss: 0.033455025404691696, acc: 0.96, recall: 0.96, precision: 0.9607371794871795, f_beta: 0.959983993597439
train: step: 3228, loss: 0.026794567704200745, acc: 0.97, recall: 0.97, precision: 0.9716981132075472, f_beta: 0.9699729756781104
train: step: 3229, loss: 0.06663016229867935, acc: 0.91, recall: 0.9099999999999999, precision: 0.9141414141414141, f_beta: 0.9097744360902256
train: step: 3230, loss: 0.06882688403129578, acc: 0.91, recall: 0.9099999999999999, precision: 0.923728813559322, f_beta: 0.9092650468797258
train: step: 3231, loss: 0.026574157178401947, acc: 0.96, recall: 0.96, precision: 0.962962962962963, f_beta: 0.9599358974358974
train: step: 3232, loss: 0.021638046950101852, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 3233, loss: 0.026210995391011238, acc: 0.97, recall: 0.97, precision: 0.9716981132075472, f_beta: 0.9699729756781104
train: step: 3234, loss: 0.027995262295007706, acc: 0.95, recall: 0.95, precision: 0.9516258530710557, f_beta: 0.949954959463517
train: step: 3235, loss: 0.042352572083473206, acc: 0.95, recall: 0.95, precision: 0.9501800720288115, f_beta: 0.9499949994999499
train: step: 3236, loss: 0.022853875532746315, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 3237, loss: 0.04599423334002495, acc: 0.95, recall: 0.95, precision: 0.9501800720288115, f_beta: 0.9499949994999499
train: step: 3238, loss: 0.052467748522758484, acc: 0.94, recall: 0.94, precision: 0.9428341384863124, f_beta: 0.9399038461538463
train: step: 3239, loss: 0.04396578669548035, acc: 0.96, recall: 0.96, precision: 0.96, f_beta: 0.96
train: step: 3240, loss: 0.02455945499241352, acc: 0.97, recall: 0.97, precision: 0.9716981132075472, f_beta: 0.9699729756781104
train: step: 3241, loss: 0.03965741768479347, acc: 0.96, recall: 0.96, precision: 0.9607371794871795, f_beta: 0.959983993597439
train: step: 3242, loss: 0.03810631483793259, acc: 0.96, recall: 0.96, precision: 0.9607371794871795, f_beta: 0.959983993597439
train: step: 3243, loss: 0.06186821311712265, acc: 0.93, recall: 0.9299999999999999, precision: 0.930172068827531, f_beta: 0.9299929992999301
train: step: 3244, loss: 0.053763248026371, acc: 0.93, recall: 0.9299999999999999, precision: 0.930172068827531, f_beta: 0.9299929992999301
train: step: 3245, loss: 0.02167230099439621, acc: 0.98, recall: 0.98, precision: 0.98, f_beta: 0.98
train: step: 3246, loss: 0.034454699605703354, acc: 0.95, recall: 0.95, precision: 0.9545454545454546, f_beta: 0.949874686716792
train: step: 3247, loss: 0.03246988356113434, acc: 0.94, recall: 0.94, precision: 0.9407051282051282, f_beta: 0.9399759903961584
train: step: 3248, loss: 0.053190652281045914, acc: 0.95, recall: 0.95, precision: 0.9501800720288115, f_beta: 0.9499949994999499
train: step: 3249, loss: 0.03618950769305229, acc: 0.96, recall: 0.96, precision: 0.9607371794871795, f_beta: 0.959983993597439
train: step: 3250, loss: 0.060488615185022354, acc: 0.93, recall: 0.9299999999999999, precision: 0.9315535929345644, f_beta: 0.9299369432489241
train: step: 3251, loss: 0.05468539893627167, acc: 0.93, recall: 0.9299999999999999, precision: 0.9343434343434343, f_beta: 0.9298245614035088
train: step: 3252, loss: 0.047825708985328674, acc: 0.92, recall: 0.9199999999999999, precision: 0.9310344827586207, f_beta: 0.9194847020933978
train: step: 3253, loss: 0.06568822264671326, acc: 0.94, recall: 0.94, precision: 0.9407051282051282, f_beta: 0.9399759903961584
train: step: 3254, loss: 0.030940599739551544, acc: 0.96, recall: 0.96, precision: 0.96, f_beta: 0.96
train: step: 3255, loss: 0.027727928012609482, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 3256, loss: 0.05393637716770172, acc: 0.94, recall: 0.94, precision: 0.9407051282051282, f_beta: 0.9399759903961584
train: step: 3257, loss: 0.04151640832424164, acc: 0.95, recall: 0.95, precision: 0.9501800720288115, f_beta: 0.9499949994999499
train: step: 3258, loss: 0.04702354967594147, acc: 0.94, recall: 0.94, precision: 0.94, f_beta: 0.94
train: step: 3259, loss: 0.06365296244621277, acc: 0.92, recall: 0.9199999999999999, precision: 0.9261363636363636, f_beta: 0.9197109594540345
train: step: 3260, loss: 0.048451997339725494, acc: 0.95, recall: 0.95, precision: 0.9501800720288115, f_beta: 0.9499949994999499
train: step: 3261, loss: 0.006146193947643042, acc: 0.99, recall: 0.99, precision: 0.9901960784313726, f_beta: 0.98999899989999
train: step: 3262, loss: 0.049776479601860046, acc: 0.94, recall: 0.94, precision: 0.9428341384863124, f_beta: 0.9399038461538463
train: step: 3263, loss: 0.04172907769680023, acc: 0.93, recall: 0.9299999999999999, precision: 0.9385964912280702, f_beta: 0.9296553110240177
train: step: 3264, loss: 0.03349811211228371, acc: 0.94, recall: 0.94, precision: 0.9407051282051282, f_beta: 0.9399759903961584
train: step: 3265, loss: 0.031792059540748596, acc: 0.97, recall: 0.97, precision: 0.9716981132075472, f_beta: 0.9699729756781104
train: step: 3266, loss: 0.03688567131757736, acc: 0.96, recall: 0.96, precision: 0.9607371794871795, f_beta: 0.959983993597439
train: step: 3267, loss: 0.01612100936472416, acc: 0.99, recall: 0.99, precision: 0.9901960784313726, f_beta: 0.98999899989999
train: step: 3268, loss: 0.0202033668756485, acc: 0.98, recall: 0.98, precision: 0.9807692307692308, f_beta: 0.9799919967987194
train: step: 3269, loss: 0.014377393759787083, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 3270, loss: 0.0059599494561553, acc: 0.99, recall: 0.99, precision: 0.9901960784313726, f_beta: 0.98999899989999
train: step: 3271, loss: 0.05278174579143524, acc: 0.93, recall: 0.9299999999999999, precision: 0.930172068827531, f_beta: 0.9299929992999301
train: step: 3272, loss: 0.035280562937259674, acc: 0.96, recall: 0.96, precision: 0.9607371794871795, f_beta: 0.959983993597439
train: step: 3273, loss: 0.01866178959608078, acc: 0.99, recall: 0.99, precision: 0.9901960784313726, f_beta: 0.98999899989999
train: step: 3274, loss: 0.027970189228653908, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 3275, loss: 0.021786389872431755, acc: 0.98, recall: 0.98, precision: 0.9807692307692308, f_beta: 0.9799919967987194
train: step: 3276, loss: 0.028728041797876358, acc: 0.96, recall: 0.96, precision: 0.962962962962963, f_beta: 0.9599358974358974
train: step: 3277, loss: 0.06187599524855614, acc: 0.93, recall: 0.9299999999999999, precision: 0.9343434343434343, f_beta: 0.9298245614035088
train: step: 3278, loss: 0.024580059573054314, acc: 0.97, recall: 0.97, precision: 0.9716981132075472, f_beta: 0.9699729756781104
train: step: 3279, loss: 0.011867053806781769, acc: 0.99, recall: 0.99, precision: 0.9901960784313726, f_beta: 0.98999899989999
train: step: 3280, loss: 0.019883135333657265, acc: 0.98, recall: 0.98, precision: 0.98, f_beta: 0.98
train: step: 3281, loss: 0.014545699581503868, acc: 0.98, recall: 0.98, precision: 0.98, f_beta: 0.98
train: step: 3282, loss: 0.03956842049956322, acc: 0.95, recall: 0.95, precision: 0.9501800720288115, f_beta: 0.9499949994999499
train: step: 3283, loss: 0.022420736029744148, acc: 0.98, recall: 0.98, precision: 0.9807692307692308, f_beta: 0.9799919967987194
train: step: 3284, loss: 0.04305541142821312, acc: 0.95, recall: 0.95, precision: 0.9516258530710557, f_beta: 0.949954959463517
train: step: 3285, loss: 0.030624650418758392, acc: 0.97, recall: 0.97, precision: 0.9716981132075472, f_beta: 0.9699729756781104
train: step: 3286, loss: 0.04567258805036545, acc: 0.94, recall: 0.94, precision: 0.94, f_beta: 0.94
train: step: 3287, loss: 0.046237003058195114, acc: 0.94, recall: 0.94, precision: 0.94, f_beta: 0.94
train: step: 3288, loss: 0.04345713183283806, acc: 0.96, recall: 0.96, precision: 0.962962962962963, f_beta: 0.9599358974358974
train: step: 3289, loss: 0.029313793405890465, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 3290, loss: 0.03292468935251236, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 3291, loss: 0.05633709952235222, acc: 0.91, recall: 0.91, precision: 0.9101640656262505, f_beta: 0.90999099909991
train: step: 3292, loss: 0.02944377437233925, acc: 0.96, recall: 0.96, precision: 0.96, f_beta: 0.96
train: step: 3293, loss: 0.0316062793135643, acc: 0.96, recall: 0.96, precision: 0.96, f_beta: 0.96
train: step: 3294, loss: 0.05892154574394226, acc: 0.92, recall: 0.9199999999999999, precision: 0.9206730769230769, f_beta: 0.919967987194878
train: step: 3295, loss: 0.03914863243699074, acc: 0.96, recall: 0.96, precision: 0.9607371794871795, f_beta: 0.959983993597439
train: step: 3296, loss: 0.03510914742946625, acc: 0.96, recall: 0.96, precision: 0.962962962962963, f_beta: 0.9599358974358974
train: step: 3297, loss: 0.029854560270905495, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 3298, loss: 0.02872040681540966, acc: 0.96, recall: 0.96, precision: 0.962962962962963, f_beta: 0.9599358974358974
train: step: 3299, loss: 0.030589239671826363, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 3300, loss: 0.03152906522154808, acc: 0.96, recall: 0.96, precision: 0.96, f_beta: 0.96
train: step: 3301, loss: 0.023612195625901222, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 3302, loss: 0.06443016976118088, acc: 0.92, recall: 0.9199999999999999, precision: 0.9206730769230769, f_beta: 0.919967987194878
train: step: 3303, loss: 0.02665102481842041, acc: 0.97, recall: 0.97, precision: 0.9716981132075472, f_beta: 0.9699729756781104
train: step: 3304, loss: 0.05958430469036102, acc: 0.93, recall: 0.9299999999999999, precision: 0.9385964912280702, f_beta: 0.9296553110240177
train: step: 3305, loss: 0.039057131856679916, acc: 0.96, recall: 0.96, precision: 0.9607371794871795, f_beta: 0.959983993597439
train: step: 3306, loss: 0.08412384986877441, acc: 0.89, recall: 0.89, precision: 0.8914090726615818, f_beta: 0.8899009108197378
train: step: 3307, loss: 0.05472927913069725, acc: 0.93, recall: 0.9299999999999999, precision: 0.930172068827531, f_beta: 0.9299929992999301
train: step: 3308, loss: 0.02429916337132454, acc: 0.97, recall: 0.97, precision: 0.9716981132075472, f_beta: 0.9699729756781104
train: step: 3309, loss: 0.02993321418762207, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 3310, loss: 0.032815661281347275, acc: 0.95, recall: 0.95, precision: 0.9501800720288115, f_beta: 0.9499949994999499
train: step: 3311, loss: 0.021303901448845863, acc: 0.98, recall: 0.98, precision: 0.98, f_beta: 0.98
train: step: 3312, loss: 0.0281508918851614, acc: 0.97, recall: 0.97, precision: 0.9716981132075472, f_beta: 0.9699729756781104
train: step: 3313, loss: 0.04407153278589249, acc: 0.96, recall: 0.96, precision: 0.9607371794871795, f_beta: 0.959983993597439
train: step: 3314, loss: 0.014358949847519398, acc: 0.98, recall: 0.98, precision: 0.9807692307692308, f_beta: 0.9799919967987194
train: step: 3315, loss: 0.014878812246024609, acc: 0.98, recall: 0.98, precision: 0.98, f_beta: 0.98
train: step: 3316, loss: 0.028646139428019524, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 3317, loss: 0.03265312686562538, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 3318, loss: 0.03871173784136772, acc: 0.95, recall: 0.95, precision: 0.9545454545454546, f_beta: 0.949874686716792
train: step: 3319, loss: 0.020400309935212135, acc: 0.98, recall: 0.98, precision: 0.9807692307692308, f_beta: 0.9799919967987194
train: step: 3320, loss: 0.025925494730472565, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 3321, loss: 0.010506348684430122, acc: 0.99, recall: 0.99, precision: 0.9901960784313726, f_beta: 0.98999899989999
train: step: 3322, loss: 0.06860144436359406, acc: 0.92, recall: 0.9199999999999999, precision: 0.9206730769230769, f_beta: 0.919967987194878
train: step: 3323, loss: 0.06234070658683777, acc: 0.92, recall: 0.92, precision: 0.92, f_beta: 0.92
train: step: 3324, loss: 0.017764009535312653, acc: 0.98, recall: 0.98, precision: 0.98, f_beta: 0.98
train: step: 3325, loss: 0.01825430430471897, acc: 0.98, recall: 0.98, precision: 0.9807692307692308, f_beta: 0.9799919967987194
train: step: 3326, loss: 0.046482153236866, acc: 0.94, recall: 0.94, precision: 0.9428341384863124, f_beta: 0.9399038461538463
train: step: 3327, loss: 0.0405937097966671, acc: 0.95, recall: 0.95, precision: 0.9516258530710557, f_beta: 0.949954959463517
train: step: 3328, loss: 0.05904286354780197, acc: 0.92, recall: 0.9199999999999999, precision: 0.9206730769230769, f_beta: 0.919967987194878
train: step: 3329, loss: 0.058828845620155334, acc: 0.93, recall: 0.9299999999999999, precision: 0.930172068827531, f_beta: 0.9299929992999301
train: step: 3330, loss: 0.02396487258374691, acc: 0.98, recall: 0.98, precision: 0.98, f_beta: 0.98
train: step: 3331, loss: 0.028636150062084198, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 3332, loss: 0.03602219000458717, acc: 0.94, recall: 0.94, precision: 0.9407051282051282, f_beta: 0.9399759903961584
train: step: 3333, loss: 0.047609634697437286, acc: 0.95, recall: 0.95, precision: 0.9501800720288115, f_beta: 0.9499949994999499
train: step: 3334, loss: 0.06308351457118988, acc: 0.93, recall: 0.9299999999999999, precision: 0.930172068827531, f_beta: 0.9299929992999301
train: step: 3335, loss: 0.04747884348034859, acc: 0.95, recall: 0.95, precision: 0.9501800720288115, f_beta: 0.9499949994999499
train: step: 3336, loss: 0.07100526243448257, acc: 0.92, recall: 0.9199999999999999, precision: 0.9227053140096618, f_beta: 0.9198717948717948
train: step: 3337, loss: 0.03764696419239044, acc: 0.95, recall: 0.95, precision: 0.9501800720288115, f_beta: 0.9499949994999499
train: step: 3338, loss: 0.016265088692307472, acc: 0.98, recall: 0.98, precision: 0.9807692307692308, f_beta: 0.9799919967987194
train: step: 3339, loss: 0.07162471115589142, acc: 0.91, recall: 0.91, precision: 0.9101640656262505, f_beta: 0.90999099909991
train: step: 3340, loss: 0.03575005754828453, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 3341, loss: 0.03809874877333641, acc: 0.95, recall: 0.95, precision: 0.9501800720288115, f_beta: 0.9499949994999499
train: step: 3342, loss: 0.06738941371440887, acc: 0.9, recall: 0.9, precision: 0.9006410256410255, f_beta: 0.8999599839935974
train: step: 3343, loss: 0.023668890818953514, acc: 0.97, recall: 0.97, precision: 0.9716981132075472, f_beta: 0.9699729756781104
train: step: 3344, loss: 0.03077108785510063, acc: 0.96, recall: 0.96, precision: 0.9607371794871795, f_beta: 0.959983993597439
train: step: 3345, loss: 0.014956560917198658, acc: 0.98, recall: 0.98, precision: 0.9807692307692308, f_beta: 0.9799919967987194
train: step: 3346, loss: 0.02267143316566944, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 3347, loss: 0.03901507332921028, acc: 0.96, recall: 0.96, precision: 0.9607371794871795, f_beta: 0.959983993597439
train: step: 3348, loss: 0.03622179105877876, acc: 0.96, recall: 0.96, precision: 0.9607371794871795, f_beta: 0.959983993597439
train: step: 3349, loss: 0.026467109099030495, acc: 0.96, recall: 0.96, precision: 0.96, f_beta: 0.96
train: step: 3350, loss: 0.03007337637245655, acc: 0.95, recall: 0.95, precision: 0.9501800720288115, f_beta: 0.9499949994999499
train: step: 3351, loss: 0.027694081887602806, acc: 0.95, recall: 0.95, precision: 0.9501800720288115, f_beta: 0.9499949994999499
train: step: 3352, loss: 0.04337432235479355, acc: 0.93, recall: 0.9299999999999999, precision: 0.930172068827531, f_beta: 0.9299929992999301
train: step: 3353, loss: 0.03353001922369003, acc: 0.96, recall: 0.96, precision: 0.96, f_beta: 0.96
train: step: 3354, loss: 0.046501483768224716, acc: 0.94, recall: 0.94, precision: 0.9407051282051282, f_beta: 0.9399759903961584
train: step: 3355, loss: 0.014485558494925499, acc: 0.99, recall: 0.99, precision: 0.9901960784313726, f_beta: 0.98999899989999
train: step: 3356, loss: 0.025637874379754066, acc: 0.97, recall: 0.97, precision: 0.9716981132075472, f_beta: 0.9699729756781104
train: step: 3357, loss: 0.04784177616238594, acc: 0.93, recall: 0.9299999999999999, precision: 0.9315535929345644, f_beta: 0.9299369432489241
train: step: 3358, loss: 0.03665686026215553, acc: 0.95, recall: 0.95, precision: 0.9501800720288115, f_beta: 0.9499949994999499
train: step: 3359, loss: 0.03323265165090561, acc: 0.95, recall: 0.95, precision: 0.9516258530710557, f_beta: 0.949954959463517
train: step: 3360, loss: 0.026793360710144043, acc: 0.97, recall: 0.97, precision: 0.9716981132075472, f_beta: 0.9699729756781104
train: step: 3361, loss: 0.03856271877884865, acc: 0.96, recall: 0.96, precision: 0.9607371794871795, f_beta: 0.959983993597439
train: step: 3362, loss: 0.039417777210474014, acc: 0.95, recall: 0.95, precision: 0.9516258530710557, f_beta: 0.949954959463517
train: step: 3363, loss: 0.021419383585453033, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 3364, loss: 0.08610241860151291, acc: 0.87, recall: 0.87, precision: 0.8737373737373737, f_beta: 0.8696741854636592
train: step: 3365, loss: 0.05650702491402626, acc: 0.93, recall: 0.9299999999999999, precision: 0.930172068827531, f_beta: 0.9299929992999301
train: step: 3366, loss: 0.03680344298481941, acc: 0.94, recall: 0.94, precision: 0.9407051282051282, f_beta: 0.9399759903961584
train: step: 3367, loss: 0.005113937892019749, acc: 0.99, recall: 0.99, precision: 0.9901960784313726, f_beta: 0.98999899989999
train: step: 3368, loss: 0.03282899037003517, acc: 0.96, recall: 0.96, precision: 0.9607371794871795, f_beta: 0.959983993597439
train: step: 3369, loss: 0.06187451258301735, acc: 0.92, recall: 0.9199999999999999, precision: 0.9261363636363636, f_beta: 0.9197109594540345
train: step: 3370, loss: 0.03212533891201019, acc: 0.96, recall: 0.96, precision: 0.96, f_beta: 0.96
train: step: 3371, loss: 0.03573671728372574, acc: 0.95, recall: 0.95, precision: 0.9501800720288115, f_beta: 0.9499949994999499
train: step: 3372, loss: 0.018304161727428436, acc: 0.98, recall: 0.98, precision: 0.98, f_beta: 0.98
train: step: 3373, loss: 0.03161704167723656, acc: 0.94, recall: 0.94, precision: 0.9428341384863124, f_beta: 0.9399038461538463
train: step: 3374, loss: 0.012421446852385998, acc: 0.98, recall: 0.98, precision: 0.9807692307692308, f_beta: 0.9799919967987194
train: step: 3375, loss: 0.037398356944322586, acc: 0.96, recall: 0.96, precision: 0.962962962962963, f_beta: 0.9599358974358974
train: step: 3376, loss: 0.03334788978099823, acc: 0.96, recall: 0.96, precision: 0.962962962962963, f_beta: 0.9599358974358974
train: step: 3377, loss: 0.031750548630952835, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 3378, loss: 0.05103308707475662, acc: 0.94, recall: 0.94, precision: 0.9407051282051282, f_beta: 0.9399759903961584
train: step: 3379, loss: 0.042808715254068375, acc: 0.95, recall: 0.95, precision: 0.9516258530710557, f_beta: 0.949954959463517
train: step: 3380, loss: 0.026261355727910995, acc: 0.96, recall: 0.96, precision: 0.9607371794871795, f_beta: 0.959983993597439
train: step: 3381, loss: 0.044967204332351685, acc: 0.95, recall: 0.95, precision: 0.9501800720288115, f_beta: 0.9499949994999499
train: step: 3382, loss: 0.027305305004119873, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 3383, loss: 0.0604608878493309, acc: 0.95, recall: 0.95, precision: 0.9516258530710557, f_beta: 0.949954959463517
train: step: 3384, loss: 0.029941000044345856, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 3385, loss: 0.029740549623966217, acc: 0.96, recall: 0.96, precision: 0.9607371794871795, f_beta: 0.959983993597439
train: step: 3386, loss: 0.01638098992407322, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 3387, loss: 0.03686980530619621, acc: 0.95, recall: 0.95, precision: 0.9501800720288115, f_beta: 0.9499949994999499
train: step: 3388, loss: 0.037892576307058334, acc: 0.94, recall: 0.94, precision: 0.94, f_beta: 0.94
train: step: 3389, loss: 0.03796183317899704, acc: 0.95, recall: 0.95, precision: 0.9545454545454546, f_beta: 0.949874686716792
train: step: 3390, loss: 0.030954336747527122, acc: 0.95, recall: 0.95, precision: 0.9501800720288115, f_beta: 0.9499949994999499
train: step: 3391, loss: 0.02600746415555477, acc: 0.96, recall: 0.96, precision: 0.96, f_beta: 0.96
train: step: 3392, loss: 0.014400593936443329, acc: 0.98, recall: 0.98, precision: 0.98, f_beta: 0.98
train: step: 3393, loss: 0.038961321115493774, acc: 0.94, recall: 0.94, precision: 0.9407051282051282, f_beta: 0.9399759903961584
train: step: 3394, loss: 0.020171718671917915, acc: 0.98, recall: 0.98, precision: 0.98, f_beta: 0.98
train: step: 3395, loss: 0.034496355801820755, acc: 0.95, recall: 0.95, precision: 0.9516258530710557, f_beta: 0.949954959463517
train: step: 3396, loss: 0.0016248100437223911, acc: 1.0, recall: 1.0, precision: 1.0, f_beta: 1.0
train: step: 3397, loss: 0.026259107515215874, acc: 0.96, recall: 0.96, precision: 0.9607371794871795, f_beta: 0.959983993597439
train: step: 3398, loss: 0.029012197628617287, acc: 0.96, recall: 0.96, precision: 0.9607371794871795, f_beta: 0.959983993597439
train: step: 3399, loss: 0.06035089120268822, acc: 0.93, recall: 0.9299999999999999, precision: 0.930172068827531, f_beta: 0.9299929992999301
train: step: 3400, loss: 0.00975814089179039, acc: 0.99, recall: 0.99, precision: 0.9901960784313726, f_beta: 0.98999899989999
train: step: 3401, loss: 0.020878104493021965, acc: 0.98, recall: 0.98, precision: 0.98, f_beta: 0.98
train: step: 3402, loss: 0.062107209116220474, acc: 0.93, recall: 0.9299999999999999, precision: 0.930172068827531, f_beta: 0.9299929992999301
train: step: 3403, loss: 0.02085370384156704, acc: 0.97, recall: 0.97, precision: 0.9716981132075472, f_beta: 0.9699729756781104
train: step: 3404, loss: 0.026001447811722755, acc: 0.98, recall: 0.98, precision: 0.9807692307692308, f_beta: 0.9799919967987194
train: step: 3405, loss: 0.033374689519405365, acc: 0.97, recall: 0.97, precision: 0.9716981132075472, f_beta: 0.9699729756781104
train: step: 3406, loss: 0.022629471495747566, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 3407, loss: 0.026220034807920456, acc: 0.97, recall: 0.97, precision: 0.9716981132075472, f_beta: 0.9699729756781104
train: step: 3408, loss: 0.03070853278040886, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 3409, loss: 0.032194893807172775, acc: 0.96, recall: 0.96, precision: 0.9607371794871795, f_beta: 0.959983993597439
train: step: 3410, loss: 0.01951839029788971, acc: 0.98, recall: 0.98, precision: 0.9807692307692308, f_beta: 0.9799919967987194
train: step: 3411, loss: 0.05415520817041397, acc: 0.93, recall: 0.9299999999999999, precision: 0.9315535929345644, f_beta: 0.9299369432489241
train: step: 3412, loss: 0.018617762252688408, acc: 0.98, recall: 0.98, precision: 0.9807692307692308, f_beta: 0.9799919967987194
train: step: 3413, loss: 0.06155740097165108, acc: 0.92, recall: 0.9199999999999999, precision: 0.9227053140096618, f_beta: 0.9198717948717948
train: step: 3414, loss: 0.0269533209502697, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 3415, loss: 0.022889424115419388, acc: 0.98, recall: 0.98, precision: 0.98, f_beta: 0.98
train: step: 3416, loss: 0.09225711971521378, acc: 0.87, recall: 0.87, precision: 0.8701480592236894, f_beta: 0.8699869986998701
train: step: 3417, loss: 0.03444630652666092, acc: 0.95, recall: 0.95, precision: 0.9501800720288115, f_beta: 0.9499949994999499
train: step: 3418, loss: 0.031686700880527496, acc: 0.95, recall: 0.95, precision: 0.9516258530710557, f_beta: 0.949954959463517
train: step: 3419, loss: 0.018536271527409554, acc: 0.98, recall: 0.98, precision: 0.98, f_beta: 0.98
train: step: 3420, loss: 0.011413109488785267, acc: 0.99, recall: 0.99, precision: 0.9901960784313726, f_beta: 0.98999899989999
train: step: 3421, loss: 0.027373678982257843, acc: 0.96, recall: 0.96, precision: 0.96, f_beta: 0.96
train: step: 3422, loss: 0.036822132766246796, acc: 0.95, recall: 0.95, precision: 0.9516258530710557, f_beta: 0.949954959463517
train: step: 3423, loss: 0.05060650408267975, acc: 0.93, recall: 0.9299999999999999, precision: 0.930172068827531, f_beta: 0.9299929992999301
train: step: 3424, loss: 0.02298879064619541, acc: 0.98, recall: 0.98, precision: 0.98, f_beta: 0.98
train: step: 3425, loss: 0.0348350889980793, acc: 0.96, recall: 0.96, precision: 0.96, f_beta: 0.96
train: step: 3426, loss: 0.018795359879732132, acc: 0.98, recall: 0.98, precision: 0.9807692307692308, f_beta: 0.9799919967987194
train: step: 3427, loss: 0.04718573018908501, acc: 0.95, recall: 0.95, precision: 0.9516258530710557, f_beta: 0.949954959463517
train: step: 3428, loss: 0.02145729586482048, acc: 0.98, recall: 0.98, precision: 0.9807692307692308, f_beta: 0.9799919967987194
train: step: 3429, loss: 0.0173193272203207, acc: 0.99, recall: 0.99, precision: 0.9901960784313726, f_beta: 0.98999899989999
train: step: 3430, loss: 0.015225895680487156, acc: 0.98, recall: 0.98, precision: 0.98, f_beta: 0.98
train: step: 3431, loss: 0.005876211915165186, acc: 0.99, recall: 0.99, precision: 0.9901960784313726, f_beta: 0.98999899989999
train: step: 3432, loss: 0.02704331837594509, acc: 0.96, recall: 0.96, precision: 0.962962962962963, f_beta: 0.9599358974358974
train: step: 3433, loss: 0.03254929557442665, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 3434, loss: 0.046929702162742615, acc: 0.92, recall: 0.9199999999999999, precision: 0.9206730769230769, f_beta: 0.919967987194878
train: step: 3435, loss: 0.03805501386523247, acc: 0.96, recall: 0.96, precision: 0.9607371794871795, f_beta: 0.959983993597439
train: step: 3436, loss: 0.017945950850844383, acc: 0.99, recall: 0.99, precision: 0.9901960784313726, f_beta: 0.98999899989999
train: step: 3437, loss: 0.02567044459283352, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 3438, loss: 0.026968633756041527, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 3439, loss: 0.07848788052797318, acc: 0.89, recall: 0.89, precision: 0.8914090726615818, f_beta: 0.8899009108197378
train: step: 3440, loss: 0.020255813375115395, acc: 0.97, recall: 0.97, precision: 0.9716981132075472, f_beta: 0.9699729756781104
train: step: 3441, loss: 0.018438130617141724, acc: 0.98, recall: 0.98, precision: 0.9807692307692308, f_beta: 0.9799919967987194
train: step: 3442, loss: 0.0478934608399868, acc: 0.94, recall: 0.94, precision: 0.9464285714285714, f_beta: 0.939783219590526
train: step: 3443, loss: 0.012537162750959396, acc: 0.99, recall: 0.99, precision: 0.9901960784313726, f_beta: 0.98999899989999
train: step: 3444, loss: 0.08988699316978455, acc: 0.88, recall: 0.88, precision: 0.8824476650563606, f_beta: 0.8798076923076923
train: step: 3445, loss: 0.06166796758770943, acc: 0.92, recall: 0.9199999999999999, precision: 0.9206730769230769, f_beta: 0.919967987194878
train: step: 3446, loss: 0.025479692965745926, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 3447, loss: 0.05479824170470238, acc: 0.94, recall: 0.94, precision: 0.94, f_beta: 0.94
train: step: 3448, loss: 0.04915515333414078, acc: 0.94, recall: 0.94, precision: 0.94, f_beta: 0.94
train: step: 3449, loss: 0.05006536468863487, acc: 0.94, recall: 0.94, precision: 0.9407051282051282, f_beta: 0.9399759903961584
train: step: 3450, loss: 0.049178630113601685, acc: 0.93, recall: 0.9299999999999999, precision: 0.930172068827531, f_beta: 0.9299929992999301
train: step: 3451, loss: 0.032525673508644104, acc: 0.96, recall: 0.96, precision: 0.96, f_beta: 0.96
train: step: 3452, loss: 0.02224596031010151, acc: 0.98, recall: 0.98, precision: 0.9807692307692308, f_beta: 0.9799919967987194
train: step: 3453, loss: 0.00263162050396204, acc: 1.0, recall: 1.0, precision: 1.0, f_beta: 1.0
train: step: 3454, loss: 0.03145674988627434, acc: 0.96, recall: 0.96, precision: 0.9607371794871795, f_beta: 0.959983993597439
train: step: 3455, loss: 0.057966459542512894, acc: 0.94, recall: 0.94, precision: 0.94, f_beta: 0.94
train: step: 3456, loss: 0.03681511804461479, acc: 0.95, recall: 0.95, precision: 0.9516258530710557, f_beta: 0.949954959463517
train: step: 3457, loss: 0.024048279970884323, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 3458, loss: 0.03917393833398819, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 3459, loss: 0.022801857441663742, acc: 0.96, recall: 0.96, precision: 0.9607371794871795, f_beta: 0.959983993597439
train: step: 3460, loss: 0.0007426867377944291, acc: 1.0, recall: 1.0, precision: 1.0, f_beta: 1.0
train: step: 3461, loss: 0.023142831400036812, acc: 0.97, recall: 0.97, precision: 0.9716981132075472, f_beta: 0.9699729756781104
train: step: 3462, loss: 0.014572403393685818, acc: 0.99, recall: 0.99, precision: 0.9901960784313726, f_beta: 0.98999899989999
train: step: 3463, loss: 0.016818758100271225, acc: 0.98, recall: 0.98, precision: 0.9807692307692308, f_beta: 0.9799919967987194
train: step: 3464, loss: 0.041317787021398544, acc: 0.95, recall: 0.95, precision: 0.9501800720288115, f_beta: 0.9499949994999499
train: step: 3465, loss: 0.012802495621144772, acc: 0.99, recall: 0.99, precision: 0.9901960784313726, f_beta: 0.98999899989999
train: step: 3466, loss: 0.004263207316398621, acc: 0.99, recall: 0.99, precision: 0.9901960784313726, f_beta: 0.98999899989999
train: step: 3467, loss: 0.037509966641664505, acc: 0.96, recall: 0.96, precision: 0.9607371794871795, f_beta: 0.959983993597439
train: step: 3468, loss: 0.014104501344263554, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 3469, loss: 0.03535358980298042, acc: 0.95, recall: 0.95, precision: 0.9501800720288115, f_beta: 0.9499949994999499
train: step: 3470, loss: 0.024762144312262535, acc: 0.96, recall: 0.96, precision: 0.9607371794871795, f_beta: 0.959983993597439
train: step: 3471, loss: 0.048591699451208115, acc: 0.94, recall: 0.94, precision: 0.9407051282051282, f_beta: 0.9399759903961584
train: step: 3472, loss: 0.012240094132721424, acc: 0.99, recall: 0.99, precision: 0.9901960784313726, f_beta: 0.98999899989999
train: step: 3473, loss: 0.0015934434486553073, acc: 1.0, recall: 1.0, precision: 1.0, f_beta: 1.0
train: step: 3474, loss: 0.0353790707886219, acc: 0.96, recall: 0.96, precision: 0.962962962962963, f_beta: 0.9599358974358974
train: step: 3475, loss: 0.041268061846494675, acc: 0.95, recall: 0.95, precision: 0.9501800720288115, f_beta: 0.9499949994999499
train: step: 3476, loss: 0.03943578898906708, acc: 0.95, recall: 0.95, precision: 0.9516258530710557, f_beta: 0.949954959463517
train: step: 3477, loss: 0.05574134737253189, acc: 0.94, recall: 0.94, precision: 0.9407051282051282, f_beta: 0.9399759903961584
train: step: 3478, loss: 0.009154137223958969, acc: 0.99, recall: 0.99, precision: 0.9901960784313726, f_beta: 0.98999899989999
train: step: 3479, loss: 0.019125714898109436, acc: 0.97, recall: 0.97, precision: 0.9716981132075472, f_beta: 0.9699729756781104
train: step: 3480, loss: 0.017135296016931534, acc: 0.98, recall: 0.98, precision: 0.98, f_beta: 0.98
train: step: 3481, loss: 0.00815995316952467, acc: 0.99, recall: 0.99, precision: 0.9901960784313726, f_beta: 0.98999899989999
train: step: 3482, loss: 0.0307907834649086, acc: 0.96, recall: 0.96, precision: 0.9607371794871795, f_beta: 0.959983993597439
train: step: 3483, loss: 0.02770691178739071, acc: 0.96, recall: 0.96, precision: 0.9607371794871795, f_beta: 0.959983993597439
train: step: 3484, loss: 0.048859853297472, acc: 0.95, recall: 0.95, precision: 0.9545454545454546, f_beta: 0.949874686716792
train: step: 3485, loss: 0.019979624077677727, acc: 0.96, recall: 0.96, precision: 0.9607371794871795, f_beta: 0.959983993597439
train: step: 3486, loss: 0.08084570616483688, acc: 0.91, recall: 0.91, precision: 0.9101640656262505, f_beta: 0.90999099909991
train: step: 3487, loss: 0.018364721909165382, acc: 0.98, recall: 0.98, precision: 0.9807692307692308, f_beta: 0.9799919967987194
train: step: 3488, loss: 0.03352915495634079, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 3489, loss: 0.05940208584070206, acc: 0.93, recall: 0.9299999999999999, precision: 0.9315535929345644, f_beta: 0.9299369432489241
train: step: 3490, loss: 0.02358924224972725, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 3491, loss: 0.056505393236875534, acc: 0.93, recall: 0.9299999999999999, precision: 0.930172068827531, f_beta: 0.9299929992999301
train: step: 3492, loss: 0.02888142503798008, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 3493, loss: 0.04343615099787712, acc: 0.96, recall: 0.96, precision: 0.96, f_beta: 0.96
train: step: 3494, loss: 0.05794372037053108, acc: 0.93, recall: 0.9299999999999999, precision: 0.9343434343434343, f_beta: 0.9298245614035088
train: step: 3495, loss: 0.024185895919799805, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 3496, loss: 0.019644038751721382, acc: 0.98, recall: 0.98, precision: 0.98, f_beta: 0.98
train: step: 3497, loss: 0.00967229064553976, acc: 0.98, recall: 0.98, precision: 0.98, f_beta: 0.98
train: step: 3498, loss: 0.03446585312485695, acc: 0.96, recall: 0.96, precision: 0.9607371794871795, f_beta: 0.959983993597439
train: step: 3499, loss: 0.041654787957668304, acc: 0.96, recall: 0.96, precision: 0.9607371794871795, f_beta: 0.959983993597439
train: step: 3500, loss: 0.02707853354513645, acc: 0.96, recall: 0.96, precision: 0.9607371794871795, f_beta: 0.959983993597439
train: step: 3501, loss: 0.05153750255703926, acc: 0.93, recall: 0.9299999999999999, precision: 0.9343434343434343, f_beta: 0.9298245614035088
train: step: 3502, loss: 0.02009407803416252, acc: 0.98, recall: 0.98, precision: 0.9807692307692308, f_beta: 0.9799919967987194
train: step: 3503, loss: 0.06262829899787903, acc: 0.92, recall: 0.9199999999999999, precision: 0.9227053140096618, f_beta: 0.9198717948717948
train: step: 3504, loss: 0.018016057088971138, acc: 0.98, recall: 0.98, precision: 0.98, f_beta: 0.98
train: step: 3505, loss: 0.04349636659026146, acc: 0.94, recall: 0.94, precision: 0.94, f_beta: 0.94
train: step: 3506, loss: 0.05837372690439224, acc: 0.93, recall: 0.9299999999999999, precision: 0.9385964912280702, f_beta: 0.9296553110240177
train: step: 3507, loss: 0.042260292917490005, acc: 0.96, recall: 0.96, precision: 0.9607371794871795, f_beta: 0.959983993597439
train: step: 3508, loss: 0.028602931648492813, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 3509, loss: 0.022121209651231766, acc: 0.97, recall: 0.97, precision: 0.9716981132075472, f_beta: 0.9699729756781104
train: step: 3510, loss: 0.04626566916704178, acc: 0.94, recall: 0.94, precision: 0.9407051282051282, f_beta: 0.9399759903961584
train: step: 3511, loss: 0.031197642907500267, acc: 0.96, recall: 0.96, precision: 0.9607371794871795, f_beta: 0.959983993597439
train: step: 3512, loss: 0.0456700325012207, acc: 0.95, recall: 0.95, precision: 0.9501800720288115, f_beta: 0.9499949994999499
train: step: 3513, loss: 0.016765384003520012, acc: 0.98, recall: 0.98, precision: 0.9807692307692308, f_beta: 0.9799919967987194
train: step: 3514, loss: 0.03596045449376106, acc: 0.96, recall: 0.96, precision: 0.96, f_beta: 0.96
train: step: 3515, loss: 0.018173910677433014, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 3516, loss: 0.018270619213581085, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 3517, loss: 0.028450602665543556, acc: 0.96, recall: 0.96, precision: 0.9607371794871795, f_beta: 0.959983993597439
train: step: 3518, loss: 0.0363590307533741, acc: 0.96, recall: 0.96, precision: 0.9607371794871795, f_beta: 0.959983993597439
train: step: 3519, loss: 0.0530339814722538, acc: 0.93, recall: 0.9299999999999999, precision: 0.930172068827531, f_beta: 0.9299929992999301
train: step: 3520, loss: 0.01911688596010208, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 3521, loss: 0.03720468655228615, acc: 0.95, recall: 0.95, precision: 0.9501800720288115, f_beta: 0.9499949994999499
train: step: 3522, loss: 0.01981750689446926, acc: 0.97, recall: 0.97, precision: 0.9716981132075472, f_beta: 0.9699729756781104
train: step: 3523, loss: 0.020510582253336906, acc: 0.98, recall: 0.98, precision: 0.98, f_beta: 0.98
train: step: 3524, loss: 0.038807984441518784, acc: 0.96, recall: 0.96, precision: 0.96, f_beta: 0.96
train: step: 3525, loss: 0.0439855195581913, acc: 0.95, recall: 0.95, precision: 0.9501800720288115, f_beta: 0.9499949994999499
train: step: 3526, loss: 0.047353558242321014, acc: 0.93, recall: 0.9299999999999999, precision: 0.930172068827531, f_beta: 0.9299929992999301
train: step: 3527, loss: 0.032462991774082184, acc: 0.96, recall: 0.96, precision: 0.9607371794871795, f_beta: 0.959983993597439
train: step: 3528, loss: 0.010208181105554104, acc: 0.99, recall: 0.99, precision: 0.9901960784313726, f_beta: 0.98999899989999
train: step: 3529, loss: 0.025676453486084938, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 3530, loss: 0.04437864199280739, acc: 0.94, recall: 0.94, precision: 0.94, f_beta: 0.94
train: step: 3531, loss: 0.04895399510860443, acc: 0.94, recall: 0.94, precision: 0.9428341384863124, f_beta: 0.9399038461538463
train: step: 3532, loss: 0.03249482810497284, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 3533, loss: 0.03450627624988556, acc: 0.96, recall: 0.96, precision: 0.96, f_beta: 0.96
train: step: 3534, loss: 0.0295345950871706, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 3535, loss: 0.031956370919942856, acc: 0.96, recall: 0.96, precision: 0.96, f_beta: 0.96
train: step: 3536, loss: 0.02570420317351818, acc: 0.98, recall: 0.98, precision: 0.9807692307692308, f_beta: 0.9799919967987194
train: step: 3537, loss: 0.03707621619105339, acc: 0.95, recall: 0.95, precision: 0.9516258530710557, f_beta: 0.949954959463517
train: step: 3538, loss: 0.015845492482185364, acc: 0.98, recall: 0.98, precision: 0.98, f_beta: 0.98
train: step: 3539, loss: 0.04729507490992546, acc: 0.94, recall: 0.94, precision: 0.9428341384863124, f_beta: 0.9399038461538463
train: step: 3540, loss: 0.026598749682307243, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 3541, loss: 0.03590469807386398, acc: 0.96, recall: 0.96, precision: 0.96, f_beta: 0.96
train: step: 3542, loss: 0.03104516491293907, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 3543, loss: 0.03430987894535065, acc: 0.95, recall: 0.95, precision: 0.9501800720288115, f_beta: 0.9499949994999499
train: step: 3544, loss: 0.04966267570853233, acc: 0.92, recall: 0.92, precision: 0.92, f_beta: 0.92
train: step: 3545, loss: 0.06454002857208252, acc: 0.92, recall: 0.9199999999999999, precision: 0.9310344827586207, f_beta: 0.9194847020933978
train: step: 3546, loss: 0.01287718303501606, acc: 0.98, recall: 0.98, precision: 0.9807692307692308, f_beta: 0.9799919967987194
train: step: 3547, loss: 0.020287206396460533, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 3548, loss: 0.018310172483325005, acc: 0.98, recall: 0.98, precision: 0.98, f_beta: 0.98
train: step: 3549, loss: 0.049391698092222214, acc: 0.93, recall: 0.9299999999999999, precision: 0.930172068827531, f_beta: 0.9299929992999301
train: step: 3550, loss: 0.018825914710760117, acc: 0.98, recall: 0.98, precision: 0.9807692307692308, f_beta: 0.9799919967987194
train: step: 3551, loss: 0.0313643254339695, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 3552, loss: 0.025106923654675484, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 3553, loss: 0.07415156066417694, acc: 0.91, recall: 0.91, precision: 0.9101640656262505, f_beta: 0.90999099909991
train: step: 3554, loss: 0.03867455571889877, acc: 0.96, recall: 0.96, precision: 0.962962962962963, f_beta: 0.9599358974358974
train: step: 3555, loss: 0.030146727338433266, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 3556, loss: 0.03377971425652504, acc: 0.96, recall: 0.96, precision: 0.9607371794871795, f_beta: 0.959983993597439
train: step: 3557, loss: 0.03341059386730194, acc: 0.97, recall: 0.97, precision: 0.9716981132075472, f_beta: 0.9699729756781104
train: step: 3558, loss: 0.037995949387550354, acc: 0.96, recall: 0.96, precision: 0.9607371794871795, f_beta: 0.959983993597439
train: step: 3559, loss: 0.0702071338891983, acc: 0.9, recall: 0.9, precision: 0.9, f_beta: 0.9
train: step: 3560, loss: 0.0635281354188919, acc: 0.92, recall: 0.9199999999999999, precision: 0.9206730769230769, f_beta: 0.919967987194878
train: step: 3561, loss: 0.024494964629411697, acc: 0.96, recall: 0.96, precision: 0.96, f_beta: 0.96
train: step: 3562, loss: 0.011915585026144981, acc: 0.99, recall: 0.99, precision: 0.9901960784313726, f_beta: 0.98999899989999
train: step: 3563, loss: 0.021674856543540955, acc: 0.98, recall: 0.98, precision: 0.98, f_beta: 0.98
train: step: 3564, loss: 0.034437499940395355, acc: 0.96, recall: 0.96, precision: 0.96, f_beta: 0.96
train: step: 3565, loss: 0.029591187834739685, acc: 0.95, recall: 0.95, precision: 0.9501800720288115, f_beta: 0.9499949994999499
train: step: 3566, loss: 0.0036506045144051313, acc: 1.0, recall: 1.0, precision: 1.0, f_beta: 1.0
train: step: 3567, loss: 0.04383577033877373, acc: 0.95, recall: 0.95, precision: 0.9501800720288115, f_beta: 0.9499949994999499
train: step: 3568, loss: 0.019407613202929497, acc: 0.98, recall: 0.98, precision: 0.9807692307692308, f_beta: 0.9799919967987194
train: step: 3569, loss: 0.04871632531285286, acc: 0.94, recall: 0.94, precision: 0.94, f_beta: 0.94
train: step: 3570, loss: 0.030048592016100883, acc: 0.98, recall: 0.98, precision: 0.98, f_beta: 0.98
train: step: 3571, loss: 0.022117123007774353, acc: 0.98, recall: 0.98, precision: 0.9807692307692308, f_beta: 0.9799919967987194
train: step: 3572, loss: 0.030632751062512398, acc: 0.96, recall: 0.96, precision: 0.96, f_beta: 0.96
train: step: 3573, loss: 0.03192693367600441, acc: 0.96, recall: 0.96, precision: 0.96, f_beta: 0.96
train: step: 3574, loss: 0.10630063712596893, acc: 0.87, recall: 0.87, precision: 0.8823894171145101, f_beta: 0.8689384010484928
train: step: 3575, loss: 0.04025750607252121, acc: 0.94, recall: 0.94, precision: 0.9428341384863124, f_beta: 0.9399038461538463
train: step: 3576, loss: 0.05517635494470596, acc: 0.95, recall: 0.95, precision: 0.9501800720288115, f_beta: 0.9499949994999499
train: step: 3577, loss: 0.010258703492581844, acc: 1.0, recall: 1.0, precision: 1.0, f_beta: 1.0
train: step: 3578, loss: 0.03874645754694939, acc: 0.96, recall: 0.96, precision: 0.96, f_beta: 0.96
train: step: 3579, loss: 0.03884454071521759, acc: 0.96, recall: 0.96, precision: 0.9607371794871795, f_beta: 0.959983993597439
train: step: 3580, loss: 0.018290480598807335, acc: 0.98, recall: 0.98, precision: 0.98, f_beta: 0.98
train: step: 3581, loss: 0.011582132428884506, acc: 0.99, recall: 0.99, precision: 0.9901960784313726, f_beta: 0.98999899989999
train: step: 3582, loss: 0.03997430205345154, acc: 0.94, recall: 0.94, precision: 0.9428341384863124, f_beta: 0.9399038461538463
train: step: 3583, loss: 0.042008962482213974, acc: 0.96, recall: 0.96, precision: 0.96, f_beta: 0.96
train: step: 3584, loss: 0.01979127712547779, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 3585, loss: 0.017030009999871254, acc: 0.98, recall: 0.98, precision: 0.98, f_beta: 0.98
train: step: 3586, loss: 0.052169069647789, acc: 0.94, recall: 0.94, precision: 0.94, f_beta: 0.94
train: step: 3587, loss: 0.051422085613012314, acc: 0.92, recall: 0.9199999999999999, precision: 0.9227053140096618, f_beta: 0.9198717948717948
train: step: 3588, loss: 0.01852845586836338, acc: 0.98, recall: 0.98, precision: 0.9807692307692308, f_beta: 0.9799919967987194
train: step: 3589, loss: 0.017267391085624695, acc: 0.98, recall: 0.98, precision: 0.9807692307692308, f_beta: 0.9799919967987194
train: step: 3590, loss: 0.0600925050675869, acc: 0.92, recall: 0.9199999999999999, precision: 0.9227053140096618, f_beta: 0.9198717948717948
train: step: 3591, loss: 0.009510711766779423, acc: 1.0, recall: 1.0, precision: 1.0, f_beta: 1.0
train: step: 3592, loss: 0.01877472549676895, acc: 0.98, recall: 0.98, precision: 0.9807692307692308, f_beta: 0.9799919967987194
train: step: 3593, loss: 0.030545875430107117, acc: 0.96, recall: 0.96, precision: 0.96, f_beta: 0.96
train: step: 3594, loss: 0.012063205242156982, acc: 0.99, recall: 0.99, precision: 0.9901960784313726, f_beta: 0.98999899989999
train: step: 3595, loss: 0.03192862495779991, acc: 0.95, recall: 0.95, precision: 0.9501800720288115, f_beta: 0.9499949994999499
train: step: 3596, loss: 0.03946639969944954, acc: 0.96, recall: 0.96, precision: 0.9607371794871795, f_beta: 0.959983993597439
train: step: 3597, loss: 0.024882245808839798, acc: 0.97, recall: 0.97, precision: 0.9716981132075472, f_beta: 0.9699729756781104
train: step: 3598, loss: 0.003323106560856104, acc: 1.0, recall: 1.0, precision: 1.0, f_beta: 1.0
train: step: 3599, loss: 0.03619655221700668, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 3600, loss: 0.030298717319965363, acc: 0.95, recall: 0.95, precision: 0.9501800720288115, f_beta: 0.9499949994999499
train: step: 3601, loss: 0.05969563499093056, acc: 0.93, recall: 0.9299999999999999, precision: 0.9385964912280702, f_beta: 0.9296553110240177
train: step: 3602, loss: 0.050822172313928604, acc: 0.93, recall: 0.9299999999999999, precision: 0.9343434343434343, f_beta: 0.9298245614035088
train: step: 3603, loss: 0.020340528339147568, acc: 0.98, recall: 0.98, precision: 0.9807692307692308, f_beta: 0.9799919967987194
train: step: 3604, loss: 0.03177812322974205, acc: 0.95, recall: 0.95, precision: 0.9516258530710557, f_beta: 0.949954959463517
train: step: 3605, loss: 0.036918554455041885, acc: 0.95, recall: 0.95, precision: 0.9516258530710557, f_beta: 0.949954959463517
train: step: 3606, loss: 0.015679502859711647, acc: 0.99, recall: 0.99, precision: 0.9901960784313726, f_beta: 0.98999899989999
train: step: 3607, loss: 0.03100118599832058, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 3608, loss: 0.02024727314710617, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 3609, loss: 0.017001468688249588, acc: 0.98, recall: 0.98, precision: 0.98, f_beta: 0.98
train: step: 3610, loss: 0.037859026342630386, acc: 0.95, recall: 0.95, precision: 0.9516258530710557, f_beta: 0.949954959463517
train: step: 3611, loss: 0.015098785050213337, acc: 0.98, recall: 0.98, precision: 0.9807692307692308, f_beta: 0.9799919967987194
train: step: 3612, loss: 0.02737708017230034, acc: 0.96, recall: 0.96, precision: 0.9607371794871795, f_beta: 0.959983993597439
train: step: 3613, loss: 0.02314087375998497, acc: 0.97, recall: 0.97, precision: 0.9716981132075472, f_beta: 0.9699729756781104
train: step: 3614, loss: 0.015333443880081177, acc: 0.99, recall: 0.99, precision: 0.9901960784313726, f_beta: 0.98999899989999
train: step: 3615, loss: 0.03294353559613228, acc: 0.96, recall: 0.96, precision: 0.96, f_beta: 0.96
train: step: 3616, loss: 0.02996184304356575, acc: 0.96, recall: 0.96, precision: 0.9607371794871795, f_beta: 0.959983993597439
train: step: 3617, loss: 0.0346529595553875, acc: 0.96, recall: 0.96, precision: 0.962962962962963, f_beta: 0.9599358974358974
train: step: 3618, loss: 0.020985672250390053, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 3619, loss: 0.013601584360003471, acc: 0.99, recall: 0.99, precision: 0.9901960784313726, f_beta: 0.98999899989999
train: step: 3620, loss: 0.05027526244521141, acc: 0.95, recall: 0.95, precision: 0.9501800720288115, f_beta: 0.9499949994999499
train: step: 3621, loss: 0.007863651029765606, acc: 0.99, recall: 0.99, precision: 0.9901960784313726, f_beta: 0.98999899989999
train: step: 3622, loss: 0.011995227076113224, acc: 0.99, recall: 0.99, precision: 0.9901960784313726, f_beta: 0.98999899989999
train: step: 3623, loss: 0.03316431865096092, acc: 0.96, recall: 0.96, precision: 0.962962962962963, f_beta: 0.9599358974358974
train: step: 3624, loss: 0.03673512861132622, acc: 0.96, recall: 0.96, precision: 0.96, f_beta: 0.96
train: step: 3625, loss: 0.044851094484329224, acc: 0.94, recall: 0.94, precision: 0.9428341384863124, f_beta: 0.9399038461538463
train: step: 3626, loss: 0.03405854105949402, acc: 0.95, recall: 0.95, precision: 0.9501800720288115, f_beta: 0.9499949994999499
train: step: 3627, loss: 0.01321637723594904, acc: 0.98, recall: 0.98, precision: 0.98, f_beta: 0.98
train: step: 3628, loss: 0.03908606246113777, acc: 0.95, recall: 0.95, precision: 0.9501800720288115, f_beta: 0.9499949994999499
train: step: 3629, loss: 0.01717882603406906, acc: 0.98, recall: 0.98, precision: 0.98, f_beta: 0.98
train: step: 3630, loss: 0.029185539111495018, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 3631, loss: 0.01761901006102562, acc: 0.97, recall: 0.97, precision: 0.9716981132075472, f_beta: 0.9699729756781104
train: step: 3632, loss: 0.0490395650267601, acc: 0.94, recall: 0.94, precision: 0.9407051282051282, f_beta: 0.9399759903961584
train: step: 3633, loss: 0.04106104373931885, acc: 0.96, recall: 0.96, precision: 0.9607371794871795, f_beta: 0.959983993597439
train: step: 3634, loss: 0.0318126305937767, acc: 0.97, recall: 0.97, precision: 0.9716981132075472, f_beta: 0.9699729756781104
train: step: 3635, loss: 0.047040414065122604, acc: 0.95, recall: 0.95, precision: 0.9501800720288115, f_beta: 0.9499949994999499
train: step: 3636, loss: 0.030725207179784775, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 3637, loss: 0.04349784925580025, acc: 0.94, recall: 0.94, precision: 0.94, f_beta: 0.94
train: step: 3638, loss: 0.03283993899822235, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 3639, loss: 0.01714797504246235, acc: 0.98, recall: 0.98, precision: 0.9807692307692308, f_beta: 0.9799919967987194
train: step: 3640, loss: 0.03323403745889664, acc: 0.95, recall: 0.95, precision: 0.9516258530710557, f_beta: 0.949954959463517
train: step: 3641, loss: 0.01912916637957096, acc: 0.98, recall: 0.98, precision: 0.9807692307692308, f_beta: 0.9799919967987194
train: step: 3642, loss: 0.038964446634054184, acc: 0.95, recall: 0.95, precision: 0.9501800720288115, f_beta: 0.9499949994999499
train: step: 3643, loss: 0.015313230454921722, acc: 0.98, recall: 0.98, precision: 0.98, f_beta: 0.98
train: step: 3644, loss: 0.034918591380119324, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 3645, loss: 0.030699284747242928, acc: 0.96, recall: 0.96, precision: 0.96, f_beta: 0.96
train: step: 3646, loss: 0.06944704055786133, acc: 0.93, recall: 0.9299999999999999, precision: 0.9315535929345644, f_beta: 0.9299369432489241
train: step: 3647, loss: 0.049444980919361115, acc: 0.94, recall: 0.94, precision: 0.9407051282051282, f_beta: 0.9399759903961584
train: step: 3648, loss: 0.0035686588380485773, acc: 0.99, recall: 0.99, precision: 0.9901960784313726, f_beta: 0.98999899989999
train: step: 3649, loss: 0.07663328945636749, acc: 0.9, recall: 0.9, precision: 0.9006410256410255, f_beta: 0.8999599839935974
train: step: 3650, loss: 0.03235793113708496, acc: 0.96, recall: 0.96, precision: 0.9607371794871795, f_beta: 0.959983993597439
train: step: 3651, loss: 0.02203795127570629, acc: 0.98, recall: 0.98, precision: 0.98, f_beta: 0.98
train: step: 3652, loss: 0.061804674565792084, acc: 0.93, recall: 0.9299999999999999, precision: 0.9315535929345644, f_beta: 0.9299369432489241
train: step: 3653, loss: 0.03487703576683998, acc: 0.96, recall: 0.96, precision: 0.96, f_beta: 0.96
train: step: 3654, loss: 0.07800845056772232, acc: 0.88, recall: 0.88, precision: 0.8824476650563606, f_beta: 0.8798076923076923
train: step: 3655, loss: 0.06195944920182228, acc: 0.94, recall: 0.94, precision: 0.9428341384863124, f_beta: 0.9399038461538463
train: step: 3656, loss: 0.03765120357275009, acc: 0.96, recall: 0.96, precision: 0.9607371794871795, f_beta: 0.959983993597439
train: step: 3657, loss: 0.03629017621278763, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 3658, loss: 0.07262882590293884, acc: 0.9, recall: 0.9, precision: 0.9006410256410255, f_beta: 0.8999599839935974
train: step: 3659, loss: 0.020414292812347412, acc: 0.98, recall: 0.98, precision: 0.9807692307692308, f_beta: 0.9799919967987194
train: step: 3660, loss: 0.026623744517564774, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 3661, loss: 0.04180608689785004, acc: 0.94, recall: 0.94, precision: 0.9407051282051282, f_beta: 0.9399759903961584
train: step: 3662, loss: 0.05115845799446106, acc: 0.93, recall: 0.9299999999999999, precision: 0.930172068827531, f_beta: 0.9299929992999301
train: step: 3663, loss: 0.04791436716914177, acc: 0.93, recall: 0.9299999999999999, precision: 0.930172068827531, f_beta: 0.9299929992999301
train: step: 3664, loss: 0.035760119557380676, acc: 0.96, recall: 0.96, precision: 0.9607371794871795, f_beta: 0.959983993597439
train: step: 3665, loss: 0.0221145898103714, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 3666, loss: 0.05159224569797516, acc: 0.94, recall: 0.94, precision: 0.9407051282051282, f_beta: 0.9399759903961584
train: step: 3667, loss: 0.013377880677580833, acc: 0.99, recall: 0.99, precision: 0.9901960784313726, f_beta: 0.98999899989999
train: step: 3668, loss: 0.026305273175239563, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 3669, loss: 0.0011306089581921697, acc: 1.0, recall: 1.0, precision: 1.0, f_beta: 1.0
train: step: 3670, loss: 0.0443500317633152, acc: 0.95, recall: 0.95, precision: 0.9501800720288115, f_beta: 0.9499949994999499
train: step: 3671, loss: 0.016735441982746124, acc: 0.98, recall: 0.98, precision: 0.9807692307692308, f_beta: 0.9799919967987194
train: step: 3672, loss: 0.01792418770492077, acc: 0.98, recall: 0.98, precision: 0.9807692307692308, f_beta: 0.9799919967987194
train: step: 3673, loss: 0.024905286729335785, acc: 0.96, recall: 0.96, precision: 0.96, f_beta: 0.96
train: step: 3674, loss: 0.013282394036650658, acc: 0.99, recall: 0.99, precision: 0.9901960784313726, f_beta: 0.98999899989999
train: step: 3675, loss: 0.03363991901278496, acc: 0.97, recall: 0.97, precision: 0.9716981132075472, f_beta: 0.9699729756781104
train: step: 3676, loss: 0.02540060132741928, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 3677, loss: 0.043732237070798874, acc: 0.94, recall: 0.94, precision: 0.9407051282051282, f_beta: 0.9399759903961584
train: step: 3678, loss: 0.04765300452709198, acc: 0.94, recall: 0.94, precision: 0.9428341384863124, f_beta: 0.9399038461538463
train: step: 3679, loss: 0.026457810774445534, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 3680, loss: 0.06485015153884888, acc: 0.92, recall: 0.92, precision: 0.92, f_beta: 0.92
train: step: 3681, loss: 0.030678538605570793, acc: 0.96, recall: 0.96, precision: 0.962962962962963, f_beta: 0.9599358974358974
train: step: 3682, loss: 0.02067074365913868, acc: 0.97, recall: 0.97, precision: 0.9716981132075472, f_beta: 0.9699729756781104
train: step: 3683, loss: 0.023860745131969452, acc: 0.98, recall: 0.98, precision: 0.9807692307692308, f_beta: 0.9799919967987194
train: step: 3684, loss: 0.01792978122830391, acc: 0.98, recall: 0.98, precision: 0.9807692307692308, f_beta: 0.9799919967987194
train: step: 3685, loss: 0.02984793670475483, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 3686, loss: 0.021776549518108368, acc: 0.96, recall: 0.96, precision: 0.9607371794871795, f_beta: 0.959983993597439
train: step: 3687, loss: 0.022013630717992783, acc: 0.98, recall: 0.98, precision: 0.98, f_beta: 0.98
train: step: 3688, loss: 0.047262534499168396, acc: 0.94, recall: 0.94, precision: 0.94, f_beta: 0.94
train: step: 3689, loss: 0.03398270532488823, acc: 0.95, recall: 0.95, precision: 0.9501800720288115, f_beta: 0.9499949994999499
train: step: 3690, loss: 0.04349621385335922, acc: 0.94, recall: 0.94, precision: 0.9428341384863124, f_beta: 0.9399038461538463
train: step: 3691, loss: 0.040009383112192154, acc: 0.95, recall: 0.95, precision: 0.9501800720288115, f_beta: 0.9499949994999499
train: step: 3692, loss: 0.03458476439118385, acc: 0.96, recall: 0.96, precision: 0.9607371794871795, f_beta: 0.959983993597439
train: step: 3693, loss: 0.06027960777282715, acc: 0.94, recall: 0.94, precision: 0.9407051282051282, f_beta: 0.9399759903961584
train: step: 3694, loss: 0.01236653421074152, acc: 0.99, recall: 0.99, precision: 0.9901960784313726, f_beta: 0.98999899989999
train: step: 3695, loss: 0.0043591465801000595, acc: 1.0, recall: 1.0, precision: 1.0, f_beta: 1.0
train: step: 3696, loss: 0.015899378806352615, acc: 0.99, recall: 0.99, precision: 0.9901960784313726, f_beta: 0.98999899989999
train: step: 3697, loss: 0.03791695833206177, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 3698, loss: 0.020789314061403275, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 3699, loss: 0.01482765469700098, acc: 0.99, recall: 0.99, precision: 0.9901960784313726, f_beta: 0.98999899989999
train: step: 3700, loss: 0.0014582340372726321, acc: 1.0, recall: 1.0, precision: 1.0, f_beta: 1.0
train: step: 3701, loss: 0.020292330533266068, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 3702, loss: 0.031734127551317215, acc: 0.96, recall: 0.96, precision: 0.962962962962963, f_beta: 0.9599358974358974
train: step: 3703, loss: 0.02807055227458477, acc: 0.96, recall: 0.96, precision: 0.96, f_beta: 0.96
train: step: 3704, loss: 0.03495967760682106, acc: 0.95, recall: 0.95, precision: 0.9516258530710557, f_beta: 0.949954959463517
train: step: 3705, loss: 0.04340384528040886, acc: 0.95, recall: 0.95, precision: 0.9501800720288115, f_beta: 0.9499949994999499
train: step: 3706, loss: 0.04219045117497444, acc: 0.96, recall: 0.96, precision: 0.96, f_beta: 0.96
train: step: 3707, loss: 0.009836625307798386, acc: 0.99, recall: 0.99, precision: 0.9901960784313726, f_beta: 0.98999899989999
train: step: 3708, loss: 0.02537422627210617, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 3709, loss: 0.04375922679901123, acc: 0.95, recall: 0.95, precision: 0.9545454545454546, f_beta: 0.949874686716792
train: step: 3710, loss: 0.02134908176958561, acc: 0.98, recall: 0.98, precision: 0.98, f_beta: 0.98
train: step: 3711, loss: 0.06434181332588196, acc: 0.92, recall: 0.9199999999999999, precision: 0.9227053140096618, f_beta: 0.9198717948717948
train: step: 3712, loss: 0.07608912140130997, acc: 0.91, recall: 0.91, precision: 0.9101640656262505, f_beta: 0.90999099909991
train: step: 3713, loss: 0.03191964328289032, acc: 0.95, recall: 0.95, precision: 0.9501800720288115, f_beta: 0.9499949994999499
train: step: 3714, loss: 0.03173352777957916, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 3715, loss: 0.012897636741399765, acc: 0.98, recall: 0.98, precision: 0.98, f_beta: 0.98
train: step: 3716, loss: 0.0009414705564267933, acc: 1.0, recall: 1.0, precision: 1.0, f_beta: 1.0
train: step: 3717, loss: 0.032127365469932556, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 3718, loss: 0.021164722740650177, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 3719, loss: 0.053260430693626404, acc: 0.93, recall: 0.9299999999999999, precision: 0.9343434343434343, f_beta: 0.9298245614035088
train: step: 3720, loss: 0.07064113765954971, acc: 0.92, recall: 0.9199999999999999, precision: 0.9206730769230769, f_beta: 0.919967987194878
train: step: 3721, loss: 0.039367616176605225, acc: 0.94, recall: 0.94, precision: 0.9407051282051282, f_beta: 0.9399759903961584
train: step: 3722, loss: 0.03027237392961979, acc: 0.95, recall: 0.95, precision: 0.9545454545454546, f_beta: 0.949874686716792
train: step: 3723, loss: 0.027371525764465332, acc: 0.98, recall: 0.98, precision: 0.98, f_beta: 0.98
train: step: 3724, loss: 0.0397360622882843, acc: 0.96, recall: 0.96, precision: 0.96, f_beta: 0.96
train: step: 3725, loss: 0.03591686114668846, acc: 0.95, recall: 0.95, precision: 0.9501800720288115, f_beta: 0.9499949994999499
train: step: 3726, loss: 0.034119199961423874, acc: 0.96, recall: 0.96, precision: 0.962962962962963, f_beta: 0.9599358974358974
train: step: 3727, loss: 0.04353182762861252, acc: 0.95, recall: 0.95, precision: 0.9501800720288115, f_beta: 0.9499949994999499
train: step: 3728, loss: 0.0025773567613214254, acc: 1.0, recall: 1.0, precision: 1.0, f_beta: 1.0
train: step: 3729, loss: 0.05360366031527519, acc: 0.94, recall: 0.94, precision: 0.9407051282051282, f_beta: 0.9399759903961584
train: step: 3730, loss: 0.035324856638908386, acc: 0.95, recall: 0.95, precision: 0.9516258530710557, f_beta: 0.949954959463517
train: step: 3731, loss: 0.01894294284284115, acc: 0.98, recall: 0.98, precision: 0.98, f_beta: 0.98
train: step: 3732, loss: 0.054877281188964844, acc: 0.95, recall: 0.95, precision: 0.9501800720288115, f_beta: 0.9499949994999499
train: step: 3733, loss: 0.023763684555888176, acc: 0.98, recall: 0.98, precision: 0.98, f_beta: 0.98
train: step: 3734, loss: 0.005434040445834398, acc: 0.99, recall: 0.99, precision: 0.9901960784313726, f_beta: 0.98999899989999
train: step: 3735, loss: 0.04607323184609413, acc: 0.95, recall: 0.95, precision: 0.9501800720288115, f_beta: 0.9499949994999499
train: step: 3736, loss: 0.05494382977485657, acc: 0.94, recall: 0.94, precision: 0.94, f_beta: 0.94
train: step: 3737, loss: 0.02344714105129242, acc: 0.95, recall: 0.95, precision: 0.9516258530710557, f_beta: 0.949954959463517
train: step: 3738, loss: 0.019590741023421288, acc: 0.98, recall: 0.98, precision: 0.9807692307692308, f_beta: 0.9799919967987194
train: step: 3739, loss: 0.04205561801791191, acc: 0.94, recall: 0.94, precision: 0.94, f_beta: 0.94
train: step: 3740, loss: 0.03961041569709778, acc: 0.95, recall: 0.95, precision: 0.9501800720288115, f_beta: 0.9499949994999499
train: step: 3741, loss: 0.05471022054553032, acc: 0.95, recall: 0.95, precision: 0.9501800720288115, f_beta: 0.9499949994999499
train: step: 3742, loss: 0.04290369898080826, acc: 0.94, recall: 0.94, precision: 0.9428341384863124, f_beta: 0.9399038461538463
train: step: 3743, loss: 0.044103365391492844, acc: 0.95, recall: 0.95, precision: 0.9501800720288115, f_beta: 0.9499949994999499
train: step: 3744, loss: 0.04055281728506088, acc: 0.95, recall: 0.95, precision: 0.9501800720288115, f_beta: 0.9499949994999499
train: step: 3745, loss: 0.02911887690424919, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 3746, loss: 0.03402281552553177, acc: 0.96, recall: 0.96, precision: 0.9607371794871795, f_beta: 0.959983993597439
train: step: 3747, loss: 0.04255548119544983, acc: 0.95, recall: 0.95, precision: 0.9501800720288115, f_beta: 0.9499949994999499
train: step: 3748, loss: 0.03780052065849304, acc: 0.96, recall: 0.96, precision: 0.96, f_beta: 0.96
train: step: 3749, loss: 0.050194501876831055, acc: 0.93, recall: 0.9299999999999999, precision: 0.9343434343434343, f_beta: 0.9298245614035088
train: step: 3750, loss: 0.03389862924814224, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 3751, loss: 0.03725521266460419, acc: 0.96, recall: 0.96, precision: 0.96, f_beta: 0.96
train: step: 3752, loss: 0.04165782034397125, acc: 0.95, recall: 0.95, precision: 0.9501800720288115, f_beta: 0.9499949994999499
train: step: 3753, loss: 0.032033562660217285, acc: 0.96, recall: 0.96, precision: 0.96, f_beta: 0.96
train: step: 3754, loss: 0.0259403083473444, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 3755, loss: 0.03988312557339668, acc: 0.96, recall: 0.96, precision: 0.96, f_beta: 0.96
train: step: 3756, loss: 0.052414342761039734, acc: 0.94, recall: 0.94, precision: 0.9407051282051282, f_beta: 0.9399759903961584
train: step: 3757, loss: 0.03725828975439072, acc: 0.95, recall: 0.95, precision: 0.9516258530710557, f_beta: 0.949954959463517
train: step: 3758, loss: 0.025991620495915413, acc: 0.95, recall: 0.95, precision: 0.9501800720288115, f_beta: 0.9499949994999499
train: step: 3759, loss: 0.030172644183039665, acc: 0.97, recall: 0.97, precision: 0.9716981132075472, f_beta: 0.9699729756781104
train: step: 3760, loss: 0.025096401572227478, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 3761, loss: 0.007226886693388224, acc: 0.99, recall: 0.99, precision: 0.9901960784313726, f_beta: 0.98999899989999
train: step: 3762, loss: 0.04615979269146919, acc: 0.92, recall: 0.9199999999999999, precision: 0.9227053140096618, f_beta: 0.9198717948717948
train: step: 3763, loss: 0.040558043867349625, acc: 0.95, recall: 0.95, precision: 0.9545454545454546, f_beta: 0.949874686716792
train: step: 3764, loss: 0.030568132176995277, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 3765, loss: 0.039595965296030045, acc: 0.95, recall: 0.95, precision: 0.9545454545454546, f_beta: 0.949874686716792
train: step: 3766, loss: 0.043660491704940796, acc: 0.95, recall: 0.95, precision: 0.9516258530710557, f_beta: 0.949954959463517
train: step: 3767, loss: 0.0352814644575119, acc: 0.96, recall: 0.96, precision: 0.962962962962963, f_beta: 0.9599358974358974
train: step: 3768, loss: 0.050842780619859695, acc: 0.94, recall: 0.94, precision: 0.9407051282051282, f_beta: 0.9399759903961584
train: step: 3769, loss: 0.04609527438879013, acc: 0.94, recall: 0.94, precision: 0.9428341384863124, f_beta: 0.9399038461538463
train: step: 3770, loss: 0.010556456632912159, acc: 0.99, recall: 0.99, precision: 0.9901960784313726, f_beta: 0.98999899989999
train: step: 3771, loss: 0.015192549675703049, acc: 0.97, recall: 0.97, precision: 0.9716981132075472, f_beta: 0.9699729756781104
train: step: 3772, loss: 0.022181391716003418, acc: 0.98, recall: 0.98, precision: 0.9807692307692308, f_beta: 0.9799919967987194
train: step: 3773, loss: 0.037292610853910446, acc: 0.96, recall: 0.96, precision: 0.96, f_beta: 0.96
train: step: 3774, loss: 0.05487530678510666, acc: 0.93, recall: 0.9299999999999999, precision: 0.930172068827531, f_beta: 0.9299929992999301
train: step: 3775, loss: 0.04794397950172424, acc: 0.94, recall: 0.94, precision: 0.9464285714285714, f_beta: 0.939783219590526
train: step: 3776, loss: 0.061381325125694275, acc: 0.92, recall: 0.9199999999999999, precision: 0.9261363636363636, f_beta: 0.9197109594540345
train: step: 3777, loss: 0.03187035024166107, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 3778, loss: 0.02959543652832508, acc: 0.96, recall: 0.96, precision: 0.9607371794871795, f_beta: 0.959983993597439
train: step: 3779, loss: 0.04872940108180046, acc: 0.94, recall: 0.94, precision: 0.94, f_beta: 0.94
train: step: 3780, loss: 0.04573412239551544, acc: 0.94, recall: 0.94, precision: 0.9407051282051282, f_beta: 0.9399759903961584
train: step: 3781, loss: 0.030247006565332413, acc: 0.96, recall: 0.96, precision: 0.96, f_beta: 0.96
train: step: 3782, loss: 0.017995646223425865, acc: 0.98, recall: 0.98, precision: 0.9807692307692308, f_beta: 0.9799919967987194
train: step: 3783, loss: 0.028696348890662193, acc: 0.96, recall: 0.96, precision: 0.962962962962963, f_beta: 0.9599358974358974
train: step: 3784, loss: 0.02850155346095562, acc: 0.97, recall: 0.97, precision: 0.9716981132075472, f_beta: 0.9699729756781104
train: step: 3785, loss: 0.03206599876284599, acc: 0.96, recall: 0.96, precision: 0.9607371794871795, f_beta: 0.959983993597439
train: step: 3786, loss: 0.025703338906168938, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 3787, loss: 0.015427691861987114, acc: 0.99, recall: 0.99, precision: 0.9901960784313726, f_beta: 0.98999899989999
train: step: 3788, loss: 0.04700285568833351, acc: 0.95, recall: 0.95, precision: 0.9545454545454546, f_beta: 0.949874686716792
train: step: 3789, loss: 0.003041580319404602, acc: 1.0, recall: 1.0, precision: 1.0, f_beta: 1.0
train: step: 3790, loss: 0.046173952519893646, acc: 0.95, recall: 0.95, precision: 0.9545454545454546, f_beta: 0.949874686716792
train: step: 3791, loss: 0.06135232746601105, acc: 0.93, recall: 0.9299999999999999, precision: 0.930172068827531, f_beta: 0.9299929992999301
train: step: 3792, loss: 0.013218947686254978, acc: 0.99, recall: 0.99, precision: 0.9901960784313726, f_beta: 0.98999899989999
train: step: 3793, loss: 0.036147549748420715, acc: 0.96, recall: 0.96, precision: 0.9607371794871795, f_beta: 0.959983993597439
train: step: 3794, loss: 0.03032444231212139, acc: 0.96, recall: 0.96, precision: 0.9607371794871795, f_beta: 0.959983993597439
train: step: 3795, loss: 0.032266631722450256, acc: 0.95, recall: 0.95, precision: 0.9545454545454546, f_beta: 0.949874686716792
train: step: 3796, loss: 0.015655849128961563, acc: 0.98, recall: 0.98, precision: 0.9807692307692308, f_beta: 0.9799919967987194
train: step: 3797, loss: 0.01040062215179205, acc: 0.99, recall: 0.99, precision: 0.9901960784313726, f_beta: 0.98999899989999
train: step: 3798, loss: 0.06618435680866241, acc: 0.92, recall: 0.9199999999999999, precision: 0.9227053140096618, f_beta: 0.9198717948717948
train: step: 3799, loss: 0.057973772287368774, acc: 0.94, recall: 0.94, precision: 0.9407051282051282, f_beta: 0.9399759903961584
train: step: 3800, loss: 0.03839937224984169, acc: 0.94, recall: 0.94, precision: 0.9428341384863124, f_beta: 0.9399038461538463
train: step: 3801, loss: 0.028458725661039352, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 3802, loss: 0.01045531127601862, acc: 0.98, recall: 0.98, precision: 0.9807692307692308, f_beta: 0.9799919967987194
train: step: 3803, loss: 0.026236310601234436, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 3804, loss: 0.018381964415311813, acc: 0.98, recall: 0.98, precision: 0.98, f_beta: 0.98
train: step: 3805, loss: 0.013416026718914509, acc: 0.98, recall: 0.98, precision: 0.98, f_beta: 0.98
train: step: 3806, loss: 0.0559072345495224, acc: 0.94, recall: 0.94, precision: 0.9407051282051282, f_beta: 0.9399759903961584
train: step: 3807, loss: 0.00322645902633667, acc: 1.0, recall: 1.0, precision: 1.0, f_beta: 1.0
train: step: 3808, loss: 0.02046182192862034, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 3809, loss: 0.038704756647348404, acc: 0.95, recall: 0.95, precision: 0.9501800720288115, f_beta: 0.9499949994999499
train: step: 3810, loss: 0.05138619244098663, acc: 0.94, recall: 0.94, precision: 0.94, f_beta: 0.94
train: step: 3811, loss: 0.023203950375318527, acc: 0.98, recall: 0.98, precision: 0.98, f_beta: 0.98
train: step: 3812, loss: 0.02882339432835579, acc: 0.96, recall: 0.96, precision: 0.9607371794871795, f_beta: 0.959983993597439
train: step: 3813, loss: 0.017293935641646385, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 3814, loss: 0.02533966489136219, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 3815, loss: 0.010903250426054, acc: 0.99, recall: 0.99, precision: 0.9901960784313726, f_beta: 0.98999899989999
train: step: 3816, loss: 0.044148463755846024, acc: 0.95, recall: 0.95, precision: 0.9501800720288115, f_beta: 0.9499949994999499
train: step: 3817, loss: 0.025294523686170578, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 3818, loss: 0.00285230646841228, acc: 1.0, recall: 1.0, precision: 1.0, f_beta: 1.0
train: step: 3819, loss: 0.0037715602666139603, acc: 0.99, recall: 0.99, precision: 0.9901960784313726, f_beta: 0.98999899989999
train: step: 3820, loss: 0.048845142126083374, acc: 0.93, recall: 0.9299999999999999, precision: 0.9315535929345644, f_beta: 0.9299369432489241
train: step: 3821, loss: 0.04259848594665527, acc: 0.94, recall: 0.94, precision: 0.9428341384863124, f_beta: 0.9399038461538463
train: step: 3822, loss: 0.05417700111865997, acc: 0.93, recall: 0.9299999999999999, precision: 0.9315535929345644, f_beta: 0.9299369432489241
train: step: 3823, loss: 0.024116335436701775, acc: 0.98, recall: 0.98, precision: 0.98, f_beta: 0.98
train: step: 3824, loss: 0.030331861227750778, acc: 0.96, recall: 0.96, precision: 0.96, f_beta: 0.96
train: step: 3825, loss: 0.013841794803738594, acc: 0.98, recall: 0.98, precision: 0.9807692307692308, f_beta: 0.9799919967987194
train: step: 3826, loss: 0.006390899419784546, acc: 1.0, recall: 1.0, precision: 1.0, f_beta: 1.0
train: step: 3827, loss: 0.02115757204592228, acc: 0.98, recall: 0.98, precision: 0.98, f_beta: 0.98
train: step: 3828, loss: 0.043421708047389984, acc: 0.95, recall: 0.95, precision: 0.9501800720288115, f_beta: 0.9499949994999499
train: step: 3829, loss: 0.044703006744384766, acc: 0.95, recall: 0.95, precision: 0.9501800720288115, f_beta: 0.9499949994999499
train: step: 3830, loss: 0.013878648169338703, acc: 0.99, recall: 0.99, precision: 0.9901960784313726, f_beta: 0.98999899989999
train: step: 3831, loss: 0.03942443057894707, acc: 0.96, recall: 0.96, precision: 0.9607371794871795, f_beta: 0.959983993597439
train: step: 3832, loss: 0.033756278455257416, acc: 0.95, recall: 0.95, precision: 0.9516258530710557, f_beta: 0.949954959463517
train: step: 3833, loss: 0.031329989433288574, acc: 0.97, recall: 0.97, precision: 0.9716981132075472, f_beta: 0.9699729756781104
train: step: 3834, loss: 0.03754010051488876, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 3835, loss: 0.02437814697623253, acc: 0.98, recall: 0.98, precision: 0.98, f_beta: 0.98
train: step: 3836, loss: 0.026859447360038757, acc: 0.96, recall: 0.96, precision: 0.9607371794871795, f_beta: 0.959983993597439
train: step: 3837, loss: 0.026686066761612892, acc: 0.98, recall: 0.98, precision: 0.9807692307692308, f_beta: 0.9799919967987194
train: step: 3838, loss: 0.05234907940030098, acc: 0.92, recall: 0.9199999999999999, precision: 0.9310344827586207, f_beta: 0.9194847020933978
train: step: 3839, loss: 0.04515521228313446, acc: 0.94, recall: 0.94, precision: 0.9428341384863124, f_beta: 0.9399038461538463
train: step: 3840, loss: 0.037692513316869736, acc: 0.96, recall: 0.96, precision: 0.9607371794871795, f_beta: 0.959983993597439
train: step: 3841, loss: 0.006106278859078884, acc: 0.99, recall: 0.99, precision: 0.9901960784313726, f_beta: 0.98999899989999
train: step: 3842, loss: 0.08443674445152283, acc: 0.91, recall: 0.9099999999999999, precision: 0.9114813327980731, f_beta: 0.9099189270343309
train: step: 3843, loss: 0.03684353828430176, acc: 0.96, recall: 0.96, precision: 0.96, f_beta: 0.96
train: step: 3844, loss: 0.05087221786379814, acc: 0.93, recall: 0.9299999999999999, precision: 0.930172068827531, f_beta: 0.9299929992999301
train: step: 3845, loss: 0.01628333330154419, acc: 0.99, recall: 0.99, precision: 0.9901960784313726, f_beta: 0.98999899989999
train: step: 3846, loss: 0.06111649423837662, acc: 0.93, recall: 0.9299999999999999, precision: 0.9315535929345644, f_beta: 0.9299369432489241
train: step: 3847, loss: 0.019871186465024948, acc: 0.98, recall: 0.98, precision: 0.98, f_beta: 0.98
train: step: 3848, loss: 0.04518885537981987, acc: 0.95, recall: 0.95, precision: 0.9501800720288115, f_beta: 0.9499949994999499
train: step: 3849, loss: 0.04930552840232849, acc: 0.93, recall: 0.9299999999999999, precision: 0.930172068827531, f_beta: 0.9299929992999301
train: step: 3850, loss: 0.04059918224811554, acc: 0.95, recall: 0.95, precision: 0.9501800720288115, f_beta: 0.9499949994999499
train: step: 3851, loss: 0.03184764087200165, acc: 0.95, recall: 0.95, precision: 0.9501800720288115, f_beta: 0.9499949994999499
train: step: 3852, loss: 0.028109123930335045, acc: 0.96, recall: 0.96, precision: 0.9607371794871795, f_beta: 0.959983993597439
train: step: 3853, loss: 0.01850460097193718, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 3854, loss: 0.0069425213150680065, acc: 0.99, recall: 0.99, precision: 0.9901960784313726, f_beta: 0.98999899989999
train: step: 3855, loss: 0.036247748881578445, acc: 0.95, recall: 0.95, precision: 0.9501800720288115, f_beta: 0.9499949994999499
train: step: 3856, loss: 0.03673544526100159, acc: 0.95, recall: 0.95, precision: 0.9501800720288115, f_beta: 0.9499949994999499
train: step: 3857, loss: 0.03638460487127304, acc: 0.96, recall: 0.96, precision: 0.96, f_beta: 0.96
train: step: 3858, loss: 0.02290613390505314, acc: 0.98, recall: 0.98, precision: 0.98, f_beta: 0.98
train: step: 3859, loss: 0.06195366755127907, acc: 0.92, recall: 0.9199999999999999, precision: 0.9206730769230769, f_beta: 0.919967987194878
train: step: 3860, loss: 0.024714475497603416, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 3861, loss: 0.015945179387927055, acc: 0.98, recall: 0.98, precision: 0.9807692307692308, f_beta: 0.9799919967987194
train: step: 3862, loss: 0.049614906311035156, acc: 0.93, recall: 0.9299999999999999, precision: 0.930172068827531, f_beta: 0.9299929992999301
train: step: 3863, loss: 0.010820278897881508, acc: 0.99, recall: 0.99, precision: 0.9901960784313726, f_beta: 0.98999899989999
train: step: 3864, loss: 0.011826119385659695, acc: 0.99, recall: 0.99, precision: 0.9901960784313726, f_beta: 0.98999899989999
train: step: 3865, loss: 0.033831216394901276, acc: 0.96, recall: 0.96, precision: 0.96, f_beta: 0.96
train: step: 3866, loss: 0.027368968352675438, acc: 0.96, recall: 0.96, precision: 0.96, f_beta: 0.96
train: step: 3867, loss: 0.0073096067644655704, acc: 0.99, recall: 0.99, precision: 0.9901960784313726, f_beta: 0.98999899989999
train: step: 3868, loss: 0.02106579951941967, acc: 0.97, recall: 0.97, precision: 0.9716981132075472, f_beta: 0.9699729756781104
train: step: 3869, loss: 0.04527558386325836, acc: 0.93, recall: 0.9299999999999999, precision: 0.930172068827531, f_beta: 0.9299929992999301
train: step: 3870, loss: 0.020898249000310898, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 3871, loss: 0.008899050764739513, acc: 0.99, recall: 0.99, precision: 0.9901960784313726, f_beta: 0.98999899989999
train: step: 3872, loss: 0.06563248485326767, acc: 0.93, recall: 0.9299999999999999, precision: 0.930172068827531, f_beta: 0.9299929992999301
train: step: 3873, loss: 0.05401323363184929, acc: 0.95, recall: 0.95, precision: 0.9516258530710557, f_beta: 0.949954959463517
train: step: 3874, loss: 0.013929372653365135, acc: 0.98, recall: 0.98, precision: 0.9807692307692308, f_beta: 0.9799919967987194
train: step: 3875, loss: 0.02092014253139496, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 3876, loss: 0.03745201975107193, acc: 0.95, recall: 0.95, precision: 0.9516258530710557, f_beta: 0.949954959463517
train: step: 3877, loss: 0.024226879701018333, acc: 0.98, recall: 0.98, precision: 0.9807692307692308, f_beta: 0.9799919967987194
train: step: 3878, loss: 0.026677846908569336, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 3879, loss: 0.025855226442217827, acc: 0.96, recall: 0.96, precision: 0.96, f_beta: 0.96
train: step: 3880, loss: 0.03335995227098465, acc: 0.96, recall: 0.96, precision: 0.9607371794871795, f_beta: 0.959983993597439
train: step: 3881, loss: 0.02925260365009308, acc: 0.96, recall: 0.96, precision: 0.96, f_beta: 0.96
train: step: 3882, loss: 0.029006021097302437, acc: 0.96, recall: 0.96, precision: 0.962962962962963, f_beta: 0.9599358974358974
train: step: 3883, loss: 0.03067687898874283, acc: 0.96, recall: 0.96, precision: 0.962962962962963, f_beta: 0.9599358974358974
train: step: 3884, loss: 0.021630439907312393, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 3885, loss: 0.025427313521504402, acc: 0.98, recall: 0.98, precision: 0.9807692307692308, f_beta: 0.9799919967987194
train: step: 3886, loss: 0.033713020384311676, acc: 0.97, recall: 0.97, precision: 0.9716981132075472, f_beta: 0.9699729756781104
train: step: 3887, loss: 0.013981902971863747, acc: 0.98, recall: 0.98, precision: 0.9807692307692308, f_beta: 0.9799919967987194
train: step: 3888, loss: 0.04276701807975769, acc: 0.94, recall: 0.94, precision: 0.94, f_beta: 0.94
train: step: 3889, loss: 0.023026149719953537, acc: 0.96, recall: 0.96, precision: 0.96, f_beta: 0.96
train: step: 3890, loss: 0.0261519867926836, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 3891, loss: 0.020036835223436356, acc: 0.98, recall: 0.98, precision: 0.9807692307692308, f_beta: 0.9799919967987194
train: step: 3892, loss: 0.09058026224374771, acc: 0.9, recall: 0.9, precision: 0.9006410256410255, f_beta: 0.8999599839935974
train: step: 3893, loss: 0.07619988173246384, acc: 0.92, recall: 0.9199999999999999, precision: 0.9310344827586207, f_beta: 0.9194847020933978
train: step: 3894, loss: 0.03728606924414635, acc: 0.96, recall: 0.96, precision: 0.96, f_beta: 0.96
train: step: 3895, loss: 0.03819289058446884, acc: 0.95, recall: 0.95, precision: 0.9501800720288115, f_beta: 0.9499949994999499
train: step: 3896, loss: 0.017174916341900826, acc: 0.98, recall: 0.98, precision: 0.98, f_beta: 0.98
train: step: 3897, loss: 0.05377557873725891, acc: 0.94, recall: 0.94, precision: 0.9464285714285714, f_beta: 0.939783219590526
train: step: 3898, loss: 0.026626933366060257, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 3899, loss: 0.054883748292922974, acc: 0.93, recall: 0.9299999999999999, precision: 0.930172068827531, f_beta: 0.9299929992999301
train: step: 3900, loss: 0.007008659187704325, acc: 0.99, recall: 0.99, precision: 0.9901960784313726, f_beta: 0.98999899989999
train: step: 3901, loss: 0.024183208122849464, acc: 0.95, recall: 0.95, precision: 0.9516258530710557, f_beta: 0.949954959463517
train: step: 3902, loss: 0.03759963437914848, acc: 0.95, recall: 0.95, precision: 0.9545454545454546, f_beta: 0.949874686716792
train: step: 3903, loss: 0.03827307000756264, acc: 0.96, recall: 0.96, precision: 0.96, f_beta: 0.96
train: step: 3904, loss: 0.07349585741758347, acc: 0.89, recall: 0.8899999999999999, precision: 0.8977968176254589, f_beta: 0.8894583458948848
train: step: 3905, loss: 0.01804618537425995, acc: 0.98, recall: 0.98, precision: 0.9807692307692308, f_beta: 0.9799919967987194
train: step: 3906, loss: 0.035848990082740784, acc: 0.96, recall: 0.96, precision: 0.9607371794871795, f_beta: 0.959983993597439
train: step: 3907, loss: 0.029716672375798225, acc: 0.96, recall: 0.96, precision: 0.9607371794871795, f_beta: 0.959983993597439
train: step: 3908, loss: 0.015015320852398872, acc: 0.98, recall: 0.98, precision: 0.9807692307692308, f_beta: 0.9799919967987194
train: step: 3909, loss: 0.06071783974766731, acc: 0.92, recall: 0.9199999999999999, precision: 0.9227053140096618, f_beta: 0.9198717948717948
train: step: 3910, loss: 0.03185178339481354, acc: 0.98, recall: 0.98, precision: 0.98, f_beta: 0.98
train: step: 3911, loss: 0.051767755299806595, acc: 0.94, recall: 0.94, precision: 0.9407051282051282, f_beta: 0.9399759903961584
train: step: 3912, loss: 0.04632188007235527, acc: 0.94, recall: 0.94, precision: 0.9428341384863124, f_beta: 0.9399038461538463
train: step: 3913, loss: 0.0341988280415535, acc: 0.95, recall: 0.95, precision: 0.9501800720288115, f_beta: 0.9499949994999499
train: step: 3914, loss: 0.05446428433060646, acc: 0.94, recall: 0.94, precision: 0.94, f_beta: 0.94
train: step: 3915, loss: 0.03461019694805145, acc: 0.95, recall: 0.95, precision: 0.9501800720288115, f_beta: 0.9499949994999499
train: step: 3916, loss: 0.02747618593275547, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 3917, loss: 0.005718064494431019, acc: 0.99, recall: 0.99, precision: 0.9901960784313726, f_beta: 0.98999899989999
train: step: 3918, loss: 0.028249304741621017, acc: 0.96, recall: 0.96, precision: 0.962962962962963, f_beta: 0.9599358974358974
train: step: 3919, loss: 0.021772947162389755, acc: 0.98, recall: 0.98, precision: 0.98, f_beta: 0.98
train: step: 3920, loss: 0.0011172693921253085, acc: 1.0, recall: 1.0, precision: 1.0, f_beta: 1.0
train: step: 3921, loss: 0.026535363867878914, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 3922, loss: 0.022936608642339706, acc: 0.98, recall: 0.98, precision: 0.9807692307692308, f_beta: 0.9799919967987194
train: step: 3923, loss: 0.062362346798181534, acc: 0.93, recall: 0.9299999999999999, precision: 0.930172068827531, f_beta: 0.9299929992999301
train: step: 3924, loss: 0.023245930671691895, acc: 0.97, recall: 0.97, precision: 0.9716981132075472, f_beta: 0.9699729756781104
train: step: 3925, loss: 0.019349751994013786, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 3926, loss: 0.025803105905652046, acc: 0.95, recall: 0.95, precision: 0.9501800720288115, f_beta: 0.9499949994999499
train: step: 3927, loss: 0.013165955431759357, acc: 0.98, recall: 0.98, precision: 0.9807692307692308, f_beta: 0.9799919967987194
train: step: 3928, loss: 0.034789811819791794, acc: 0.94, recall: 0.94, precision: 0.9428341384863124, f_beta: 0.9399038461538463
train: step: 3929, loss: 0.02589631825685501, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 3930, loss: 0.018997078761458397, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 3931, loss: 0.035163771361112595, acc: 0.95, recall: 0.95, precision: 0.9501800720288115, f_beta: 0.9499949994999499
train: step: 3932, loss: 0.019211092963814735, acc: 0.98, recall: 0.98, precision: 0.98, f_beta: 0.98
train: step: 3933, loss: 0.02012147195637226, acc: 0.98, recall: 0.98, precision: 0.98, f_beta: 0.98
train: step: 3934, loss: 0.05124200880527496, acc: 0.93, recall: 0.9299999999999999, precision: 0.930172068827531, f_beta: 0.9299929992999301
train: step: 3935, loss: 0.028323249891400337, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 3936, loss: 0.03435244411230087, acc: 0.96, recall: 0.96, precision: 0.96, f_beta: 0.96
train: step: 3937, loss: 0.07284252345561981, acc: 0.9, recall: 0.8999999999999999, precision: 0.9025764895330113, f_beta: 0.8998397435897436
train: step: 3938, loss: 0.030533405020833015, acc: 0.96, recall: 0.96, precision: 0.962962962962963, f_beta: 0.9599358974358974
train: step: 3939, loss: 0.03687773272395134, acc: 0.95, recall: 0.95, precision: 0.9516258530710557, f_beta: 0.949954959463517
train: step: 3940, loss: 0.037926726043224335, acc: 0.93, recall: 0.9299999999999999, precision: 0.9315535929345644, f_beta: 0.9299369432489241
train: step: 3941, loss: 0.03187042474746704, acc: 0.97, recall: 0.97, precision: 0.9716981132075472, f_beta: 0.9699729756781104
train: step: 3942, loss: 0.01991777867078781, acc: 0.96, recall: 0.96, precision: 0.96, f_beta: 0.96
train: step: 3943, loss: 0.014864877797663212, acc: 0.99, recall: 0.99, precision: 0.9901960784313726, f_beta: 0.98999899989999
train: step: 3944, loss: 0.01676502265036106, acc: 0.96, recall: 0.96, precision: 0.9607371794871795, f_beta: 0.959983993597439
train: step: 3945, loss: 0.024915354326367378, acc: 0.98, recall: 0.98, precision: 0.9807692307692308, f_beta: 0.9799919967987194
train: step: 3946, loss: 0.04521448537707329, acc: 0.95, recall: 0.95, precision: 0.9501800720288115, f_beta: 0.9499949994999499
train: step: 3947, loss: 0.03129909187555313, acc: 0.96, recall: 0.96, precision: 0.9607371794871795, f_beta: 0.959983993597439
train: step: 3948, loss: 0.03382469713687897, acc: 0.96, recall: 0.96, precision: 0.9607371794871795, f_beta: 0.959983993597439
train: step: 3949, loss: 0.04263148456811905, acc: 0.95, recall: 0.95, precision: 0.9501800720288115, f_beta: 0.9499949994999499
train: step: 3950, loss: 0.033417221158742905, acc: 0.96, recall: 0.96, precision: 0.9607371794871795, f_beta: 0.959983993597439
train: step: 3951, loss: 0.03903202712535858, acc: 0.95, recall: 0.95, precision: 0.9501800720288115, f_beta: 0.9499949994999499
train: step: 3952, loss: 0.06715812534093857, acc: 0.92, recall: 0.9199999999999999, precision: 0.9206730769230769, f_beta: 0.919967987194878
train: step: 3953, loss: 0.02525661513209343, acc: 0.96, recall: 0.96, precision: 0.96, f_beta: 0.96
train: step: 3954, loss: 0.04346103593707085, acc: 0.94, recall: 0.94, precision: 0.9407051282051282, f_beta: 0.9399759903961584
train: step: 3955, loss: 0.027394942939281464, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 3956, loss: 0.010102294385433197, acc: 0.98, recall: 0.98, precision: 0.9807692307692308, f_beta: 0.9799919967987194
train: step: 3957, loss: 0.03350340947508812, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 3958, loss: 0.025411561131477356, acc: 0.98, recall: 0.98, precision: 0.9807692307692308, f_beta: 0.9799919967987194
train: step: 3959, loss: 0.04651028662919998, acc: 0.94, recall: 0.94, precision: 0.94, f_beta: 0.94
train: step: 3960, loss: 0.0310819074511528, acc: 0.95, recall: 0.95, precision: 0.9501800720288115, f_beta: 0.9499949994999499
train: step: 3961, loss: 0.03648575022816658, acc: 0.96, recall: 0.96, precision: 0.9607371794871795, f_beta: 0.959983993597439
train: step: 3962, loss: 0.003023468889296055, acc: 1.0, recall: 1.0, precision: 1.0, f_beta: 1.0
train: step: 3963, loss: 0.03888293728232384, acc: 0.96, recall: 0.96, precision: 0.9607371794871795, f_beta: 0.959983993597439
train: step: 3964, loss: 0.02479015849530697, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 3965, loss: 0.023829031735658646, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 3966, loss: 0.017278945073485374, acc: 0.97, recall: 0.97, precision: 0.9716981132075472, f_beta: 0.9699729756781104
train: step: 3967, loss: 0.01826084777712822, acc: 0.97, recall: 0.97, precision: 0.9716981132075472, f_beta: 0.9699729756781104
train: step: 3968, loss: 0.014867258258163929, acc: 0.98, recall: 0.98, precision: 0.98, f_beta: 0.98
train: step: 3969, loss: 0.008614269085228443, acc: 0.98, recall: 0.98, precision: 0.9807692307692308, f_beta: 0.9799919967987194
train: step: 3970, loss: 0.011483993381261826, acc: 0.99, recall: 0.99, precision: 0.9901960784313726, f_beta: 0.98999899989999
train: step: 3971, loss: 0.03575009107589722, acc: 0.95, recall: 0.95, precision: 0.9501800720288115, f_beta: 0.9499949994999499
train: step: 3972, loss: 0.027894215658307076, acc: 0.96, recall: 0.96, precision: 0.96, f_beta: 0.96
train: step: 3973, loss: 0.01602851413190365, acc: 0.99, recall: 0.99, precision: 0.9901960784313726, f_beta: 0.98999899989999
train: step: 3974, loss: 0.009127963334321976, acc: 0.99, recall: 0.99, precision: 0.9901960784313726, f_beta: 0.98999899989999
train: step: 3975, loss: 0.02380949817597866, acc: 0.98, recall: 0.98, precision: 0.9807692307692308, f_beta: 0.9799919967987194
train: step: 3976, loss: 0.032452020794153214, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 3977, loss: 0.006386665627360344, acc: 1.0, recall: 1.0, precision: 1.0, f_beta: 1.0
train: step: 3978, loss: 0.03582210838794708, acc: 0.96, recall: 0.96, precision: 0.96, f_beta: 0.96
train: step: 3979, loss: 0.027806028723716736, acc: 0.96, recall: 0.96, precision: 0.962962962962963, f_beta: 0.9599358974358974
train: step: 3980, loss: 0.016286931931972504, acc: 0.98, recall: 0.98, precision: 0.98, f_beta: 0.98
train: step: 3981, loss: 0.010757621377706528, acc: 0.99, recall: 0.99, precision: 0.9901960784313726, f_beta: 0.98999899989999
train: step: 3982, loss: 0.014943855814635754, acc: 0.99, recall: 0.99, precision: 0.9901960784313726, f_beta: 0.98999899989999
train: step: 3983, loss: 0.03475043177604675, acc: 0.96, recall: 0.96, precision: 0.9607371794871795, f_beta: 0.959983993597439
train: step: 3984, loss: 0.009223909117281437, acc: 0.99, recall: 0.99, precision: 0.9901960784313726, f_beta: 0.98999899989999
train: step: 3985, loss: 0.00649309391155839, acc: 1.0, recall: 1.0, precision: 1.0, f_beta: 1.0
train: step: 3986, loss: 0.03666233643889427, acc: 0.96, recall: 0.96, precision: 0.96, f_beta: 0.96
train: step: 3987, loss: 0.027389101684093475, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 3988, loss: 0.011784550733864307, acc: 0.98, recall: 0.98, precision: 0.9807692307692308, f_beta: 0.9799919967987194
train: step: 3989, loss: 0.020597953349351883, acc: 0.98, recall: 0.98, precision: 0.9807692307692308, f_beta: 0.9799919967987194
train: step: 3990, loss: 0.03806256875395775, acc: 0.95, recall: 0.95, precision: 0.9501800720288115, f_beta: 0.9499949994999499
train: step: 3991, loss: 0.03220906853675842, acc: 0.96, recall: 0.96, precision: 0.9607371794871795, f_beta: 0.959983993597439
train: step: 3992, loss: 0.018406489863991737, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 3993, loss: 0.034392088651657104, acc: 0.96, recall: 0.96, precision: 0.96, f_beta: 0.96
train: step: 3994, loss: 0.019545642659068108, acc: 0.98, recall: 0.98, precision: 0.98, f_beta: 0.98
train: step: 3995, loss: 0.014470729976892471, acc: 0.98, recall: 0.98, precision: 0.9807692307692308, f_beta: 0.9799919967987194
train: step: 3996, loss: 0.02399534173309803, acc: 0.96, recall: 0.96, precision: 0.9607371794871795, f_beta: 0.959983993597439
train: step: 3997, loss: 0.08226003497838974, acc: 0.89, recall: 0.89, precision: 0.89015606242497, f_beta: 0.88998899889989
train: step: 3998, loss: 0.01929449290037155, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 3999, loss: 0.03656337782740593, acc: 0.96, recall: 0.96, precision: 0.96, f_beta: 0.96
train: step: 4000, loss: 0.0007369831437245011, acc: 1.0, recall: 1.0, precision: 1.0, f_beta: 1.0
train: step: 4001, loss: 0.05499134957790375, acc: 0.93, recall: 0.9299999999999999, precision: 0.930172068827531, f_beta: 0.9299929992999301
train: step: 4002, loss: 0.008064734749495983, acc: 0.99, recall: 0.99, precision: 0.9901960784313726, f_beta: 0.98999899989999
train: step: 4003, loss: 0.02350781485438347, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 4004, loss: 0.018426841124892235, acc: 0.98, recall: 0.98, precision: 0.9807692307692308, f_beta: 0.9799919967987194
train: step: 4005, loss: 0.03645631670951843, acc: 0.95, recall: 0.95, precision: 0.9501800720288115, f_beta: 0.9499949994999499
train: step: 4006, loss: 0.06027955561876297, acc: 0.93, recall: 0.9299999999999999, precision: 0.930172068827531, f_beta: 0.9299929992999301
train: step: 4007, loss: 0.022096781060099602, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 4008, loss: 0.010332461446523666, acc: 0.99, recall: 0.99, precision: 0.9901960784313726, f_beta: 0.98999899989999
train: step: 4009, loss: 0.027240285649895668, acc: 0.96, recall: 0.96, precision: 0.9607371794871795, f_beta: 0.959983993597439
train: step: 4010, loss: 0.04047191143035889, acc: 0.96, recall: 0.96, precision: 0.9607371794871795, f_beta: 0.959983993597439
train: step: 4011, loss: 0.038621608167886734, acc: 0.95, recall: 0.95, precision: 0.9501800720288115, f_beta: 0.9499949994999499
train: step: 4012, loss: 0.03128543868660927, acc: 0.96, recall: 0.96, precision: 0.96, f_beta: 0.96
train: step: 4013, loss: 0.058099303394556046, acc: 0.94, recall: 0.94, precision: 0.9407051282051282, f_beta: 0.9399759903961584
train: step: 4014, loss: 0.025242524221539497, acc: 0.98, recall: 0.98, precision: 0.98, f_beta: 0.98
train: step: 4015, loss: 0.04136667773127556, acc: 0.95, recall: 0.95, precision: 0.9516258530710557, f_beta: 0.949954959463517
train: step: 4016, loss: 0.03579641506075859, acc: 0.97, recall: 0.97, precision: 0.9716981132075472, f_beta: 0.9699729756781104
train: step: 4017, loss: 0.01743461564183235, acc: 0.98, recall: 0.98, precision: 0.98, f_beta: 0.98
train: step: 4018, loss: 0.014750463888049126, acc: 0.99, recall: 0.99, precision: 0.9901960784313726, f_beta: 0.98999899989999
train: step: 4019, loss: 0.02057843841612339, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 4020, loss: 0.05318111553788185, acc: 0.95, recall: 0.95, precision: 0.9516258530710557, f_beta: 0.949954959463517
train: step: 4021, loss: 0.004976506344974041, acc: 1.0, recall: 1.0, precision: 1.0, f_beta: 1.0
train: step: 4022, loss: 0.029550937935709953, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 4023, loss: 0.004866947419941425, acc: 1.0, recall: 1.0, precision: 1.0, f_beta: 1.0
train: step: 4024, loss: 0.015284285880625248, acc: 0.99, recall: 0.99, precision: 0.9901960784313726, f_beta: 0.98999899989999
train: step: 4025, loss: 0.028492223471403122, acc: 0.96, recall: 0.96, precision: 0.962962962962963, f_beta: 0.9599358974358974
train: step: 4026, loss: 0.016343697905540466, acc: 0.99, recall: 0.99, precision: 0.9901960784313726, f_beta: 0.98999899989999
train: step: 4027, loss: 0.03167544677853584, acc: 0.96, recall: 0.96, precision: 0.9607371794871795, f_beta: 0.959983993597439
train: step: 4028, loss: 0.026955967769026756, acc: 0.96, recall: 0.96, precision: 0.962962962962963, f_beta: 0.9599358974358974
train: step: 4029, loss: 0.02100117690861225, acc: 0.98, recall: 0.98, precision: 0.98, f_beta: 0.98
train: step: 4030, loss: 0.0286286361515522, acc: 0.96, recall: 0.96, precision: 0.9607371794871795, f_beta: 0.959983993597439
train: step: 4031, loss: 0.014396402053534985, acc: 0.98, recall: 0.98, precision: 0.98, f_beta: 0.98
train: step: 4032, loss: 0.03196660429239273, acc: 0.96, recall: 0.96, precision: 0.9607371794871795, f_beta: 0.959983993597439
train: step: 4033, loss: 0.012429636903107166, acc: 0.99, recall: 0.99, precision: 0.9901960784313726, f_beta: 0.98999899989999
train: step: 4034, loss: 0.016969170421361923, acc: 0.98, recall: 0.98, precision: 0.98, f_beta: 0.98
train: step: 4035, loss: 0.01383954007178545, acc: 0.99, recall: 0.99, precision: 0.9901960784313726, f_beta: 0.98999899989999
train: step: 4036, loss: 0.011864167638123035, acc: 0.99, recall: 0.99, precision: 0.9901960784313726, f_beta: 0.98999899989999
train: step: 4037, loss: 0.06496258825063705, acc: 0.92, recall: 0.9199999999999999, precision: 0.9261363636363636, f_beta: 0.9197109594540345
train: step: 4038, loss: 0.05486239492893219, acc: 0.93, recall: 0.9299999999999999, precision: 0.9315535929345644, f_beta: 0.9299369432489241
train: step: 4039, loss: 0.03841479495167732, acc: 0.95, recall: 0.95, precision: 0.9516258530710557, f_beta: 0.949954959463517
train: step: 4040, loss: 0.0028617263305932283, acc: 1.0, recall: 1.0, precision: 1.0, f_beta: 1.0
train: step: 4041, loss: 0.025727057829499245, acc: 0.98, recall: 0.98, precision: 0.98, f_beta: 0.98
train: step: 4042, loss: 0.06209157779812813, acc: 0.91, recall: 0.91, precision: 0.9101640656262505, f_beta: 0.90999099909991
train: step: 4043, loss: 0.012756817042827606, acc: 0.98, recall: 0.98, precision: 0.98, f_beta: 0.98
train: step: 4044, loss: 0.03133419528603554, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 4045, loss: 0.008843682706356049, acc: 0.99, recall: 0.99, precision: 0.9901960784313726, f_beta: 0.98999899989999
train: step: 4046, loss: 0.02696283906698227, acc: 0.96, recall: 0.96, precision: 0.96, f_beta: 0.96
train: step: 4047, loss: 0.034999508410692215, acc: 0.96, recall: 0.96, precision: 0.96, f_beta: 0.96
train: step: 4048, loss: 0.02022199146449566, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 4049, loss: 0.02810629829764366, acc: 0.96, recall: 0.96, precision: 0.96, f_beta: 0.96
train: step: 4050, loss: 0.027927404269576073, acc: 0.97, recall: 0.97, precision: 0.9716981132075472, f_beta: 0.9699729756781104
train: step: 4051, loss: 0.012102510780096054, acc: 0.99, recall: 0.99, precision: 0.9901960784313726, f_beta: 0.98999899989999
train: step: 4052, loss: 0.055492889136075974, acc: 0.94, recall: 0.94, precision: 0.9407051282051282, f_beta: 0.9399759903961584
train: step: 4053, loss: 0.06006127968430519, acc: 0.93, recall: 0.9299999999999999, precision: 0.9385964912280702, f_beta: 0.9296553110240177
train: step: 4054, loss: 0.024169765412807465, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 4055, loss: 0.022910721600055695, acc: 0.98, recall: 0.98, precision: 0.98, f_beta: 0.98
train: step: 4056, loss: 0.035158149898052216, acc: 0.96, recall: 0.96, precision: 0.96, f_beta: 0.96
train: step: 4057, loss: 0.008198365569114685, acc: 0.98, recall: 0.98, precision: 0.9807692307692308, f_beta: 0.9799919967987194
train: step: 4058, loss: 0.025484271347522736, acc: 0.97, recall: 0.97, precision: 0.9716981132075472, f_beta: 0.9699729756781104
train: step: 4059, loss: 0.02172859013080597, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 4060, loss: 0.03409972041845322, acc: 0.96, recall: 0.96, precision: 0.96, f_beta: 0.96
train: step: 4061, loss: 0.055110521614551544, acc: 0.92, recall: 0.9199999999999999, precision: 0.9261363636363636, f_beta: 0.9197109594540345
train: step: 4062, loss: 0.04848787188529968, acc: 0.93, recall: 0.9299999999999999, precision: 0.930172068827531, f_beta: 0.9299929992999301
train: step: 4063, loss: 0.04753905162215233, acc: 0.94, recall: 0.94, precision: 0.9428341384863124, f_beta: 0.9399038461538463
train: step: 4064, loss: 0.043827105313539505, acc: 0.96, recall: 0.96, precision: 0.962962962962963, f_beta: 0.9599358974358974
train: step: 4065, loss: 0.03488849103450775, acc: 0.94, recall: 0.94, precision: 0.9464285714285714, f_beta: 0.939783219590526
train: step: 4066, loss: 0.010323651134967804, acc: 0.99, recall: 0.99, precision: 0.9901960784313726, f_beta: 0.98999899989999
train: step: 4067, loss: 0.03915025666356087, acc: 0.96, recall: 0.96, precision: 0.9607371794871795, f_beta: 0.959983993597439
train: step: 4068, loss: 0.021281957626342773, acc: 0.98, recall: 0.98, precision: 0.98, f_beta: 0.98
train: step: 4069, loss: 0.028685051947832108, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 4070, loss: 0.04823652282357216, acc: 0.95, recall: 0.95, precision: 0.9516258530710557, f_beta: 0.949954959463517
train: step: 4071, loss: 0.04316667467355728, acc: 0.95, recall: 0.95, precision: 0.9545454545454546, f_beta: 0.949874686716792
train: step: 4072, loss: 0.04796457663178444, acc: 0.94, recall: 0.94, precision: 0.9464285714285714, f_beta: 0.939783219590526
train: step: 4073, loss: 0.038574133068323135, acc: 0.96, recall: 0.96, precision: 0.96, f_beta: 0.96
train: step: 4074, loss: 0.020555991679430008, acc: 0.97, recall: 0.97, precision: 0.9716981132075472, f_beta: 0.9699729756781104
train: step: 4075, loss: 0.028696417808532715, acc: 0.96, recall: 0.96, precision: 0.9607371794871795, f_beta: 0.959983993597439
train: step: 4076, loss: 0.0640031173825264, acc: 0.94, recall: 0.94, precision: 0.9428341384863124, f_beta: 0.9399038461538463
train: step: 4077, loss: 0.019945524632930756, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 4078, loss: 0.046940889209508896, acc: 0.94, recall: 0.94, precision: 0.9464285714285714, f_beta: 0.939783219590526
train: step: 4079, loss: 0.022133884951472282, acc: 0.97, recall: 0.97, precision: 0.9716981132075472, f_beta: 0.9699729756781104
train: step: 4080, loss: 0.018220605328679085, acc: 0.98, recall: 0.98, precision: 0.98, f_beta: 0.98
train: step: 4081, loss: 0.07092490792274475, acc: 0.92, recall: 0.9199999999999999, precision: 0.9227053140096618, f_beta: 0.9198717948717948
train: step: 4082, loss: 0.05218091607093811, acc: 0.93, recall: 0.9299999999999999, precision: 0.9343434343434343, f_beta: 0.9298245614035088
train: step: 4083, loss: 0.045861855149269104, acc: 0.95, recall: 0.95, precision: 0.9501800720288115, f_beta: 0.9499949994999499
train: step: 4084, loss: 0.03068634495139122, acc: 0.97, recall: 0.97, precision: 0.9716981132075472, f_beta: 0.9699729756781104
train: step: 4085, loss: 0.06522141396999359, acc: 0.91, recall: 0.9099999999999999, precision: 0.9141414141414141, f_beta: 0.9097744360902256
train: step: 4086, loss: 0.06629624217748642, acc: 0.91, recall: 0.9099999999999999, precision: 0.9114813327980731, f_beta: 0.9099189270343309
train: step: 4087, loss: 0.05931645259261131, acc: 0.92, recall: 0.9199999999999999, precision: 0.9206730769230769, f_beta: 0.919967987194878
train: step: 4088, loss: 0.05217673256993294, acc: 0.94, recall: 0.94, precision: 0.94, f_beta: 0.94
train: step: 4089, loss: 0.013122932985424995, acc: 0.99, recall: 0.99, precision: 0.9901960784313726, f_beta: 0.98999899989999
train: step: 4090, loss: 0.029308248311281204, acc: 0.96, recall: 0.96, precision: 0.96, f_beta: 0.96
train: step: 4091, loss: 0.024562906473875046, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 4092, loss: 0.010514339432120323, acc: 0.99, recall: 0.99, precision: 0.9901960784313726, f_beta: 0.98999899989999
train: step: 4093, loss: 0.06812726706266403, acc: 0.91, recall: 0.9099999999999999, precision: 0.923728813559322, f_beta: 0.9092650468797258
train: step: 4094, loss: 0.043498698621988297, acc: 0.94, recall: 0.94, precision: 0.9428341384863124, f_beta: 0.9399038461538463
train: step: 4095, loss: 0.022876610979437828, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 4096, loss: 0.017062334343791008, acc: 0.98, recall: 0.98, precision: 0.9807692307692308, f_beta: 0.9799919967987194
train: step: 4097, loss: 0.0202053003013134, acc: 0.98, recall: 0.98, precision: 0.9807692307692308, f_beta: 0.9799919967987194
train: step: 4098, loss: 0.03735236078500748, acc: 0.95, recall: 0.95, precision: 0.9501800720288115, f_beta: 0.9499949994999499
train: step: 4099, loss: 0.013512709178030491, acc: 0.99, recall: 0.99, precision: 0.9901960784313726, f_beta: 0.98999899989999
train: step: 4100, loss: 0.03267068788409233, acc: 0.95, recall: 0.95, precision: 0.9501800720288115, f_beta: 0.9499949994999499
train: step: 4101, loss: 0.06837322562932968, acc: 0.93, recall: 0.9299999999999999, precision: 0.9385964912280702, f_beta: 0.9296553110240177
train: step: 4102, loss: 0.03310836851596832, acc: 0.96, recall: 0.96, precision: 0.962962962962963, f_beta: 0.9599358974358974
train: step: 4103, loss: 0.05138717591762543, acc: 0.94, recall: 0.94, precision: 0.9464285714285714, f_beta: 0.939783219590526
train: step: 4104, loss: 0.04517345502972603, acc: 0.94, recall: 0.94, precision: 0.9428341384863124, f_beta: 0.9399038461538463
train: step: 4105, loss: 0.016071729362010956, acc: 0.98, recall: 0.98, precision: 0.9807692307692308, f_beta: 0.9799919967987194
train: step: 4106, loss: 0.017564671114087105, acc: 0.98, recall: 0.98, precision: 0.9807692307692308, f_beta: 0.9799919967987194
train: step: 4107, loss: 0.008789557963609695, acc: 0.99, recall: 0.99, precision: 0.9901960784313726, f_beta: 0.98999899989999
train: step: 4108, loss: 0.05024610087275505, acc: 0.93, recall: 0.9299999999999999, precision: 0.930172068827531, f_beta: 0.9299929992999301
train: step: 4109, loss: 0.03076593391597271, acc: 0.96, recall: 0.96, precision: 0.962962962962963, f_beta: 0.9599358974358974
train: step: 4110, loss: 0.024567827582359314, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 4111, loss: 0.029331231489777565, acc: 0.97, recall: 0.97, precision: 0.9716981132075472, f_beta: 0.9699729756781104
train: step: 4112, loss: 0.03449976071715355, acc: 0.96, recall: 0.96, precision: 0.9607371794871795, f_beta: 0.959983993597439
train: step: 4113, loss: 0.029407968744635582, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 4114, loss: 0.02782503142952919, acc: 0.97, recall: 0.97, precision: 0.9716981132075472, f_beta: 0.9699729756781104
train: step: 4115, loss: 0.030089307576417923, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 4116, loss: 0.02230226807296276, acc: 0.96, recall: 0.96, precision: 0.9607371794871795, f_beta: 0.959983993597439
train: step: 4117, loss: 0.02498251385986805, acc: 0.97, recall: 0.97, precision: 0.9716981132075472, f_beta: 0.9699729756781104
train: step: 4118, loss: 0.0717782974243164, acc: 0.92, recall: 0.92, precision: 0.92, f_beta: 0.92
train: step: 4119, loss: 0.03806179389357567, acc: 0.95, recall: 0.95, precision: 0.9501800720288115, f_beta: 0.9499949994999499
train: step: 4120, loss: 0.01582251861691475, acc: 0.96, recall: 0.96, precision: 0.9607371794871795, f_beta: 0.959983993597439
train: step: 4121, loss: 0.01371678151190281, acc: 0.98, recall: 0.98, precision: 0.9807692307692308, f_beta: 0.9799919967987194
train: step: 4122, loss: 0.027303600683808327, acc: 0.96, recall: 0.96, precision: 0.9607371794871795, f_beta: 0.959983993597439
train: step: 4123, loss: 0.006115449592471123, acc: 0.99, recall: 0.99, precision: 0.9901960784313726, f_beta: 0.98999899989999
train: step: 4124, loss: 0.05294225364923477, acc: 0.94, recall: 0.94, precision: 0.9407051282051282, f_beta: 0.9399759903961584
train: step: 4125, loss: 0.011825921945273876, acc: 0.99, recall: 0.99, precision: 0.9901960784313726, f_beta: 0.98999899989999
train: step: 4126, loss: 0.006531256251037121, acc: 1.0, recall: 1.0, precision: 1.0, f_beta: 1.0
train: step: 4127, loss: 0.04019965976476669, acc: 0.96, recall: 0.96, precision: 0.9607371794871795, f_beta: 0.959983993597439
train: step: 4128, loss: 0.026255981996655464, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 4129, loss: 0.017344404011964798, acc: 0.98, recall: 0.98, precision: 0.98, f_beta: 0.98
train: step: 4130, loss: 0.01955513097345829, acc: 0.98, recall: 0.98, precision: 0.98, f_beta: 0.98
train: step: 4131, loss: 0.02419481799006462, acc: 0.96, recall: 0.96, precision: 0.962962962962963, f_beta: 0.9599358974358974
train: step: 4132, loss: 0.008508589118719101, acc: 0.99, recall: 0.99, precision: 0.9901960784313726, f_beta: 0.98999899989999
train: step: 4133, loss: 0.06987759470939636, acc: 0.91, recall: 0.9099999999999999, precision: 0.9114813327980731, f_beta: 0.9099189270343309
train: step: 4134, loss: 0.012810311280190945, acc: 0.99, recall: 0.99, precision: 0.9901960784313726, f_beta: 0.98999899989999
train: step: 4135, loss: 0.023039786145091057, acc: 0.98, recall: 0.98, precision: 0.98, f_beta: 0.98
train: step: 4136, loss: 0.012152155861258507, acc: 0.98, recall: 0.98, precision: 0.98, f_beta: 0.98
train: step: 4137, loss: 0.011669912375509739, acc: 0.98, recall: 0.98, precision: 0.9807692307692308, f_beta: 0.9799919967987194
train: step: 4138, loss: 0.026316599920392036, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 4139, loss: 0.00454575615003705, acc: 1.0, recall: 1.0, precision: 1.0, f_beta: 1.0
train: step: 4140, loss: 0.03197315335273743, acc: 0.97, recall: 0.97, precision: 0.9716981132075472, f_beta: 0.9699729756781104
train: step: 4141, loss: 0.008395235985517502, acc: 0.98, recall: 0.98, precision: 0.9807692307692308, f_beta: 0.9799919967987194
train: step: 4142, loss: 0.00918311532586813, acc: 0.98, recall: 0.98, precision: 0.9807692307692308, f_beta: 0.9799919967987194
train: step: 4143, loss: 0.033056724816560745, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 4144, loss: 0.0374520905315876, acc: 0.95, recall: 0.95, precision: 0.9501800720288115, f_beta: 0.9499949994999499
train: step: 4145, loss: 0.017503613606095314, acc: 0.98, recall: 0.98, precision: 0.9807692307692308, f_beta: 0.9799919967987194
train: step: 4146, loss: 0.03427249565720558, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 4147, loss: 0.011537035927176476, acc: 0.99, recall: 0.99, precision: 0.9901960784313726, f_beta: 0.98999899989999
train: step: 4148, loss: 0.02248241938650608, acc: 0.98, recall: 0.98, precision: 0.98, f_beta: 0.98
train: step: 4149, loss: 0.02652723342180252, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 4150, loss: 0.030793629586696625, acc: 0.97, recall: 0.97, precision: 0.9716981132075472, f_beta: 0.9699729756781104
train: step: 4151, loss: 0.04926262050867081, acc: 0.95, recall: 0.95, precision: 0.9516258530710557, f_beta: 0.949954959463517
train: step: 4152, loss: 0.020535264164209366, acc: 0.97, recall: 0.97, precision: 0.9716981132075472, f_beta: 0.9699729756781104
train: step: 4153, loss: 0.02282888814806938, acc: 0.98, recall: 0.98, precision: 0.98, f_beta: 0.98
train: step: 4154, loss: 0.021890435367822647, acc: 0.98, recall: 0.98, precision: 0.98, f_beta: 0.98
train: step: 4155, loss: 0.016446823254227638, acc: 0.98, recall: 0.98, precision: 0.9807692307692308, f_beta: 0.9799919967987194
train: step: 4156, loss: 0.008340531960129738, acc: 0.99, recall: 0.99, precision: 0.9901960784313726, f_beta: 0.98999899989999
train: step: 4157, loss: 0.047704122960567474, acc: 0.94, recall: 0.94, precision: 0.94, f_beta: 0.94
train: step: 4158, loss: 0.006204994861036539, acc: 0.99, recall: 0.99, precision: 0.9901960784313726, f_beta: 0.98999899989999
train: step: 4159, loss: 0.029008015990257263, acc: 0.96, recall: 0.96, precision: 0.9607371794871795, f_beta: 0.959983993597439
train: step: 4160, loss: 0.015325355343520641, acc: 0.99, recall: 0.99, precision: 0.9901960784313726, f_beta: 0.98999899989999
train: step: 4161, loss: 0.0034249655436724424, acc: 1.0, recall: 1.0, precision: 1.0, f_beta: 1.0
train: step: 4162, loss: 0.04223082959651947, acc: 0.95, recall: 0.95, precision: 0.9501800720288115, f_beta: 0.9499949994999499
train: step: 4163, loss: 0.007970317266881466, acc: 0.98, recall: 0.98, precision: 0.9807692307692308, f_beta: 0.9799919967987194
train: step: 4164, loss: 0.05092655122280121, acc: 0.93, recall: 0.9299999999999999, precision: 0.9343434343434343, f_beta: 0.9298245614035088
train: step: 4165, loss: 0.012435956858098507, acc: 0.98, recall: 0.98, precision: 0.9807692307692308, f_beta: 0.9799919967987194
train: step: 4166, loss: 0.04481624439358711, acc: 0.95, recall: 0.95, precision: 0.9516258530710557, f_beta: 0.949954959463517
train: step: 4167, loss: 0.053990114480257034, acc: 0.95, recall: 0.95, precision: 0.9501800720288115, f_beta: 0.9499949994999499
train: step: 4168, loss: 0.048663292080163956, acc: 0.95, recall: 0.95, precision: 0.9501800720288115, f_beta: 0.9499949994999499
train: step: 4169, loss: 0.0328717976808548, acc: 0.95, recall: 0.95, precision: 0.9501800720288115, f_beta: 0.9499949994999499
train: step: 4170, loss: 0.029366319999098778, acc: 0.97, recall: 0.97, precision: 0.9716981132075472, f_beta: 0.9699729756781104
train: step: 4171, loss: 0.02761037088930607, acc: 0.97, recall: 0.97, precision: 0.9716981132075472, f_beta: 0.9699729756781104
train: step: 4172, loss: 0.023512031883001328, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 4173, loss: 0.03886449709534645, acc: 0.96, recall: 0.96, precision: 0.9607371794871795, f_beta: 0.959983993597439
train: step: 4174, loss: 0.03083382174372673, acc: 0.97, recall: 0.97, precision: 0.9716981132075472, f_beta: 0.9699729756781104
train: step: 4175, loss: 0.05849606916308403, acc: 0.93, recall: 0.9299999999999999, precision: 0.9343434343434343, f_beta: 0.9298245614035088
train: step: 4176, loss: 0.04258827120065689, acc: 0.95, recall: 0.95, precision: 0.9501800720288115, f_beta: 0.9499949994999499
train: step: 4177, loss: 0.03408996760845184, acc: 0.96, recall: 0.96, precision: 0.9607371794871795, f_beta: 0.959983993597439
train: step: 4178, loss: 0.02683732472360134, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 4179, loss: 0.015157051384449005, acc: 0.98, recall: 0.98, precision: 0.98, f_beta: 0.98
train: step: 4180, loss: 0.01109500415623188, acc: 0.99, recall: 0.99, precision: 0.9901960784313726, f_beta: 0.98999899989999
train: step: 4181, loss: 0.024423984810709953, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 4182, loss: 0.04429000988602638, acc: 0.94, recall: 0.94, precision: 0.9407051282051282, f_beta: 0.9399759903961584
train: step: 4183, loss: 0.04851211607456207, acc: 0.95, recall: 0.95, precision: 0.9545454545454546, f_beta: 0.949874686716792
train: step: 4184, loss: 0.046005282551050186, acc: 0.94, recall: 0.94, precision: 0.9428341384863124, f_beta: 0.9399038461538463
train: step: 4185, loss: 0.018199339509010315, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 4186, loss: 0.03376990929245949, acc: 0.96, recall: 0.96, precision: 0.9607371794871795, f_beta: 0.959983993597439
train: step: 4187, loss: 0.013923024758696556, acc: 0.98, recall: 0.98, precision: 0.9807692307692308, f_beta: 0.9799919967987194
train: step: 4188, loss: 0.04119052737951279, acc: 0.94, recall: 0.94, precision: 0.94, f_beta: 0.94
train: step: 4189, loss: 0.023433895781636238, acc: 0.98, recall: 0.98, precision: 0.98, f_beta: 0.98
train: step: 4190, loss: 0.04797448590397835, acc: 0.93, recall: 0.9299999999999999, precision: 0.9385964912280702, f_beta: 0.9296553110240177
train: step: 4191, loss: 0.04632524400949478, acc: 0.94, recall: 0.94, precision: 0.9407051282051282, f_beta: 0.9399759903961584
train: step: 4192, loss: 0.012531053274869919, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 4193, loss: 0.03505627065896988, acc: 0.96, recall: 0.96, precision: 0.96, f_beta: 0.96
train: step: 4194, loss: 0.03441460058093071, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 4195, loss: 0.018867788836359978, acc: 0.99, recall: 0.99, precision: 0.9901960784313726, f_beta: 0.98999899989999
train: step: 4196, loss: 0.013623804785311222, acc: 0.98, recall: 0.98, precision: 0.98, f_beta: 0.98
train: step: 4197, loss: 0.02030865103006363, acc: 0.97, recall: 0.97, precision: 0.9716981132075472, f_beta: 0.9699729756781104
train: step: 4198, loss: 0.005353282205760479, acc: 1.0, recall: 1.0, precision: 1.0, f_beta: 1.0
train: step: 4199, loss: 0.03059214912354946, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 4200, loss: 0.01865389756858349, acc: 0.98, recall: 0.98, precision: 0.98, f_beta: 0.98
train: step: 4201, loss: 0.04079841449856758, acc: 0.95, recall: 0.95, precision: 0.9545454545454546, f_beta: 0.949874686716792
train: step: 4202, loss: 0.013684346340596676, acc: 0.98, recall: 0.98, precision: 0.9807692307692308, f_beta: 0.9799919967987194
train: step: 4203, loss: 0.030934328213334084, acc: 0.95, recall: 0.95, precision: 0.9501800720288115, f_beta: 0.9499949994999499
train: step: 4204, loss: 0.0372159518301487, acc: 0.95, recall: 0.95, precision: 0.9501800720288115, f_beta: 0.9499949994999499
train: step: 4205, loss: 0.03834155201911926, acc: 0.95, recall: 0.95, precision: 0.9516258530710557, f_beta: 0.949954959463517
train: step: 4206, loss: 0.0025779532734304667, acc: 1.0, recall: 1.0, precision: 1.0, f_beta: 1.0
train: step: 4207, loss: 0.027179835364222527, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 4208, loss: 0.020208504050970078, acc: 0.98, recall: 0.98, precision: 0.9807692307692308, f_beta: 0.9799919967987194
train: step: 4209, loss: 0.004837841261178255, acc: 1.0, recall: 1.0, precision: 1.0, f_beta: 1.0
train: step: 4210, loss: 0.04160210117697716, acc: 0.96, recall: 0.96, precision: 0.962962962962963, f_beta: 0.9599358974358974
train: step: 4211, loss: 0.036897722631692886, acc: 0.95, recall: 0.95, precision: 0.9501800720288115, f_beta: 0.9499949994999499
train: step: 4212, loss: 0.05958262458443642, acc: 0.93, recall: 0.9299999999999999, precision: 0.9343434343434343, f_beta: 0.9298245614035088
train: step: 4213, loss: 0.021939659491181374, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 4214, loss: 0.011686062440276146, acc: 0.98, recall: 0.98, precision: 0.98, f_beta: 0.98
train: step: 4215, loss: 0.03446730971336365, acc: 0.94, recall: 0.94, precision: 0.94, f_beta: 0.94
train: step: 4216, loss: 0.007861417718231678, acc: 0.99, recall: 0.99, precision: 0.9901960784313726, f_beta: 0.98999899989999
train: step: 4217, loss: 0.030720634385943413, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 4218, loss: 0.02136874943971634, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 4219, loss: 0.07151462882757187, acc: 0.9, recall: 0.9, precision: 0.9006410256410255, f_beta: 0.8999599839935974
train: step: 4220, loss: 0.01727154850959778, acc: 0.98, recall: 0.98, precision: 0.98, f_beta: 0.98
train: step: 4221, loss: 0.03229989856481552, acc: 0.96, recall: 0.96, precision: 0.9607371794871795, f_beta: 0.959983993597439
train: step: 4222, loss: 0.008714972995221615, acc: 1.0, recall: 1.0, precision: 1.0, f_beta: 1.0
train: step: 4223, loss: 0.022608203813433647, acc: 0.98, recall: 0.98, precision: 0.9807692307692308, f_beta: 0.9799919967987194
train: step: 4224, loss: 0.0077931820414960384, acc: 0.98, recall: 0.98, precision: 0.98, f_beta: 0.98
train: step: 4225, loss: 0.03921316936612129, acc: 0.96, recall: 0.96, precision: 0.9607371794871795, f_beta: 0.959983993597439
train: step: 4226, loss: 0.02369367517530918, acc: 0.96, recall: 0.96, precision: 0.9607371794871795, f_beta: 0.959983993597439
train: step: 4227, loss: 0.016139565035700798, acc: 0.98, recall: 0.98, precision: 0.9807692307692308, f_beta: 0.9799919967987194
train: step: 4228, loss: 0.01259070634841919, acc: 0.99, recall: 0.99, precision: 0.9901960784313726, f_beta: 0.98999899989999
train: step: 4229, loss: 0.07988134026527405, acc: 0.9, recall: 0.8999999999999999, precision: 0.9025764895330113, f_beta: 0.8998397435897436
train: step: 4230, loss: 0.016769390553236008, acc: 0.98, recall: 0.98, precision: 0.9807692307692308, f_beta: 0.9799919967987194
train: step: 4231, loss: 0.4368426203727722, acc: 0.98, recall: 0.98, precision: 0.9807692307692308, f_beta: 0.9799919967987194
train: step: 4232, loss: 0.02752777561545372, acc: 0.97, recall: 0.97, precision: 0.9716981132075472, f_beta: 0.9699729756781104
train: step: 4233, loss: 0.022967500612139702, acc: 0.98, recall: 0.98, precision: 0.9807692307692308, f_beta: 0.9799919967987194
train: step: 4234, loss: 0.08896682411432266, acc: 0.9, recall: 0.8999999999999999, precision: 0.9105090311986863, f_beta: 0.8993558776167472
train: step: 4235, loss: 0.05701263248920441, acc: 0.94, recall: 0.94, precision: 0.9464285714285714, f_beta: 0.939783219590526
train: step: 4236, loss: 0.4708345830440521, acc: 0.49, recall: 0.49, precision: 0.4140893470790378, f_beta: 0.3453985367731998
train: step: 4237, loss: 0.06375705450773239, acc: 0.93, recall: 0.9299999999999999, precision: 0.9315535929345644, f_beta: 0.9299369432489241
train: step: 4238, loss: 0.08643221110105515, acc: 0.89, recall: 0.89, precision: 0.89015606242497, f_beta: 0.88998899889989
train: step: 4239, loss: 0.022704022005200386, acc: 0.97, recall: 0.97, precision: 0.9716981132075472, f_beta: 0.9699729756781104
train: step: 4240, loss: 0.4708545207977295, acc: 0.95, recall: 0.95, precision: 0.9545454545454546, f_beta: 0.949874686716792
train: step: 4241, loss: 0.07319515198469162, acc: 0.92, recall: 0.9199999999999999, precision: 0.9206730769230769, f_beta: 0.919967987194878
train: step: 4242, loss: 0.47644588351249695, acc: 0.95, recall: 0.95, precision: 0.9501800720288115, f_beta: 0.9499949994999499
train: step: 4243, loss: 0.026883313432335854, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 4244, loss: 0.458933025598526, acc: 0.89, recall: 0.89, precision: 0.9098360655737705, f_beta: 0.8886526976414617
train: step: 4245, loss: 0.4606861174106598, acc: 0.82, recall: 0.8200000000000001, precision: 0.8676470588235294, f_beta: 0.8139727159983464
train: step: 4246, loss: 0.0382424034178257, acc: 0.96, recall: 0.96, precision: 0.9607371794871795, f_beta: 0.959983993597439
train: step: 4247, loss: 0.4442085921764374, acc: 0.95, recall: 0.95, precision: 0.9545454545454546, f_beta: 0.949874686716792
train: step: 4248, loss: 0.3989344835281372, acc: 0.55, recall: 0.55, precision: 0.6526251526251526, f_beta: 0.45906959971150385
train: step: 4249, loss: 0.38909652829170227, acc: 0.5, recall: 0.5, precision: 0.5, f_beta: 0.3503118503118503
train: step: 4250, loss: 0.12493972480297089, acc: 0.96, recall: 0.96, precision: 0.962962962962963, f_beta: 0.9599358974358974
train: step: 4251, loss: 0.3795607388019562, acc: 0.49, recall: 0.49, precision: 0.4140893470790378, f_beta: 0.3453985367731998
train: step: 4252, loss: 0.3608197867870331, acc: 0.49, recall: 0.49, precision: 0.2474747474747475, f_beta: 0.32885906040268453
train: step: 4253, loss: 0.3568150997161865, acc: 0.5, recall: 0.5, precision: 0.5, f_beta: 0.3503118503118503
train: step: 4254, loss: 0.06545118987560272, acc: 0.96, recall: 0.96, precision: 0.96, f_beta: 0.96
train: step: 4255, loss: 0.07941362261772156, acc: 0.94, recall: 0.94, precision: 0.9407051282051282, f_beta: 0.9399759903961584
train: step: 4256, loss: 0.3472326397895813, acc: 0.53, recall: 0.53, precision: 0.6152073732718895, f_beta: 0.42338363390994965
train: step: 4257, loss: 0.3371294140815735, acc: 0.8, recall: 0.8, precision: 0.8255208333333333, f_beta: 0.7960016319869441
train: step: 4258, loss: 0.2934820353984833, acc: 0.93, recall: 0.9299999999999999, precision: 0.9343434343434343, f_beta: 0.9298245614035088
train: step: 4259, loss: 0.3101981580257416, acc: 0.93, recall: 0.9299999999999999, precision: 0.9385964912280702, f_beta: 0.9296553110240177
train: step: 4260, loss: 0.2645642161369324, acc: 0.91, recall: 0.9099999999999999, precision: 0.923728813559322, f_beta: 0.9092650468797258
train: step: 4261, loss: 0.28427040576934814, acc: 0.96, recall: 0.96, precision: 0.962962962962963, f_beta: 0.9599358974358974
train: step: 4262, loss: 0.26324132084846497, acc: 0.82, recall: 0.8200000000000001, precision: 0.8472222222222222, f_beta: 0.8164014687882497
train: step: 4263, loss: 0.28482237458229065, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 4264, loss: 0.2682848572731018, acc: 0.96, recall: 0.96, precision: 0.962962962962963, f_beta: 0.9599358974358974
train: step: 4265, loss: 0.2632223963737488, acc: 0.88, recall: 0.88, precision: 0.8958333333333333, f_beta: 0.8787878787878789
train: step: 4266, loss: 0.2921990156173706, acc: 0.07, recall: 0.06999999999999999, precision: 0.06565656565656565, f_beta: 0.06766917293233084
train: step: 4267, loss: 0.26571914553642273, acc: 0.8, recall: 0.8, precision: 0.8571428571428572, f_beta: 0.7916666666666665
train: step: 4268, loss: 0.2580656111240387, acc: 0.85, recall: 0.85, precision: 0.8753753753753755, f_beta: 0.8474214220323468
train: step: 4269, loss: 0.24105960130691528, acc: 0.91, recall: 0.9099999999999999, precision: 0.9114813327980731, f_beta: 0.9099189270343309
train: step: 4270, loss: 0.24411094188690186, acc: 0.91, recall: 0.9099999999999999, precision: 0.9141414141414141, f_beta: 0.9097744360902256
train: step: 4271, loss: 0.24309514462947845, acc: 0.88, recall: 0.88, precision: 0.8958333333333333, f_beta: 0.8787878787878789
train: step: 4272, loss: 0.21355290710926056, acc: 0.9, recall: 0.9, precision: 0.9166666666666667, f_beta: 0.898989898989899
train: step: 4273, loss: 0.2276187688112259, acc: 0.83, recall: 0.8300000000000001, precision: 0.8626373626373627, f_beta: 0.8260869565217391
train: step: 4274, loss: 0.21462871134281158, acc: 0.96, recall: 0.96, precision: 0.962962962962963, f_beta: 0.9599358974358974
train: step: 4275, loss: 0.15867137908935547, acc: 0.97, recall: 0.97, precision: 0.9716981132075472, f_beta: 0.9699729756781104
train: step: 4276, loss: 0.16678382456302643, acc: 0.88, recall: 0.88, precision: 0.8824476650563606, f_beta: 0.8798076923076923
train: step: 4277, loss: 0.12671837210655212, acc: 0.91, recall: 0.9099999999999999, precision: 0.9141414141414141, f_beta: 0.9097744360902256
train: step: 4278, loss: 0.12997204065322876, acc: 0.9, recall: 0.8999999999999999, precision: 0.9058441558441559, f_beta: 0.8996386993175431
train: step: 4279, loss: 0.07082244753837585, acc: 0.9, recall: 0.8999999999999999, precision: 0.9058441558441559, f_beta: 0.8996386993175431
train: step: 4280, loss: 0.03288305550813675, acc: 0.98, recall: 0.98, precision: 0.98, f_beta: 0.98
train: step: 4281, loss: 0.36733877658843994, acc: 0.89, recall: 0.89, precision: 0.8914090726615818, f_beta: 0.8899009108197378
train: step: 4282, loss: 0.042177945375442505, acc: 0.94, recall: 0.94, precision: 0.9464285714285714, f_beta: 0.939783219590526
train: step: 4283, loss: 0.07328377664089203, acc: 0.9, recall: 0.8999999999999999, precision: 0.9058441558441559, f_beta: 0.8996386993175431
train: step: 4284, loss: 0.05291105806827545, acc: 0.92, recall: 0.9199999999999999, precision: 0.9227053140096618, f_beta: 0.9198717948717948
train: step: 4285, loss: 0.013788637705147266, acc: 1.0, recall: 1.0, precision: 1.0, f_beta: 1.0
train: step: 4286, loss: 0.053657416254282, acc: 0.94, recall: 0.94, precision: 0.94, f_beta: 0.94
train: step: 4287, loss: 0.04014153406023979, acc: 0.95, recall: 0.95, precision: 0.9516258530710557, f_beta: 0.949954959463517
train: step: 4288, loss: 0.04617113992571831, acc: 0.95, recall: 0.95, precision: 0.9501800720288115, f_beta: 0.9499949994999499
train: step: 4289, loss: 0.04164513573050499, acc: 0.95, recall: 0.95, precision: 0.9501800720288115, f_beta: 0.9499949994999499
train: step: 4290, loss: 0.06544993817806244, acc: 0.93, recall: 0.9299999999999999, precision: 0.9315535929345644, f_beta: 0.9299369432489241
train: step: 4291, loss: 0.021978383883833885, acc: 0.98, recall: 0.98, precision: 0.98, f_beta: 0.98
train: step: 4292, loss: 0.038673028349876404, acc: 0.96, recall: 0.96, precision: 0.962962962962963, f_beta: 0.9599358974358974
train: step: 4293, loss: 0.07886233925819397, acc: 0.9, recall: 0.9, precision: 0.9006410256410255, f_beta: 0.8999599839935974
train: step: 4294, loss: 0.04178623855113983, acc: 0.96, recall: 0.96, precision: 0.9607371794871795, f_beta: 0.959983993597439
train: step: 4295, loss: 0.01852603629231453, acc: 0.98, recall: 0.98, precision: 0.98, f_beta: 0.98
train: step: 4296, loss: 0.04936467111110687, acc: 0.94, recall: 0.94, precision: 0.9407051282051282, f_beta: 0.9399759903961584
train: step: 4297, loss: 0.05567007139325142, acc: 0.93, recall: 0.9299999999999999, precision: 0.9315535929345644, f_beta: 0.9299369432489241
train: step: 4298, loss: 0.06166800484061241, acc: 0.92, recall: 0.9199999999999999, precision: 0.9206730769230769, f_beta: 0.919967987194878
train: step: 4299, loss: 0.04179977998137474, acc: 0.96, recall: 0.96, precision: 0.9607371794871795, f_beta: 0.959983993597439
train: step: 4300, loss: 0.04038417339324951, acc: 0.95, recall: 0.95, precision: 0.9516258530710557, f_beta: 0.949954959463517
train: step: 4301, loss: 0.04980875924229622, acc: 0.95, recall: 0.95, precision: 0.9501800720288115, f_beta: 0.9499949994999499
train: step: 4302, loss: 0.08245335519313812, acc: 0.88, recall: 0.88, precision: 0.88, f_beta: 0.88
train: step: 4303, loss: 0.039250873029232025, acc: 0.95, recall: 0.95, precision: 0.9501800720288115, f_beta: 0.9499949994999499
train: step: 4304, loss: 0.031960248947143555, acc: 0.96, recall: 0.96, precision: 0.9607371794871795, f_beta: 0.959983993597439
train: step: 4305, loss: 0.02216087281703949, acc: 0.99, recall: 0.99, precision: 0.9901960784313726, f_beta: 0.98999899989999
train: step: 4306, loss: 0.03371388837695122, acc: 0.95, recall: 0.95, precision: 0.9516258530710557, f_beta: 0.949954959463517
train: step: 4307, loss: 0.01916451007127762, acc: 0.96, recall: 0.96, precision: 0.9607371794871795, f_beta: 0.959983993597439
train: step: 4308, loss: 0.014612138271331787, acc: 0.99, recall: 0.99, precision: 0.9901960784313726, f_beta: 0.98999899989999
train: step: 4309, loss: 0.02386624366044998, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 4310, loss: 0.036148834973573685, acc: 0.95, recall: 0.95, precision: 0.9545454545454546, f_beta: 0.949874686716792
train: step: 4311, loss: 0.062193840742111206, acc: 0.93, recall: 0.9299999999999999, precision: 0.9315535929345644, f_beta: 0.9299369432489241
train: step: 4312, loss: 0.06569352000951767, acc: 0.91, recall: 0.9099999999999999, precision: 0.9181966544267646, f_beta: 0.9095568284594513
train: step: 4313, loss: 0.034661468118429184, acc: 0.95, recall: 0.95, precision: 0.9516258530710557, f_beta: 0.949954959463517
train: step: 4314, loss: 0.030159296467900276, acc: 0.95, recall: 0.95, precision: 0.9545454545454546, f_beta: 0.949874686716792
train: step: 4315, loss: 0.023345384746789932, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 4316, loss: 0.04161378741264343, acc: 0.96, recall: 0.96, precision: 0.962962962962963, f_beta: 0.9599358974358974
train: step: 4317, loss: 0.038136448711156845, acc: 0.96, recall: 0.96, precision: 0.9607371794871795, f_beta: 0.959983993597439
train: step: 4318, loss: 0.03754077106714249, acc: 0.95, recall: 0.95, precision: 0.9501800720288115, f_beta: 0.9499949994999499
train: step: 4319, loss: 0.028651319444179535, acc: 0.95, recall: 0.95, precision: 0.9545454545454546, f_beta: 0.949874686716792
train: step: 4320, loss: 0.05938904732465744, acc: 0.93, recall: 0.9299999999999999, precision: 0.9343434343434343, f_beta: 0.9298245614035088
train: step: 4321, loss: 0.010863935574889183, acc: 0.99, recall: 0.99, precision: 0.9901960784313726, f_beta: 0.98999899989999
train: step: 4322, loss: 0.0335044302046299, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 4323, loss: 0.026326224207878113, acc: 0.96, recall: 0.96, precision: 0.96, f_beta: 0.96
train: step: 4324, loss: 0.03672494739294052, acc: 0.95, recall: 0.95, precision: 0.9516258530710557, f_beta: 0.949954959463517
train: step: 4325, loss: 0.026149552315473557, acc: 0.96, recall: 0.96, precision: 0.96, f_beta: 0.96
train: step: 4326, loss: 0.036070216447114944, acc: 0.94, recall: 0.94, precision: 0.94, f_beta: 0.94
train: step: 4327, loss: 0.0532621294260025, acc: 0.93, recall: 0.9299999999999999, precision: 0.9343434343434343, f_beta: 0.9298245614035088
train: step: 4328, loss: 0.02857217937707901, acc: 0.98, recall: 0.98, precision: 0.98, f_beta: 0.98
train: step: 4329, loss: 0.03959156200289726, acc: 0.94, recall: 0.94, precision: 0.9464285714285714, f_beta: 0.939783219590526
train: step: 4330, loss: 0.06679540872573853, acc: 0.92, recall: 0.9199999999999999, precision: 0.9206730769230769, f_beta: 0.919967987194878
train: step: 4331, loss: 0.07239074259996414, acc: 0.92, recall: 0.9199999999999999, precision: 0.9227053140096618, f_beta: 0.9198717948717948
train: step: 4332, loss: 0.048961151391267776, acc: 0.94, recall: 0.94, precision: 0.9407051282051282, f_beta: 0.9399759903961584
train: step: 4333, loss: 0.04252997413277626, acc: 0.95, recall: 0.95, precision: 0.9501800720288115, f_beta: 0.9499949994999499
train: step: 4334, loss: 0.026937492191791534, acc: 0.96, recall: 0.96, precision: 0.962962962962963, f_beta: 0.9599358974358974
train: step: 4335, loss: 0.03791527822613716, acc: 0.95, recall: 0.95, precision: 0.9545454545454546, f_beta: 0.949874686716792
train: step: 4336, loss: 0.048222895711660385, acc: 0.94, recall: 0.94, precision: 0.9407051282051282, f_beta: 0.9399759903961584
train: step: 4337, loss: 0.04181673005223274, acc: 0.95, recall: 0.95, precision: 0.9516258530710557, f_beta: 0.949954959463517
train: step: 4338, loss: 0.024742282927036285, acc: 0.98, recall: 0.98, precision: 0.9807692307692308, f_beta: 0.9799919967987194
train: step: 4339, loss: 0.026522619649767876, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 4340, loss: 0.009305765852332115, acc: 0.99, recall: 0.99, precision: 0.9901960784313726, f_beta: 0.98999899989999
train: step: 4341, loss: 0.0508260615170002, acc: 0.94, recall: 0.94, precision: 0.9464285714285714, f_beta: 0.939783219590526
train: step: 4342, loss: 0.013755780644714832, acc: 0.98, recall: 0.98, precision: 0.9807692307692308, f_beta: 0.9799919967987194
train: step: 4343, loss: 0.0699467733502388, acc: 0.89, recall: 0.8899999999999999, precision: 0.8939393939393939, f_beta: 0.8897243107769424
train: step: 4344, loss: 0.039445556700229645, acc: 0.95, recall: 0.95, precision: 0.9501800720288115, f_beta: 0.9499949994999499
train: step: 4345, loss: 0.01593572273850441, acc: 0.99, recall: 0.99, precision: 0.9901960784313726, f_beta: 0.98999899989999
train: step: 4346, loss: 0.03162959963083267, acc: 0.96, recall: 0.96, precision: 0.9607371794871795, f_beta: 0.959983993597439
train: step: 4347, loss: 0.03311740234494209, acc: 0.95, recall: 0.95, precision: 0.9501800720288115, f_beta: 0.9499949994999499
train: step: 4348, loss: 0.04784709960222244, acc: 0.95, recall: 0.95, precision: 0.9516258530710557, f_beta: 0.949954959463517
train: step: 4349, loss: 0.016268892213702202, acc: 0.99, recall: 0.99, precision: 0.9901960784313726, f_beta: 0.98999899989999
train: step: 4350, loss: 0.030221642926335335, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 4351, loss: 0.03871969133615494, acc: 0.96, recall: 0.96, precision: 0.96, f_beta: 0.96
train: step: 4352, loss: 0.026364916935563087, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 4353, loss: 0.044614702463150024, acc: 0.95, recall: 0.95, precision: 0.9516258530710557, f_beta: 0.949954959463517
train: step: 4354, loss: 0.04810382425785065, acc: 0.94, recall: 0.94, precision: 0.9464285714285714, f_beta: 0.939783219590526
train: step: 4355, loss: 0.04115850850939751, acc: 0.96, recall: 0.96, precision: 0.962962962962963, f_beta: 0.9599358974358974
train: step: 4356, loss: 0.052893150597810745, acc: 0.93, recall: 0.9299999999999999, precision: 0.9343434343434343, f_beta: 0.9298245614035088
train: step: 4357, loss: 0.04803173243999481, acc: 0.95, recall: 0.95, precision: 0.9516258530710557, f_beta: 0.949954959463517
train: step: 4358, loss: 0.031672608107328415, acc: 0.96, recall: 0.96, precision: 0.9607371794871795, f_beta: 0.959983993597439
train: step: 4359, loss: 0.04087875410914421, acc: 0.95, recall: 0.95, precision: 0.9545454545454546, f_beta: 0.949874686716792
train: step: 4360, loss: 0.037555254995822906, acc: 0.95, recall: 0.95, precision: 0.9501800720288115, f_beta: 0.9499949994999499
train: step: 4361, loss: 0.03984993323683739, acc: 0.94, recall: 0.94, precision: 0.9428341384863124, f_beta: 0.9399038461538463
train: step: 4362, loss: 0.02522134780883789, acc: 0.96, recall: 0.96, precision: 0.96, f_beta: 0.96
train: step: 4363, loss: 0.0315883494913578, acc: 0.95, recall: 0.95, precision: 0.9545454545454546, f_beta: 0.949874686716792
train: step: 4364, loss: 0.03211411088705063, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 4365, loss: 0.036011915653944016, acc: 0.95, recall: 0.95, precision: 0.9516258530710557, f_beta: 0.949954959463517
train: step: 4366, loss: 0.015760736539959908, acc: 0.99, recall: 0.99, precision: 0.9901960784313726, f_beta: 0.98999899989999
train: step: 4367, loss: 0.027725592255592346, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 4368, loss: 0.0244891457259655, acc: 0.97, recall: 0.97, precision: 0.9716981132075472, f_beta: 0.9699729756781104
train: step: 4369, loss: 0.024288415908813477, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 4370, loss: 0.05129672959446907, acc: 0.94, recall: 0.94, precision: 0.9407051282051282, f_beta: 0.9399759903961584
train: step: 4371, loss: 0.02289251796901226, acc: 0.96, recall: 0.96, precision: 0.9607371794871795, f_beta: 0.959983993597439
train: step: 4372, loss: 0.016608547419309616, acc: 0.98, recall: 0.98, precision: 0.9807692307692308, f_beta: 0.9799919967987194
train: step: 4373, loss: 0.039166249334812164, acc: 0.96, recall: 0.96, precision: 0.9607371794871795, f_beta: 0.959983993597439
train: step: 4374, loss: 0.002668746979907155, acc: 1.0, recall: 1.0, precision: 1.0, f_beta: 1.0
train: step: 4375, loss: 0.01811971142888069, acc: 0.97, recall: 0.97, precision: 0.9716981132075472, f_beta: 0.9699729756781104
train: step: 4376, loss: 0.02146245911717415, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 4377, loss: 0.02797807939350605, acc: 0.98, recall: 0.98, precision: 0.9807692307692308, f_beta: 0.9799919967987194
train: step: 4378, loss: 0.019955938681960106, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 4379, loss: 0.01548653282225132, acc: 0.98, recall: 0.98, precision: 0.9807692307692308, f_beta: 0.9799919967987194
train: step: 4380, loss: 0.031061025336384773, acc: 0.96, recall: 0.96, precision: 0.962962962962963, f_beta: 0.9599358974358974
train: step: 4381, loss: 0.018835989758372307, acc: 0.97, recall: 0.97, precision: 0.9716981132075472, f_beta: 0.9699729756781104
train: step: 4382, loss: 0.021220002323389053, acc: 0.97, recall: 0.97, precision: 0.9716981132075472, f_beta: 0.9699729756781104
train: step: 4383, loss: 0.02803167887032032, acc: 0.96, recall: 0.96, precision: 0.962962962962963, f_beta: 0.9599358974358974
train: step: 4384, loss: 0.04658401012420654, acc: 0.95, recall: 0.95, precision: 0.9545454545454546, f_beta: 0.949874686716792
train: step: 4385, loss: 0.008072346448898315, acc: 0.99, recall: 0.99, precision: 0.9901960784313726, f_beta: 0.98999899989999
train: step: 4386, loss: 0.044115275144577026, acc: 0.94, recall: 0.94, precision: 0.9428341384863124, f_beta: 0.9399038461538463
train: step: 4387, loss: 0.019377291202545166, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 4388, loss: 0.033685460686683655, acc: 0.97, recall: 0.97, precision: 0.9716981132075472, f_beta: 0.9699729756781104
train: step: 4389, loss: 0.03213728591799736, acc: 0.96, recall: 0.96, precision: 0.962962962962963, f_beta: 0.9599358974358974
train: step: 4390, loss: 0.02717503532767296, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 4391, loss: 0.026940666139125824, acc: 0.96, recall: 0.96, precision: 0.9607371794871795, f_beta: 0.959983993597439
train: step: 4392, loss: 0.005217267666012049, acc: 1.0, recall: 1.0, precision: 1.0, f_beta: 1.0
train: step: 4393, loss: 0.03976568579673767, acc: 0.96, recall: 0.96, precision: 0.96, f_beta: 0.96
train: step: 4394, loss: 0.07068755477666855, acc: 0.92, recall: 0.9199999999999999, precision: 0.9206730769230769, f_beta: 0.919967987194878
train: step: 4395, loss: 0.028051115572452545, acc: 0.96, recall: 0.96, precision: 0.962962962962963, f_beta: 0.9599358974358974
train: step: 4396, loss: 0.004214676562696695, acc: 0.99, recall: 0.99, precision: 0.9901960784313726, f_beta: 0.98999899989999
train: step: 4397, loss: 0.03255048766732216, acc: 0.95, recall: 0.95, precision: 0.9516258530710557, f_beta: 0.949954959463517
train: step: 4398, loss: 0.039623767137527466, acc: 0.96, recall: 0.96, precision: 0.9607371794871795, f_beta: 0.959983993597439
train: step: 4399, loss: 0.04544995725154877, acc: 0.95, recall: 0.95, precision: 0.9516258530710557, f_beta: 0.949954959463517
train: step: 4400, loss: 0.01371800061315298, acc: 0.99, recall: 0.99, precision: 0.9901960784313726, f_beta: 0.98999899989999
train: step: 4401, loss: 0.03552405908703804, acc: 0.96, recall: 0.96, precision: 0.9607371794871795, f_beta: 0.959983993597439
train: step: 4402, loss: 0.01916518062353134, acc: 0.98, recall: 0.98, precision: 0.9807692307692308, f_beta: 0.9799919967987194
train: step: 4403, loss: 0.02511591464281082, acc: 0.96, recall: 0.96, precision: 0.96, f_beta: 0.96
train: step: 4404, loss: 0.004785677418112755, acc: 0.99, recall: 0.99, precision: 0.9901960784313726, f_beta: 0.98999899989999
train: step: 4405, loss: 0.040112245827913284, acc: 0.96, recall: 0.96, precision: 0.9607371794871795, f_beta: 0.959983993597439
train: step: 4406, loss: 0.02056286297738552, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 4407, loss: 0.04019283130764961, acc: 0.94, recall: 0.94, precision: 0.94, f_beta: 0.94
train: step: 4408, loss: 0.025964386761188507, acc: 0.96, recall: 0.96, precision: 0.9607371794871795, f_beta: 0.959983993597439
train: step: 4409, loss: 0.04150215536355972, acc: 0.96, recall: 0.96, precision: 0.96, f_beta: 0.96
train: step: 4410, loss: 0.020393531769514084, acc: 0.98, recall: 0.98, precision: 0.98, f_beta: 0.98
train: step: 4411, loss: 0.034319858998060226, acc: 0.96, recall: 0.96, precision: 0.962962962962963, f_beta: 0.9599358974358974
train: step: 4412, loss: 0.012566855177283287, acc: 0.98, recall: 0.98, precision: 0.98, f_beta: 0.98
train: step: 4413, loss: 0.03989363834261894, acc: 0.96, recall: 0.96, precision: 0.96, f_beta: 0.96
train: step: 4414, loss: 0.041755128651857376, acc: 0.95, recall: 0.95, precision: 0.9516258530710557, f_beta: 0.949954959463517
train: step: 4415, loss: 0.055816423147916794, acc: 0.94, recall: 0.94, precision: 0.9407051282051282, f_beta: 0.9399759903961584
train: step: 4416, loss: 0.02302997186779976, acc: 0.98, recall: 0.98, precision: 0.98, f_beta: 0.98
train: step: 4417, loss: 0.02283390983939171, acc: 0.97, recall: 0.97, precision: 0.9716981132075472, f_beta: 0.9699729756781104
train: step: 4418, loss: 0.03824303299188614, acc: 0.96, recall: 0.96, precision: 0.9607371794871795, f_beta: 0.959983993597439
train: step: 4419, loss: 0.029150690883398056, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 4420, loss: 0.05094558745622635, acc: 0.95, recall: 0.95, precision: 0.9516258530710557, f_beta: 0.949954959463517
train: step: 4421, loss: 0.02005581371486187, acc: 0.98, recall: 0.98, precision: 0.98, f_beta: 0.98
train: step: 4422, loss: 0.041193779557943344, acc: 0.96, recall: 0.96, precision: 0.9607371794871795, f_beta: 0.959983993597439
train: step: 4423, loss: 0.050423745065927505, acc: 0.94, recall: 0.94, precision: 0.9428341384863124, f_beta: 0.9399038461538463
train: step: 4424, loss: 0.03409070894122124, acc: 0.96, recall: 0.96, precision: 0.96, f_beta: 0.96
train: step: 4425, loss: 0.03148850053548813, acc: 0.95, recall: 0.95, precision: 0.9516258530710557, f_beta: 0.949954959463517
train: step: 4426, loss: 0.015653572976589203, acc: 0.98, recall: 0.98, precision: 0.98, f_beta: 0.98
train: step: 4427, loss: 0.008523442782461643, acc: 0.99, recall: 0.99, precision: 0.9901960784313726, f_beta: 0.98999899989999
train: step: 4428, loss: 0.07232941687107086, acc: 0.9, recall: 0.8999999999999999, precision: 0.9058441558441559, f_beta: 0.8996386993175431
train: step: 4429, loss: 0.01655999943614006, acc: 0.98, recall: 0.98, precision: 0.98, f_beta: 0.98
train: step: 4430, loss: 0.04117815941572189, acc: 0.95, recall: 0.95, precision: 0.9501800720288115, f_beta: 0.9499949994999499
train: step: 4431, loss: 0.04083891957998276, acc: 0.94, recall: 0.94, precision: 0.9464285714285714, f_beta: 0.939783219590526
train: step: 4432, loss: 0.034896064549684525, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 4433, loss: 0.022588424384593964, acc: 0.96, recall: 0.96, precision: 0.9607371794871795, f_beta: 0.959983993597439
train: step: 4434, loss: 0.019074689596891403, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 4435, loss: 0.017786061391234398, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 4436, loss: 0.033614132553339005, acc: 0.95, recall: 0.95, precision: 0.9516258530710557, f_beta: 0.949954959463517
train: step: 4437, loss: 0.019553350284695625, acc: 0.98, recall: 0.98, precision: 0.98, f_beta: 0.98
train: step: 4438, loss: 0.04957885295152664, acc: 0.93, recall: 0.9299999999999999, precision: 0.930172068827531, f_beta: 0.9299929992999301
train: step: 4439, loss: 0.0553533174097538, acc: 0.91, recall: 0.91, precision: 0.9101640656262505, f_beta: 0.90999099909991
train: step: 4440, loss: 0.05901473015546799, acc: 0.93, recall: 0.9299999999999999, precision: 0.9315535929345644, f_beta: 0.9299369432489241
train: step: 4441, loss: 0.03427829593420029, acc: 0.96, recall: 0.96, precision: 0.9607371794871795, f_beta: 0.959983993597439
train: step: 4442, loss: 0.016053607687354088, acc: 0.98, recall: 0.98, precision: 0.98, f_beta: 0.98
train: step: 4443, loss: 0.02956966869533062, acc: 0.98, recall: 0.98, precision: 0.98, f_beta: 0.98
train: step: 4444, loss: 0.018411777913570404, acc: 0.99, recall: 0.99, precision: 0.9901960784313726, f_beta: 0.98999899989999
train: step: 4445, loss: 0.020721321925520897, acc: 0.98, recall: 0.98, precision: 0.9807692307692308, f_beta: 0.9799919967987194
train: step: 4446, loss: 0.017310142517089844, acc: 0.98, recall: 0.98, precision: 0.98, f_beta: 0.98
train: step: 4447, loss: 0.04859830066561699, acc: 0.93, recall: 0.9299999999999999, precision: 0.930172068827531, f_beta: 0.9299929992999301
train: step: 4448, loss: 0.003794958582147956, acc: 1.0, recall: 1.0, precision: 1.0, f_beta: 1.0
train: step: 4449, loss: 0.031569644808769226, acc: 0.95, recall: 0.95, precision: 0.9501800720288115, f_beta: 0.9499949994999499
train: step: 4450, loss: 0.029394935816526413, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 4451, loss: 0.023660726845264435, acc: 0.98, recall: 0.98, precision: 0.9807692307692308, f_beta: 0.9799919967987194
train: step: 4452, loss: 0.02286679670214653, acc: 0.97, recall: 0.97, precision: 0.9716981132075472, f_beta: 0.9699729756781104
train: step: 4453, loss: 0.041743312031030655, acc: 0.95, recall: 0.95, precision: 0.9501800720288115, f_beta: 0.9499949994999499
train: step: 4454, loss: 0.014580478891730309, acc: 0.99, recall: 0.99, precision: 0.9901960784313726, f_beta: 0.98999899989999
train: step: 4455, loss: 0.015550322830677032, acc: 0.99, recall: 0.99, precision: 0.9901960784313726, f_beta: 0.98999899989999
train: step: 4456, loss: 0.01874866522848606, acc: 0.98, recall: 0.98, precision: 0.9807692307692308, f_beta: 0.9799919967987194
train: step: 4457, loss: 0.004743167664855719, acc: 1.0, recall: 1.0, precision: 1.0, f_beta: 1.0
train: step: 4458, loss: 0.01989353634417057, acc: 0.97, recall: 0.97, precision: 0.9716981132075472, f_beta: 0.9699729756781104
train: step: 4459, loss: 0.03747739642858505, acc: 0.96, recall: 0.96, precision: 0.96, f_beta: 0.96
train: step: 4460, loss: 0.020985841751098633, acc: 0.96, recall: 0.96, precision: 0.9607371794871795, f_beta: 0.959983993597439
train: step: 4461, loss: 0.053593385964632034, acc: 0.93, recall: 0.9299999999999999, precision: 0.9315535929345644, f_beta: 0.9299369432489241
train: step: 4462, loss: 0.015065562911331654, acc: 0.99, recall: 0.99, precision: 0.9901960784313726, f_beta: 0.98999899989999
train: step: 4463, loss: 0.026874832808971405, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 4464, loss: 0.03917356953024864, acc: 0.95, recall: 0.95, precision: 0.9501800720288115, f_beta: 0.9499949994999499
train: step: 4465, loss: 0.019211387261748314, acc: 0.97, recall: 0.97, precision: 0.9716981132075472, f_beta: 0.9699729756781104
train: step: 4466, loss: 0.030502839013934135, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 4467, loss: 0.01372308935970068, acc: 0.99, recall: 0.99, precision: 0.9901960784313726, f_beta: 0.98999899989999
train: step: 4468, loss: 0.02295506000518799, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 4469, loss: 0.021822897717356682, acc: 0.96, recall: 0.96, precision: 0.962962962962963, f_beta: 0.9599358974358974
train: step: 4470, loss: 0.045585598796606064, acc: 0.94, recall: 0.94, precision: 0.9407051282051282, f_beta: 0.9399759903961584
train: step: 4471, loss: 0.004223320633172989, acc: 0.99, recall: 0.99, precision: 0.9901960784313726, f_beta: 0.98999899989999
train: step: 4472, loss: 0.06327871233224869, acc: 0.92, recall: 0.9199999999999999, precision: 0.9261363636363636, f_beta: 0.9197109594540345
train: step: 4473, loss: 0.012129643931984901, acc: 0.99, recall: 0.99, precision: 0.9901960784313726, f_beta: 0.98999899989999
train: step: 4474, loss: 0.03506152704358101, acc: 0.96, recall: 0.96, precision: 0.9607371794871795, f_beta: 0.959983993597439
train: step: 4475, loss: 0.026460709050297737, acc: 0.97, recall: 0.97, precision: 0.9716981132075472, f_beta: 0.9699729756781104
train: step: 4476, loss: 0.016761964187026024, acc: 0.97, recall: 0.97, precision: 0.9716981132075472, f_beta: 0.9699729756781104
train: step: 4477, loss: 0.027661414816975594, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 4478, loss: 0.07875030487775803, acc: 0.91, recall: 0.9099999999999999, precision: 0.9114813327980731, f_beta: 0.9099189270343309
train: step: 4479, loss: 0.03796364367008209, acc: 0.95, recall: 0.95, precision: 0.9516258530710557, f_beta: 0.949954959463517
train: step: 4480, loss: 0.010081462562084198, acc: 0.98, recall: 0.98, precision: 0.9807692307692308, f_beta: 0.9799919967987194
train: step: 4481, loss: 0.031441595405340195, acc: 0.95, recall: 0.95, precision: 0.9516258530710557, f_beta: 0.949954959463517
train: step: 4482, loss: 0.05297485366463661, acc: 0.94, recall: 0.94, precision: 0.94, f_beta: 0.94
train: step: 4483, loss: 0.04729219526052475, acc: 0.92, recall: 0.9199999999999999, precision: 0.9206730769230769, f_beta: 0.919967987194878
train: step: 4484, loss: 0.03372392803430557, acc: 0.96, recall: 0.96, precision: 0.96, f_beta: 0.96
train: step: 4485, loss: 0.003915620502084494, acc: 1.0, recall: 1.0, precision: 1.0, f_beta: 1.0
train: step: 4486, loss: 0.024599986150860786, acc: 0.98, recall: 0.98, precision: 0.9807692307692308, f_beta: 0.9799919967987194
train: step: 4487, loss: 0.03614822030067444, acc: 0.96, recall: 0.96, precision: 0.9607371794871795, f_beta: 0.959983993597439
train: step: 4488, loss: 0.045984067022800446, acc: 0.94, recall: 0.94, precision: 0.9407051282051282, f_beta: 0.9399759903961584
train: step: 4489, loss: 0.027938971295952797, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 4490, loss: 0.0710645392537117, acc: 0.91, recall: 0.9099999999999999, precision: 0.9181966544267646, f_beta: 0.9095568284594513
train: step: 4491, loss: 0.04009521007537842, acc: 0.95, recall: 0.95, precision: 0.9545454545454546, f_beta: 0.949874686716792
train: step: 4492, loss: 0.028263550251722336, acc: 0.96, recall: 0.96, precision: 0.9607371794871795, f_beta: 0.959983993597439
train: step: 4493, loss: 0.012646418064832687, acc: 0.99, recall: 0.99, precision: 0.9901960784313726, f_beta: 0.98999899989999
train: step: 4494, loss: 0.021093793213367462, acc: 0.98, recall: 0.98, precision: 0.9807692307692308, f_beta: 0.9799919967987194
train: step: 4495, loss: 0.0515921413898468, acc: 0.94, recall: 0.94, precision: 0.9407051282051282, f_beta: 0.9399759903961584
train: step: 4496, loss: 0.03601570427417755, acc: 0.96, recall: 0.96, precision: 0.96, f_beta: 0.96
train: step: 4497, loss: 0.02277211844921112, acc: 0.96, recall: 0.96, precision: 0.9607371794871795, f_beta: 0.959983993597439
train: step: 4498, loss: 0.020425403490662575, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 4499, loss: 0.02095114067196846, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 4500, loss: 0.016350381076335907, acc: 0.98, recall: 0.98, precision: 0.9807692307692308, f_beta: 0.9799919967987194
train: step: 4501, loss: 0.03419114276766777, acc: 0.96, recall: 0.96, precision: 0.9607371794871795, f_beta: 0.959983993597439
train: step: 4502, loss: 0.015989094972610474, acc: 0.98, recall: 0.98, precision: 0.9807692307692308, f_beta: 0.9799919967987194
train: step: 4503, loss: 0.036951541900634766, acc: 0.96, recall: 0.96, precision: 0.9607371794871795, f_beta: 0.959983993597439
train: step: 4504, loss: 0.05081428587436676, acc: 0.95, recall: 0.95, precision: 0.9501800720288115, f_beta: 0.9499949994999499
train: step: 4505, loss: 0.0324503555893898, acc: 0.97, recall: 0.97, precision: 0.9716981132075472, f_beta: 0.9699729756781104
train: step: 4506, loss: 0.039627671241760254, acc: 0.95, recall: 0.95, precision: 0.9501800720288115, f_beta: 0.9499949994999499
train: step: 4507, loss: 0.023004470393061638, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 4508, loss: 0.06765612214803696, acc: 0.91, recall: 0.9099999999999999, precision: 0.9181966544267646, f_beta: 0.9095568284594513
train: step: 4509, loss: 0.013468055985867977, acc: 0.99, recall: 0.99, precision: 0.9901960784313726, f_beta: 0.98999899989999
train: step: 4510, loss: 0.013740206137299538, acc: 0.98, recall: 0.98, precision: 0.98, f_beta: 0.98
train: step: 4511, loss: 0.05664370208978653, acc: 0.94, recall: 0.94, precision: 0.9428341384863124, f_beta: 0.9399038461538463
train: step: 4512, loss: 0.06116684898734093, acc: 0.93, recall: 0.9299999999999999, precision: 0.9343434343434343, f_beta: 0.9298245614035088
train: step: 4513, loss: 0.018939632922410965, acc: 0.98, recall: 0.98, precision: 0.98, f_beta: 0.98
train: step: 4514, loss: 0.027478933334350586, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 4515, loss: 0.02903698943555355, acc: 0.96, recall: 0.96, precision: 0.9607371794871795, f_beta: 0.959983993597439
train: step: 4516, loss: 0.024198142811655998, acc: 0.97, recall: 0.97, precision: 0.9716981132075472, f_beta: 0.9699729756781104
train: step: 4517, loss: 0.011030621826648712, acc: 1.0, recall: 1.0, precision: 1.0, f_beta: 1.0
train: step: 4518, loss: 0.049752868711948395, acc: 0.93, recall: 0.9299999999999999, precision: 0.9343434343434343, f_beta: 0.9298245614035088
train: step: 4519, loss: 0.030003780499100685, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 4520, loss: 0.06117596477270126, acc: 0.9, recall: 0.9, precision: 0.9006410256410255, f_beta: 0.8999599839935974
train: step: 4521, loss: 0.035711925476789474, acc: 0.95, recall: 0.95, precision: 0.9501800720288115, f_beta: 0.9499949994999499
train: step: 4522, loss: 0.04403592646121979, acc: 0.95, recall: 0.95, precision: 0.9545454545454546, f_beta: 0.949874686716792
train: step: 4523, loss: 0.04137324169278145, acc: 0.95, recall: 0.95, precision: 0.9501800720288115, f_beta: 0.9499949994999499
train: step: 4524, loss: 0.044569119811058044, acc: 0.94, recall: 0.94, precision: 0.9428341384863124, f_beta: 0.9399038461538463
train: step: 4525, loss: 0.0104088569059968, acc: 0.99, recall: 0.99, precision: 0.9901960784313726, f_beta: 0.98999899989999
train: step: 4526, loss: 0.0549020953476429, acc: 0.93, recall: 0.9299999999999999, precision: 0.9315535929345644, f_beta: 0.9299369432489241
train: step: 4527, loss: 0.01424186211079359, acc: 0.98, recall: 0.98, precision: 0.9807692307692308, f_beta: 0.9799919967987194
train: step: 4528, loss: 0.01103412639349699, acc: 0.99, recall: 0.99, precision: 0.9901960784313726, f_beta: 0.98999899989999
train: step: 4529, loss: 0.012805464677512646, acc: 0.99, recall: 0.99, precision: 0.9901960784313726, f_beta: 0.98999899989999
train: step: 4530, loss: 0.028245583176612854, acc: 0.96, recall: 0.96, precision: 0.962962962962963, f_beta: 0.9599358974358974
train: step: 4531, loss: 0.02319427579641342, acc: 0.98, recall: 0.98, precision: 0.9807692307692308, f_beta: 0.9799919967987194
train: step: 4532, loss: 0.08808746188879013, acc: 0.89, recall: 0.89, precision: 0.89015606242497, f_beta: 0.88998899889989
train: step: 4533, loss: 0.014803209342062473, acc: 0.98, recall: 0.98, precision: 0.98, f_beta: 0.98
train: step: 4534, loss: 0.027609623968601227, acc: 0.96, recall: 0.96, precision: 0.9607371794871795, f_beta: 0.959983993597439
train: step: 4535, loss: 0.028508581221103668, acc: 0.96, recall: 0.96, precision: 0.9607371794871795, f_beta: 0.959983993597439
train: step: 4536, loss: 0.03017876110970974, acc: 0.96, recall: 0.96, precision: 0.9607371794871795, f_beta: 0.959983993597439
train: step: 4537, loss: 0.0399920716881752, acc: 0.96, recall: 0.96, precision: 0.96, f_beta: 0.96
train: step: 4538, loss: 0.023669151589274406, acc: 0.97, recall: 0.97, precision: 0.9716981132075472, f_beta: 0.9699729756781104
train: step: 4539, loss: 0.026487812399864197, acc: 0.96, recall: 0.96, precision: 0.9607371794871795, f_beta: 0.959983993597439
train: step: 4540, loss: 0.041494984179735184, acc: 0.95, recall: 0.95, precision: 0.9501800720288115, f_beta: 0.9499949994999499
train: step: 4541, loss: 0.020773427560925484, acc: 0.98, recall: 0.98, precision: 0.98, f_beta: 0.98
train: step: 4542, loss: 0.012817483395338058, acc: 0.99, recall: 0.99, precision: 0.9901960784313726, f_beta: 0.98999899989999
train: step: 4543, loss: 0.033703237771987915, acc: 0.95, recall: 0.95, precision: 0.9501800720288115, f_beta: 0.9499949994999499
train: step: 4544, loss: 0.03218744695186615, acc: 0.95, recall: 0.95, precision: 0.9545454545454546, f_beta: 0.949874686716792
train: step: 4545, loss: 0.017753146588802338, acc: 0.97, recall: 0.97, precision: 0.9716981132075472, f_beta: 0.9699729756781104
train: step: 4546, loss: 0.023617420345544815, acc: 0.97, recall: 0.97, precision: 0.9716981132075472, f_beta: 0.9699729756781104
train: step: 4547, loss: 0.04849635437130928, acc: 0.94, recall: 0.94, precision: 0.9464285714285714, f_beta: 0.939783219590526
train: step: 4548, loss: 0.03182050585746765, acc: 0.96, recall: 0.96, precision: 0.9607371794871795, f_beta: 0.959983993597439
train: step: 4549, loss: 0.04396192729473114, acc: 0.94, recall: 0.94, precision: 0.9407051282051282, f_beta: 0.9399759903961584
train: step: 4550, loss: 0.010403506457805634, acc: 0.98, recall: 0.98, precision: 0.98, f_beta: 0.98
train: step: 4551, loss: 0.02564997225999832, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 4552, loss: 0.03491847217082977, acc: 0.96, recall: 0.96, precision: 0.9607371794871795, f_beta: 0.959983993597439
train: step: 4553, loss: 0.033981721848249435, acc: 0.96, recall: 0.96, precision: 0.9607371794871795, f_beta: 0.959983993597439
train: step: 4554, loss: 0.034297868609428406, acc: 0.96, recall: 0.96, precision: 0.96, f_beta: 0.96
train: step: 4555, loss: 0.04252295568585396, acc: 0.94, recall: 0.94, precision: 0.9407051282051282, f_beta: 0.9399759903961584
train: step: 4556, loss: 0.004787735641002655, acc: 1.0, recall: 1.0, precision: 1.0, f_beta: 1.0
train: step: 4557, loss: 0.034503091126680374, acc: 0.96, recall: 0.96, precision: 0.9607371794871795, f_beta: 0.959983993597439
train: step: 4558, loss: 0.017530495300889015, acc: 0.98, recall: 0.98, precision: 0.9807692307692308, f_beta: 0.9799919967987194
train: step: 4559, loss: 0.024118438363075256, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 4560, loss: 0.052089765667915344, acc: 0.9, recall: 0.9, precision: 0.9, f_beta: 0.9
train: step: 4561, loss: 0.03831316530704498, acc: 0.95, recall: 0.95, precision: 0.9516258530710557, f_beta: 0.949954959463517
train: step: 4562, loss: 0.041210804134607315, acc: 0.95, recall: 0.95, precision: 0.9501800720288115, f_beta: 0.9499949994999499
train: step: 4563, loss: 0.003417251631617546, acc: 1.0, recall: 1.0, precision: 1.0, f_beta: 1.0
train: step: 4564, loss: 0.024486582726240158, acc: 0.96, recall: 0.96, precision: 0.962962962962963, f_beta: 0.9599358974358974
train: step: 4565, loss: 0.030809983611106873, acc: 0.95, recall: 0.95, precision: 0.9501800720288115, f_beta: 0.9499949994999499
train: step: 4566, loss: 0.013513206504285336, acc: 0.99, recall: 0.99, precision: 0.9901960784313726, f_beta: 0.98999899989999
train: step: 4567, loss: 0.04019301012158394, acc: 0.95, recall: 0.95, precision: 0.9516258530710557, f_beta: 0.949954959463517
train: step: 4568, loss: 0.03561069816350937, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 4569, loss: 0.0385238379240036, acc: 0.94, recall: 0.94, precision: 0.9407051282051282, f_beta: 0.9399759903961584
train: step: 4570, loss: 0.009305976331233978, acc: 0.99, recall: 0.99, precision: 0.9901960784313726, f_beta: 0.98999899989999
train: step: 4571, loss: 0.03640371933579445, acc: 0.96, recall: 0.96, precision: 0.9607371794871795, f_beta: 0.959983993597439
train: step: 4572, loss: 0.03422141447663307, acc: 0.95, recall: 0.95, precision: 0.9501800720288115, f_beta: 0.9499949994999499
train: step: 4573, loss: 0.02077300101518631, acc: 0.98, recall: 0.98, precision: 0.9807692307692308, f_beta: 0.9799919967987194
train: step: 4574, loss: 0.011151779443025589, acc: 0.98, recall: 0.98, precision: 0.98, f_beta: 0.98
train: step: 4575, loss: 0.014460289850831032, acc: 0.98, recall: 0.98, precision: 0.9807692307692308, f_beta: 0.9799919967987194
train: step: 4576, loss: 0.0231180340051651, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 4577, loss: 0.013270859606564045, acc: 0.98, recall: 0.98, precision: 0.9807692307692308, f_beta: 0.9799919967987194
train: step: 4578, loss: 0.01824291981756687, acc: 0.98, recall: 0.98, precision: 0.9807692307692308, f_beta: 0.9799919967987194
train: step: 4579, loss: 0.0360974445939064, acc: 0.96, recall: 0.96, precision: 0.9607371794871795, f_beta: 0.959983993597439
train: step: 4580, loss: 0.012107052840292454, acc: 0.98, recall: 0.98, precision: 0.98, f_beta: 0.98
train: step: 4581, loss: 0.034331947565078735, acc: 0.96, recall: 0.96, precision: 0.9607371794871795, f_beta: 0.959983993597439
train: step: 4582, loss: 0.011995903216302395, acc: 0.99, recall: 0.99, precision: 0.9901960784313726, f_beta: 0.98999899989999
train: step: 4583, loss: 0.047889262437820435, acc: 0.93, recall: 0.9299999999999999, precision: 0.9315535929345644, f_beta: 0.9299369432489241
train: step: 4584, loss: 0.010773321613669395, acc: 0.98, recall: 0.98, precision: 0.9807692307692308, f_beta: 0.9799919967987194
train: step: 4585, loss: 0.036343906074762344, acc: 0.96, recall: 0.96, precision: 0.962962962962963, f_beta: 0.9599358974358974
train: step: 4586, loss: 0.02902153506875038, acc: 0.98, recall: 0.98, precision: 0.9807692307692308, f_beta: 0.9799919967987194
train: step: 4587, loss: 0.04275639355182648, acc: 0.94, recall: 0.94, precision: 0.94, f_beta: 0.94
train: step: 4588, loss: 0.03492284193634987, acc: 0.95, recall: 0.95, precision: 0.9545454545454546, f_beta: 0.949874686716792
train: step: 4589, loss: 0.014873543754220009, acc: 0.98, recall: 0.98, precision: 0.9807692307692308, f_beta: 0.9799919967987194
train: step: 4590, loss: 0.019334256649017334, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 4591, loss: 0.025232873857021332, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 4592, loss: 0.04663649946451187, acc: 0.95, recall: 0.95, precision: 0.9501800720288115, f_beta: 0.9499949994999499
train: step: 4593, loss: 0.03297515958547592, acc: 0.96, recall: 0.96, precision: 0.9607371794871795, f_beta: 0.959983993597439
train: step: 4594, loss: 0.014872346073389053, acc: 0.99, recall: 0.99, precision: 0.9901960784313726, f_beta: 0.98999899989999
train: step: 4595, loss: 0.020530549809336662, acc: 0.98, recall: 0.98, precision: 0.9807692307692308, f_beta: 0.9799919967987194
train: step: 4596, loss: 0.010269943624734879, acc: 0.99, recall: 0.99, precision: 0.9901960784313726, f_beta: 0.98999899989999
train: step: 4597, loss: 0.06756077706813812, acc: 0.92, recall: 0.9199999999999999, precision: 0.9206730769230769, f_beta: 0.919967987194878
train: step: 4598, loss: 0.019527100026607513, acc: 0.98, recall: 0.98, precision: 0.9807692307692308, f_beta: 0.9799919967987194
train: step: 4599, loss: 0.008316371589899063, acc: 0.99, recall: 0.99, precision: 0.9901960784313726, f_beta: 0.98999899989999
train: step: 4600, loss: 0.011138655245304108, acc: 0.99, recall: 0.99, precision: 0.9901960784313726, f_beta: 0.98999899989999
train: step: 4601, loss: 0.032138582319021225, acc: 0.96, recall: 0.96, precision: 0.96, f_beta: 0.96
train: step: 4602, loss: 0.024564776569604874, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 4603, loss: 0.03952416777610779, acc: 0.95, recall: 0.95, precision: 0.9545454545454546, f_beta: 0.949874686716792
train: step: 4604, loss: 0.0431218296289444, acc: 0.94, recall: 0.94, precision: 0.9407051282051282, f_beta: 0.9399759903961584
train: step: 4605, loss: 0.02139287069439888, acc: 0.97, recall: 0.97, precision: 0.9716981132075472, f_beta: 0.9699729756781104
train: step: 4606, loss: 0.011373517103493214, acc: 0.99, recall: 0.99, precision: 0.9901960784313726, f_beta: 0.98999899989999
train: step: 4607, loss: 0.03234928473830223, acc: 0.97, recall: 0.97, precision: 0.9716981132075472, f_beta: 0.9699729756781104
train: step: 4608, loss: 0.07957087457180023, acc: 0.9, recall: 0.8999999999999999, precision: 0.9025764895330113, f_beta: 0.8998397435897436
train: step: 4609, loss: 0.06856650859117508, acc: 0.91, recall: 0.9099999999999999, precision: 0.9114813327980731, f_beta: 0.9099189270343309
train: step: 4610, loss: 0.05298234522342682, acc: 0.93, recall: 0.9299999999999999, precision: 0.9315535929345644, f_beta: 0.9299369432489241
train: step: 4611, loss: 0.02481343224644661, acc: 0.96, recall: 0.96, precision: 0.9607371794871795, f_beta: 0.959983993597439
train: step: 4612, loss: 0.024322766810655594, acc: 0.96, recall: 0.96, precision: 0.96, f_beta: 0.96
train: step: 4613, loss: 0.020490331575274467, acc: 0.98, recall: 0.98, precision: 0.9807692307692308, f_beta: 0.9799919967987194
train: step: 4614, loss: 0.017349956557154655, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 4615, loss: 0.045122984796762466, acc: 0.95, recall: 0.95, precision: 0.9501800720288115, f_beta: 0.9499949994999499
train: step: 4616, loss: 0.02957465872168541, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 4617, loss: 0.011013252660632133, acc: 0.99, recall: 0.99, precision: 0.9901960784313726, f_beta: 0.98999899989999
train: step: 4618, loss: 0.03200647234916687, acc: 0.96, recall: 0.96, precision: 0.9607371794871795, f_beta: 0.959983993597439
train: step: 4619, loss: 0.0168907567858696, acc: 0.98, recall: 0.98, precision: 0.9807692307692308, f_beta: 0.9799919967987194
train: step: 4620, loss: 0.019386932253837585, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 4621, loss: 0.024463243782520294, acc: 0.98, recall: 0.98, precision: 0.9807692307692308, f_beta: 0.9799919967987194
train: step: 4622, loss: 0.028014320880174637, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 4623, loss: 0.04553249850869179, acc: 0.94, recall: 0.94, precision: 0.9407051282051282, f_beta: 0.9399759903961584
train: step: 4624, loss: 0.016734449192881584, acc: 0.98, recall: 0.98, precision: 0.9807692307692308, f_beta: 0.9799919967987194
train: step: 4625, loss: 0.0196659155189991, acc: 0.98, recall: 0.98, precision: 0.9807692307692308, f_beta: 0.9799919967987194
train: step: 4626, loss: 0.03137211501598358, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 4627, loss: 0.03186416253447533, acc: 0.95, recall: 0.95, precision: 0.9545454545454546, f_beta: 0.949874686716792
train: step: 4628, loss: 0.018299587070941925, acc: 0.98, recall: 0.98, precision: 0.9807692307692308, f_beta: 0.9799919967987194
train: step: 4629, loss: 0.02491690218448639, acc: 0.98, recall: 0.98, precision: 0.9807692307692308, f_beta: 0.9799919967987194
train: step: 4630, loss: 0.015831366181373596, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 4631, loss: 0.0062925745733082294, acc: 0.99, recall: 0.99, precision: 0.9901960784313726, f_beta: 0.98999899989999
train: step: 4632, loss: 0.04845193773508072, acc: 0.94, recall: 0.94, precision: 0.94, f_beta: 0.94
train: step: 4633, loss: 0.03232213854789734, acc: 0.96, recall: 0.96, precision: 0.962962962962963, f_beta: 0.9599358974358974
train: step: 4634, loss: 0.04274293780326843, acc: 0.96, recall: 0.96, precision: 0.96, f_beta: 0.96
train: step: 4635, loss: 0.027126627042889595, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 4636, loss: 0.022088460624217987, acc: 0.98, recall: 0.98, precision: 0.98, f_beta: 0.98
train: step: 4637, loss: 0.031282294541597366, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 4638, loss: 0.030302662402391434, acc: 0.97, recall: 0.97, precision: 0.9716981132075472, f_beta: 0.9699729756781104
train: step: 4639, loss: 0.021727556362748146, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 4640, loss: 0.041944924741983414, acc: 0.94, recall: 0.94, precision: 0.94, f_beta: 0.94
train: step: 4641, loss: 0.0238959901034832, acc: 0.98, recall: 0.98, precision: 0.98, f_beta: 0.98
train: step: 4642, loss: 0.01118145976215601, acc: 0.98, recall: 0.98, precision: 0.98, f_beta: 0.98
train: step: 4643, loss: 0.03971618041396141, acc: 0.94, recall: 0.94, precision: 0.9407051282051282, f_beta: 0.9399759903961584
train: step: 4644, loss: 0.018212538212537766, acc: 0.98, recall: 0.98, precision: 0.9807692307692308, f_beta: 0.9799919967987194
train: step: 4645, loss: 0.05137251317501068, acc: 0.93, recall: 0.9299999999999999, precision: 0.9343434343434343, f_beta: 0.9298245614035088
train: step: 4646, loss: 0.02733619138598442, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 4647, loss: 0.02151557058095932, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 4648, loss: 0.003141270950436592, acc: 0.99, recall: 0.99, precision: 0.9901960784313726, f_beta: 0.98999899989999
train: step: 4649, loss: 0.0248381569981575, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 4650, loss: 0.020567631348967552, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 4651, loss: 0.03989183157682419, acc: 0.93, recall: 0.9299999999999999, precision: 0.9315535929345644, f_beta: 0.9299369432489241
train: step: 4652, loss: 0.031425729393959045, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 4653, loss: 0.033291418105363846, acc: 0.96, recall: 0.96, precision: 0.9607371794871795, f_beta: 0.959983993597439
train: step: 4654, loss: 0.027624711394309998, acc: 0.96, recall: 0.96, precision: 0.962962962962963, f_beta: 0.9599358974358974
train: step: 4655, loss: 0.021183906123042107, acc: 0.98, recall: 0.98, precision: 0.98, f_beta: 0.98
train: step: 4656, loss: 0.04893733561038971, acc: 0.95, recall: 0.95, precision: 0.9516258530710557, f_beta: 0.949954959463517
train: step: 4657, loss: 0.009282732382416725, acc: 0.99, recall: 0.99, precision: 0.9901960784313726, f_beta: 0.98999899989999
train: step: 4658, loss: 0.02447277121245861, acc: 0.96, recall: 0.96, precision: 0.9607371794871795, f_beta: 0.959983993597439
train: step: 4659, loss: 0.0571397989988327, acc: 0.94, recall: 0.94, precision: 0.9407051282051282, f_beta: 0.9399759903961584
train: step: 4660, loss: 0.03895903751254082, acc: 0.96, recall: 0.96, precision: 0.9607371794871795, f_beta: 0.959983993597439
train: step: 4661, loss: 0.04062609747052193, acc: 0.95, recall: 0.95, precision: 0.9501800720288115, f_beta: 0.9499949994999499
train: step: 4662, loss: 0.015221662819385529, acc: 0.98, recall: 0.98, precision: 0.9807692307692308, f_beta: 0.9799919967987194
train: step: 4663, loss: 0.05470180884003639, acc: 0.92, recall: 0.9199999999999999, precision: 0.9227053140096618, f_beta: 0.9198717948717948
train: step: 4664, loss: 0.029276210814714432, acc: 0.96, recall: 0.96, precision: 0.96, f_beta: 0.96
train: step: 4665, loss: 0.046815261244773865, acc: 0.95, recall: 0.95, precision: 0.9516258530710557, f_beta: 0.949954959463517
train: step: 4666, loss: 0.014036035165190697, acc: 0.98, recall: 0.98, precision: 0.9807692307692308, f_beta: 0.9799919967987194
train: step: 4667, loss: 0.01996961422264576, acc: 0.98, recall: 0.98, precision: 0.9807692307692308, f_beta: 0.9799919967987194
train: step: 4668, loss: 0.059565600007772446, acc: 0.94, recall: 0.94, precision: 0.9407051282051282, f_beta: 0.9399759903961584
train: step: 4669, loss: 0.022781910374760628, acc: 0.96, recall: 0.96, precision: 0.9607371794871795, f_beta: 0.959983993597439
train: step: 4670, loss: 0.040230169892311096, acc: 0.96, recall: 0.96, precision: 0.962962962962963, f_beta: 0.9599358974358974
train: step: 4671, loss: 0.03905127942562103, acc: 0.95, recall: 0.95, precision: 0.9516258530710557, f_beta: 0.949954959463517
train: step: 4672, loss: 0.03652934357523918, acc: 0.95, recall: 0.95, precision: 0.9501800720288115, f_beta: 0.9499949994999499
train: step: 4673, loss: 0.034805990755558014, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 4674, loss: 0.028756700456142426, acc: 0.97, recall: 0.97, precision: 0.9716981132075472, f_beta: 0.9699729756781104
train: step: 4675, loss: 0.015489615499973297, acc: 0.98, recall: 0.98, precision: 0.9807692307692308, f_beta: 0.9799919967987194
train: step: 4676, loss: 0.036463718861341476, acc: 0.96, recall: 0.96, precision: 0.962962962962963, f_beta: 0.9599358974358974
train: step: 4677, loss: 0.02729858085513115, acc: 0.97, recall: 0.97, precision: 0.9716981132075472, f_beta: 0.9699729756781104
train: step: 4678, loss: 0.030166978016495705, acc: 0.96, recall: 0.96, precision: 0.9607371794871795, f_beta: 0.959983993597439
train: step: 4679, loss: 0.04037366062402725, acc: 0.93, recall: 0.9299999999999999, precision: 0.930172068827531, f_beta: 0.9299929992999301
train: step: 4680, loss: 0.01448776200413704, acc: 0.99, recall: 0.99, precision: 0.9901960784313726, f_beta: 0.98999899989999
train: step: 4681, loss: 0.022526323795318604, acc: 0.97, recall: 0.97, precision: 0.9716981132075472, f_beta: 0.9699729756781104
train: step: 4682, loss: 0.042377013713121414, acc: 0.95, recall: 0.95, precision: 0.9516258530710557, f_beta: 0.949954959463517
train: step: 4683, loss: 0.04024466499686241, acc: 0.95, recall: 0.95, precision: 0.9501800720288115, f_beta: 0.9499949994999499
train: step: 4684, loss: 0.032679881900548935, acc: 0.94, recall: 0.94, precision: 0.9407051282051282, f_beta: 0.9399759903961584
train: step: 4685, loss: 0.03270406648516655, acc: 0.95, recall: 0.95, precision: 0.9501800720288115, f_beta: 0.9499949994999499
train: step: 4686, loss: 0.03152467682957649, acc: 0.95, recall: 0.95, precision: 0.9516258530710557, f_beta: 0.949954959463517
train: step: 4687, loss: 0.028800148516893387, acc: 0.95, recall: 0.95, precision: 0.9545454545454546, f_beta: 0.949874686716792
train: step: 4688, loss: 0.024815697222948074, acc: 0.96, recall: 0.96, precision: 0.9607371794871795, f_beta: 0.959983993597439
train: step: 4689, loss: 0.0220757145434618, acc: 0.97, recall: 0.97, precision: 0.9716981132075472, f_beta: 0.9699729756781104
train: step: 4690, loss: 0.03794298693537712, acc: 0.96, recall: 0.96, precision: 0.9607371794871795, f_beta: 0.959983993597439
train: step: 4691, loss: 0.018342606723308563, acc: 0.98, recall: 0.98, precision: 0.98, f_beta: 0.98
train: step: 4692, loss: 0.008597374893724918, acc: 0.99, recall: 0.99, precision: 0.9901960784313726, f_beta: 0.98999899989999
train: step: 4693, loss: 0.050588227808475494, acc: 0.94, recall: 0.94, precision: 0.94, f_beta: 0.94
train: step: 4694, loss: 0.03629127889871597, acc: 0.95, recall: 0.95, precision: 0.9516258530710557, f_beta: 0.949954959463517
train: step: 4695, loss: 0.012451239861547947, acc: 0.99, recall: 0.99, precision: 0.9901960784313726, f_beta: 0.98999899989999
train: step: 4696, loss: 0.03134005889296532, acc: 0.96, recall: 0.96, precision: 0.96, f_beta: 0.96
train: step: 4697, loss: 0.0356023795902729, acc: 0.95, recall: 0.95, precision: 0.9516258530710557, f_beta: 0.949954959463517
train: step: 4698, loss: 0.03443915396928787, acc: 0.95, recall: 0.95, precision: 0.9516258530710557, f_beta: 0.949954959463517
train: step: 4699, loss: 0.010243769735097885, acc: 0.98, recall: 0.98, precision: 0.98, f_beta: 0.98
train: step: 4700, loss: 0.05410009250044823, acc: 0.92, recall: 0.9199999999999999, precision: 0.9227053140096618, f_beta: 0.9198717948717948
train: step: 4701, loss: 0.00689359474927187, acc: 0.99, recall: 0.99, precision: 0.9901960784313726, f_beta: 0.98999899989999
train: step: 4702, loss: 0.02366890385746956, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 4703, loss: 0.0220961757004261, acc: 0.97, recall: 0.97, precision: 0.9716981132075472, f_beta: 0.9699729756781104
train: step: 4704, loss: 0.008613090962171555, acc: 0.99, recall: 0.99, precision: 0.9901960784313726, f_beta: 0.98999899989999
train: step: 4705, loss: 0.0575876459479332, acc: 0.94, recall: 0.94, precision: 0.9464285714285714, f_beta: 0.939783219590526
train: step: 4706, loss: 0.03256189450621605, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 4707, loss: 0.016817452386021614, acc: 0.98, recall: 0.98, precision: 0.9807692307692308, f_beta: 0.9799919967987194
train: step: 4708, loss: 0.029069948941469193, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 4709, loss: 0.01821732707321644, acc: 0.99, recall: 0.99, precision: 0.9901960784313726, f_beta: 0.98999899989999
train: step: 4710, loss: 0.039927467703819275, acc: 0.94, recall: 0.94, precision: 0.9407051282051282, f_beta: 0.9399759903961584
train: step: 4711, loss: 0.05597348138689995, acc: 0.93, recall: 0.9299999999999999, precision: 0.9315535929345644, f_beta: 0.9299369432489241
train: step: 4712, loss: 0.03645075857639313, acc: 0.96, recall: 0.96, precision: 0.9607371794871795, f_beta: 0.959983993597439
train: step: 4713, loss: 0.02222589962184429, acc: 0.98, recall: 0.98, precision: 0.98, f_beta: 0.98
train: step: 4714, loss: 0.02483430691063404, acc: 0.96, recall: 0.96, precision: 0.9607371794871795, f_beta: 0.959983993597439
train: step: 4715, loss: 0.0238356851041317, acc: 0.98, recall: 0.98, precision: 0.9807692307692308, f_beta: 0.9799919967987194
train: step: 4716, loss: 0.04697937145829201, acc: 0.94, recall: 0.94, precision: 0.9464285714285714, f_beta: 0.939783219590526
train: step: 4717, loss: 0.017443135380744934, acc: 0.98, recall: 0.98, precision: 0.98, f_beta: 0.98
train: step: 4718, loss: 0.010125655680894852, acc: 0.99, recall: 0.99, precision: 0.9901960784313726, f_beta: 0.98999899989999
train: step: 4719, loss: 0.0103134261444211, acc: 0.98, recall: 0.98, precision: 0.98, f_beta: 0.98
train: step: 4720, loss: 0.021921295672655106, acc: 0.97, recall: 0.97, precision: 0.9716981132075472, f_beta: 0.9699729756781104
train: step: 4721, loss: 0.025067681446671486, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 4722, loss: 0.005128221586346626, acc: 0.99, recall: 0.99, precision: 0.9901960784313726, f_beta: 0.98999899989999
train: step: 4723, loss: 0.017479171976447105, acc: 0.98, recall: 0.98, precision: 0.98, f_beta: 0.98
train: step: 4724, loss: 0.026729684323072433, acc: 0.95, recall: 0.95, precision: 0.9545454545454546, f_beta: 0.949874686716792
train: step: 4725, loss: 0.01121017336845398, acc: 0.99, recall: 0.99, precision: 0.9901960784313726, f_beta: 0.98999899989999
train: step: 4726, loss: 0.022384963929653168, acc: 0.98, recall: 0.98, precision: 0.9807692307692308, f_beta: 0.9799919967987194
train: step: 4727, loss: 0.016791243106126785, acc: 0.98, recall: 0.98, precision: 0.9807692307692308, f_beta: 0.9799919967987194
train: step: 4728, loss: 0.027615807950496674, acc: 0.96, recall: 0.96, precision: 0.96, f_beta: 0.96
train: step: 4729, loss: 0.00879798736423254, acc: 0.98, recall: 0.98, precision: 0.9807692307692308, f_beta: 0.9799919967987194
train: step: 4730, loss: 0.0272840429097414, acc: 0.96, recall: 0.96, precision: 0.9607371794871795, f_beta: 0.959983993597439
train: step: 4731, loss: 0.023195112124085426, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 4732, loss: 0.02055453322827816, acc: 0.96, recall: 0.96, precision: 0.96, f_beta: 0.96
train: step: 4733, loss: 0.0419980064034462, acc: 0.94, recall: 0.94, precision: 0.9428341384863124, f_beta: 0.9399038461538463
train: step: 4734, loss: 0.04294029250741005, acc: 0.94, recall: 0.94, precision: 0.9407051282051282, f_beta: 0.9399759903961584
train: step: 4735, loss: 0.014519994147121906, acc: 0.99, recall: 0.99, precision: 0.9901960784313726, f_beta: 0.98999899989999
train: step: 4736, loss: 0.04600505903363228, acc: 0.95, recall: 0.95, precision: 0.9501800720288115, f_beta: 0.9499949994999499
train: step: 4737, loss: 0.01867150142788887, acc: 0.99, recall: 0.99, precision: 0.9901960784313726, f_beta: 0.98999899989999
train: step: 4738, loss: 0.01903604157269001, acc: 0.98, recall: 0.98, precision: 0.9807692307692308, f_beta: 0.9799919967987194
train: step: 4739, loss: 0.007588963955640793, acc: 0.99, recall: 0.99, precision: 0.9901960784313726, f_beta: 0.98999899989999
train: step: 4740, loss: 0.01626969687640667, acc: 0.98, recall: 0.98, precision: 0.9807692307692308, f_beta: 0.9799919967987194
train: step: 4741, loss: 0.029087355360388756, acc: 0.96, recall: 0.96, precision: 0.9607371794871795, f_beta: 0.959983993597439
train: step: 4742, loss: 0.031571172177791595, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 4743, loss: 0.018136247992515564, acc: 0.98, recall: 0.98, precision: 0.9807692307692308, f_beta: 0.9799919967987194
train: step: 4744, loss: 0.03872433677315712, acc: 0.95, recall: 0.95, precision: 0.9516258530710557, f_beta: 0.949954959463517
train: step: 4745, loss: 0.03371094912290573, acc: 0.96, recall: 0.96, precision: 0.96, f_beta: 0.96
train: step: 4746, loss: 0.03401128202676773, acc: 0.96, recall: 0.96, precision: 0.962962962962963, f_beta: 0.9599358974358974
train: step: 4747, loss: 0.03372810035943985, acc: 0.96, recall: 0.96, precision: 0.9607371794871795, f_beta: 0.959983993597439
train: step: 4748, loss: 0.014994203113019466, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 4749, loss: 0.0143592469394207, acc: 0.98, recall: 0.98, precision: 0.98, f_beta: 0.98
train: step: 4750, loss: 0.03791258856654167, acc: 0.95, recall: 0.95, precision: 0.9501800720288115, f_beta: 0.9499949994999499
train: step: 4751, loss: 0.03356092423200607, acc: 0.95, recall: 0.95, precision: 0.9516258530710557, f_beta: 0.949954959463517
train: step: 4752, loss: 0.012683863751590252, acc: 0.98, recall: 0.98, precision: 0.98, f_beta: 0.98
train: step: 4753, loss: 0.02036248706281185, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 4754, loss: 0.02020260877907276, acc: 0.98, recall: 0.98, precision: 0.98, f_beta: 0.98
train: step: 4755, loss: 0.029904555529356003, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 4756, loss: 0.024577414616942406, acc: 0.98, recall: 0.98, precision: 0.98, f_beta: 0.98
train: step: 4757, loss: 0.029662616550922394, acc: 0.94, recall: 0.94, precision: 0.9428341384863124, f_beta: 0.9399038461538463
train: step: 4758, loss: 0.03421350568532944, acc: 0.95, recall: 0.95, precision: 0.9501800720288115, f_beta: 0.9499949994999499
train: step: 4759, loss: 0.029603343456983566, acc: 0.95, recall: 0.95, precision: 0.9516258530710557, f_beta: 0.949954959463517
train: step: 4760, loss: 0.049211349338293076, acc: 0.94, recall: 0.94, precision: 0.94, f_beta: 0.94
train: step: 4761, loss: 0.03070814162492752, acc: 0.97, recall: 0.97, precision: 0.9716981132075472, f_beta: 0.9699729756781104
train: step: 4762, loss: 0.008395383134484291, acc: 0.99, recall: 0.99, precision: 0.9901960784313726, f_beta: 0.98999899989999
train: step: 4763, loss: 0.022511567920446396, acc: 0.98, recall: 0.98, precision: 0.98, f_beta: 0.98
train: step: 4764, loss: 0.01795409806072712, acc: 0.98, recall: 0.98, precision: 0.98, f_beta: 0.98
train: step: 4765, loss: 0.0025999920908361673, acc: 1.0, recall: 1.0, precision: 1.0, f_beta: 1.0
train: step: 4766, loss: 0.015767253935337067, acc: 0.97, recall: 0.97, precision: 0.9716981132075472, f_beta: 0.9699729756781104
train: step: 4767, loss: 0.0185353085398674, acc: 0.98, recall: 0.98, precision: 0.98, f_beta: 0.98
train: step: 4768, loss: 0.012297004461288452, acc: 0.99, recall: 0.99, precision: 0.9901960784313726, f_beta: 0.98999899989999
train: step: 4769, loss: 0.02406863123178482, acc: 0.97, recall: 0.97, precision: 0.9716981132075472, f_beta: 0.9699729756781104
train: step: 4770, loss: 0.037098802626132965, acc: 0.96, recall: 0.96, precision: 0.9607371794871795, f_beta: 0.959983993597439
train: step: 4771, loss: 0.053876638412475586, acc: 0.93, recall: 0.9299999999999999, precision: 0.9343434343434343, f_beta: 0.9298245614035088
train: step: 4772, loss: 0.01218721829354763, acc: 0.98, recall: 0.98, precision: 0.98, f_beta: 0.98
train: step: 4773, loss: 0.010602503083646297, acc: 0.99, recall: 0.99, precision: 0.9901960784313726, f_beta: 0.98999899989999
train: step: 4774, loss: 0.037420131266117096, acc: 0.96, recall: 0.96, precision: 0.962962962962963, f_beta: 0.9599358974358974
train: step: 4775, loss: 0.03506992384791374, acc: 0.93, recall: 0.9299999999999999, precision: 0.9385964912280702, f_beta: 0.9296553110240177
train: step: 4776, loss: 0.004224342759698629, acc: 1.0, recall: 1.0, precision: 1.0, f_beta: 1.0
train: step: 4777, loss: 0.03667721897363663, acc: 0.96, recall: 0.96, precision: 0.9607371794871795, f_beta: 0.959983993597439
train: step: 4778, loss: 0.036061424762010574, acc: 0.96, recall: 0.96, precision: 0.9607371794871795, f_beta: 0.959983993597439
train: step: 4779, loss: 0.025192489847540855, acc: 0.96, recall: 0.96, precision: 0.96, f_beta: 0.96
train: step: 4780, loss: 0.025461258366703987, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 4781, loss: 0.06669317185878754, acc: 0.92, recall: 0.9199999999999999, precision: 0.9206730769230769, f_beta: 0.919967987194878
train: step: 4782, loss: 0.029905948787927628, acc: 0.97, recall: 0.97, precision: 0.9716981132075472, f_beta: 0.9699729756781104
train: step: 4783, loss: 0.013925492763519287, acc: 0.98, recall: 0.98, precision: 0.9807692307692308, f_beta: 0.9799919967987194
train: step: 4784, loss: 0.024966062977910042, acc: 0.98, recall: 0.98, precision: 0.9807692307692308, f_beta: 0.9799919967987194
train: step: 4785, loss: 0.02389262244105339, acc: 0.98, recall: 0.98, precision: 0.98, f_beta: 0.98
train: step: 4786, loss: 0.026211107149720192, acc: 0.96, recall: 0.96, precision: 0.96, f_beta: 0.96
train: step: 4787, loss: 0.027304816991090775, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 4788, loss: 0.015002421103417873, acc: 0.98, recall: 0.98, precision: 0.98, f_beta: 0.98
train: step: 4789, loss: 0.01583564467728138, acc: 0.98, recall: 0.98, precision: 0.98, f_beta: 0.98
train: step: 4790, loss: 0.02747349813580513, acc: 0.95, recall: 0.95, precision: 0.9501800720288115, f_beta: 0.9499949994999499
train: step: 4791, loss: 0.028966043144464493, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 4792, loss: 0.03404610976576805, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 4793, loss: 0.027900652959942818, acc: 0.96, recall: 0.96, precision: 0.9607371794871795, f_beta: 0.959983993597439
train: step: 4794, loss: 0.006526237819343805, acc: 0.99, recall: 0.99, precision: 0.9901960784313726, f_beta: 0.98999899989999
train: step: 4795, loss: 0.02173059992492199, acc: 0.98, recall: 0.98, precision: 0.9807692307692308, f_beta: 0.9799919967987194
train: step: 4796, loss: 0.0292041152715683, acc: 0.95, recall: 0.95, precision: 0.9516258530710557, f_beta: 0.949954959463517
train: step: 4797, loss: 0.025095820426940918, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 4798, loss: 0.0159881841391325, acc: 0.98, recall: 0.98, precision: 0.9807692307692308, f_beta: 0.9799919967987194
train: step: 4799, loss: 0.020091846585273743, acc: 0.96, recall: 0.96, precision: 0.9607371794871795, f_beta: 0.959983993597439
train: step: 4800, loss: 0.0008741481578908861, acc: 1.0, recall: 1.0, precision: 1.0, f_beta: 1.0
train: step: 4801, loss: 0.04424256831407547, acc: 0.96, recall: 0.96, precision: 0.9607371794871795, f_beta: 0.959983993597439
train: step: 4802, loss: 0.020020686089992523, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 4803, loss: 0.05047483742237091, acc: 0.93, recall: 0.9299999999999999, precision: 0.930172068827531, f_beta: 0.9299929992999301
train: step: 4804, loss: 0.03472388908267021, acc: 0.96, recall: 0.96, precision: 0.96, f_beta: 0.96
train: step: 4805, loss: 0.028312135487794876, acc: 0.97, recall: 0.97, precision: 0.9716981132075472, f_beta: 0.9699729756781104
train: step: 4806, loss: 0.04123982414603233, acc: 0.95, recall: 0.95, precision: 0.9501800720288115, f_beta: 0.9499949994999499
train: step: 4807, loss: 0.0363752655684948, acc: 0.95, recall: 0.95, precision: 0.9501800720288115, f_beta: 0.9499949994999499
train: step: 4808, loss: 0.05403309687972069, acc: 0.93, recall: 0.9299999999999999, precision: 0.9343434343434343, f_beta: 0.9298245614035088
train: step: 4809, loss: 0.02258959226310253, acc: 0.97, recall: 0.97, precision: 0.9716981132075472, f_beta: 0.9699729756781104
train: step: 4810, loss: 0.025225581601262093, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 4811, loss: 0.028059082105755806, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 4812, loss: 0.06034982204437256, acc: 0.93, recall: 0.9299999999999999, precision: 0.9315535929345644, f_beta: 0.9299369432489241
train: step: 4813, loss: 0.022412599995732307, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 4814, loss: 0.01615951955318451, acc: 0.98, recall: 0.98, precision: 0.9807692307692308, f_beta: 0.9799919967987194
train: step: 4815, loss: 0.008423405699431896, acc: 0.98, recall: 0.98, precision: 0.9807692307692308, f_beta: 0.9799919967987194
train: step: 4816, loss: 0.03965996205806732, acc: 0.96, recall: 0.96, precision: 0.96, f_beta: 0.96
train: step: 4817, loss: 0.040873412042856216, acc: 0.95, recall: 0.95, precision: 0.9501800720288115, f_beta: 0.9499949994999499
train: step: 4818, loss: 0.019595274701714516, acc: 0.98, recall: 0.98, precision: 0.9807692307692308, f_beta: 0.9799919967987194
train: step: 4819, loss: 0.038963433355093, acc: 0.93, recall: 0.9299999999999999, precision: 0.9315535929345644, f_beta: 0.9299369432489241
train: step: 4820, loss: 0.01750025525689125, acc: 0.98, recall: 0.98, precision: 0.9807692307692308, f_beta: 0.9799919967987194
train: step: 4821, loss: 0.013195345178246498, acc: 0.98, recall: 0.98, precision: 0.98, f_beta: 0.98
train: step: 4822, loss: 0.007914946414530277, acc: 1.0, recall: 1.0, precision: 1.0, f_beta: 1.0
train: step: 4823, loss: 0.01694680191576481, acc: 0.98, recall: 0.98, precision: 0.9807692307692308, f_beta: 0.9799919967987194
train: step: 4824, loss: 0.028513705357909203, acc: 0.94, recall: 0.94, precision: 0.9407051282051282, f_beta: 0.9399759903961584
train: step: 4825, loss: 0.03334750235080719, acc: 0.96, recall: 0.96, precision: 0.9607371794871795, f_beta: 0.959983993597439
train: step: 4826, loss: 0.03668123111128807, acc: 0.95, recall: 0.95, precision: 0.9501800720288115, f_beta: 0.9499949994999499
train: step: 4827, loss: 0.028810175135731697, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 4828, loss: 0.017412949353456497, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 4829, loss: 0.03587772324681282, acc: 0.96, recall: 0.96, precision: 0.9607371794871795, f_beta: 0.959983993597439
train: step: 4830, loss: 0.029782624915242195, acc: 0.95, recall: 0.95, precision: 0.9516258530710557, f_beta: 0.949954959463517
train: step: 4831, loss: 0.05134628340601921, acc: 0.95, recall: 0.95, precision: 0.9545454545454546, f_beta: 0.949874686716792
train: step: 4832, loss: 0.054613810032606125, acc: 0.93, recall: 0.9299999999999999, precision: 0.9343434343434343, f_beta: 0.9298245614035088
train: step: 4833, loss: 0.05073949694633484, acc: 0.95, recall: 0.95, precision: 0.9501800720288115, f_beta: 0.9499949994999499
train: step: 4834, loss: 0.001948408200405538, acc: 1.0, recall: 1.0, precision: 1.0, f_beta: 1.0
train: step: 4835, loss: 0.04490791633725166, acc: 0.94, recall: 0.94, precision: 0.94, f_beta: 0.94
train: step: 4836, loss: 0.01669054478406906, acc: 0.98, recall: 0.98, precision: 0.98, f_beta: 0.98
train: step: 4837, loss: 0.02626778557896614, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 4838, loss: 0.03722595050930977, acc: 0.95, recall: 0.95, precision: 0.9501800720288115, f_beta: 0.9499949994999499
train: step: 4839, loss: 0.01743270829319954, acc: 0.98, recall: 0.98, precision: 0.98, f_beta: 0.98
train: step: 4840, loss: 0.0549398809671402, acc: 0.94, recall: 0.94, precision: 0.9407051282051282, f_beta: 0.9399759903961584
train: step: 4841, loss: 0.023156583309173584, acc: 0.97, recall: 0.97, precision: 0.9716981132075472, f_beta: 0.9699729756781104
train: step: 4842, loss: 0.021772004663944244, acc: 0.96, recall: 0.96, precision: 0.9607371794871795, f_beta: 0.959983993597439
train: step: 4843, loss: 0.029003359377384186, acc: 0.96, recall: 0.96, precision: 0.9607371794871795, f_beta: 0.959983993597439
train: step: 4844, loss: 0.03447909280657768, acc: 0.96, recall: 0.96, precision: 0.96, f_beta: 0.96
train: step: 4845, loss: 0.03661738708615303, acc: 0.96, recall: 0.96, precision: 0.96, f_beta: 0.96
train: step: 4846, loss: 0.03192850202322006, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 4847, loss: 0.004375501070171595, acc: 1.0, recall: 1.0, precision: 1.0, f_beta: 1.0
train: step: 4848, loss: 0.029051117599010468, acc: 0.96, recall: 0.96, precision: 0.96, f_beta: 0.96
train: step: 4849, loss: 0.040740109980106354, acc: 0.96, recall: 0.96, precision: 0.9607371794871795, f_beta: 0.959983993597439
train: step: 4850, loss: 0.02353665977716446, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 4851, loss: 0.05239592492580414, acc: 0.93, recall: 0.9299999999999999, precision: 0.9343434343434343, f_beta: 0.9298245614035088
train: step: 4852, loss: 0.0183759443461895, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 4853, loss: 0.04134121537208557, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 4854, loss: 0.03721810504794121, acc: 0.95, recall: 0.95, precision: 0.9501800720288115, f_beta: 0.9499949994999499
train: step: 4855, loss: 0.015741437673568726, acc: 0.98, recall: 0.98, precision: 0.9807692307692308, f_beta: 0.9799919967987194
train: step: 4856, loss: 0.0034281450789421797, acc: 1.0, recall: 1.0, precision: 1.0, f_beta: 1.0
train: step: 4857, loss: 0.06160604581236839, acc: 0.9, recall: 0.8999999999999999, precision: 0.9025764895330113, f_beta: 0.8998397435897436
train: step: 4858, loss: 0.03676018863916397, acc: 0.94, recall: 0.94, precision: 0.9428341384863124, f_beta: 0.9399038461538463
train: step: 4859, loss: 0.024173762649297714, acc: 0.98, recall: 0.98, precision: 0.9807692307692308, f_beta: 0.9799919967987194
train: step: 4860, loss: 0.0542912594974041, acc: 0.92, recall: 0.9199999999999999, precision: 0.9206730769230769, f_beta: 0.919967987194878
train: step: 4861, loss: 0.009387009777128696, acc: 0.99, recall: 0.99, precision: 0.9901960784313726, f_beta: 0.98999899989999
train: step: 4862, loss: 0.003071010112762451, acc: 1.0, recall: 1.0, precision: 1.0, f_beta: 1.0
train: step: 4863, loss: 0.04566524177789688, acc: 0.94, recall: 0.94, precision: 0.9407051282051282, f_beta: 0.9399759903961584
train: step: 4864, loss: 0.002082984894514084, acc: 1.0, recall: 1.0, precision: 1.0, f_beta: 1.0
train: step: 4865, loss: 0.009576751850545406, acc: 0.99, recall: 0.99, precision: 0.9901960784313726, f_beta: 0.98999899989999
train: step: 4866, loss: 0.013966011814773083, acc: 0.98, recall: 0.98, precision: 0.98, f_beta: 0.98
train: step: 4867, loss: 0.013985340483486652, acc: 0.98, recall: 0.98, precision: 0.9807692307692308, f_beta: 0.9799919967987194
train: step: 4868, loss: 0.020839614793658257, acc: 0.98, recall: 0.98, precision: 0.98, f_beta: 0.98
train: step: 4869, loss: 0.01970689930021763, acc: 0.98, recall: 0.98, precision: 0.98, f_beta: 0.98
train: step: 4870, loss: 0.01596899703145027, acc: 0.98, recall: 0.98, precision: 0.9807692307692308, f_beta: 0.9799919967987194
train: step: 4871, loss: 0.017196262255311012, acc: 0.98, recall: 0.98, precision: 0.98, f_beta: 0.98
train: step: 4872, loss: 0.004511496052145958, acc: 0.99, recall: 0.99, precision: 0.9901960784313726, f_beta: 0.98999899989999
train: step: 4873, loss: 0.028377318754792213, acc: 0.95, recall: 0.95, precision: 0.9501800720288115, f_beta: 0.9499949994999499
train: step: 4874, loss: 0.01773659884929657, acc: 0.98, recall: 0.98, precision: 0.9807692307692308, f_beta: 0.9799919967987194
train: step: 4875, loss: 0.03979743644595146, acc: 0.94, recall: 0.94, precision: 0.9407051282051282, f_beta: 0.9399759903961584
train: step: 4876, loss: 0.03631286695599556, acc: 0.96, recall: 0.96, precision: 0.9607371794871795, f_beta: 0.959983993597439
train: step: 4877, loss: 0.014089684933423996, acc: 0.99, recall: 0.99, precision: 0.9901960784313726, f_beta: 0.98999899989999
train: step: 4878, loss: 0.012840529903769493, acc: 0.98, recall: 0.98, precision: 0.9807692307692308, f_beta: 0.9799919967987194
train: step: 4879, loss: 0.025422506034374237, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 4880, loss: 0.033990636467933655, acc: 0.95, recall: 0.95, precision: 0.9516258530710557, f_beta: 0.949954959463517
train: step: 4881, loss: 0.02445770800113678, acc: 0.96, recall: 0.96, precision: 0.96, f_beta: 0.96
train: step: 4882, loss: 0.061779145151376724, acc: 0.92, recall: 0.9199999999999999, precision: 0.9206730769230769, f_beta: 0.919967987194878
train: step: 4883, loss: 0.011145937256515026, acc: 0.99, recall: 0.99, precision: 0.9901960784313726, f_beta: 0.98999899989999
train: step: 4884, loss: 0.014772312715649605, acc: 0.98, recall: 0.98, precision: 0.98, f_beta: 0.98
train: step: 4885, loss: 0.0017273601843044162, acc: 1.0, recall: 1.0, precision: 1.0, f_beta: 1.0
train: step: 4886, loss: 0.01927334815263748, acc: 0.98, recall: 0.98, precision: 0.98, f_beta: 0.98
train: step: 4887, loss: 0.04214819520711899, acc: 0.94, recall: 0.94, precision: 0.9428341384863124, f_beta: 0.9399038461538463
train: step: 4888, loss: 0.02970845252275467, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 4889, loss: 0.029883846640586853, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 4890, loss: 0.013687917962670326, acc: 0.98, recall: 0.98, precision: 0.9807692307692308, f_beta: 0.9799919967987194
train: step: 4891, loss: 0.04781285300850868, acc: 0.93, recall: 0.9299999999999999, precision: 0.9315535929345644, f_beta: 0.9299369432489241
train: step: 4892, loss: 0.03468169644474983, acc: 0.96, recall: 0.96, precision: 0.9607371794871795, f_beta: 0.959983993597439
train: step: 4893, loss: 0.014122238382697105, acc: 0.98, recall: 0.98, precision: 0.9807692307692308, f_beta: 0.9799919967987194
train: step: 4894, loss: 0.0408954918384552, acc: 0.95, recall: 0.95, precision: 0.9501800720288115, f_beta: 0.9499949994999499
train: step: 4895, loss: 0.03452330455183983, acc: 0.96, recall: 0.96, precision: 0.9607371794871795, f_beta: 0.959983993597439
train: step: 4896, loss: 0.023105615749955177, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 4897, loss: 0.013992245309054852, acc: 0.98, recall: 0.98, precision: 0.9807692307692308, f_beta: 0.9799919967987194
train: step: 4898, loss: 0.05182325839996338, acc: 0.94, recall: 0.94, precision: 0.9428341384863124, f_beta: 0.9399038461538463
train: step: 4899, loss: 0.019569167867302895, acc: 0.98, recall: 0.98, precision: 0.9807692307692308, f_beta: 0.9799919967987194
train: step: 4900, loss: 0.023304929956793785, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 4901, loss: 0.040894798934459686, acc: 0.96, recall: 0.96, precision: 0.96, f_beta: 0.96
train: step: 4902, loss: 0.01383378729224205, acc: 0.99, recall: 0.99, precision: 0.9901960784313726, f_beta: 0.98999899989999
train: step: 4903, loss: 0.030123362317681313, acc: 0.97, recall: 0.97, precision: 0.9716981132075472, f_beta: 0.9699729756781104
train: step: 4904, loss: 0.018472980707883835, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 4905, loss: 0.019731294363737106, acc: 0.98, recall: 0.98, precision: 0.98, f_beta: 0.98
train: step: 4906, loss: 0.043654754757881165, acc: 0.96, recall: 0.96, precision: 0.9607371794871795, f_beta: 0.959983993597439
train: step: 4907, loss: 0.009491159580647945, acc: 0.99, recall: 0.99, precision: 0.9901960784313726, f_beta: 0.98999899989999
train: step: 4908, loss: 0.017087306827306747, acc: 0.99, recall: 0.99, precision: 0.9901960784313726, f_beta: 0.98999899989999
train: step: 4909, loss: 0.0146273672580719, acc: 0.99, recall: 0.99, precision: 0.9901960784313726, f_beta: 0.98999899989999
train: step: 4910, loss: 0.027645034715533257, acc: 0.96, recall: 0.96, precision: 0.9607371794871795, f_beta: 0.959983993597439
train: step: 4911, loss: 0.0397057831287384, acc: 0.95, recall: 0.95, precision: 0.9516258530710557, f_beta: 0.949954959463517
train: step: 4912, loss: 0.003468005917966366, acc: 1.0, recall: 1.0, precision: 1.0, f_beta: 1.0
train: step: 4913, loss: 0.02241150289773941, acc: 0.98, recall: 0.98, precision: 0.98, f_beta: 0.98
train: step: 4914, loss: 0.01938716694712639, acc: 0.98, recall: 0.98, precision: 0.9807692307692308, f_beta: 0.9799919967987194
train: step: 4915, loss: 0.03217418119311333, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 4916, loss: 0.010016598738729954, acc: 0.99, recall: 0.99, precision: 0.9901960784313726, f_beta: 0.98999899989999
train: step: 4917, loss: 0.025631660595536232, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 4918, loss: 0.04160257428884506, acc: 0.96, recall: 0.96, precision: 0.96, f_beta: 0.96
train: step: 4919, loss: 0.06843923777341843, acc: 0.91, recall: 0.9099999999999999, precision: 0.9114813327980731, f_beta: 0.9099189270343309
train: step: 4920, loss: 0.0008053256315179169, acc: 1.0, recall: 1.0, precision: 1.0, f_beta: 1.0
train: step: 4921, loss: 0.027479052543640137, acc: 0.97, recall: 0.97, precision: 0.9716981132075472, f_beta: 0.9699729756781104
train: step: 4922, loss: 0.023095818236470222, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 4923, loss: 0.018863340839743614, acc: 0.98, recall: 0.98, precision: 0.9807692307692308, f_beta: 0.9799919967987194
train: step: 4924, loss: 0.02860049158334732, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 4925, loss: 0.026566240936517715, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 4926, loss: 0.012052671983838081, acc: 0.98, recall: 0.98, precision: 0.9807692307692308, f_beta: 0.9799919967987194
train: step: 4927, loss: 0.049144867807626724, acc: 0.95, recall: 0.95, precision: 0.9516258530710557, f_beta: 0.949954959463517
train: step: 4928, loss: 0.02922917902469635, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 4929, loss: 0.034735627472400665, acc: 0.95, recall: 0.95, precision: 0.9501800720288115, f_beta: 0.9499949994999499
train: step: 4930, loss: 0.013764357194304466, acc: 0.99, recall: 0.99, precision: 0.9901960784313726, f_beta: 0.98999899989999
train: step: 4931, loss: 0.03524357080459595, acc: 0.96, recall: 0.96, precision: 0.96, f_beta: 0.96
train: step: 4932, loss: 0.03198159113526344, acc: 0.95, recall: 0.95, precision: 0.9501800720288115, f_beta: 0.9499949994999499
train: step: 4933, loss: 0.0025252802297472954, acc: 1.0, recall: 1.0, precision: 1.0, f_beta: 1.0
train: step: 4934, loss: 0.01988622173666954, acc: 0.97, recall: 0.97, precision: 0.9716981132075472, f_beta: 0.9699729756781104
train: step: 4935, loss: 0.03620051220059395, acc: 0.94, recall: 0.94, precision: 0.94, f_beta: 0.94
train: step: 4936, loss: 0.006102957762777805, acc: 0.99, recall: 0.99, precision: 0.9901960784313726, f_beta: 0.98999899989999
train: step: 4937, loss: 0.02295038290321827, acc: 0.97, recall: 0.97, precision: 0.9716981132075472, f_beta: 0.9699729756781104
train: step: 4938, loss: 0.032998692244291306, acc: 0.96, recall: 0.96, precision: 0.962962962962963, f_beta: 0.9599358974358974
train: step: 4939, loss: 0.01851109229028225, acc: 0.98, recall: 0.98, precision: 0.9807692307692308, f_beta: 0.9799919967987194
train: step: 4940, loss: 0.005582164507359266, acc: 1.0, recall: 1.0, precision: 1.0, f_beta: 1.0
train: step: 4941, loss: 0.026259595528244972, acc: 0.96, recall: 0.96, precision: 0.962962962962963, f_beta: 0.9599358974358974
train: step: 4942, loss: 0.0040295119397342205, acc: 1.0, recall: 1.0, precision: 1.0, f_beta: 1.0
train: step: 4943, loss: 0.01689983904361725, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 4944, loss: 0.020987244322896004, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 4945, loss: 0.02344393916428089, acc: 0.97, recall: 0.97, precision: 0.9716981132075472, f_beta: 0.9699729756781104
train: step: 4946, loss: 0.017825689166784286, acc: 0.98, recall: 0.98, precision: 0.98, f_beta: 0.98
train: step: 4947, loss: 0.024850869551301003, acc: 0.97, recall: 0.97, precision: 0.9716981132075472, f_beta: 0.9699729756781104
train: step: 4948, loss: 0.029178613796830177, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 4949, loss: 0.017903825268149376, acc: 0.97, recall: 0.97, precision: 0.9716981132075472, f_beta: 0.9699729756781104
train: step: 4950, loss: 0.01173139363527298, acc: 0.99, recall: 0.99, precision: 0.9901960784313726, f_beta: 0.98999899989999
train: step: 4951, loss: 0.021386971697211266, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 4952, loss: 0.04928695783019066, acc: 0.94, recall: 0.94, precision: 0.9407051282051282, f_beta: 0.9399759903961584
train: step: 4953, loss: 0.0841071680188179, acc: 0.91, recall: 0.9099999999999999, precision: 0.9141414141414141, f_beta: 0.9097744360902256
train: step: 4954, loss: 0.023474164307117462, acc: 0.96, recall: 0.96, precision: 0.9607371794871795, f_beta: 0.959983993597439
train: step: 4955, loss: 0.02379152551293373, acc: 0.98, recall: 0.98, precision: 0.9807692307692308, f_beta: 0.9799919967987194
train: step: 4956, loss: 0.014431159943342209, acc: 0.98, recall: 0.98, precision: 0.9807692307692308, f_beta: 0.9799919967987194
train: step: 4957, loss: 0.013373445719480515, acc: 0.99, recall: 0.99, precision: 0.9901960784313726, f_beta: 0.98999899989999
train: step: 4958, loss: 0.019641490653157234, acc: 0.98, recall: 0.98, precision: 0.98, f_beta: 0.98
train: step: 4959, loss: 0.029923444613814354, acc: 0.96, recall: 0.96, precision: 0.9607371794871795, f_beta: 0.959983993597439
train: step: 4960, loss: 0.00099860446061939, acc: 1.0, recall: 1.0, precision: 1.0, f_beta: 1.0
train: step: 4961, loss: 0.030490197241306305, acc: 0.96, recall: 0.96, precision: 0.962962962962963, f_beta: 0.9599358974358974
train: step: 4962, loss: 0.008861958980560303, acc: 0.98, recall: 0.98, precision: 0.98, f_beta: 0.98
train: step: 4963, loss: 0.0013407454825937748, acc: 1.0, recall: 1.0, precision: 1.0, f_beta: 1.0
train: step: 4964, loss: 0.02589728869497776, acc: 0.98, recall: 0.98, precision: 0.9807692307692308, f_beta: 0.9799919967987194
train: step: 4965, loss: 0.04607309773564339, acc: 0.95, recall: 0.95, precision: 0.9516258530710557, f_beta: 0.949954959463517
train: step: 4966, loss: 0.056661464273929596, acc: 0.93, recall: 0.9299999999999999, precision: 0.930172068827531, f_beta: 0.9299929992999301
train: step: 4967, loss: 0.028528301045298576, acc: 0.96, recall: 0.96, precision: 0.9607371794871795, f_beta: 0.959983993597439
train: step: 4968, loss: 0.033791638910770416, acc: 0.96, recall: 0.96, precision: 0.9607371794871795, f_beta: 0.959983993597439
train: step: 4969, loss: 0.02094215713441372, acc: 0.98, recall: 0.98, precision: 0.9807692307692308, f_beta: 0.9799919967987194
train: step: 4970, loss: 0.051724378019571304, acc: 0.95, recall: 0.95, precision: 0.9516258530710557, f_beta: 0.949954959463517
train: step: 4971, loss: 0.028773881494998932, acc: 0.96, recall: 0.96, precision: 0.9607371794871795, f_beta: 0.959983993597439
train: step: 4972, loss: 0.030570339411497116, acc: 0.96, recall: 0.96, precision: 0.96, f_beta: 0.96
train: step: 4973, loss: 0.034632857888936996, acc: 0.96, recall: 0.96, precision: 0.962962962962963, f_beta: 0.9599358974358974
train: step: 4974, loss: 0.012345364317297935, acc: 0.99, recall: 0.99, precision: 0.9901960784313726, f_beta: 0.98999899989999
train: step: 4975, loss: 0.03230945020914078, acc: 0.97, recall: 0.97, precision: 0.9716981132075472, f_beta: 0.9699729756781104
train: step: 4976, loss: 0.027498465031385422, acc: 0.96, recall: 0.96, precision: 0.96, f_beta: 0.96
train: step: 4977, loss: 0.03148668631911278, acc: 0.96, recall: 0.96, precision: 0.962962962962963, f_beta: 0.9599358974358974
train: step: 4978, loss: 0.032137151807546616, acc: 0.95, recall: 0.95, precision: 0.9501800720288115, f_beta: 0.9499949994999499
train: step: 4979, loss: 0.0444297157227993, acc: 0.95, recall: 0.95, precision: 0.9501800720288115, f_beta: 0.9499949994999499
train: step: 4980, loss: 0.012499618344008923, acc: 0.99, recall: 0.99, precision: 0.9901960784313726, f_beta: 0.98999899989999
train: step: 4981, loss: 0.015441378578543663, acc: 0.98, recall: 0.98, precision: 0.9807692307692308, f_beta: 0.9799919967987194
train: step: 4982, loss: 0.0026017690543085337, acc: 1.0, recall: 1.0, precision: 1.0, f_beta: 1.0
train: step: 4983, loss: 0.01749824732542038, acc: 0.98, recall: 0.98, precision: 0.98, f_beta: 0.98
train: step: 4984, loss: 0.008848879486322403, acc: 0.98, recall: 0.98, precision: 0.98, f_beta: 0.98
train: step: 4985, loss: 0.037434324622154236, acc: 0.96, recall: 0.96, precision: 0.96, f_beta: 0.96
train: step: 4986, loss: 0.05058656632900238, acc: 0.94, recall: 0.94, precision: 0.94, f_beta: 0.94
train: step: 4987, loss: 0.0330803245306015, acc: 0.95, recall: 0.95, precision: 0.9545454545454546, f_beta: 0.949874686716792
train: step: 4988, loss: 0.027034135535359383, acc: 0.96, recall: 0.96, precision: 0.9607371794871795, f_beta: 0.959983993597439
train: step: 4989, loss: 0.02388337068259716, acc: 0.96, recall: 0.96, precision: 0.96, f_beta: 0.96
train: step: 4990, loss: 0.03509172797203064, acc: 0.96, recall: 0.96, precision: 0.9607371794871795, f_beta: 0.959983993597439
train: step: 4991, loss: 0.013772953301668167, acc: 0.98, recall: 0.98, precision: 0.98, f_beta: 0.98
train: step: 4992, loss: 0.016416646540164948, acc: 0.98, recall: 0.98, precision: 0.9807692307692308, f_beta: 0.9799919967987194
train: step: 4993, loss: 0.028702368959784508, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 4994, loss: 0.039459194988012314, acc: 0.96, recall: 0.96, precision: 0.9607371794871795, f_beta: 0.959983993597439
train: step: 4995, loss: 0.013409064151346684, acc: 0.98, recall: 0.98, precision: 0.9807692307692308, f_beta: 0.9799919967987194
train: step: 4996, loss: 0.022089803591370583, acc: 0.98, recall: 0.98, precision: 0.98, f_beta: 0.98
train: step: 4997, loss: 0.02600180171430111, acc: 0.96, recall: 0.96, precision: 0.9607371794871795, f_beta: 0.959983993597439
train: step: 4998, loss: 0.05370473489165306, acc: 0.92, recall: 0.9199999999999999, precision: 0.9261363636363636, f_beta: 0.9197109594540345
train: step: 4999, loss: 0.01714923232793808, acc: 0.98, recall: 0.98, precision: 0.98, f_beta: 0.98
train: step: 5000, loss: 0.01465571392327547, acc: 0.98, recall: 0.98, precision: 0.98, f_beta: 0.98
train: step: 5001, loss: 0.03515143319964409, acc: 0.95, recall: 0.95, precision: 0.9516258530710557, f_beta: 0.949954959463517
train: step: 5002, loss: 0.047627776861190796, acc: 0.95, recall: 0.95, precision: 0.9501800720288115, f_beta: 0.9499949994999499
train: step: 5003, loss: 0.030074866488575935, acc: 0.96, recall: 0.96, precision: 0.96, f_beta: 0.96
train: step: 5004, loss: 0.031236281618475914, acc: 0.96, recall: 0.96, precision: 0.962962962962963, f_beta: 0.9599358974358974
train: step: 5005, loss: 0.02778582274913788, acc: 0.96, recall: 0.96, precision: 0.9607371794871795, f_beta: 0.959983993597439
train: step: 5006, loss: 0.006311217322945595, acc: 1.0, recall: 1.0, precision: 1.0, f_beta: 1.0
train: step: 5007, loss: 0.02094395086169243, acc: 0.98, recall: 0.98, precision: 0.98, f_beta: 0.98
train: step: 5008, loss: 0.027510592713952065, acc: 0.96, recall: 0.96, precision: 0.962962962962963, f_beta: 0.9599358974358974
train: step: 5009, loss: 0.03398218750953674, acc: 0.96, recall: 0.96, precision: 0.962962962962963, f_beta: 0.9599358974358974
train: step: 5010, loss: 0.023460572585463524, acc: 0.98, recall: 0.98, precision: 0.98, f_beta: 0.98
train: step: 5011, loss: 0.02837914042174816, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 5012, loss: 0.022582175210118294, acc: 0.98, recall: 0.98, precision: 0.9807692307692308, f_beta: 0.9799919967987194
train: step: 5013, loss: 0.024896670132875443, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 5014, loss: 0.0031244687270373106, acc: 1.0, recall: 1.0, precision: 1.0, f_beta: 1.0
train: step: 5015, loss: 0.003147888695821166, acc: 1.0, recall: 1.0, precision: 1.0, f_beta: 1.0
train: step: 5016, loss: 0.0487142913043499, acc: 0.93, recall: 0.9299999999999999, precision: 0.9385964912280702, f_beta: 0.9296553110240177
train: step: 5017, loss: 0.017221717163920403, acc: 0.99, recall: 0.99, precision: 0.9901960784313726, f_beta: 0.98999899989999
train: step: 5018, loss: 0.0017112678615376353, acc: 1.0, recall: 1.0, precision: 1.0, f_beta: 1.0
train: step: 5019, loss: 0.014085414819419384, acc: 0.98, recall: 0.98, precision: 0.98, f_beta: 0.98
train: step: 5020, loss: 0.019635699689388275, acc: 0.98, recall: 0.98, precision: 0.98, f_beta: 0.98
train: step: 5021, loss: 0.08734793961048126, acc: 0.89, recall: 0.8899999999999999, precision: 0.8939393939393939, f_beta: 0.8897243107769424
train: step: 5022, loss: 0.07240354269742966, acc: 0.91, recall: 0.9099999999999999, precision: 0.9141414141414141, f_beta: 0.9097744360902256
train: step: 5023, loss: 0.02243795618414879, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 5024, loss: 0.01193440891802311, acc: 0.99, recall: 0.99, precision: 0.9901960784313726, f_beta: 0.98999899989999
train: step: 5025, loss: 0.043809786438941956, acc: 0.96, recall: 0.96, precision: 0.9607371794871795, f_beta: 0.959983993597439
train: step: 5026, loss: 0.017672426998615265, acc: 0.98, recall: 0.98, precision: 0.98, f_beta: 0.98
train: step: 5027, loss: 0.03216208145022392, acc: 0.96, recall: 0.96, precision: 0.96, f_beta: 0.96
train: step: 5028, loss: 0.05377727374434471, acc: 0.95, recall: 0.95, precision: 0.9501800720288115, f_beta: 0.9499949994999499
train: step: 5029, loss: 0.011896621435880661, acc: 0.99, recall: 0.99, precision: 0.9901960784313726, f_beta: 0.98999899989999
train: step: 5030, loss: 0.016202254220843315, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 5031, loss: 0.006735760718584061, acc: 0.99, recall: 0.99, precision: 0.9901960784313726, f_beta: 0.98999899989999
train: step: 5032, loss: 0.021329890936613083, acc: 0.98, recall: 0.98, precision: 0.98, f_beta: 0.98
train: step: 5033, loss: 0.03162769228219986, acc: 0.95, recall: 0.95, precision: 0.9516258530710557, f_beta: 0.949954959463517
train: step: 5034, loss: 0.051857177168130875, acc: 0.94, recall: 0.94, precision: 0.9428341384863124, f_beta: 0.9399038461538463
train: step: 5035, loss: 0.027387037873268127, acc: 0.96, recall: 0.96, precision: 0.9607371794871795, f_beta: 0.959983993597439
train: step: 5036, loss: 0.03555000200867653, acc: 0.96, recall: 0.96, precision: 0.96, f_beta: 0.96
train: step: 5037, loss: 0.010240072384476662, acc: 0.99, recall: 0.99, precision: 0.9901960784313726, f_beta: 0.98999899989999
train: step: 5038, loss: 0.009517738595604897, acc: 0.99, recall: 0.99, precision: 0.9901960784313726, f_beta: 0.98999899989999
train: step: 5039, loss: 0.03071114607155323, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 5040, loss: 0.010925824753940105, acc: 0.99, recall: 0.99, precision: 0.9901960784313726, f_beta: 0.98999899989999
train: step: 5041, loss: 0.03970833122730255, acc: 0.96, recall: 0.96, precision: 0.9607371794871795, f_beta: 0.959983993597439
train: step: 5042, loss: 0.05303202569484711, acc: 0.94, recall: 0.94, precision: 0.94, f_beta: 0.94
train: step: 5043, loss: 0.006832452956587076, acc: 0.99, recall: 0.99, precision: 0.9901960784313726, f_beta: 0.98999899989999
train: step: 5044, loss: 0.033011771738529205, acc: 0.96, recall: 0.96, precision: 0.9607371794871795, f_beta: 0.959983993597439
train: step: 5045, loss: 0.015259013511240482, acc: 0.98, recall: 0.98, precision: 0.9807692307692308, f_beta: 0.9799919967987194
train: step: 5046, loss: 0.03479353338479996, acc: 0.95, recall: 0.95, precision: 0.9501800720288115, f_beta: 0.9499949994999499
train: step: 5047, loss: 0.02385987713932991, acc: 0.97, recall: 0.97, precision: 0.9716981132075472, f_beta: 0.9699729756781104
train: step: 5048, loss: 0.009182090871036053, acc: 0.98, recall: 0.98, precision: 0.9807692307692308, f_beta: 0.9799919967987194
train: step: 5049, loss: 0.04399876669049263, acc: 0.94, recall: 0.94, precision: 0.94, f_beta: 0.94
train: step: 5050, loss: 0.007913222536444664, acc: 1.0, recall: 1.0, precision: 1.0, f_beta: 1.0
train: step: 5051, loss: 0.031636934727430344, acc: 0.97, recall: 0.97, precision: 0.9716981132075472, f_beta: 0.9699729756781104
train: step: 5052, loss: 0.021687058731913567, acc: 0.98, recall: 0.98, precision: 0.98, f_beta: 0.98
train: step: 5053, loss: 0.034640319645404816, acc: 0.96, recall: 0.96, precision: 0.96, f_beta: 0.96
train: step: 5054, loss: 0.06280352175235748, acc: 0.92, recall: 0.92, precision: 0.92, f_beta: 0.92
train: step: 5055, loss: 0.01975361257791519, acc: 0.98, recall: 0.98, precision: 0.98, f_beta: 0.98
train: step: 5056, loss: 0.0423976331949234, acc: 0.96, recall: 0.96, precision: 0.96, f_beta: 0.96
train: step: 5057, loss: 0.0681850016117096, acc: 0.92, recall: 0.9199999999999999, precision: 0.9206730769230769, f_beta: 0.919967987194878
train: step: 5058, loss: 0.021636495366692543, acc: 0.98, recall: 0.98, precision: 0.9807692307692308, f_beta: 0.9799919967987194
train: step: 5059, loss: 0.02411142736673355, acc: 0.98, recall: 0.98, precision: 0.98, f_beta: 0.98
train: step: 5060, loss: 0.054152149707078934, acc: 0.94, recall: 0.94, precision: 0.94, f_beta: 0.94
train: step: 5061, loss: 0.02311435155570507, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 5062, loss: 0.049069758504629135, acc: 0.94, recall: 0.94, precision: 0.9407051282051282, f_beta: 0.9399759903961584
train: step: 5063, loss: 0.039136745035648346, acc: 0.96, recall: 0.96, precision: 0.962962962962963, f_beta: 0.9599358974358974
train: step: 5064, loss: 0.022260569036006927, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 5065, loss: 0.01365299429744482, acc: 0.99, recall: 0.99, precision: 0.9901960784313726, f_beta: 0.98999899989999
train: step: 5066, loss: 0.006469397339969873, acc: 0.99, recall: 0.99, precision: 0.9901960784313726, f_beta: 0.98999899989999
train: step: 5067, loss: 0.04026316478848457, acc: 0.95, recall: 0.95, precision: 0.9501800720288115, f_beta: 0.9499949994999499
train: step: 5068, loss: 0.023266581818461418, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 5069, loss: 0.015675587579607964, acc: 0.98, recall: 0.98, precision: 0.98, f_beta: 0.98
train: step: 5070, loss: 0.02710648626089096, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 5071, loss: 0.028044097125530243, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 5072, loss: 0.01525090355426073, acc: 0.98, recall: 0.98, precision: 0.9807692307692308, f_beta: 0.9799919967987194
train: step: 5073, loss: 0.02781878411769867, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 5074, loss: 0.011103007942438126, acc: 0.99, recall: 0.99, precision: 0.9901960784313726, f_beta: 0.98999899989999
train: step: 5075, loss: 0.05421227589249611, acc: 0.93, recall: 0.9299999999999999, precision: 0.9343434343434343, f_beta: 0.9298245614035088
train: step: 5076, loss: 0.053543273359537125, acc: 0.94, recall: 0.94, precision: 0.9407051282051282, f_beta: 0.9399759903961584
train: step: 5077, loss: 0.010349427349865437, acc: 0.99, recall: 0.99, precision: 0.9901960784313726, f_beta: 0.98999899989999
train: step: 5078, loss: 0.03154176473617554, acc: 0.95, recall: 0.95, precision: 0.9501800720288115, f_beta: 0.9499949994999499
train: step: 5079, loss: 0.06296578049659729, acc: 0.92, recall: 0.9199999999999999, precision: 0.9206730769230769, f_beta: 0.919967987194878
train: step: 5080, loss: 0.03217237815260887, acc: 0.95, recall: 0.95, precision: 0.9501800720288115, f_beta: 0.9499949994999499
train: step: 5081, loss: 0.04069778323173523, acc: 0.95, recall: 0.95, precision: 0.9501800720288115, f_beta: 0.9499949994999499
train: step: 5082, loss: 0.04489404708147049, acc: 0.95, recall: 0.95, precision: 0.9501800720288115, f_beta: 0.9499949994999499
train: step: 5083, loss: 0.047711290419101715, acc: 0.92, recall: 0.9199999999999999, precision: 0.9206730769230769, f_beta: 0.919967987194878
train: step: 5084, loss: 0.007858958095312119, acc: 0.99, recall: 0.99, precision: 0.9901960784313726, f_beta: 0.98999899989999
train: step: 5085, loss: 0.045235615223646164, acc: 0.94, recall: 0.94, precision: 0.9407051282051282, f_beta: 0.9399759903961584
train: step: 5086, loss: 0.030705127865076065, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 5087, loss: 0.03706122934818268, acc: 0.96, recall: 0.96, precision: 0.9607371794871795, f_beta: 0.959983993597439
train: step: 5088, loss: 0.012867474928498268, acc: 0.98, recall: 0.98, precision: 0.98, f_beta: 0.98
train: step: 5089, loss: 0.016646195203065872, acc: 0.98, recall: 0.98, precision: 0.98, f_beta: 0.98
train: step: 5090, loss: 0.025313520804047585, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 5091, loss: 0.03976944088935852, acc: 0.96, recall: 0.96, precision: 0.9607371794871795, f_beta: 0.959983993597439
train: step: 5092, loss: 0.007939701899886131, acc: 0.99, recall: 0.99, precision: 0.9901960784313726, f_beta: 0.98999899989999
train: step: 5093, loss: 0.029388215392827988, acc: 0.95, recall: 0.95, precision: 0.9501800720288115, f_beta: 0.9499949994999499
train: step: 5094, loss: 0.02402564510703087, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 5095, loss: 0.022485867142677307, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 5096, loss: 0.03326831012964249, acc: 0.97, recall: 0.97, precision: 0.9716981132075472, f_beta: 0.9699729756781104
train: step: 5097, loss: 0.012685392051935196, acc: 0.99, recall: 0.99, precision: 0.9901960784313726, f_beta: 0.98999899989999
train: step: 5098, loss: 0.01686267927289009, acc: 0.98, recall: 0.98, precision: 0.9807692307692308, f_beta: 0.9799919967987194
train: step: 5099, loss: 0.023110313341021538, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 5100, loss: 0.04350792244076729, acc: 0.95, recall: 0.95, precision: 0.9516258530710557, f_beta: 0.949954959463517
train: step: 5101, loss: 0.0317520797252655, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 5102, loss: 0.037030838429927826, acc: 0.95, recall: 0.95, precision: 0.9545454545454546, f_beta: 0.949874686716792
train: step: 5103, loss: 0.022784583270549774, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 5104, loss: 0.045764654874801636, acc: 0.95, recall: 0.95, precision: 0.9516258530710557, f_beta: 0.949954959463517
train: step: 5105, loss: 0.020460648462176323, acc: 0.98, recall: 0.98, precision: 0.9807692307692308, f_beta: 0.9799919967987194
train: step: 5106, loss: 0.010989784263074398, acc: 0.99, recall: 0.99, precision: 0.9901960784313726, f_beta: 0.98999899989999
train: step: 5107, loss: 0.02034628950059414, acc: 0.96, recall: 0.96, precision: 0.96, f_beta: 0.96
train: step: 5108, loss: 0.01928352192044258, acc: 0.98, recall: 0.98, precision: 0.9807692307692308, f_beta: 0.9799919967987194
train: step: 5109, loss: 0.031958628445863724, acc: 0.96, recall: 0.96, precision: 0.9607371794871795, f_beta: 0.959983993597439
train: step: 5110, loss: 0.022387640550732613, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 5111, loss: 0.026482539251446724, acc: 0.96, recall: 0.96, precision: 0.96, f_beta: 0.96
train: step: 5112, loss: 0.01344285998493433, acc: 0.99, recall: 0.99, precision: 0.9901960784313726, f_beta: 0.98999899989999
train: step: 5113, loss: 0.01791105419397354, acc: 0.98, recall: 0.98, precision: 0.9807692307692308, f_beta: 0.9799919967987194
train: step: 5114, loss: 0.04733177274465561, acc: 0.95, recall: 0.95, precision: 0.9501800720288115, f_beta: 0.9499949994999499
train: step: 5115, loss: 0.01652868092060089, acc: 0.99, recall: 0.99, precision: 0.9901960784313726, f_beta: 0.98999899989999
train: step: 5116, loss: 0.021531013771891594, acc: 0.98, recall: 0.98, precision: 0.9807692307692308, f_beta: 0.9799919967987194
train: step: 5117, loss: 0.010906555689871311, acc: 0.99, recall: 0.99, precision: 0.9901960784313726, f_beta: 0.98999899989999
train: step: 5118, loss: 0.050603222101926804, acc: 0.94, recall: 0.94, precision: 0.9428341384863124, f_beta: 0.9399038461538463
train: step: 5119, loss: 0.027586817741394043, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 5120, loss: 0.0243853610008955, acc: 0.99, recall: 0.99, precision: 0.9901960784313726, f_beta: 0.98999899989999
train: step: 5121, loss: 0.002209235215559602, acc: 1.0, recall: 1.0, precision: 1.0, f_beta: 1.0
train: step: 5122, loss: 0.02216990478336811, acc: 0.98, recall: 0.98, precision: 0.98, f_beta: 0.98
train: step: 5123, loss: 0.01983559876680374, acc: 0.98, recall: 0.98, precision: 0.9807692307692308, f_beta: 0.9799919967987194
train: step: 5124, loss: 0.016525378450751305, acc: 0.99, recall: 0.99, precision: 0.9901960784313726, f_beta: 0.98999899989999
train: step: 5125, loss: 0.021070435643196106, acc: 0.96, recall: 0.96, precision: 0.96, f_beta: 0.96
train: step: 5126, loss: 0.011304030194878578, acc: 0.99, recall: 0.99, precision: 0.9901960784313726, f_beta: 0.98999899989999
train: step: 5127, loss: 0.014131622388958931, acc: 0.99, recall: 0.99, precision: 0.9901960784313726, f_beta: 0.98999899989999
train: step: 5128, loss: 0.022740187123417854, acc: 0.98, recall: 0.98, precision: 0.9807692307692308, f_beta: 0.9799919967987194
train: step: 5129, loss: 0.0029947375878691673, acc: 1.0, recall: 1.0, precision: 1.0, f_beta: 1.0
train: step: 5130, loss: 0.010400909930467606, acc: 0.99, recall: 0.99, precision: 0.9901960784313726, f_beta: 0.98999899989999
train: step: 5131, loss: 0.049150142818689346, acc: 0.95, recall: 0.95, precision: 0.9501800720288115, f_beta: 0.9499949994999499
train: step: 5132, loss: 0.02149093709886074, acc: 0.97, recall: 0.97, precision: 0.9716981132075472, f_beta: 0.9699729756781104
train: step: 5133, loss: 0.020164089277386665, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 5134, loss: 0.031566400080919266, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 5135, loss: 0.0371461883187294, acc: 0.96, recall: 0.96, precision: 0.9607371794871795, f_beta: 0.959983993597439
train: step: 5136, loss: 0.0018412673380225897, acc: 1.0, recall: 1.0, precision: 1.0, f_beta: 1.0
train: step: 5137, loss: 0.027705984190106392, acc: 0.97, recall: 0.97, precision: 0.9716981132075472, f_beta: 0.9699729756781104
train: step: 5138, loss: 0.023837627843022346, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 5139, loss: 0.03398026525974274, acc: 0.96, recall: 0.96, precision: 0.9607371794871795, f_beta: 0.959983993597439
train: step: 5140, loss: 0.02817578800022602, acc: 0.98, recall: 0.98, precision: 0.9807692307692308, f_beta: 0.9799919967987194
train: step: 5141, loss: 0.0005722499918192625, acc: 1.0, recall: 1.0, precision: 1.0, f_beta: 1.0
train: step: 5142, loss: 0.014779244549572468, acc: 0.99, recall: 0.99, precision: 0.9901960784313726, f_beta: 0.98999899989999
train: step: 5143, loss: 0.011528696864843369, acc: 0.98, recall: 0.98, precision: 0.9807692307692308, f_beta: 0.9799919967987194
train: step: 5144, loss: 0.00093836709856987, acc: 1.0, recall: 1.0, precision: 1.0, f_beta: 1.0
train: step: 5145, loss: 0.0029357681050896645, acc: 1.0, recall: 1.0, precision: 1.0, f_beta: 1.0
train: step: 5146, loss: 0.015554821118712425, acc: 0.98, recall: 0.98, precision: 0.9807692307692308, f_beta: 0.9799919967987194
train: step: 5147, loss: 0.04676066339015961, acc: 0.93, recall: 0.9299999999999999, precision: 0.9315535929345644, f_beta: 0.9299369432489241
train: step: 5148, loss: 0.004662542138248682, acc: 1.0, recall: 1.0, precision: 1.0, f_beta: 1.0
train: step: 5149, loss: 0.015613529831171036, acc: 0.98, recall: 0.98, precision: 0.9807692307692308, f_beta: 0.9799919967987194
train: step: 5150, loss: 0.042346905916929245, acc: 0.94, recall: 0.94, precision: 0.9464285714285714, f_beta: 0.939783219590526
train: step: 5151, loss: 0.014524879865348339, acc: 0.99, recall: 0.99, precision: 0.9901960784313726, f_beta: 0.98999899989999
train: step: 5152, loss: 0.02009047195315361, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 5153, loss: 0.01017477735877037, acc: 0.99, recall: 0.99, precision: 0.9901960784313726, f_beta: 0.98999899989999
train: step: 5154, loss: 0.030636673793196678, acc: 0.96, recall: 0.96, precision: 0.9607371794871795, f_beta: 0.959983993597439
train: step: 5155, loss: 0.0309675931930542, acc: 0.95, recall: 0.95, precision: 0.9545454545454546, f_beta: 0.949874686716792
train: step: 5156, loss: 0.041887883096933365, acc: 0.96, recall: 0.96, precision: 0.9607371794871795, f_beta: 0.959983993597439
train: step: 5157, loss: 0.03431602567434311, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 5158, loss: 0.041950926184654236, acc: 0.95, recall: 0.95, precision: 0.9501800720288115, f_beta: 0.9499949994999499
train: step: 5159, loss: 0.03868016600608826, acc: 0.96, recall: 0.96, precision: 0.9607371794871795, f_beta: 0.959983993597439
train: step: 5160, loss: 0.021125297993421555, acc: 0.98, recall: 0.98, precision: 0.98, f_beta: 0.98
train: step: 5161, loss: 0.0033197402954101562, acc: 0.99, recall: 0.99, precision: 0.9901960784313726, f_beta: 0.98999899989999
train: step: 5162, loss: 0.004360884893685579, acc: 1.0, recall: 1.0, precision: 1.0, f_beta: 1.0
train: step: 5163, loss: 0.02080395072698593, acc: 0.98, recall: 0.98, precision: 0.9807692307692308, f_beta: 0.9799919967987194
train: step: 5164, loss: 0.032805729657411575, acc: 0.97, recall: 0.97, precision: 0.9716981132075472, f_beta: 0.9699729756781104
train: step: 5165, loss: 0.027162540704011917, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 5166, loss: 0.03382470831274986, acc: 0.95, recall: 0.95, precision: 0.9545454545454546, f_beta: 0.949874686716792
train: step: 5167, loss: 0.032221511006355286, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 5168, loss: 0.012645578011870384, acc: 0.99, recall: 0.99, precision: 0.9901960784313726, f_beta: 0.98999899989999
train: step: 5169, loss: 0.020455198362469673, acc: 0.98, recall: 0.98, precision: 0.9807692307692308, f_beta: 0.9799919967987194
train: step: 5170, loss: 0.018282108008861542, acc: 0.98, recall: 0.98, precision: 0.9807692307692308, f_beta: 0.9799919967987194
train: step: 5171, loss: 0.01603667438030243, acc: 0.99, recall: 0.99, precision: 0.9901960784313726, f_beta: 0.98999899989999
train: step: 5172, loss: 0.01487760990858078, acc: 0.99, recall: 0.99, precision: 0.9901960784313726, f_beta: 0.98999899989999
train: step: 5173, loss: 0.013253629207611084, acc: 0.99, recall: 0.99, precision: 0.9901960784313726, f_beta: 0.98999899989999
train: step: 5174, loss: 0.03524657338857651, acc: 0.96, recall: 0.96, precision: 0.9607371794871795, f_beta: 0.959983993597439
train: step: 5175, loss: 0.028268644586205482, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 5176, loss: 0.023188984021544456, acc: 0.98, recall: 0.98, precision: 0.98, f_beta: 0.98
train: step: 5177, loss: 0.03182978183031082, acc: 0.96, recall: 0.96, precision: 0.962962962962963, f_beta: 0.9599358974358974
train: step: 5178, loss: 0.025585390627384186, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 5179, loss: 0.03994360566139221, acc: 0.94, recall: 0.94, precision: 0.9407051282051282, f_beta: 0.9399759903961584
train: step: 5180, loss: 0.03062525764107704, acc: 0.95, recall: 0.95, precision: 0.9516258530710557, f_beta: 0.949954959463517
train: step: 5181, loss: 0.04586606100201607, acc: 0.94, recall: 0.94, precision: 0.9407051282051282, f_beta: 0.9399759903961584
train: step: 5182, loss: 0.04828678071498871, acc: 0.93, recall: 0.9299999999999999, precision: 0.9343434343434343, f_beta: 0.9298245614035088
train: step: 5183, loss: 0.052250321954488754, acc: 0.94, recall: 0.94, precision: 0.94, f_beta: 0.94
train: step: 5184, loss: 0.03348616510629654, acc: 0.97, recall: 0.97, precision: 0.9716981132075472, f_beta: 0.9699729756781104
train: step: 5185, loss: 0.012844927608966827, acc: 0.98, recall: 0.98, precision: 0.9807692307692308, f_beta: 0.9799919967987194
train: step: 5186, loss: 0.03303884342312813, acc: 0.95, recall: 0.95, precision: 0.9501800720288115, f_beta: 0.9499949994999499
train: step: 5187, loss: 0.015934329479932785, acc: 0.98, recall: 0.98, precision: 0.98, f_beta: 0.98
train: step: 5188, loss: 0.039234112948179245, acc: 0.95, recall: 0.95, precision: 0.9516258530710557, f_beta: 0.949954959463517
train: step: 5189, loss: 0.020982608199119568, acc: 0.98, recall: 0.98, precision: 0.9807692307692308, f_beta: 0.9799919967987194
train: step: 5190, loss: 0.021508917212486267, acc: 0.98, recall: 0.98, precision: 0.9807692307692308, f_beta: 0.9799919967987194
train: step: 5191, loss: 0.04480409622192383, acc: 0.94, recall: 0.94, precision: 0.94, f_beta: 0.94
train: step: 5192, loss: 0.05053659528493881, acc: 0.94, recall: 0.94, precision: 0.9464285714285714, f_beta: 0.939783219590526
train: step: 5193, loss: 0.0006321189575828612, acc: 1.0, recall: 1.0, precision: 1.0, f_beta: 1.0
train: step: 5194, loss: 0.021183202043175697, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 5195, loss: 0.021203670650720596, acc: 0.98, recall: 0.98, precision: 0.98, f_beta: 0.98
train: step: 5196, loss: 0.03050033561885357, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 5197, loss: 0.008831051178276539, acc: 0.99, recall: 0.99, precision: 0.9901960784313726, f_beta: 0.98999899989999
train: step: 5198, loss: 0.007400514092296362, acc: 0.99, recall: 0.99, precision: 0.9901960784313726, f_beta: 0.98999899989999
train: step: 5199, loss: 0.014350559562444687, acc: 0.98, recall: 0.98, precision: 0.9807692307692308, f_beta: 0.9799919967987194
train: step: 5200, loss: 0.05211775377392769, acc: 0.94, recall: 0.94, precision: 0.94, f_beta: 0.94
train: step: 5201, loss: 0.0174575038254261, acc: 0.98, recall: 0.98, precision: 0.98, f_beta: 0.98
train: step: 5202, loss: 0.05277417600154877, acc: 0.93, recall: 0.9299999999999999, precision: 0.9343434343434343, f_beta: 0.9298245614035088
train: step: 5203, loss: 0.04545183107256889, acc: 0.95, recall: 0.95, precision: 0.9501800720288115, f_beta: 0.9499949994999499
train: step: 5204, loss: 0.012029571458697319, acc: 0.99, recall: 0.99, precision: 0.9901960784313726, f_beta: 0.98999899989999
train: step: 5205, loss: 0.002108463551849127, acc: 1.0, recall: 1.0, precision: 1.0, f_beta: 1.0
train: step: 5206, loss: 0.016866514459252357, acc: 0.98, recall: 0.98, precision: 0.9807692307692308, f_beta: 0.9799919967987194
train: step: 5207, loss: 0.03891829401254654, acc: 0.96, recall: 0.96, precision: 0.96, f_beta: 0.96
train: step: 5208, loss: 0.027766739949584007, acc: 0.96, recall: 0.96, precision: 0.9607371794871795, f_beta: 0.959983993597439
train: step: 5209, loss: 0.01712416484951973, acc: 0.98, recall: 0.98, precision: 0.9807692307692308, f_beta: 0.9799919967987194
train: step: 5210, loss: 0.03320106491446495, acc: 0.96, recall: 0.96, precision: 0.9607371794871795, f_beta: 0.959983993597439
train: step: 5211, loss: 0.013925867155194283, acc: 0.98, recall: 0.98, precision: 0.98, f_beta: 0.98
train: step: 5212, loss: 0.034821175038814545, acc: 0.96, recall: 0.96, precision: 0.962962962962963, f_beta: 0.9599358974358974
train: step: 5213, loss: 0.026805682107806206, acc: 0.96, recall: 0.96, precision: 0.96, f_beta: 0.96
train: step: 5214, loss: 0.023633794859051704, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 5215, loss: 0.03173782676458359, acc: 0.96, recall: 0.96, precision: 0.962962962962963, f_beta: 0.9599358974358974
train: step: 5216, loss: 0.01072798389941454, acc: 0.99, recall: 0.99, precision: 0.9901960784313726, f_beta: 0.98999899989999
train: step: 5217, loss: 0.018995966762304306, acc: 0.98, recall: 0.98, precision: 0.9807692307692308, f_beta: 0.9799919967987194
train: step: 5218, loss: 0.008860244415700436, acc: 0.99, recall: 0.99, precision: 0.9901960784313726, f_beta: 0.98999899989999
train: step: 5219, loss: 0.052501119673252106, acc: 0.94, recall: 0.94, precision: 0.9428341384863124, f_beta: 0.9399038461538463
train: step: 5220, loss: 0.028940841555595398, acc: 0.96, recall: 0.96, precision: 0.962962962962963, f_beta: 0.9599358974358974
train: step: 5221, loss: 0.03503843769431114, acc: 0.96, recall: 0.96, precision: 0.962962962962963, f_beta: 0.9599358974358974
train: step: 5222, loss: 0.02750037983059883, acc: 0.97, recall: 0.97, precision: 0.9716981132075472, f_beta: 0.9699729756781104
train: step: 5223, loss: 0.05218898877501488, acc: 0.94, recall: 0.94, precision: 0.9407051282051282, f_beta: 0.9399759903961584
train: step: 5224, loss: 0.012252295389771461, acc: 0.99, recall: 0.99, precision: 0.9901960784313726, f_beta: 0.98999899989999
train: step: 5225, loss: 0.002518883440643549, acc: 1.0, recall: 1.0, precision: 1.0, f_beta: 1.0
train: step: 5226, loss: 0.0025554513558745384, acc: 1.0, recall: 1.0, precision: 1.0, f_beta: 1.0
train: step: 5227, loss: 0.036825116723775864, acc: 0.96, recall: 0.96, precision: 0.962962962962963, f_beta: 0.9599358974358974
train: step: 5228, loss: 0.03987877815961838, acc: 0.95, recall: 0.95, precision: 0.9501800720288115, f_beta: 0.9499949994999499
train: step: 5229, loss: 0.010580477304756641, acc: 0.99, recall: 0.99, precision: 0.9901960784313726, f_beta: 0.98999899989999
train: step: 5230, loss: 0.012058684602379799, acc: 0.99, recall: 0.99, precision: 0.9901960784313726, f_beta: 0.98999899989999
train: step: 5231, loss: 0.0095607228577137, acc: 0.98, recall: 0.98, precision: 0.9807692307692308, f_beta: 0.9799919967987194
train: step: 5232, loss: 0.025807280093431473, acc: 0.96, recall: 0.96, precision: 0.96, f_beta: 0.96
train: step: 5233, loss: 0.007286245934665203, acc: 0.99, recall: 0.99, precision: 0.9901960784313726, f_beta: 0.98999899989999
train: step: 5234, loss: 0.0031515168957412243, acc: 1.0, recall: 1.0, precision: 1.0, f_beta: 1.0
train: step: 5235, loss: 0.02557883784174919, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 5236, loss: 0.0190737321972847, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 5237, loss: 0.0015549161471426487, acc: 1.0, recall: 1.0, precision: 1.0, f_beta: 1.0
train: step: 5238, loss: 0.004337315447628498, acc: 0.99, recall: 0.99, precision: 0.9901960784313726, f_beta: 0.98999899989999
train: step: 5239, loss: 0.03125191852450371, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 5240, loss: 0.02264711819589138, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 5241, loss: 0.015724094584584236, acc: 0.97, recall: 0.97, precision: 0.9716981132075472, f_beta: 0.9699729756781104
train: step: 5242, loss: 0.02721654810011387, acc: 0.97, recall: 0.97, precision: 0.9716981132075472, f_beta: 0.9699729756781104
train: step: 5243, loss: 0.039513230323791504, acc: 0.95, recall: 0.95, precision: 0.9516258530710557, f_beta: 0.949954959463517
train: step: 5244, loss: 0.030113520100712776, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 5245, loss: 0.011208947747945786, acc: 0.99, recall: 0.99, precision: 0.9901960784313726, f_beta: 0.98999899989999
train: step: 5246, loss: 0.035949934273958206, acc: 0.96, recall: 0.96, precision: 0.962962962962963, f_beta: 0.9599358974358974
train: step: 5247, loss: 0.0236625075340271, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 5248, loss: 0.06608015298843384, acc: 0.93, recall: 0.9299999999999999, precision: 0.930172068827531, f_beta: 0.9299929992999301
train: step: 5249, loss: 0.02838120236992836, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 5250, loss: 0.050976723432540894, acc: 0.94, recall: 0.94, precision: 0.94, f_beta: 0.94
train: step: 5251, loss: 0.01480382215231657, acc: 0.99, recall: 0.99, precision: 0.9901960784313726, f_beta: 0.98999899989999
train: step: 5252, loss: 0.020236432552337646, acc: 0.98, recall: 0.98, precision: 0.98, f_beta: 0.98
train: step: 5253, loss: 0.027660755440592766, acc: 0.96, recall: 0.96, precision: 0.962962962962963, f_beta: 0.9599358974358974
train: step: 5254, loss: 0.01700715534389019, acc: 0.98, recall: 0.98, precision: 0.9807692307692308, f_beta: 0.9799919967987194
train: step: 5255, loss: 0.057710811495780945, acc: 0.93, recall: 0.9299999999999999, precision: 0.9315535929345644, f_beta: 0.9299369432489241
train: step: 5256, loss: 0.04084642231464386, acc: 0.92, recall: 0.9199999999999999, precision: 0.9206730769230769, f_beta: 0.919967987194878
train: step: 5257, loss: 0.02972891367971897, acc: 0.96, recall: 0.96, precision: 0.96, f_beta: 0.96
train: step: 5258, loss: 0.005637658294290304, acc: 1.0, recall: 1.0, precision: 1.0, f_beta: 1.0
train: step: 5259, loss: 0.03028368391096592, acc: 0.96, recall: 0.96, precision: 0.9607371794871795, f_beta: 0.959983993597439
train: step: 5260, loss: 0.01345372200012207, acc: 0.98, recall: 0.98, precision: 0.98, f_beta: 0.98
train: step: 5261, loss: 0.026010259985923767, acc: 0.96, recall: 0.96, precision: 0.962962962962963, f_beta: 0.9599358974358974
train: step: 5262, loss: 0.009010802954435349, acc: 0.98, recall: 0.98, precision: 0.9807692307692308, f_beta: 0.9799919967987194
train: step: 5263, loss: 0.03420243412256241, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 5264, loss: 0.046397142112255096, acc: 0.94, recall: 0.94, precision: 0.94, f_beta: 0.94
train: step: 5265, loss: 0.01264495961368084, acc: 0.99, recall: 0.99, precision: 0.9901960784313726, f_beta: 0.98999899989999
train: step: 5266, loss: 0.012485773302614689, acc: 0.98, recall: 0.98, precision: 0.98, f_beta: 0.98
train: step: 5267, loss: 0.0124724255874753, acc: 0.98, recall: 0.98, precision: 0.98, f_beta: 0.98
train: step: 5268, loss: 0.015086951665580273, acc: 0.98, recall: 0.98, precision: 0.9807692307692308, f_beta: 0.9799919967987194
train: step: 5269, loss: 0.07803338766098022, acc: 0.9, recall: 0.8999999999999999, precision: 0.9025764895330113, f_beta: 0.8998397435897436
train: step: 5270, loss: 0.020669637247920036, acc: 0.98, recall: 0.98, precision: 0.9807692307692308, f_beta: 0.9799919967987194
train: step: 5271, loss: 0.03754214942455292, acc: 0.95, recall: 0.95, precision: 0.9501800720288115, f_beta: 0.9499949994999499
train: step: 5272, loss: 0.03744836151599884, acc: 0.96, recall: 0.96, precision: 0.962962962962963, f_beta: 0.9599358974358974
train: step: 5273, loss: 0.023278484120965004, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 5274, loss: 0.02318064495921135, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 5275, loss: 0.00896509550511837, acc: 0.99, recall: 0.99, precision: 0.9901960784313726, f_beta: 0.98999899989999
train: step: 5276, loss: 0.01868615858256817, acc: 0.98, recall: 0.98, precision: 0.98, f_beta: 0.98
train: step: 5277, loss: 0.03567542880773544, acc: 0.96, recall: 0.96, precision: 0.9607371794871795, f_beta: 0.959983993597439
train: step: 5278, loss: 0.0007793973200023174, acc: 1.0, recall: 1.0, precision: 1.0, f_beta: 1.0
train: step: 5279, loss: 0.04326147958636284, acc: 0.96, recall: 0.96, precision: 0.9607371794871795, f_beta: 0.959983993597439
train: step: 5280, loss: 0.04103516414761543, acc: 0.95, recall: 0.95, precision: 0.9516258530710557, f_beta: 0.949954959463517
train: step: 5281, loss: 0.0353994257748127, acc: 0.96, recall: 0.96, precision: 0.96, f_beta: 0.96
train: step: 5282, loss: 0.0401168167591095, acc: 0.94, recall: 0.94, precision: 0.9428341384863124, f_beta: 0.9399038461538463
train: step: 5283, loss: 0.01988721825182438, acc: 0.98, recall: 0.98, precision: 0.9807692307692308, f_beta: 0.9799919967987194
train: step: 5284, loss: 0.04202040284872055, acc: 0.94, recall: 0.94, precision: 0.9464285714285714, f_beta: 0.939783219590526
train: step: 5285, loss: 0.03824443742632866, acc: 0.96, recall: 0.96, precision: 0.96, f_beta: 0.96
train: step: 5286, loss: 0.008121874183416367, acc: 0.99, recall: 0.99, precision: 0.9901960784313726, f_beta: 0.98999899989999
train: step: 5287, loss: 0.025637583807110786, acc: 0.96, recall: 0.96, precision: 0.96, f_beta: 0.96
train: step: 5288, loss: 0.02124887891113758, acc: 0.98, recall: 0.98, precision: 0.98, f_beta: 0.98
train: step: 5289, loss: 0.028350593522191048, acc: 0.97, recall: 0.97, precision: 0.9716981132075472, f_beta: 0.9699729756781104
train: step: 5290, loss: 0.03748004510998726, acc: 0.96, recall: 0.96, precision: 0.96, f_beta: 0.96
train: step: 5291, loss: 0.03772357851266861, acc: 0.96, recall: 0.96, precision: 0.9607371794871795, f_beta: 0.959983993597439
train: step: 5292, loss: 0.07643413543701172, acc: 0.91, recall: 0.9099999999999999, precision: 0.9114813327980731, f_beta: 0.9099189270343309
train: step: 5293, loss: 0.023477530106902122, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 5294, loss: 0.0016870425315573812, acc: 1.0, recall: 1.0, precision: 1.0, f_beta: 1.0
train: step: 5295, loss: 0.02974317967891693, acc: 0.97, recall: 0.97, precision: 0.9716981132075472, f_beta: 0.9699729756781104
train: step: 5296, loss: 0.03706073388457298, acc: 0.95, recall: 0.95, precision: 0.9501800720288115, f_beta: 0.9499949994999499
train: step: 5297, loss: 0.0159038957208395, acc: 0.98, recall: 0.98, precision: 0.9807692307692308, f_beta: 0.9799919967987194
train: step: 5298, loss: 0.01458040438592434, acc: 0.98, recall: 0.98, precision: 0.9807692307692308, f_beta: 0.9799919967987194
train: step: 5299, loss: 0.02003897726535797, acc: 0.98, recall: 0.98, precision: 0.98, f_beta: 0.98
train: step: 5300, loss: 0.020351307466626167, acc: 0.98, recall: 0.98, precision: 0.9807692307692308, f_beta: 0.9799919967987194
train: step: 5301, loss: 0.0015599536709487438, acc: 1.0, recall: 1.0, precision: 1.0, f_beta: 1.0
train: step: 5302, loss: 0.0134141705930233, acc: 0.98, recall: 0.98, precision: 0.98, f_beta: 0.98
train: step: 5303, loss: 0.02044571377336979, acc: 0.98, recall: 0.98, precision: 0.9807692307692308, f_beta: 0.9799919967987194
train: step: 5304, loss: 0.06271757185459137, acc: 0.93, recall: 0.9299999999999999, precision: 0.9343434343434343, f_beta: 0.9298245614035088
train: step: 5305, loss: 0.03363794833421707, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 5306, loss: 0.04893743619322777, acc: 0.95, recall: 0.95, precision: 0.9516258530710557, f_beta: 0.949954959463517
train: step: 5307, loss: 0.0030740411020815372, acc: 1.0, recall: 1.0, precision: 1.0, f_beta: 1.0
train: step: 5308, loss: 0.02516474574804306, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 5309, loss: 0.013714727014303207, acc: 0.98, recall: 0.98, precision: 0.98, f_beta: 0.98
train: step: 5310, loss: 0.04065127670764923, acc: 0.95, recall: 0.95, precision: 0.9545454545454546, f_beta: 0.949874686716792
train: step: 5311, loss: 0.030183015391230583, acc: 0.96, recall: 0.96, precision: 0.9607371794871795, f_beta: 0.959983993597439
train: step: 5312, loss: 0.02287198044359684, acc: 0.98, recall: 0.98, precision: 0.98, f_beta: 0.98
train: step: 5313, loss: 0.021002531051635742, acc: 0.98, recall: 0.98, precision: 0.9807692307692308, f_beta: 0.9799919967987194
train: step: 5314, loss: 0.014982882887125015, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 5315, loss: 0.022730570286512375, acc: 0.98, recall: 0.98, precision: 0.98, f_beta: 0.98
train: step: 5316, loss: 0.022903773933649063, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 5317, loss: 0.024588465690612793, acc: 0.98, recall: 0.98, precision: 0.98, f_beta: 0.98
train: step: 5318, loss: 0.02111867442727089, acc: 0.98, recall: 0.98, precision: 0.98, f_beta: 0.98
train: step: 5319, loss: 0.012397892773151398, acc: 0.99, recall: 0.99, precision: 0.9901960784313726, f_beta: 0.98999899989999
train: step: 5320, loss: 0.0005089419428259134, acc: 1.0, recall: 1.0, precision: 1.0, f_beta: 1.0
train: step: 5321, loss: 0.049034345895051956, acc: 0.94, recall: 0.94, precision: 0.9407051282051282, f_beta: 0.9399759903961584
train: step: 5322, loss: 0.02209349162876606, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 5323, loss: 0.02324735000729561, acc: 0.97, recall: 0.97, precision: 0.9716981132075472, f_beta: 0.9699729756781104
train: step: 5324, loss: 0.0016085893148556352, acc: 1.0, recall: 1.0, precision: 1.0, f_beta: 1.0
train: step: 5325, loss: 0.05283757299184799, acc: 0.92, recall: 0.92, precision: 0.92, f_beta: 0.92
train: step: 5326, loss: 0.018397323787212372, acc: 0.97, recall: 0.97, precision: 0.9716981132075472, f_beta: 0.9699729756781104
train: step: 5327, loss: 0.04418318718671799, acc: 0.94, recall: 0.94, precision: 0.9464285714285714, f_beta: 0.939783219590526
train: step: 5328, loss: 0.001920436043292284, acc: 1.0, recall: 1.0, precision: 1.0, f_beta: 1.0
train: step: 5329, loss: 0.02582022175192833, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 5330, loss: 0.022760281339287758, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 5331, loss: 0.0016026321100071073, acc: 1.0, recall: 1.0, precision: 1.0, f_beta: 1.0
train: step: 5332, loss: 0.0336366705596447, acc: 0.96, recall: 0.96, precision: 0.9607371794871795, f_beta: 0.959983993597439
train: step: 5333, loss: 0.013359620235860348, acc: 0.98, recall: 0.98, precision: 0.98, f_beta: 0.98
train: step: 5334, loss: 0.016353406012058258, acc: 0.98, recall: 0.98, precision: 0.9807692307692308, f_beta: 0.9799919967987194
train: step: 5335, loss: 0.0048528145998716354, acc: 1.0, recall: 1.0, precision: 1.0, f_beta: 1.0
train: step: 5336, loss: 0.02738749422132969, acc: 0.97, recall: 0.97, precision: 0.9716981132075472, f_beta: 0.9699729756781104
train: step: 5337, loss: 0.038119979202747345, acc: 0.96, recall: 0.96, precision: 0.9607371794871795, f_beta: 0.959983993597439
train: step: 5338, loss: 0.03069930337369442, acc: 0.96, recall: 0.96, precision: 0.9607371794871795, f_beta: 0.959983993597439
train: step: 5339, loss: 0.04856186360120773, acc: 0.94, recall: 0.94, precision: 0.9464285714285714, f_beta: 0.939783219590526
train: step: 5340, loss: 0.02709263563156128, acc: 0.97, recall: 0.97, precision: 0.9716981132075472, f_beta: 0.9699729756781104
train: step: 5341, loss: 0.0221954844892025, acc: 0.97, recall: 0.97, precision: 0.9716981132075472, f_beta: 0.9699729756781104
train: step: 5342, loss: 0.01000452321022749, acc: 0.99, recall: 0.99, precision: 0.9901960784313726, f_beta: 0.98999899989999
train: step: 5343, loss: 0.019947558641433716, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 5344, loss: 0.006584439426660538, acc: 1.0, recall: 1.0, precision: 1.0, f_beta: 1.0
train: step: 5345, loss: 0.019634276628494263, acc: 0.98, recall: 0.98, precision: 0.9807692307692308, f_beta: 0.9799919967987194
train: step: 5346, loss: 0.02300206571817398, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 5347, loss: 0.029091697186231613, acc: 0.97, recall: 0.97, precision: 0.9716981132075472, f_beta: 0.9699729756781104
train: step: 5348, loss: 0.015762876719236374, acc: 0.98, recall: 0.98, precision: 0.9807692307692308, f_beta: 0.9799919967987194
train: step: 5349, loss: 0.022346463054418564, acc: 0.98, recall: 0.98, precision: 0.98, f_beta: 0.98
train: step: 5350, loss: 0.03225557506084442, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 5351, loss: 0.031062090769410133, acc: 0.96, recall: 0.96, precision: 0.96, f_beta: 0.96
train: step: 5352, loss: 0.034835029393434525, acc: 0.96, recall: 0.96, precision: 0.9607371794871795, f_beta: 0.959983993597439
train: step: 5353, loss: 0.01582449860870838, acc: 0.98, recall: 0.98, precision: 0.9807692307692308, f_beta: 0.9799919967987194
train: step: 5354, loss: 0.026560407131910324, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 5355, loss: 0.048304203897714615, acc: 0.94, recall: 0.94, precision: 0.9407051282051282, f_beta: 0.9399759903961584
train: step: 5356, loss: 0.010366794653236866, acc: 0.99, recall: 0.99, precision: 0.9901960784313726, f_beta: 0.98999899989999
train: step: 5357, loss: 0.030093804001808167, acc: 0.97, recall: 0.97, precision: 0.9716981132075472, f_beta: 0.9699729756781104
train: step: 5358, loss: 0.04081099107861519, acc: 0.96, recall: 0.96, precision: 0.96, f_beta: 0.96
train: step: 5359, loss: 0.035075847059488297, acc: 0.95, recall: 0.95, precision: 0.9516258530710557, f_beta: 0.949954959463517
train: step: 5360, loss: 0.03051534667611122, acc: 0.97, recall: 0.97, precision: 0.9716981132075472, f_beta: 0.9699729756781104
train: step: 5361, loss: 0.005266077350825071, acc: 1.0, recall: 1.0, precision: 1.0, f_beta: 1.0
train: step: 5362, loss: 0.027774706482887268, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 5363, loss: 0.01292240060865879, acc: 0.98, recall: 0.98, precision: 0.98, f_beta: 0.98
train: step: 5364, loss: 0.0015839689876884222, acc: 1.0, recall: 1.0, precision: 1.0, f_beta: 1.0
train: step: 5365, loss: 0.02688007429242134, acc: 0.96, recall: 0.96, precision: 0.9607371794871795, f_beta: 0.959983993597439
train: step: 5366, loss: 0.020025476813316345, acc: 0.98, recall: 0.98, precision: 0.98, f_beta: 0.98
train: step: 5367, loss: 0.012600010260939598, acc: 0.99, recall: 0.99, precision: 0.9901960784313726, f_beta: 0.98999899989999
train: step: 5368, loss: 0.020356187596917152, acc: 0.98, recall: 0.98, precision: 0.98, f_beta: 0.98
train: step: 5369, loss: 0.009122583083808422, acc: 0.99, recall: 0.99, precision: 0.9901960784313726, f_beta: 0.98999899989999
train: step: 5370, loss: 0.0159563347697258, acc: 0.98, recall: 0.98, precision: 0.9807692307692308, f_beta: 0.9799919967987194
train: step: 5371, loss: 0.025285914540290833, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 5372, loss: 0.010598850436508656, acc: 0.98, recall: 0.98, precision: 0.98, f_beta: 0.98
train: step: 5373, loss: 0.026844244450330734, acc: 0.97, recall: 0.97, precision: 0.9716981132075472, f_beta: 0.9699729756781104
train: step: 5374, loss: 0.046249933540821075, acc: 0.95, recall: 0.95, precision: 0.9545454545454546, f_beta: 0.949874686716792
train: step: 5375, loss: 0.017239470034837723, acc: 0.98, recall: 0.98, precision: 0.98, f_beta: 0.98
train: step: 5376, loss: 0.011887099593877792, acc: 0.99, recall: 0.99, precision: 0.9901960784313726, f_beta: 0.98999899989999
train: step: 5377, loss: 0.03062603995203972, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 5378, loss: 0.04027779400348663, acc: 0.96, recall: 0.96, precision: 0.962962962962963, f_beta: 0.9599358974358974
train: step: 5379, loss: 0.05360604450106621, acc: 0.93, recall: 0.9299999999999999, precision: 0.9315535929345644, f_beta: 0.9299369432489241
train: step: 5380, loss: 0.012191054411232471, acc: 0.98, recall: 0.98, precision: 0.9807692307692308, f_beta: 0.9799919967987194
train: step: 5381, loss: 0.009078452363610268, acc: 0.98, recall: 0.98, precision: 0.9807692307692308, f_beta: 0.9799919967987194
train: step: 5382, loss: 0.022319652140140533, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 5383, loss: 0.023067859932780266, acc: 0.97, recall: 0.97, precision: 0.9716981132075472, f_beta: 0.9699729756781104
train: step: 5384, loss: 0.0160604827105999, acc: 0.98, recall: 0.98, precision: 0.9807692307692308, f_beta: 0.9799919967987194
train: step: 5385, loss: 0.023207521066069603, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 5386, loss: 0.011318950913846493, acc: 0.99, recall: 0.99, precision: 0.9901960784313726, f_beta: 0.98999899989999
train: step: 5387, loss: 0.02018006332218647, acc: 0.97, recall: 0.97, precision: 0.9716981132075472, f_beta: 0.9699729756781104
train: step: 5388, loss: 0.03666087985038757, acc: 0.95, recall: 0.95, precision: 0.9501800720288115, f_beta: 0.9499949994999499
train: step: 5389, loss: 0.01108595635741949, acc: 0.99, recall: 0.99, precision: 0.9901960784313726, f_beta: 0.98999899989999
train: step: 5390, loss: 0.041985999792814255, acc: 0.96, recall: 0.96, precision: 0.96, f_beta: 0.96
train: step: 5391, loss: 0.007814744487404823, acc: 0.99, recall: 0.99, precision: 0.9901960784313726, f_beta: 0.98999899989999
train: step: 5392, loss: 0.02772088721394539, acc: 0.96, recall: 0.96, precision: 0.9607371794871795, f_beta: 0.959983993597439
train: step: 5393, loss: 0.029076125472784042, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 5394, loss: 0.008573127910494804, acc: 0.98, recall: 0.98, precision: 0.9807692307692308, f_beta: 0.9799919967987194
train: step: 5395, loss: 0.012958270497620106, acc: 0.99, recall: 0.99, precision: 0.9901960784313726, f_beta: 0.98999899989999
train: step: 5396, loss: 0.02024700865149498, acc: 0.98, recall: 0.98, precision: 0.9807692307692308, f_beta: 0.9799919967987194
train: step: 5397, loss: 0.021735932677984238, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 5398, loss: 0.008813189342617989, acc: 0.99, recall: 0.99, precision: 0.9901960784313726, f_beta: 0.98999899989999
train: step: 5399, loss: 0.022358467802405357, acc: 0.98, recall: 0.98, precision: 0.9807692307692308, f_beta: 0.9799919967987194
train: step: 5400, loss: 0.00186641956679523, acc: 1.0, recall: 1.0, precision: 1.0, f_beta: 1.0
train: step: 5401, loss: 0.034908343106508255, acc: 0.96, recall: 0.96, precision: 0.962962962962963, f_beta: 0.9599358974358974
train: step: 5402, loss: 0.011541076004505157, acc: 0.99, recall: 0.99, precision: 0.9901960784313726, f_beta: 0.98999899989999
train: step: 5403, loss: 0.040417060256004333, acc: 0.95, recall: 0.95, precision: 0.9545454545454546, f_beta: 0.949874686716792
train: step: 5404, loss: 0.024184025824069977, acc: 0.98, recall: 0.98, precision: 0.9807692307692308, f_beta: 0.9799919967987194
train: step: 5405, loss: 0.013330855406820774, acc: 0.99, recall: 0.99, precision: 0.9901960784313726, f_beta: 0.98999899989999
train: step: 5406, loss: 0.038949862122535706, acc: 0.95, recall: 0.95, precision: 0.9516258530710557, f_beta: 0.949954959463517
train: step: 5407, loss: 0.009920736774802208, acc: 0.99, recall: 0.99, precision: 0.9901960784313726, f_beta: 0.98999899989999
train: step: 5408, loss: 0.034431010484695435, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 5409, loss: 0.019210319966077805, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 5410, loss: 0.03144161403179169, acc: 0.97, recall: 0.97, precision: 0.9716981132075472, f_beta: 0.9699729756781104
train: step: 5411, loss: 0.02930058166384697, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 5412, loss: 0.03450101613998413, acc: 0.96, recall: 0.96, precision: 0.962962962962963, f_beta: 0.9599358974358974
train: step: 5413, loss: 0.02273811586201191, acc: 0.96, recall: 0.96, precision: 0.96, f_beta: 0.96
train: step: 5414, loss: 0.05391169711947441, acc: 0.93, recall: 0.9299999999999999, precision: 0.930172068827531, f_beta: 0.9299929992999301
train: step: 5415, loss: 0.01654319278895855, acc: 0.98, recall: 0.98, precision: 0.98, f_beta: 0.98
train: step: 5416, loss: 0.02710576541721821, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 5417, loss: 0.024180449545383453, acc: 0.98, recall: 0.98, precision: 0.98, f_beta: 0.98
train: step: 5418, loss: 0.009100234135985374, acc: 0.99, recall: 0.99, precision: 0.9901960784313726, f_beta: 0.98999899989999
train: step: 5419, loss: 0.025886135175824165, acc: 0.96, recall: 0.96, precision: 0.96, f_beta: 0.96
train: step: 5420, loss: 0.01155941467732191, acc: 0.99, recall: 0.99, precision: 0.9901960784313726, f_beta: 0.98999899989999
train: step: 5421, loss: 0.025242552161216736, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 5422, loss: 0.01909523457288742, acc: 0.98, recall: 0.98, precision: 0.98, f_beta: 0.98
train: step: 5423, loss: 0.02772732824087143, acc: 0.98, recall: 0.98, precision: 0.98, f_beta: 0.98
train: step: 5424, loss: 0.010603640228509903, acc: 0.99, recall: 0.99, precision: 0.9901960784313726, f_beta: 0.98999899989999
train: step: 5425, loss: 0.04260122403502464, acc: 0.94, recall: 0.94, precision: 0.9407051282051282, f_beta: 0.9399759903961584
train: step: 5426, loss: 0.03998031094670296, acc: 0.95, recall: 0.95, precision: 0.9501800720288115, f_beta: 0.9499949994999499
train: step: 5427, loss: 0.04034094884991646, acc: 0.95, recall: 0.95, precision: 0.9501800720288115, f_beta: 0.9499949994999499
train: step: 5428, loss: 0.03855010122060776, acc: 0.95, recall: 0.95, precision: 0.9516258530710557, f_beta: 0.949954959463517
train: step: 5429, loss: 0.022354014217853546, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 5430, loss: 0.4702852666378021, acc: 0.98, recall: 0.98, precision: 0.9807692307692308, f_beta: 0.9799919967987194
train: step: 5431, loss: 0.01513376459479332, acc: 0.98, recall: 0.98, precision: 0.98, f_beta: 0.98
train: step: 5432, loss: 0.00449643237516284, acc: 1.0, recall: 1.0, precision: 1.0, f_beta: 1.0
train: step: 5433, loss: 0.03039652854204178, acc: 0.96, recall: 0.96, precision: 0.962962962962963, f_beta: 0.9599358974358974
train: step: 5434, loss: 0.005492623895406723, acc: 0.99, recall: 0.99, precision: 0.9901960784313726, f_beta: 0.98999899989999
train: step: 5435, loss: 0.006159399636089802, acc: 0.99, recall: 0.99, precision: 0.9901960784313726, f_beta: 0.98999899989999
train: step: 5436, loss: 0.019254660233855247, acc: 0.97, recall: 0.97, precision: 0.9716981132075472, f_beta: 0.9699729756781104
train: step: 5437, loss: 0.03579360246658325, acc: 0.96, recall: 0.96, precision: 0.96, f_beta: 0.96
train: step: 5438, loss: 0.0747469887137413, acc: 0.9, recall: 0.8999999999999999, precision: 0.9058441558441559, f_beta: 0.8996386993175431
train: step: 5439, loss: 0.025217093527317047, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 5440, loss: 0.01609518565237522, acc: 0.98, recall: 0.98, precision: 0.9807692307692308, f_beta: 0.9799919967987194
train: step: 5441, loss: 0.02461639791727066, acc: 0.96, recall: 0.96, precision: 0.962962962962963, f_beta: 0.9599358974358974
train: step: 5442, loss: 0.01909346505999565, acc: 0.97, recall: 0.97, precision: 0.9716981132075472, f_beta: 0.9699729756781104
train: step: 5443, loss: 0.042419012635946274, acc: 0.95, recall: 0.95, precision: 0.9501800720288115, f_beta: 0.9499949994999499
train: step: 5444, loss: 0.03821449726819992, acc: 0.95, recall: 0.95, precision: 0.9516258530710557, f_beta: 0.949954959463517
train: step: 5445, loss: 0.01032976247370243, acc: 0.99, recall: 0.99, precision: 0.9901960784313726, f_beta: 0.98999899989999
train: step: 5446, loss: 0.022415457293391228, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 5447, loss: 0.01246466115117073, acc: 0.99, recall: 0.99, precision: 0.9901960784313726, f_beta: 0.98999899989999
train: step: 5448, loss: 0.07174751162528992, acc: 0.88, recall: 0.88, precision: 0.8824476650563606, f_beta: 0.8798076923076923
train: step: 5449, loss: 0.005232372786849737, acc: 1.0, recall: 1.0, precision: 1.0, f_beta: 1.0
train: step: 5450, loss: 0.010670679621398449, acc: 0.98, recall: 0.98, precision: 0.98, f_beta: 0.98
train: step: 5451, loss: 0.023942600935697556, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 5452, loss: 0.0452161580324173, acc: 0.95, recall: 0.95, precision: 0.9501800720288115, f_beta: 0.9499949994999499
train: step: 5453, loss: 0.005447243340313435, acc: 1.0, recall: 1.0, precision: 1.0, f_beta: 1.0
train: step: 5454, loss: 0.03127431496977806, acc: 0.96, recall: 0.96, precision: 0.96, f_beta: 0.96
train: step: 5455, loss: 0.021203508600592613, acc: 0.98, recall: 0.98, precision: 0.98, f_beta: 0.98
train: step: 5456, loss: 0.006237826310098171, acc: 1.0, recall: 1.0, precision: 1.0, f_beta: 1.0
train: step: 5457, loss: 0.02697107382118702, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 5458, loss: 0.04367758333683014, acc: 0.95, recall: 0.95, precision: 0.9516258530710557, f_beta: 0.949954959463517
train: step: 5459, loss: 0.04764524847269058, acc: 0.94, recall: 0.94, precision: 0.9428341384863124, f_beta: 0.9399038461538463
train: step: 5460, loss: 0.02379283681511879, acc: 0.98, recall: 0.98, precision: 0.9807692307692308, f_beta: 0.9799919967987194
train: step: 5461, loss: 0.028747854754328728, acc: 0.96, recall: 0.96, precision: 0.9607371794871795, f_beta: 0.959983993597439
train: step: 5462, loss: 0.0008062174892984331, acc: 1.0, recall: 1.0, precision: 1.0, f_beta: 1.0
train: step: 5463, loss: 0.044073525816202164, acc: 0.95, recall: 0.95, precision: 0.9501800720288115, f_beta: 0.9499949994999499
train: step: 5464, loss: 0.03397691622376442, acc: 0.95, recall: 0.95, precision: 0.9501800720288115, f_beta: 0.9499949994999499
train: step: 5465, loss: 0.04486900195479393, acc: 0.95, recall: 0.95, precision: 0.9545454545454546, f_beta: 0.949874686716792
train: step: 5466, loss: 0.031964562833309174, acc: 0.96, recall: 0.96, precision: 0.962962962962963, f_beta: 0.9599358974358974
train: step: 5467, loss: 0.033991679549217224, acc: 0.96, recall: 0.96, precision: 0.9607371794871795, f_beta: 0.959983993597439
train: step: 5468, loss: 0.02087288536131382, acc: 0.97, recall: 0.97, precision: 0.9716981132075472, f_beta: 0.9699729756781104
train: step: 5469, loss: 0.020874524489045143, acc: 0.98, recall: 0.98, precision: 0.98, f_beta: 0.98
train: step: 5470, loss: 0.03420710936188698, acc: 0.96, recall: 0.96, precision: 0.962962962962963, f_beta: 0.9599358974358974
train: step: 5471, loss: 0.044451311230659485, acc: 0.95, recall: 0.95, precision: 0.9516258530710557, f_beta: 0.949954959463517
train: step: 5472, loss: 0.031211426481604576, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 5473, loss: 0.01877475529909134, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 5474, loss: 0.02272821217775345, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 5475, loss: 0.018108239397406578, acc: 0.97, recall: 0.97, precision: 0.9716981132075472, f_beta: 0.9699729756781104
train: step: 5476, loss: 0.02653568424284458, acc: 0.96, recall: 0.96, precision: 0.9607371794871795, f_beta: 0.959983993597439
train: step: 5477, loss: 0.06254646182060242, acc: 0.93, recall: 0.9299999999999999, precision: 0.9385964912280702, f_beta: 0.9296553110240177
train: step: 5478, loss: 0.022435517981648445, acc: 0.98, recall: 0.98, precision: 0.9807692307692308, f_beta: 0.9799919967987194
train: step: 5479, loss: 0.023240789771080017, acc: 0.96, recall: 0.96, precision: 0.96, f_beta: 0.96
train: step: 5480, loss: 0.0004288728814572096, acc: 1.0, recall: 1.0, precision: 1.0, f_beta: 1.0
train: step: 5481, loss: 0.020251145586371422, acc: 0.98, recall: 0.98, precision: 0.98, f_beta: 0.98
train: step: 5482, loss: 0.02200436033308506, acc: 0.98, recall: 0.98, precision: 0.9807692307692308, f_beta: 0.9799919967987194
train: step: 5483, loss: 0.010807855054736137, acc: 0.99, recall: 0.99, precision: 0.9901960784313726, f_beta: 0.98999899989999
train: step: 5484, loss: 0.026205014437437057, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 5485, loss: 0.0459662489593029, acc: 0.95, recall: 0.95, precision: 0.9516258530710557, f_beta: 0.949954959463517
train: step: 5486, loss: 0.021332645788788795, acc: 0.98, recall: 0.98, precision: 0.9807692307692308, f_beta: 0.9799919967987194
train: step: 5487, loss: 0.018276246264576912, acc: 0.98, recall: 0.98, precision: 0.98, f_beta: 0.98
train: step: 5488, loss: 0.0231462549418211, acc: 0.98, recall: 0.98, precision: 0.98, f_beta: 0.98
train: step: 5489, loss: 0.02760951593518257, acc: 0.96, recall: 0.96, precision: 0.9607371794871795, f_beta: 0.959983993597439
train: step: 5490, loss: 0.020419154316186905, acc: 0.98, recall: 0.98, precision: 0.98, f_beta: 0.98
train: step: 5491, loss: 0.006133961491286755, acc: 0.99, recall: 0.99, precision: 0.9901960784313726, f_beta: 0.98999899989999
train: step: 5492, loss: 0.034953948110342026, acc: 0.95, recall: 0.95, precision: 0.9501800720288115, f_beta: 0.9499949994999499
train: step: 5493, loss: 0.039115674793720245, acc: 0.94, recall: 0.94, precision: 0.9407051282051282, f_beta: 0.9399759903961584
train: step: 5494, loss: 0.027678856626152992, acc: 0.97, recall: 0.97, precision: 0.9716981132075472, f_beta: 0.9699729756781104
train: step: 5495, loss: 0.010996433906257153, acc: 0.99, recall: 0.99, precision: 0.9901960784313726, f_beta: 0.98999899989999
train: step: 5496, loss: 0.003101042006164789, acc: 1.0, recall: 1.0, precision: 1.0, f_beta: 1.0
train: step: 5497, loss: 0.02609696425497532, acc: 0.97, recall: 0.97, precision: 0.9716981132075472, f_beta: 0.9699729756781104
train: step: 5498, loss: 0.019817644730210304, acc: 0.95, recall: 0.95, precision: 0.9516258530710557, f_beta: 0.949954959463517
train: step: 5499, loss: 0.03171856701374054, acc: 0.96, recall: 0.96, precision: 0.96, f_beta: 0.96
train: step: 5500, loss: 0.027010895311832428, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 5501, loss: 0.015433542430400848, acc: 0.98, recall: 0.98, precision: 0.98, f_beta: 0.98
train: step: 5502, loss: 0.01554668415337801, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 5503, loss: 0.05429540202021599, acc: 0.93, recall: 0.9299999999999999, precision: 0.9315535929345644, f_beta: 0.9299369432489241
train: step: 5504, loss: 0.015567527152597904, acc: 0.99, recall: 0.99, precision: 0.9901960784313726, f_beta: 0.98999899989999
train: step: 5505, loss: 0.022940978407859802, acc: 0.97, recall: 0.97, precision: 0.9716981132075472, f_beta: 0.9699729756781104
train: step: 5506, loss: 0.03158116713166237, acc: 0.96, recall: 0.96, precision: 0.96, f_beta: 0.96
train: step: 5507, loss: 0.04934433102607727, acc: 0.95, recall: 0.95, precision: 0.9516258530710557, f_beta: 0.949954959463517
train: step: 5508, loss: 0.016864830628037453, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 5509, loss: 0.01409086212515831, acc: 0.98, recall: 0.98, precision: 0.9807692307692308, f_beta: 0.9799919967987194
train: step: 5510, loss: 0.01285739615559578, acc: 0.99, recall: 0.99, precision: 0.9901960784313726, f_beta: 0.98999899989999
train: step: 5511, loss: 0.013189557939767838, acc: 0.99, recall: 0.99, precision: 0.9901960784313726, f_beta: 0.98999899989999
train: step: 5512, loss: 0.01570914313197136, acc: 0.98, recall: 0.98, precision: 0.9807692307692308, f_beta: 0.9799919967987194
train: step: 5513, loss: 0.0019475036533549428, acc: 1.0, recall: 1.0, precision: 1.0, f_beta: 1.0
train: step: 5514, loss: 0.01422538049519062, acc: 0.98, recall: 0.98, precision: 0.9807692307692308, f_beta: 0.9799919967987194
train: step: 5515, loss: 0.021191127598285675, acc: 0.98, recall: 0.98, precision: 0.9807692307692308, f_beta: 0.9799919967987194
train: step: 5516, loss: 0.03834431245923042, acc: 0.95, recall: 0.95, precision: 0.9501800720288115, f_beta: 0.9499949994999499
train: step: 5517, loss: 0.01344564463943243, acc: 0.99, recall: 0.99, precision: 0.9901960784313726, f_beta: 0.98999899989999
train: step: 5518, loss: 0.04101425036787987, acc: 0.96, recall: 0.96, precision: 0.9607371794871795, f_beta: 0.959983993597439
train: step: 5519, loss: 0.007326319348067045, acc: 0.99, recall: 0.99, precision: 0.9901960784313726, f_beta: 0.98999899989999
train: step: 5520, loss: 0.029437322169542313, acc: 0.96, recall: 0.96, precision: 0.96, f_beta: 0.96
train: step: 5521, loss: 0.027002353221178055, acc: 0.96, recall: 0.96, precision: 0.96, f_beta: 0.96
train: step: 5522, loss: 0.016290806233882904, acc: 0.99, recall: 0.99, precision: 0.9901960784313726, f_beta: 0.98999899989999
train: step: 5523, loss: 0.013801868073642254, acc: 0.97, recall: 0.97, precision: 0.9716981132075472, f_beta: 0.9699729756781104
train: step: 5524, loss: 0.011110798455774784, acc: 0.99, recall: 0.99, precision: 0.9901960784313726, f_beta: 0.98999899989999
train: step: 5525, loss: 0.016367781907320023, acc: 0.98, recall: 0.98, precision: 0.9807692307692308, f_beta: 0.9799919967987194
train: step: 5526, loss: 0.024116940796375275, acc: 0.98, recall: 0.98, precision: 0.9807692307692308, f_beta: 0.9799919967987194
train: step: 5527, loss: 0.015305490233004093, acc: 0.98, recall: 0.98, precision: 0.9807692307692308, f_beta: 0.9799919967987194
train: step: 5528, loss: 0.020668916404247284, acc: 0.98, recall: 0.98, precision: 0.98, f_beta: 0.98
train: step: 5529, loss: 0.01470821350812912, acc: 0.98, recall: 0.98, precision: 0.98, f_beta: 0.98
train: step: 5530, loss: 0.010755889117717743, acc: 0.99, recall: 0.99, precision: 0.9901960784313726, f_beta: 0.98999899989999
train: step: 5531, loss: 0.029756223782896996, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 5532, loss: 0.016657641157507896, acc: 0.98, recall: 0.98, precision: 0.98, f_beta: 0.98
train: step: 5533, loss: 0.044522132724523544, acc: 0.93, recall: 0.9299999999999999, precision: 0.9343434343434343, f_beta: 0.9298245614035088
train: step: 5534, loss: 0.022690195590257645, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 5535, loss: 0.025937389582395554, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 5536, loss: 0.018986038863658905, acc: 0.98, recall: 0.98, precision: 0.9807692307692308, f_beta: 0.9799919967987194
train: step: 5537, loss: 0.02412848174571991, acc: 0.98, recall: 0.98, precision: 0.9807692307692308, f_beta: 0.9799919967987194
train: step: 5538, loss: 0.03368088975548744, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 5539, loss: 0.015907643362879753, acc: 0.99, recall: 0.99, precision: 0.9901960784313726, f_beta: 0.98999899989999
train: step: 5540, loss: 0.039865147322416306, acc: 0.94, recall: 0.94, precision: 0.9428341384863124, f_beta: 0.9399038461538463
train: step: 5541, loss: 0.013667026534676552, acc: 0.98, recall: 0.98, precision: 0.98, f_beta: 0.98
train: step: 5542, loss: 0.012529952451586723, acc: 0.99, recall: 0.99, precision: 0.9901960784313726, f_beta: 0.98999899989999
train: step: 5543, loss: 0.004486465360969305, acc: 1.0, recall: 1.0, precision: 1.0, f_beta: 1.0
train: step: 5544, loss: 0.056273411959409714, acc: 0.94, recall: 0.94, precision: 0.9428341384863124, f_beta: 0.9399038461538463
train: step: 5545, loss: 0.036866042762994766, acc: 0.96, recall: 0.96, precision: 0.9607371794871795, f_beta: 0.959983993597439
train: step: 5546, loss: 0.03269548341631889, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 5547, loss: 0.024048181250691414, acc: 0.97, recall: 0.97, precision: 0.9716981132075472, f_beta: 0.9699729756781104
train: step: 5548, loss: 0.012753489427268505, acc: 0.99, recall: 0.99, precision: 0.9901960784313726, f_beta: 0.98999899989999
train: step: 5549, loss: 0.02842104434967041, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 5550, loss: 0.03372064605355263, acc: 0.96, recall: 0.96, precision: 0.96, f_beta: 0.96
train: step: 5551, loss: 0.002976254327222705, acc: 1.0, recall: 1.0, precision: 1.0, f_beta: 1.0
train: step: 5552, loss: 0.021574396640062332, acc: 0.98, recall: 0.98, precision: 0.98, f_beta: 0.98
train: step: 5553, loss: 0.013166717253625393, acc: 0.98, recall: 0.98, precision: 0.9807692307692308, f_beta: 0.9799919967987194
train: step: 5554, loss: 0.018797479569911957, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 5555, loss: 0.02816135808825493, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 5556, loss: 0.01584971696138382, acc: 0.98, recall: 0.98, precision: 0.98, f_beta: 0.98
train: step: 5557, loss: 0.07077211141586304, acc: 0.92, recall: 0.9199999999999999, precision: 0.9227053140096618, f_beta: 0.9198717948717948
train: step: 5558, loss: 0.03578777611255646, acc: 0.96, recall: 0.96, precision: 0.96, f_beta: 0.96
train: step: 5559, loss: 0.003134984290227294, acc: 1.0, recall: 1.0, precision: 1.0, f_beta: 1.0
train: step: 5560, loss: 0.025768397375941277, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 5561, loss: 0.02937144972383976, acc: 0.96, recall: 0.96, precision: 0.9607371794871795, f_beta: 0.959983993597439
train: step: 5562, loss: 0.016162339597940445, acc: 0.98, recall: 0.98, precision: 0.9807692307692308, f_beta: 0.9799919967987194
train: step: 5563, loss: 0.019475262612104416, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 5564, loss: 0.01659870147705078, acc: 0.98, recall: 0.98, precision: 0.9807692307692308, f_beta: 0.9799919967987194
train: step: 5565, loss: 0.010267463512718678, acc: 0.99, recall: 0.99, precision: 0.9901960784313726, f_beta: 0.98999899989999
train: step: 5566, loss: 0.023337388411164284, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 5567, loss: 0.019480077549815178, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 5568, loss: 0.012541258707642555, acc: 0.99, recall: 0.99, precision: 0.9901960784313726, f_beta: 0.98999899989999
train: step: 5569, loss: 0.02001274563372135, acc: 0.98, recall: 0.98, precision: 0.98, f_beta: 0.98
train: step: 5570, loss: 0.04479379206895828, acc: 0.93, recall: 0.9299999999999999, precision: 0.930172068827531, f_beta: 0.9299929992999301
train: step: 5571, loss: 0.03201251104474068, acc: 0.95, recall: 0.95, precision: 0.9516258530710557, f_beta: 0.949954959463517
train: step: 5572, loss: 0.015464387834072113, acc: 0.98, recall: 0.98, precision: 0.9807692307692308, f_beta: 0.9799919967987194
train: step: 5573, loss: 0.03586724400520325, acc: 0.94, recall: 0.94, precision: 0.9407051282051282, f_beta: 0.9399759903961584
train: step: 5574, loss: 0.03485965356230736, acc: 0.97, recall: 0.97, precision: 0.9716981132075472, f_beta: 0.9699729756781104
train: step: 5575, loss: 0.028045525774359703, acc: 0.96, recall: 0.96, precision: 0.96, f_beta: 0.96
train: step: 5576, loss: 0.008204706944525242, acc: 0.99, recall: 0.99, precision: 0.9901960784313726, f_beta: 0.98999899989999
train: step: 5577, loss: 0.039978738874197006, acc: 0.95, recall: 0.95, precision: 0.9501800720288115, f_beta: 0.9499949994999499
train: step: 5578, loss: 0.03904467821121216, acc: 0.96, recall: 0.96, precision: 0.96, f_beta: 0.96
train: step: 5579, loss: 0.023669518530368805, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 5580, loss: 0.0017703187186270952, acc: 1.0, recall: 1.0, precision: 1.0, f_beta: 1.0
train: step: 5581, loss: 0.02227538637816906, acc: 0.98, recall: 0.98, precision: 0.9807692307692308, f_beta: 0.9799919967987194
train: step: 5582, loss: 0.007357644848525524, acc: 1.0, recall: 1.0, precision: 1.0, f_beta: 1.0
train: step: 5583, loss: 0.011116325855255127, acc: 0.99, recall: 0.99, precision: 0.9901960784313726, f_beta: 0.98999899989999
train: step: 5584, loss: 0.01726641319692135, acc: 0.98, recall: 0.98, precision: 0.98, f_beta: 0.98
train: step: 5585, loss: 0.010286800563335419, acc: 0.99, recall: 0.99, precision: 0.9901960784313726, f_beta: 0.98999899989999
train: step: 5586, loss: 0.0020913227926939726, acc: 1.0, recall: 1.0, precision: 1.0, f_beta: 1.0
train: step: 5587, loss: 0.02519599162042141, acc: 0.98, recall: 0.98, precision: 0.98, f_beta: 0.98
train: step: 5588, loss: 0.016726307570934296, acc: 0.98, recall: 0.98, precision: 0.9807692307692308, f_beta: 0.9799919967987194
train: step: 5589, loss: 0.026098575443029404, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 5590, loss: 0.019792746752500534, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 5591, loss: 0.022750845178961754, acc: 0.96, recall: 0.96, precision: 0.962962962962963, f_beta: 0.9599358974358974
train: step: 5592, loss: 0.0027232205029577017, acc: 1.0, recall: 1.0, precision: 1.0, f_beta: 1.0
train: step: 5593, loss: 0.01222617644816637, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 5594, loss: 0.02950507402420044, acc: 0.96, recall: 0.96, precision: 0.962962962962963, f_beta: 0.9599358974358974
train: step: 5595, loss: 0.08382003754377365, acc: 0.9, recall: 0.8999999999999999, precision: 0.9058441558441559, f_beta: 0.8996386993175431
train: step: 5596, loss: 0.03561588376760483, acc: 0.96, recall: 0.96, precision: 0.9607371794871795, f_beta: 0.959983993597439
train: step: 5597, loss: 0.0309283584356308, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 5598, loss: 0.03303394839167595, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 5599, loss: 0.0475783571600914, acc: 0.94, recall: 0.94, precision: 0.9464285714285714, f_beta: 0.939783219590526
train: step: 5600, loss: 0.012463870458304882, acc: 0.98, recall: 0.98, precision: 0.98, f_beta: 0.98
train: step: 5601, loss: 0.01202669832855463, acc: 0.99, recall: 0.99, precision: 0.9901960784313726, f_beta: 0.98999899989999
train: step: 5602, loss: 0.014376319944858551, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 5603, loss: 0.003458895720541477, acc: 1.0, recall: 1.0, precision: 1.0, f_beta: 1.0
train: step: 5604, loss: 0.04026390612125397, acc: 0.95, recall: 0.95, precision: 0.9501800720288115, f_beta: 0.9499949994999499
train: step: 5605, loss: 0.032848797738552094, acc: 0.96, recall: 0.96, precision: 0.96, f_beta: 0.96
train: step: 5606, loss: 0.04977481812238693, acc: 0.94, recall: 0.94, precision: 0.9407051282051282, f_beta: 0.9399759903961584
train: step: 5607, loss: 0.028275679796934128, acc: 0.97, recall: 0.97, precision: 0.9716981132075472, f_beta: 0.9699729756781104
train: step: 5608, loss: 0.024886108934879303, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 5609, loss: 0.017967119812965393, acc: 0.98, recall: 0.98, precision: 0.9807692307692308, f_beta: 0.9799919967987194
train: step: 5610, loss: 0.016899745911359787, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 5611, loss: 0.009627732448279858, acc: 0.98, recall: 0.98, precision: 0.9807692307692308, f_beta: 0.9799919967987194
train: step: 5612, loss: 0.04193969815969467, acc: 0.93, recall: 0.9299999999999999, precision: 0.9315535929345644, f_beta: 0.9299369432489241
train: step: 5613, loss: 0.02744290418922901, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 5614, loss: 0.010249695740640163, acc: 0.98, recall: 0.98, precision: 0.9807692307692308, f_beta: 0.9799919967987194
train: step: 5615, loss: 0.012254154309630394, acc: 0.98, recall: 0.98, precision: 0.98, f_beta: 0.98
train: step: 5616, loss: 0.04458797350525856, acc: 0.95, recall: 0.95, precision: 0.9501800720288115, f_beta: 0.9499949994999499
train: step: 5617, loss: 0.029395489022135735, acc: 0.97, recall: 0.97, precision: 0.9716981132075472, f_beta: 0.9699729756781104
train: step: 5618, loss: 0.02124849520623684, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 5619, loss: 0.02542164735496044, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 5620, loss: 0.025678100064396858, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 5621, loss: 0.00014295923756435513, acc: 1.0, recall: 1.0, precision: 1.0, f_beta: 1.0
train: step: 5622, loss: 0.010823304764926434, acc: 0.99, recall: 0.99, precision: 0.9901960784313726, f_beta: 0.98999899989999
train: step: 5623, loss: 0.0017482483526691794, acc: 1.0, recall: 1.0, precision: 1.0, f_beta: 1.0
train: step: 5624, loss: 0.020328983664512634, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 5625, loss: 0.012990318238735199, acc: 0.98, recall: 0.98, precision: 0.98, f_beta: 0.98
train: step: 5626, loss: 0.000665504252538085, acc: 1.0, recall: 1.0, precision: 1.0, f_beta: 1.0
train: step: 5627, loss: 0.026417119428515434, acc: 0.96, recall: 0.96, precision: 0.9607371794871795, f_beta: 0.959983993597439
train: step: 5628, loss: 0.016859162598848343, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 5629, loss: 0.022497927770018578, acc: 0.98, recall: 0.98, precision: 0.98, f_beta: 0.98
train: step: 5630, loss: 0.01652153581380844, acc: 0.98, recall: 0.98, precision: 0.9807692307692308, f_beta: 0.9799919967987194
train: step: 5631, loss: 0.022786643356084824, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 5632, loss: 0.01772073283791542, acc: 0.98, recall: 0.98, precision: 0.98, f_beta: 0.98
train: step: 5633, loss: 0.044332318007946014, acc: 0.95, recall: 0.95, precision: 0.9501800720288115, f_beta: 0.9499949994999499
train: step: 5634, loss: 0.06301994621753693, acc: 0.93, recall: 0.9299999999999999, precision: 0.930172068827531, f_beta: 0.9299929992999301
train: step: 5635, loss: 0.020199505612254143, acc: 0.98, recall: 0.98, precision: 0.9807692307692308, f_beta: 0.9799919967987194
train: step: 5636, loss: 0.044735029339790344, acc: 0.94, recall: 0.94, precision: 0.9407051282051282, f_beta: 0.9399759903961584
train: step: 5637, loss: 0.03125184401869774, acc: 0.96, recall: 0.96, precision: 0.9607371794871795, f_beta: 0.959983993597439
train: step: 5638, loss: 0.04512840136885643, acc: 0.93, recall: 0.9299999999999999, precision: 0.930172068827531, f_beta: 0.9299929992999301
train: step: 5639, loss: 0.017973583191633224, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 5640, loss: 0.05269862711429596, acc: 0.95, recall: 0.95, precision: 0.9501800720288115, f_beta: 0.9499949994999499
train: step: 5641, loss: 0.04479861259460449, acc: 0.94, recall: 0.94, precision: 0.9464285714285714, f_beta: 0.939783219590526
train: step: 5642, loss: 0.004240001086145639, acc: 0.99, recall: 0.99, precision: 0.9901960784313726, f_beta: 0.98999899989999
train: step: 5643, loss: 0.020750269293785095, acc: 0.97, recall: 0.97, precision: 0.9716981132075472, f_beta: 0.9699729756781104
train: step: 5644, loss: 0.014936319552361965, acc: 0.98, recall: 0.98, precision: 0.98, f_beta: 0.98
train: step: 5645, loss: 0.021771956235170364, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 5646, loss: 0.013152727857232094, acc: 0.99, recall: 0.99, precision: 0.9901960784313726, f_beta: 0.98999899989999
train: step: 5647, loss: 0.015020353719592094, acc: 0.99, recall: 0.99, precision: 0.9901960784313726, f_beta: 0.98999899989999
train: step: 5648, loss: 0.03441082313656807, acc: 0.96, recall: 0.96, precision: 0.9607371794871795, f_beta: 0.959983993597439
train: step: 5649, loss: 0.005591306835412979, acc: 0.99, recall: 0.99, precision: 0.9901960784313726, f_beta: 0.98999899989999
train: step: 5650, loss: 0.015491736121475697, acc: 0.97, recall: 0.97, precision: 0.9716981132075472, f_beta: 0.9699729756781104
train: step: 5651, loss: 0.025249173864722252, acc: 0.97, recall: 0.97, precision: 0.9716981132075472, f_beta: 0.9699729756781104
train: step: 5652, loss: 0.008216832764446735, acc: 0.99, recall: 0.99, precision: 0.9901960784313726, f_beta: 0.98999899989999
train: step: 5653, loss: 0.029825301840901375, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 5654, loss: 0.005954539403319359, acc: 0.99, recall: 0.99, precision: 0.9901960784313726, f_beta: 0.98999899989999
train: step: 5655, loss: 0.009796631522476673, acc: 0.99, recall: 0.99, precision: 0.9901960784313726, f_beta: 0.98999899989999
train: step: 5656, loss: 0.016183579340577126, acc: 0.98, recall: 0.98, precision: 0.9807692307692308, f_beta: 0.9799919967987194
train: step: 5657, loss: 0.015191649086773396, acc: 0.98, recall: 0.98, precision: 0.98, f_beta: 0.98
train: step: 5658, loss: 0.037138279527425766, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 5659, loss: 0.05991455167531967, acc: 0.92, recall: 0.9199999999999999, precision: 0.9227053140096618, f_beta: 0.9198717948717948
train: step: 5660, loss: 0.021992657333612442, acc: 0.98, recall: 0.98, precision: 0.9807692307692308, f_beta: 0.9799919967987194
train: step: 5661, loss: 0.018581796437501907, acc: 0.98, recall: 0.98, precision: 0.98, f_beta: 0.98
train: step: 5662, loss: 0.051543254405260086, acc: 0.94, recall: 0.94, precision: 0.9407051282051282, f_beta: 0.9399759903961584
train: step: 5663, loss: 0.017410583794116974, acc: 0.98, recall: 0.98, precision: 0.98, f_beta: 0.98
train: step: 5664, loss: 0.04404596984386444, acc: 0.95, recall: 0.95, precision: 0.9545454545454546, f_beta: 0.949874686716792
train: step: 5665, loss: 0.02857089973986149, acc: 0.96, recall: 0.96, precision: 0.9607371794871795, f_beta: 0.959983993597439
train: step: 5666, loss: 0.05091895908117294, acc: 0.95, recall: 0.95, precision: 0.9501800720288115, f_beta: 0.9499949994999499
train: step: 5667, loss: 0.04886564239859581, acc: 0.94, recall: 0.94, precision: 0.9428341384863124, f_beta: 0.9399038461538463
train: step: 5668, loss: 0.01069832593202591, acc: 0.99, recall: 0.99, precision: 0.9901960784313726, f_beta: 0.98999899989999
train: step: 5669, loss: 0.027406834065914154, acc: 0.96, recall: 0.96, precision: 0.9607371794871795, f_beta: 0.959983993597439
train: step: 5670, loss: 0.045920513570308685, acc: 0.94, recall: 0.94, precision: 0.9407051282051282, f_beta: 0.9399759903961584
train: step: 5671, loss: 0.018215229734778404, acc: 0.98, recall: 0.98, precision: 0.98, f_beta: 0.98
train: step: 5672, loss: 0.03879405930638313, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 5673, loss: 0.022040553390979767, acc: 0.98, recall: 0.98, precision: 0.9807692307692308, f_beta: 0.9799919967987194
train: step: 5674, loss: 0.03852855786681175, acc: 0.96, recall: 0.96, precision: 0.962962962962963, f_beta: 0.9599358974358974
train: step: 5675, loss: 0.04925099387764931, acc: 0.94, recall: 0.94, precision: 0.94, f_beta: 0.94
train: step: 5676, loss: 0.045091256499290466, acc: 0.95, recall: 0.95, precision: 0.9501800720288115, f_beta: 0.9499949994999499
train: step: 5677, loss: 0.050810813903808594, acc: 0.95, recall: 0.95, precision: 0.9516258530710557, f_beta: 0.949954959463517
train: step: 5678, loss: 0.02686392329633236, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 5679, loss: 0.019550912082195282, acc: 0.98, recall: 0.98, precision: 0.9807692307692308, f_beta: 0.9799919967987194
train: step: 5680, loss: 0.05609654262661934, acc: 0.91, recall: 0.9099999999999999, precision: 0.9141414141414141, f_beta: 0.9097744360902256
train: step: 5681, loss: 0.018223794177174568, acc: 0.98, recall: 0.98, precision: 0.98, f_beta: 0.98
train: step: 5682, loss: 0.027149811387062073, acc: 0.97, recall: 0.97, precision: 0.9716981132075472, f_beta: 0.9699729756781104
train: step: 5683, loss: 0.05371309816837311, acc: 0.95, recall: 0.95, precision: 0.9501800720288115, f_beta: 0.9499949994999499
train: step: 5684, loss: 0.023462969809770584, acc: 0.98, recall: 0.98, precision: 0.98, f_beta: 0.98
train: step: 5685, loss: 0.03667176142334938, acc: 0.95, recall: 0.95, precision: 0.9501800720288115, f_beta: 0.9499949994999499
train: step: 5686, loss: 0.014305710792541504, acc: 0.99, recall: 0.99, precision: 0.9901960784313726, f_beta: 0.98999899989999
train: step: 5687, loss: 0.010143251158297062, acc: 1.0, recall: 1.0, precision: 1.0, f_beta: 1.0
train: step: 5688, loss: 0.044901732355356216, acc: 0.94, recall: 0.94, precision: 0.9428341384863124, f_beta: 0.9399038461538463
train: step: 5689, loss: 0.04004095494747162, acc: 0.96, recall: 0.96, precision: 0.962962962962963, f_beta: 0.9599358974358974
train: step: 5690, loss: 0.020933764055371284, acc: 0.98, recall: 0.98, precision: 0.9807692307692308, f_beta: 0.9799919967987194
train: step: 5691, loss: 0.039281636476516724, acc: 0.96, recall: 0.96, precision: 0.96, f_beta: 0.96
train: step: 5692, loss: 0.02017100900411606, acc: 0.98, recall: 0.98, precision: 0.98, f_beta: 0.98
train: step: 5693, loss: 0.05704342946410179, acc: 0.93, recall: 0.9299999999999999, precision: 0.9385964912280702, f_beta: 0.9296553110240177
train: step: 5694, loss: 0.028892945498228073, acc: 0.96, recall: 0.96, precision: 0.96, f_beta: 0.96
train: step: 5695, loss: 0.01753263734281063, acc: 0.98, recall: 0.98, precision: 0.98, f_beta: 0.98
train: step: 5696, loss: 0.014806182123720646, acc: 0.98, recall: 0.98, precision: 0.98, f_beta: 0.98
train: step: 5697, loss: 0.015648916363716125, acc: 0.98, recall: 0.98, precision: 0.98, f_beta: 0.98
train: step: 5698, loss: 0.013175832107663155, acc: 0.99, recall: 0.99, precision: 0.9901960784313726, f_beta: 0.98999899989999
train: step: 5699, loss: 0.029922837391495705, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 5700, loss: 0.008871969766914845, acc: 0.99, recall: 0.99, precision: 0.9901960784313726, f_beta: 0.98999899989999
train: step: 5701, loss: 0.013721678406000137, acc: 0.98, recall: 0.98, precision: 0.9807692307692308, f_beta: 0.9799919967987194
train: step: 5702, loss: 0.018792133778333664, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 5703, loss: 0.010244245640933514, acc: 0.99, recall: 0.99, precision: 0.9901960784313726, f_beta: 0.98999899989999
train: step: 5704, loss: 0.010439536534249783, acc: 0.99, recall: 0.99, precision: 0.9901960784313726, f_beta: 0.98999899989999
train: step: 5705, loss: 0.012577121146023273, acc: 0.99, recall: 0.99, precision: 0.9901960784313726, f_beta: 0.98999899989999
train: step: 5706, loss: 0.020284369587898254, acc: 0.98, recall: 0.98, precision: 0.9807692307692308, f_beta: 0.9799919967987194
train: step: 5707, loss: 0.012467345222830772, acc: 0.99, recall: 0.99, precision: 0.9901960784313726, f_beta: 0.98999899989999
train: step: 5708, loss: 0.021495120599865913, acc: 0.98, recall: 0.98, precision: 0.98, f_beta: 0.98
train: step: 5709, loss: 0.03667251393198967, acc: 0.96, recall: 0.96, precision: 0.9607371794871795, f_beta: 0.959983993597439
train: step: 5710, loss: 0.02021702751517296, acc: 0.98, recall: 0.98, precision: 0.98, f_beta: 0.98
train: step: 5711, loss: 0.03575597330927849, acc: 0.96, recall: 0.96, precision: 0.96, f_beta: 0.96
train: step: 5712, loss: 0.01058261189609766, acc: 0.99, recall: 0.99, precision: 0.9901960784313726, f_beta: 0.98999899989999
train: step: 5713, loss: 0.01199929229915142, acc: 0.99, recall: 0.99, precision: 0.9901960784313726, f_beta: 0.98999899989999
train: step: 5714, loss: 0.03658977150917053, acc: 0.95, recall: 0.95, precision: 0.9545454545454546, f_beta: 0.949874686716792
train: step: 5715, loss: 0.02016364596784115, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 5716, loss: 0.04067261517047882, acc: 0.95, recall: 0.95, precision: 0.9516258530710557, f_beta: 0.949954959463517
train: step: 5717, loss: 0.005292885936796665, acc: 0.99, recall: 0.99, precision: 0.9901960784313726, f_beta: 0.98999899989999
train: step: 5718, loss: 0.03458287939429283, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 5719, loss: 0.017060399055480957, acc: 0.98, recall: 0.98, precision: 0.9807692307692308, f_beta: 0.9799919967987194
train: step: 5720, loss: 0.02209973894059658, acc: 0.98, recall: 0.98, precision: 0.98, f_beta: 0.98
train: step: 5721, loss: 0.027692941948771477, acc: 0.96, recall: 0.96, precision: 0.962962962962963, f_beta: 0.9599358974358974
train: step: 5722, loss: 0.0014388439012691379, acc: 1.0, recall: 1.0, precision: 1.0, f_beta: 1.0
train: step: 5723, loss: 0.03893057256937027, acc: 0.96, recall: 0.96, precision: 0.962962962962963, f_beta: 0.9599358974358974
train: step: 5724, loss: 0.044226840138435364, acc: 0.93, recall: 0.9299999999999999, precision: 0.9315535929345644, f_beta: 0.9299369432489241
train: step: 5725, loss: 0.020921869203448296, acc: 0.98, recall: 0.98, precision: 0.98, f_beta: 0.98
train: step: 5726, loss: 0.018501989543437958, acc: 0.98, recall: 0.98, precision: 0.9807692307692308, f_beta: 0.9799919967987194
train: step: 5727, loss: 0.016413219273090363, acc: 0.98, recall: 0.98, precision: 0.9807692307692308, f_beta: 0.9799919967987194
train: step: 5728, loss: 0.05588504299521446, acc: 0.94, recall: 0.94, precision: 0.9407051282051282, f_beta: 0.9399759903961584
train: step: 5729, loss: 0.021292459219694138, acc: 0.98, recall: 0.98, precision: 0.98, f_beta: 0.98
train: step: 5730, loss: 0.028071029111742973, acc: 0.95, recall: 0.95, precision: 0.9501800720288115, f_beta: 0.9499949994999499
train: step: 5731, loss: 0.02142155170440674, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 5732, loss: 0.012079098261892796, acc: 0.99, recall: 0.99, precision: 0.9901960784313726, f_beta: 0.98999899989999
train: step: 5733, loss: 0.004386125598102808, acc: 0.99, recall: 0.99, precision: 0.9901960784313726, f_beta: 0.98999899989999
train: step: 5734, loss: 0.031675394624471664, acc: 0.96, recall: 0.96, precision: 0.9607371794871795, f_beta: 0.959983993597439
train: step: 5735, loss: 0.016492528840899467, acc: 0.99, recall: 0.99, precision: 0.9901960784313726, f_beta: 0.98999899989999
train: step: 5736, loss: 0.016255641356110573, acc: 0.99, recall: 0.99, precision: 0.9901960784313726, f_beta: 0.98999899989999
train: step: 5737, loss: 0.10481834411621094, acc: 0.88, recall: 0.88, precision: 0.8958333333333333, f_beta: 0.8787878787878789
train: step: 5738, loss: 0.0024719845969229937, acc: 1.0, recall: 1.0, precision: 1.0, f_beta: 1.0
train: step: 5739, loss: 0.01094704307615757, acc: 0.98, recall: 0.98, precision: 0.98, f_beta: 0.98
train: step: 5740, loss: 0.029291167855262756, acc: 0.95, recall: 0.95, precision: 0.9516258530710557, f_beta: 0.949954959463517
train: step: 5741, loss: 0.030738649889826775, acc: 0.95, recall: 0.95, precision: 0.9501800720288115, f_beta: 0.9499949994999499
train: step: 5742, loss: 0.014178070239722729, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 5743, loss: 0.0259490255266428, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 5744, loss: 0.014523131772875786, acc: 0.98, recall: 0.98, precision: 0.9807692307692308, f_beta: 0.9799919967987194
train: step: 5745, loss: 0.04875510558485985, acc: 0.94, recall: 0.94, precision: 0.94, f_beta: 0.94
train: step: 5746, loss: 0.01936223916709423, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 5747, loss: 0.043621063232421875, acc: 0.96, recall: 0.96, precision: 0.96, f_beta: 0.96
train: step: 5748, loss: 0.0362304225564003, acc: 0.95, recall: 0.95, precision: 0.9501800720288115, f_beta: 0.9499949994999499
train: step: 5749, loss: 0.018048692494630814, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 5750, loss: 0.03953947499394417, acc: 0.96, recall: 0.96, precision: 0.96, f_beta: 0.96
train: step: 5751, loss: 0.00646068574860692, acc: 0.99, recall: 0.99, precision: 0.9901960784313726, f_beta: 0.98999899989999
train: step: 5752, loss: 0.03163478150963783, acc: 0.95, recall: 0.95, precision: 0.9501800720288115, f_beta: 0.9499949994999499
train: step: 5753, loss: 0.022331619635224342, acc: 0.98, recall: 0.98, precision: 0.9807692307692308, f_beta: 0.9799919967987194
train: step: 5754, loss: 0.016367845237255096, acc: 0.98, recall: 0.98, precision: 0.9807692307692308, f_beta: 0.9799919967987194
train: step: 5755, loss: 0.0014982303837314248, acc: 1.0, recall: 1.0, precision: 1.0, f_beta: 1.0
train: step: 5756, loss: 0.0013404489727690816, acc: 1.0, recall: 1.0, precision: 1.0, f_beta: 1.0
train: step: 5757, loss: 0.026840301230549812, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 5758, loss: 0.018039606511592865, acc: 0.98, recall: 0.98, precision: 0.98, f_beta: 0.98
train: step: 5759, loss: 0.010746130719780922, acc: 0.99, recall: 0.99, precision: 0.9901960784313726, f_beta: 0.98999899989999
train: step: 5760, loss: 0.040528878569602966, acc: 0.96, recall: 0.96, precision: 0.96, f_beta: 0.96
train: step: 5761, loss: 0.006959112361073494, acc: 0.99, recall: 0.99, precision: 0.9901960784313726, f_beta: 0.98999899989999
train: step: 5762, loss: 0.013542763888835907, acc: 0.98, recall: 0.98, precision: 0.98, f_beta: 0.98
train: step: 5763, loss: 0.011933999136090279, acc: 0.99, recall: 0.99, precision: 0.9901960784313726, f_beta: 0.98999899989999
train: step: 5764, loss: 0.019768040627241135, acc: 0.97, recall: 0.97, precision: 0.9716981132075472, f_beta: 0.9699729756781104
train: step: 5765, loss: 0.005449818447232246, acc: 0.99, recall: 0.99, precision: 0.9901960784313726, f_beta: 0.98999899989999
train: step: 5766, loss: 0.013859386555850506, acc: 0.98, recall: 0.98, precision: 0.98, f_beta: 0.98
train: step: 5767, loss: 0.010547326877713203, acc: 0.99, recall: 0.99, precision: 0.9901960784313726, f_beta: 0.98999899989999
train: step: 5768, loss: 0.026295147836208344, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 5769, loss: 0.01599603332579136, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 5770, loss: 0.03462499752640724, acc: 0.96, recall: 0.96, precision: 0.96, f_beta: 0.96
train: step: 5771, loss: 0.010311875492334366, acc: 0.99, recall: 0.99, precision: 0.9901960784313726, f_beta: 0.98999899989999
train: step: 5772, loss: 0.02289012260735035, acc: 0.98, recall: 0.98, precision: 0.9807692307692308, f_beta: 0.9799919967987194
train: step: 5773, loss: 0.04637608677148819, acc: 0.93, recall: 0.9299999999999999, precision: 0.930172068827531, f_beta: 0.9299929992999301
train: step: 5774, loss: 0.039058391004800797, acc: 0.95, recall: 0.95, precision: 0.9516258530710557, f_beta: 0.949954959463517
train: step: 5775, loss: 0.030642205849289894, acc: 0.96, recall: 0.96, precision: 0.9607371794871795, f_beta: 0.959983993597439
train: step: 5776, loss: 0.010638700798153877, acc: 0.99, recall: 0.99, precision: 0.9901960784313726, f_beta: 0.98999899989999
train: step: 5777, loss: 0.0372561514377594, acc: 0.96, recall: 0.96, precision: 0.9607371794871795, f_beta: 0.959983993597439
train: step: 5778, loss: 0.01940855197608471, acc: 0.98, recall: 0.98, precision: 0.98, f_beta: 0.98
train: step: 5779, loss: 0.017386773601174355, acc: 0.98, recall: 0.98, precision: 0.9807692307692308, f_beta: 0.9799919967987194
train: step: 5780, loss: 0.043497148901224136, acc: 0.95, recall: 0.95, precision: 0.9501800720288115, f_beta: 0.9499949994999499
train: step: 5781, loss: 0.009359294548630714, acc: 0.99, recall: 0.99, precision: 0.9901960784313726, f_beta: 0.98999899989999
train: step: 5782, loss: 0.028025761246681213, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 5783, loss: 0.017782799899578094, acc: 0.97, recall: 0.97, precision: 0.9716981132075472, f_beta: 0.9699729756781104
train: step: 5784, loss: 0.02091589942574501, acc: 0.98, recall: 0.98, precision: 0.9807692307692308, f_beta: 0.9799919967987194
train: step: 5785, loss: 0.023479675874114037, acc: 0.98, recall: 0.98, precision: 0.9807692307692308, f_beta: 0.9799919967987194
train: step: 5786, loss: 0.022340985015034676, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 5787, loss: 0.01370013877749443, acc: 0.98, recall: 0.98, precision: 0.9807692307692308, f_beta: 0.9799919967987194
train: step: 5788, loss: 0.008242153562605381, acc: 1.0, recall: 1.0, precision: 1.0, f_beta: 1.0
train: step: 5789, loss: 0.031800564378499985, acc: 0.96, recall: 0.96, precision: 0.9607371794871795, f_beta: 0.959983993597439
train: step: 5790, loss: 0.00358020537532866, acc: 1.0, recall: 1.0, precision: 1.0, f_beta: 1.0
train: step: 5791, loss: 0.03272613510489464, acc: 0.95, recall: 0.95, precision: 0.9501800720288115, f_beta: 0.9499949994999499
train: step: 5792, loss: 0.030772538855671883, acc: 0.97, recall: 0.97, precision: 0.9716981132075472, f_beta: 0.9699729756781104
train: step: 5793, loss: 0.025577181950211525, acc: 0.97, recall: 0.97, precision: 0.9716981132075472, f_beta: 0.9699729756781104
train: step: 5794, loss: 0.009569928981363773, acc: 0.99, recall: 0.99, precision: 0.9901960784313726, f_beta: 0.98999899989999
train: step: 5795, loss: 0.056295379996299744, acc: 0.94, recall: 0.94, precision: 0.9407051282051282, f_beta: 0.9399759903961584
train: step: 5796, loss: 0.05301244556903839, acc: 0.94, recall: 0.94, precision: 0.9407051282051282, f_beta: 0.9399759903961584
train: step: 5797, loss: 0.049779314547777176, acc: 0.94, recall: 0.94, precision: 0.94, f_beta: 0.94
train: step: 5798, loss: 0.004960397258400917, acc: 0.99, recall: 0.99, precision: 0.9901960784313726, f_beta: 0.98999899989999
train: step: 5799, loss: 0.01666085422039032, acc: 0.98, recall: 0.98, precision: 0.9807692307692308, f_beta: 0.9799919967987194
train: step: 5800, loss: 0.0007931157015264034, acc: 1.0, recall: 1.0, precision: 1.0, f_beta: 1.0
train: step: 5801, loss: 0.010645194910466671, acc: 0.98, recall: 0.98, precision: 0.98, f_beta: 0.98
train: step: 5802, loss: 0.012593069113790989, acc: 0.99, recall: 0.99, precision: 0.9901960784313726, f_beta: 0.98999899989999
train: step: 5803, loss: 0.04563022032380104, acc: 0.95, recall: 0.95, precision: 0.9516258530710557, f_beta: 0.949954959463517
train: step: 5804, loss: 0.020488452166318893, acc: 0.98, recall: 0.98, precision: 0.98, f_beta: 0.98
train: step: 5805, loss: 0.016957256942987442, acc: 0.98, recall: 0.98, precision: 0.9807692307692308, f_beta: 0.9799919967987194
train: step: 5806, loss: 0.03030790388584137, acc: 0.96, recall: 0.96, precision: 0.9607371794871795, f_beta: 0.959983993597439
train: step: 5807, loss: 0.03963135555386543, acc: 0.95, recall: 0.95, precision: 0.9501800720288115, f_beta: 0.9499949994999499
train: step: 5808, loss: 0.013929758220911026, acc: 0.98, recall: 0.98, precision: 0.98, f_beta: 0.98
train: step: 5809, loss: 0.011920413933694363, acc: 0.99, recall: 0.99, precision: 0.9901960784313726, f_beta: 0.98999899989999
train: step: 5810, loss: 0.019261859357357025, acc: 0.98, recall: 0.98, precision: 0.98, f_beta: 0.98
train: step: 5811, loss: 0.026831893250346184, acc: 0.96, recall: 0.96, precision: 0.962962962962963, f_beta: 0.9599358974358974
train: step: 5812, loss: 0.016712523996829987, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 5813, loss: 0.023725606501102448, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 5814, loss: 0.07225098460912704, acc: 0.92, recall: 0.9199999999999999, precision: 0.9206730769230769, f_beta: 0.919967987194878
train: step: 5815, loss: 0.03109677881002426, acc: 0.96, recall: 0.96, precision: 0.9607371794871795, f_beta: 0.959983993597439
train: step: 5816, loss: 0.014624432660639286, acc: 0.99, recall: 0.99, precision: 0.9901960784313726, f_beta: 0.98999899989999
train: step: 5817, loss: 0.05970173701643944, acc: 0.94, recall: 0.94, precision: 0.9407051282051282, f_beta: 0.9399759903961584
train: step: 5818, loss: 0.010914082638919353, acc: 0.99, recall: 0.99, precision: 0.9901960784313726, f_beta: 0.98999899989999
train: step: 5819, loss: 0.03663985803723335, acc: 0.96, recall: 0.96, precision: 0.96, f_beta: 0.96
train: step: 5820, loss: 0.030572323128581047, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 5821, loss: 0.043150100857019424, acc: 0.95, recall: 0.95, precision: 0.9516258530710557, f_beta: 0.949954959463517
train: step: 5822, loss: 0.02279088646173477, acc: 0.98, recall: 0.98, precision: 0.98, f_beta: 0.98
train: step: 5823, loss: 0.023647429421544075, acc: 0.98, recall: 0.98, precision: 0.98, f_beta: 0.98
train: step: 5824, loss: 0.027290090918540955, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 5825, loss: 0.015614139847457409, acc: 0.98, recall: 0.98, precision: 0.9807692307692308, f_beta: 0.9799919967987194
train: step: 5826, loss: 0.020217858254909515, acc: 0.98, recall: 0.98, precision: 0.9807692307692308, f_beta: 0.9799919967987194
train: step: 5827, loss: 0.025822114199399948, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 5828, loss: 0.022595668211579323, acc: 0.96, recall: 0.96, precision: 0.9607371794871795, f_beta: 0.959983993597439
train: step: 5829, loss: 0.029467793181538582, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 5830, loss: 0.00018758645455818623, acc: 1.0, recall: 1.0, precision: 1.0, f_beta: 1.0
train: step: 5831, loss: 0.0412980318069458, acc: 0.95, recall: 0.95, precision: 0.9516258530710557, f_beta: 0.949954959463517
train: step: 5832, loss: 0.023959798738360405, acc: 0.97, recall: 0.97, precision: 0.9716981132075472, f_beta: 0.9699729756781104
train: step: 5833, loss: 0.03527290001511574, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 5834, loss: 0.007824008353054523, acc: 0.99, recall: 0.99, precision: 0.9901960784313726, f_beta: 0.98999899989999
train: step: 5835, loss: 0.0021001335699111223, acc: 1.0, recall: 1.0, precision: 1.0, f_beta: 1.0
train: step: 5836, loss: 0.04047366976737976, acc: 0.96, recall: 0.96, precision: 0.96, f_beta: 0.96
train: step: 5837, loss: 0.012552490457892418, acc: 0.99, recall: 0.99, precision: 0.9901960784313726, f_beta: 0.98999899989999
train: step: 5838, loss: 0.013387263752520084, acc: 0.98, recall: 0.98, precision: 0.98, f_beta: 0.98
train: step: 5839, loss: 0.044989097863435745, acc: 0.94, recall: 0.94, precision: 0.9464285714285714, f_beta: 0.939783219590526
train: step: 5840, loss: 0.018937673419713974, acc: 0.97, recall: 0.97, precision: 0.9716981132075472, f_beta: 0.9699729756781104
train: step: 5841, loss: 0.013881432823836803, acc: 0.99, recall: 0.99, precision: 0.9901960784313726, f_beta: 0.98999899989999
train: step: 5842, loss: 0.01100845355540514, acc: 0.99, recall: 0.99, precision: 0.9901960784313726, f_beta: 0.98999899989999
train: step: 5843, loss: 0.012392132543027401, acc: 0.99, recall: 0.99, precision: 0.9901960784313726, f_beta: 0.98999899989999
train: step: 5844, loss: 0.025205358862876892, acc: 0.98, recall: 0.98, precision: 0.9807692307692308, f_beta: 0.9799919967987194
train: step: 5845, loss: 0.032115593552589417, acc: 0.96, recall: 0.96, precision: 0.96, f_beta: 0.96
train: step: 5846, loss: 0.013114707544445992, acc: 0.99, recall: 0.99, precision: 0.9901960784313726, f_beta: 0.98999899989999
train: step: 5847, loss: 0.025991057977080345, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 5848, loss: 0.03615352511405945, acc: 0.96, recall: 0.96, precision: 0.96, f_beta: 0.96
train: step: 5849, loss: 0.03881610929965973, acc: 0.96, recall: 0.96, precision: 0.96, f_beta: 0.96
train: step: 5850, loss: 0.018719922751188278, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 5851, loss: 0.014732594601809978, acc: 0.98, recall: 0.98, precision: 0.98, f_beta: 0.98
train: step: 5852, loss: 0.00866029504686594, acc: 0.99, recall: 0.99, precision: 0.9901960784313726, f_beta: 0.98999899989999
train: step: 5853, loss: 0.02084510400891304, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 5854, loss: 0.010200920514762402, acc: 0.99, recall: 0.99, precision: 0.9901960784313726, f_beta: 0.98999899989999
train: step: 5855, loss: 0.009020550176501274, acc: 0.99, recall: 0.99, precision: 0.9901960784313726, f_beta: 0.98999899989999
train: step: 5856, loss: 0.026241935789585114, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 5857, loss: 0.05441875383257866, acc: 0.93, recall: 0.9299999999999999, precision: 0.9315535929345644, f_beta: 0.9299369432489241
train: step: 5858, loss: 0.01448441855609417, acc: 0.99, recall: 0.99, precision: 0.9901960784313726, f_beta: 0.98999899989999
train: step: 5859, loss: 0.04283883050084114, acc: 0.96, recall: 0.96, precision: 0.962962962962963, f_beta: 0.9599358974358974
train: step: 5860, loss: 0.008892714977264404, acc: 0.99, recall: 0.99, precision: 0.9901960784313726, f_beta: 0.98999899989999
train: step: 5861, loss: 0.01403515413403511, acc: 0.99, recall: 0.99, precision: 0.9901960784313726, f_beta: 0.98999899989999
train: step: 5862, loss: 0.004064968321472406, acc: 0.99, recall: 0.99, precision: 0.9901960784313726, f_beta: 0.98999899989999
train: step: 5863, loss: 0.02952692098915577, acc: 0.96, recall: 0.96, precision: 0.9607371794871795, f_beta: 0.959983993597439
train: step: 5864, loss: 0.001986756920814514, acc: 1.0, recall: 1.0, precision: 1.0, f_beta: 1.0
train: step: 5865, loss: 0.010051589459180832, acc: 0.98, recall: 0.98, precision: 0.98, f_beta: 0.98
train: step: 5866, loss: 0.018418259918689728, acc: 0.98, recall: 0.98, precision: 0.9807692307692308, f_beta: 0.9799919967987194
train: step: 5867, loss: 0.007123786956071854, acc: 0.99, recall: 0.99, precision: 0.9901960784313726, f_beta: 0.98999899989999
train: step: 5868, loss: 0.02194681577384472, acc: 0.98, recall: 0.98, precision: 0.98, f_beta: 0.98
train: step: 5869, loss: 0.03295191377401352, acc: 0.96, recall: 0.96, precision: 0.9607371794871795, f_beta: 0.959983993597439
train: step: 5870, loss: 0.027574218809604645, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 5871, loss: 0.013850849121809006, acc: 0.99, recall: 0.99, precision: 0.9901960784313726, f_beta: 0.98999899989999
train: step: 5872, loss: 0.014343350194394588, acc: 0.99, recall: 0.99, precision: 0.9901960784313726, f_beta: 0.98999899989999
train: step: 5873, loss: 0.004328731447458267, acc: 1.0, recall: 1.0, precision: 1.0, f_beta: 1.0
train: step: 5874, loss: 0.048026472330093384, acc: 0.95, recall: 0.95, precision: 0.9516258530710557, f_beta: 0.949954959463517
train: step: 5875, loss: 0.018044380471110344, acc: 0.98, recall: 0.98, precision: 0.98, f_beta: 0.98
train: step: 5876, loss: 0.016791880130767822, acc: 0.98, recall: 0.98, precision: 0.9807692307692308, f_beta: 0.9799919967987194
train: step: 5877, loss: 0.0017713048728182912, acc: 1.0, recall: 1.0, precision: 1.0, f_beta: 1.0
train: step: 5878, loss: 0.0002805770200211555, acc: 1.0, recall: 1.0, precision: 1.0, f_beta: 1.0
train: step: 5879, loss: 0.02605959214270115, acc: 0.96, recall: 0.96, precision: 0.96, f_beta: 0.96
train: step: 5880, loss: 0.010999312624335289, acc: 0.99, recall: 0.99, precision: 0.9901960784313726, f_beta: 0.98999899989999
train: step: 5881, loss: 0.023654799908399582, acc: 0.98, recall: 0.98, precision: 0.9807692307692308, f_beta: 0.9799919967987194
train: step: 5882, loss: 0.023203687742352486, acc: 0.97, recall: 0.97, precision: 0.9716981132075472, f_beta: 0.9699729756781104
train: step: 5883, loss: 0.03482622280716896, acc: 0.96, recall: 0.96, precision: 0.9607371794871795, f_beta: 0.959983993597439
train: step: 5884, loss: 0.006813745480030775, acc: 0.99, recall: 0.99, precision: 0.9901960784313726, f_beta: 0.98999899989999
train: step: 5885, loss: 0.03093336895108223, acc: 0.96, recall: 0.96, precision: 0.96, f_beta: 0.96
train: step: 5886, loss: 0.012724974192678928, acc: 0.99, recall: 0.99, precision: 0.9901960784313726, f_beta: 0.98999899989999
train: step: 5887, loss: 0.039756890386343, acc: 0.96, recall: 0.96, precision: 0.9607371794871795, f_beta: 0.959983993597439
train: step: 5888, loss: 0.022068290039896965, acc: 0.97, recall: 0.97, precision: 0.9716981132075472, f_beta: 0.9699729756781104
train: step: 5889, loss: 0.02451760694384575, acc: 0.97, recall: 0.97, precision: 0.9716981132075472, f_beta: 0.9699729756781104
train: step: 5890, loss: 0.03492016717791557, acc: 0.96, recall: 0.96, precision: 0.9607371794871795, f_beta: 0.959983993597439
train: step: 5891, loss: 0.047630272805690765, acc: 0.93, recall: 0.9299999999999999, precision: 0.9343434343434343, f_beta: 0.9298245614035088
train: step: 5892, loss: 0.010987997055053711, acc: 0.99, recall: 0.99, precision: 0.9901960784313726, f_beta: 0.98999899989999
train: step: 5893, loss: 0.017449595034122467, acc: 0.98, recall: 0.98, precision: 0.98, f_beta: 0.98
train: step: 5894, loss: 0.023775262758135796, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 5895, loss: 0.030957955867052078, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 5896, loss: 0.03466595709323883, acc: 0.95, recall: 0.95, precision: 0.9501800720288115, f_beta: 0.9499949994999499
train: step: 5897, loss: 0.017318958416581154, acc: 0.98, recall: 0.98, precision: 0.9807692307692308, f_beta: 0.9799919967987194
train: step: 5898, loss: 0.014246635138988495, acc: 0.98, recall: 0.98, precision: 0.9807692307692308, f_beta: 0.9799919967987194
train: step: 5899, loss: 0.02397465892136097, acc: 0.98, recall: 0.98, precision: 0.9807692307692308, f_beta: 0.9799919967987194
train: step: 5900, loss: 0.014858412556350231, acc: 0.99, recall: 0.99, precision: 0.9901960784313726, f_beta: 0.98999899989999
train: step: 5901, loss: 0.048826683312654495, acc: 0.94, recall: 0.94, precision: 0.9407051282051282, f_beta: 0.9399759903961584
train: step: 5902, loss: 0.017707686871290207, acc: 0.98, recall: 0.98, precision: 0.9807692307692308, f_beta: 0.9799919967987194
train: step: 5903, loss: 0.01921871490776539, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 5904, loss: 0.010654360055923462, acc: 0.99, recall: 0.99, precision: 0.9901960784313726, f_beta: 0.98999899989999
train: step: 5905, loss: 0.00864528026431799, acc: 0.98, recall: 0.98, precision: 0.98, f_beta: 0.98
train: step: 5906, loss: 0.0127619793638587, acc: 0.99, recall: 0.99, precision: 0.9901960784313726, f_beta: 0.98999899989999
train: step: 5907, loss: 0.006779507268220186, acc: 0.99, recall: 0.99, precision: 0.9901960784313726, f_beta: 0.98999899989999
train: step: 5908, loss: 0.014840969815850258, acc: 0.98, recall: 0.98, precision: 0.9807692307692308, f_beta: 0.9799919967987194
train: step: 5909, loss: 0.020550422370433807, acc: 0.98, recall: 0.98, precision: 0.9807692307692308, f_beta: 0.9799919967987194
train: step: 5910, loss: 0.04183511808514595, acc: 0.95, recall: 0.95, precision: 0.9501800720288115, f_beta: 0.9499949994999499
train: step: 5911, loss: 0.058105822652578354, acc: 0.93, recall: 0.9299999999999999, precision: 0.9315535929345644, f_beta: 0.9299369432489241
train: step: 5912, loss: 0.013359402306377888, acc: 0.98, recall: 0.98, precision: 0.9807692307692308, f_beta: 0.9799919967987194
train: step: 5913, loss: 0.026849431917071342, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 5914, loss: 0.006235635373741388, acc: 0.99, recall: 0.99, precision: 0.9901960784313726, f_beta: 0.98999899989999
train: step: 5915, loss: 0.006328942719846964, acc: 0.99, recall: 0.99, precision: 0.9901960784313726, f_beta: 0.98999899989999
train: step: 5916, loss: 0.03878672048449516, acc: 0.96, recall: 0.96, precision: 0.962962962962963, f_beta: 0.9599358974358974
train: step: 5917, loss: 0.006541927345097065, acc: 0.99, recall: 0.99, precision: 0.9901960784313726, f_beta: 0.98999899989999
train: step: 5918, loss: 0.023885250091552734, acc: 0.97, recall: 0.97, precision: 0.9716981132075472, f_beta: 0.9699729756781104
train: step: 5919, loss: 0.031116848811507225, acc: 0.96, recall: 0.96, precision: 0.9607371794871795, f_beta: 0.959983993597439
train: step: 5920, loss: 0.01068369671702385, acc: 0.99, recall: 0.99, precision: 0.9901960784313726, f_beta: 0.98999899989999
train: step: 5921, loss: 0.039365142583847046, acc: 0.94, recall: 0.94, precision: 0.9428341384863124, f_beta: 0.9399038461538463
train: step: 5922, loss: 0.023952364921569824, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 5923, loss: 0.03174448385834694, acc: 0.96, recall: 0.96, precision: 0.9607371794871795, f_beta: 0.959983993597439
train: step: 5924, loss: 0.005312114953994751, acc: 0.99, recall: 0.99, precision: 0.9901960784313726, f_beta: 0.98999899989999
train: step: 5925, loss: 0.00848119705915451, acc: 0.99, recall: 0.99, precision: 0.9901960784313726, f_beta: 0.98999899989999
train: step: 5926, loss: 0.016946308314800262, acc: 0.98, recall: 0.98, precision: 0.9807692307692308, f_beta: 0.9799919967987194
train: step: 5927, loss: 0.00902016181498766, acc: 0.99, recall: 0.99, precision: 0.9901960784313726, f_beta: 0.98999899989999
train: step: 5928, loss: 0.012384394183754921, acc: 0.99, recall: 0.99, precision: 0.9901960784313726, f_beta: 0.98999899989999
train: step: 5929, loss: 0.030962178483605385, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 5930, loss: 0.009587750770151615, acc: 0.99, recall: 0.99, precision: 0.9901960784313726, f_beta: 0.98999899989999
train: step: 5931, loss: 0.024211609736084938, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 5932, loss: 0.016731493175029755, acc: 0.98, recall: 0.98, precision: 0.98, f_beta: 0.98
train: step: 5933, loss: 0.009979129768908024, acc: 0.99, recall: 0.99, precision: 0.9901960784313726, f_beta: 0.98999899989999
train: step: 5934, loss: 0.004073034040629864, acc: 1.0, recall: 1.0, precision: 1.0, f_beta: 1.0
train: step: 5935, loss: 0.015182189643383026, acc: 0.98, recall: 0.98, precision: 0.9807692307692308, f_beta: 0.9799919967987194
train: step: 5936, loss: 0.02301161363720894, acc: 0.96, recall: 0.96, precision: 0.9607371794871795, f_beta: 0.959983993597439
train: step: 5937, loss: 0.015353010967373848, acc: 0.98, recall: 0.98, precision: 0.9807692307692308, f_beta: 0.9799919967987194
train: step: 5938, loss: 0.0441230908036232, acc: 0.94, recall: 0.94, precision: 0.9464285714285714, f_beta: 0.939783219590526
train: step: 5939, loss: 0.00035158044192939997, acc: 1.0, recall: 1.0, precision: 1.0, f_beta: 1.0
train: step: 5940, loss: 0.043303366750478745, acc: 0.94, recall: 0.94, precision: 0.9407051282051282, f_beta: 0.9399759903961584
train: step: 5941, loss: 0.060006700456142426, acc: 0.94, recall: 0.94, precision: 0.9428341384863124, f_beta: 0.9399038461538463
train: step: 5942, loss: 0.04903928190469742, acc: 0.93, recall: 0.9299999999999999, precision: 0.930172068827531, f_beta: 0.9299929992999301
train: step: 5943, loss: 0.03093774989247322, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 5944, loss: 0.004178048577159643, acc: 1.0, recall: 1.0, precision: 1.0, f_beta: 1.0
train: step: 5945, loss: 0.02205178514122963, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 5946, loss: 0.018179209902882576, acc: 0.99, recall: 0.99, precision: 0.9901960784313726, f_beta: 0.98999899989999
train: step: 5947, loss: 0.00620797136798501, acc: 0.99, recall: 0.99, precision: 0.9901960784313726, f_beta: 0.98999899989999
train: step: 5948, loss: 0.019132332876324654, acc: 0.98, recall: 0.98, precision: 0.98, f_beta: 0.98
train: step: 5949, loss: 0.018923021852970123, acc: 0.97, recall: 0.97, precision: 0.9716981132075472, f_beta: 0.9699729756781104
train: step: 5950, loss: 0.018248176202178, acc: 0.98, recall: 0.98, precision: 0.9807692307692308, f_beta: 0.9799919967987194
train: step: 5951, loss: 0.03171851113438606, acc: 0.96, recall: 0.96, precision: 0.96, f_beta: 0.96
train: step: 5952, loss: 0.03063512034714222, acc: 0.96, recall: 0.96, precision: 0.9607371794871795, f_beta: 0.959983993597439
train: step: 5953, loss: 0.00307847885414958, acc: 1.0, recall: 1.0, precision: 1.0, f_beta: 1.0
train: step: 5954, loss: 0.02594691328704357, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 5955, loss: 0.003233697498217225, acc: 0.99, recall: 0.99, precision: 0.9901960784313726, f_beta: 0.98999899989999
train: step: 5956, loss: 0.02433610148727894, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 5957, loss: 0.015101559460163116, acc: 0.98, recall: 0.98, precision: 0.9807692307692308, f_beta: 0.9799919967987194
train: step: 5958, loss: 0.03286360204219818, acc: 0.97, recall: 0.97, precision: 0.9716981132075472, f_beta: 0.9699729756781104
train: step: 5959, loss: 0.010390806943178177, acc: 0.99, recall: 0.99, precision: 0.9901960784313726, f_beta: 0.98999899989999
train: step: 5960, loss: 0.01792193576693535, acc: 0.98, recall: 0.98, precision: 0.98, f_beta: 0.98
train: step: 5961, loss: 0.0187248345464468, acc: 0.98, recall: 0.98, precision: 0.9807692307692308, f_beta: 0.9799919967987194
train: step: 5962, loss: 0.034785982221364975, acc: 0.96, recall: 0.96, precision: 0.96, f_beta: 0.96
train: step: 5963, loss: 0.028974125161767006, acc: 0.96, recall: 0.96, precision: 0.96, f_beta: 0.96
train: step: 5964, loss: 0.033827800303697586, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 5965, loss: 0.02327258512377739, acc: 0.98, recall: 0.98, precision: 0.98, f_beta: 0.98
train: step: 5966, loss: 0.02223970741033554, acc: 0.98, recall: 0.98, precision: 0.9807692307692308, f_beta: 0.9799919967987194
train: step: 5967, loss: 0.0023052184842526913, acc: 1.0, recall: 1.0, precision: 1.0, f_beta: 1.0
train: step: 5968, loss: 0.00985831767320633, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 5969, loss: 0.037495438009500504, acc: 0.96, recall: 0.96, precision: 0.9607371794871795, f_beta: 0.959983993597439
train: step: 5970, loss: 0.02135593444108963, acc: 0.96, recall: 0.96, precision: 0.96, f_beta: 0.96
train: step: 5971, loss: 0.03380702808499336, acc: 0.96, recall: 0.96, precision: 0.9607371794871795, f_beta: 0.959983993597439
train: step: 5972, loss: 0.030591769143939018, acc: 0.96, recall: 0.96, precision: 0.96, f_beta: 0.96
train: step: 5973, loss: 0.006587663199752569, acc: 0.99, recall: 0.99, precision: 0.9901960784313726, f_beta: 0.98999899989999
train: step: 5974, loss: 0.015269491821527481, acc: 0.97, recall: 0.97, precision: 0.970188075230092, f_beta: 0.96999699969997
train: step: 5975, loss: 0.03657098114490509, acc: 0.96, recall: 0.96, precision: 0.9607371794871795, f_beta: 0.959983993597439
train: step: 5976, loss: 0.04851163551211357, acc: 0.95, recall: 0.95, precision: 0.9501800720288115, f_beta: 0.9499949994999499
train: step: 5977, loss: 0.04262833297252655, acc: 0.95, recall: 0.95, precision: 0.9516258530710557, f_beta: 0.949954959463517
train: step: 5978, loss: 0.028544418513774872, acc: 0.96, recall: 0.96, precision: 0.9607371794871795, f_beta: 0.959983993597439
train: step: 5979, loss: 0.052250269800424576, acc: 0.94, recall: 0.94, precision: 0.9407051282051282, f_beta: 0.9399759903961584
train: step: 5980, loss: 0.025775227695703506, acc: 0.96, recall: 0.96, precision: 0.9607371794871795, f_beta: 0.959983993597439
train: step: 5981, loss: 0.024059182032942772, acc: 0.96, recall: 0.96, precision: 0.96, f_beta: 0.96
train: step: 5982, loss: 0.03552597016096115, acc: 0.96, recall: 0.96, precision: 0.962962962962963, f_beta: 0.9599358974358974
train: step: 5983, loss: 0.01971113495528698, acc: 0.98, recall: 0.98, precision: 0.9807692307692308, f_beta: 0.9799919967987194
train: step: 5984, loss: 0.01317598670721054, acc: 0.99, recall: 0.99, precision: 0.9901960784313726, f_beta: 0.98999899989999
train: step: 5985, loss: 0.03128790110349655, acc: 0.97, recall: 0.97, precision: 0.9716981132075472, f_beta: 0.9699729756781104
train: step: 5986, loss: 0.02130279503762722, acc: 0.98, recall: 0.98, precision: 0.98, f_beta: 0.98
train: step: 5987, loss: 0.0393090546131134, acc: 0.96, recall: 0.96, precision: 0.96, f_beta: 0.96
train: step: 5988, loss: 0.03712301328778267, acc: 0.95, recall: 0.95, precision: 0.9516258530710557, f_beta: 0.949954959463517
train: step: 5989, loss: 0.03742637485265732, acc: 0.96, recall: 0.96, precision: 0.96, f_beta: 0.96
train: step: 5990, loss: 0.010351721197366714, acc: 0.99, recall: 0.99, precision: 0.9901960784313726, f_beta: 0.98999899989999
train: step: 5991, loss: 0.01304109301418066, acc: 0.98, recall: 0.98, precision: 0.98, f_beta: 0.98
train: step: 5992, loss: 0.010953078046441078, acc: 0.99, recall: 0.99, precision: 0.9901960784313726, f_beta: 0.98999899989999
train: step: 5993, loss: 0.02721935324370861, acc: 0.96, recall: 0.96, precision: 0.9607371794871795, f_beta: 0.959983993597439
train: step: 5994, loss: 0.01631699874997139, acc: 0.98, recall: 0.98, precision: 0.98, f_beta: 0.98
train: step: 5995, loss: 0.015061781741678715, acc: 0.99, recall: 0.99, precision: 0.9901960784313726, f_beta: 0.98999899989999
train: step: 5996, loss: 0.04853611811995506, acc: 0.94, recall: 0.94, precision: 0.9407051282051282, f_beta: 0.9399759903961584
train: step: 5997, loss: 0.01917962357401848, acc: 0.98, recall: 0.98, precision: 0.98, f_beta: 0.98
train: step: 5998, loss: 0.018705086782574654, acc: 0.98, recall: 0.98, precision: 0.9807692307692308, f_beta: 0.9799919967987194
train: step: 5999, loss: 0.037089042365550995, acc: 0.96, recall: 0.96, precision: 0.96, f_beta: 0.96