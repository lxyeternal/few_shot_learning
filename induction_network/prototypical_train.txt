train: step: 0, loss: 0.6931511759757996, acc: 0.5, recall: 0.5, precision: 0.5, f_beta: 0.3730407523510972
train: step: 1, loss: 0.6931804418563843, acc: 0.525, recall: 0.525, precision: 0.5900900900900901, f_beta: 0.4202898550724638
train: step: 2, loss: 0.6931474804878235, acc: 0.4, recall: 0.4, precision: 0.398989898989899, f_beta: 0.39849624060150374
train: step: 3, loss: 0.6931460499763489, acc: 0.45, recall: 0.45, precision: 0.3611111111111111, f_beta: 0.34523809523809523
train: step: 4, loss: 0.6934821605682373, acc: 0.45, recall: 0.45, precision: 0.23684210526315788, f_beta: 0.3103448275862069
train: step: 5, loss: 0.6931478381156921, acc: 0.475, recall: 0.475, precision: 0.47333333333333333, f_beta: 0.4666666666666666
train: step: 6, loss: 0.6931502223014832, acc: 0.475, recall: 0.475, precision: 0.24358974358974358, f_beta: 0.3220338983050847
train: step: 7, loss: 0.6931163668632507, acc: 0.525, recall: 0.525, precision: 0.7564102564102564, f_beta: 0.38660209846650523
train: step: 8, loss: 0.6931470036506653, acc: 0.55, recall: 0.55, precision: 0.5505050505050505, f_beta: 0.5488721804511277
train: step: 9, loss: 0.6931396722793579, acc: 0.5, recall: 0.5, precision: 0.25, f_beta: 0.3333333333333333
train: step: 10, loss: 0.6931475400924683, acc: 0.325, recall: 0.325, precision: 0.32097186700767266, f_beta: 0.32118164676304206
train: step: 11, loss: 0.6931318044662476, acc: 0.55, recall: 0.55, precision: 0.763157894736842, f_beta: 0.4357366771159875
train: step: 12, loss: 0.693145751953125, acc: 0.55, recall: 0.55, precision: 0.763157894736842, f_beta: 0.4357366771159875
train: step: 13, loss: 0.693139374256134, acc: 0.5, recall: 0.5, precision: 0.25, f_beta: 0.3333333333333333
train: step: 14, loss: 0.6931456327438354, acc: 0.5, recall: 0.5, precision: 0.25, f_beta: 0.3333333333333333
train: step: 15, loss: 0.6931489109992981, acc: 0.35, recall: 0.35000000000000003, precision: 0.3214285714285714, f_beta: 0.3229166666666667
train: step: 16, loss: 0.6931613683700562, acc: 0.5, recall: 0.5, precision: 0.5, f_beta: 0.3730407523510972
train: step: 17, loss: 0.6930931210517883, acc: 0.55, recall: 0.55, precision: 0.5666666666666667, f_beta: 0.52
train: step: 18, loss: 0.6931471824645996, acc: 0.55, recall: 0.55, precision: 0.5505050505050505, f_beta: 0.5488721804511277
train: step: 19, loss: 0.6931461095809937, acc: 0.4, recall: 0.4, precision: 0.39583333333333337, f_beta: 0.3939393939393939
train: step: 20, loss: 0.6931469440460205, acc: 0.5, recall: 0.5, precision: 0.5, f_beta: 0.4505494505494506
train: step: 21, loss: 0.6931493878364563, acc: 0.45, recall: 0.45, precision: 0.44791666666666663, f_beta: 0.4444444444444444
train: step: 22, loss: 0.6931471228599548, acc: 0.525, recall: 0.525, precision: 0.525062656641604, f_beta: 0.5247029393370857
train: step: 23, loss: 0.6931535005569458, acc: 0.5, recall: 0.5, precision: 0.25, f_beta: 0.3333333333333333
train: step: 24, loss: 0.6931549310684204, acc: 0.45, recall: 0.45, precision: 0.4019607843137255, f_beta: 0.37321937321937326
train: step: 25, loss: 0.6931504011154175, acc: 0.525, recall: 0.525, precision: 0.525062656641604, f_beta: 0.5247029393370857
train: step: 26, loss: 0.6931374669075012, acc: 0.5, recall: 0.5, precision: 0.5, f_beta: 0.3730407523510972
train: step: 27, loss: 0.6931365728378296, acc: 0.475, recall: 0.475, precision: 0.24358974358974358, f_beta: 0.3220338983050847
train: step: 28, loss: 0.6931419968605042, acc: 0.675, recall: 0.675, precision: 0.6754385964912281, f_beta: 0.6747967479674797
train: step: 29, loss: 0.6931459307670593, acc: 0.5, recall: 0.5, precision: 0.5, f_beta: 0.4949494949494949
train: step: 30, loss: 0.693168044090271, acc: 0.475, recall: 0.475, precision: 0.24358974358974358, f_beta: 0.3220338983050847
train: step: 31, loss: 0.6931549906730652, acc: 0.475, recall: 0.475, precision: 0.24358974358974358, f_beta: 0.3220338983050847
train: step: 32, loss: 0.6931635737419128, acc: 0.475, recall: 0.475, precision: 0.24358974358974358, f_beta: 0.3220338983050847
train: step: 33, loss: 0.69316565990448, acc: 0.5, recall: 0.5, precision: 0.25, f_beta: 0.3333333333333333
train: step: 34, loss: 0.6931467056274414, acc: 0.475, recall: 0.475, precision: 0.47333333333333333, f_beta: 0.4666666666666666
train: step: 35, loss: 0.6931152939796448, acc: 0.5, recall: 0.5, precision: 0.25, f_beta: 0.3333333333333333
train: step: 36, loss: 0.6932659149169922, acc: 0.5, recall: 0.5, precision: 0.25, f_beta: 0.3333333333333333
train: step: 37, loss: 0.6931708455085754, acc: 0.5, recall: 0.5, precision: 0.5, f_beta: 0.4505494505494506
train: step: 38, loss: 0.6932093501091003, acc: 0.475, recall: 0.475, precision: 0.44285714285714284, f_beta: 0.38909090909090915
train: step: 39, loss: 0.693152904510498, acc: 0.525, recall: 0.525, precision: 0.5266666666666666, f_beta: 0.5174603174603175
train: step: 40, loss: 0.693146824836731, acc: 0.45, recall: 0.45, precision: 0.45, f_beta: 0.45
train: step: 41, loss: 0.6931726336479187, acc: 0.45, recall: 0.45, precision: 0.23684210526315788, f_beta: 0.3103448275862069
train: step: 42, loss: 0.693151593208313, acc: 0.35, recall: 0.35, precision: 0.35, f_beta: 0.35
train: step: 43, loss: 0.6931332349777222, acc: 0.5, recall: 0.5, precision: 0.25, f_beta: 0.3333333333333333
train: step: 44, loss: 0.6931832432746887, acc: 0.5, recall: 0.5, precision: 0.5, f_beta: 0.3730407523510972
train: step: 45, loss: 0.6931448578834534, acc: 0.65, recall: 0.6499999999999999, precision: 0.6515151515151515, f_beta: 0.6491228070175439
train: step: 46, loss: 0.6930353045463562, acc: 0.5, recall: 0.5, precision: 0.25, f_beta: 0.3333333333333333
train: step: 47, loss: 0.6931493878364563, acc: 0.4, recall: 0.39999999999999997, precision: 0.34375, f_beta: 0.34065934065934067
train: step: 48, loss: 0.6931477785110474, acc: 0.5, recall: 0.5, precision: 0.5, f_beta: 0.3730407523510972
train: step: 49, loss: 0.6931468844413757, acc: 0.55, recall: 0.55, precision: 0.5520833333333333, f_beta: 0.5454545454545454
train: step: 50, loss: 0.6931825876235962, acc: 0.475, recall: 0.47500000000000003, precision: 0.40990990990990994, f_beta: 0.35926773455377575
train: step: 51, loss: 0.6931125521659851, acc: 0.575, recall: 0.575, precision: 0.7702702702702703, f_beta: 0.4813119755911518
train: step: 52, loss: 0.693245530128479, acc: 0.475, recall: 0.475, precision: 0.24358974358974358, f_beta: 0.3220338983050847
train: step: 53, loss: 0.6931470036506653, acc: 0.525, recall: 0.525, precision: 0.5432900432900433, f_beta: 0.46890286512928026
train: step: 54, loss: 0.6931443214416504, acc: 0.5, recall: 0.5, precision: 0.25, f_beta: 0.3333333333333333
train: step: 55, loss: 0.6931509375572205, acc: 0.55, recall: 0.55, precision: 0.5520833333333333, f_beta: 0.5454545454545454
train: step: 56, loss: 0.69300776720047, acc: 0.55, recall: 0.55, precision: 0.5980392156862745, f_beta: 0.4871794871794872
train: step: 57, loss: 0.6931212544441223, acc: 0.5, recall: 0.5, precision: 0.5, f_beta: 0.3730407523510972
train: step: 58, loss: 0.6931546330451965, acc: 0.5, recall: 0.5, precision: 0.25, f_beta: 0.3333333333333333
train: step: 59, loss: 0.6931484937667847, acc: 0.425, recall: 0.42500000000000004, precision: 0.42000000000000004, f_beta: 0.4158730158730159
train: step: 60, loss: 0.6930531859397888, acc: 0.525, recall: 0.525, precision: 0.5900900900900901, f_beta: 0.4202898550724638
train: step: 61, loss: 0.6931460499763489, acc: 0.45, recall: 0.44999999999999996, precision: 0.44505494505494503, f_beta: 0.4373401534526854
train: step: 62, loss: 0.693136990070343, acc: 0.45, recall: 0.45, precision: 0.4494949494949495, f_beta: 0.4486215538847118
train: step: 63, loss: 0.6931703686714172, acc: 0.5, recall: 0.5, precision: 0.25, f_beta: 0.3333333333333333
train: step: 64, loss: 0.6931481957435608, acc: 0.5, recall: 0.5, precision: 0.5, f_beta: 0.47916666666666663
train: step: 65, loss: 0.6931460499763489, acc: 0.45, recall: 0.45, precision: 0.44791666666666663, f_beta: 0.4444444444444444
train: step: 66, loss: 0.6930540800094604, acc: 0.55, recall: 0.5499999999999999, precision: 0.6388888888888888, f_beta: 0.46428571428571425
train: step: 67, loss: 0.6931585073471069, acc: 0.45, recall: 0.45, precision: 0.45, f_beta: 0.45
train: step: 68, loss: 0.6931504011154175, acc: 0.55, recall: 0.55, precision: 0.5505050505050505, f_beta: 0.5488721804511277
train: step: 69, loss: 0.6931947469711304, acc: 0.475, recall: 0.47500000000000003, precision: 0.40990990990990994, f_beta: 0.35926773455377575
train: step: 70, loss: 0.6931482553482056, acc: 0.475, recall: 0.475, precision: 0.24358974358974358, f_beta: 0.3220338983050847
train: step: 71, loss: 0.6931583285331726, acc: 0.5, recall: 0.5, precision: 0.5, f_beta: 0.3730407523510972
train: step: 72, loss: 0.6931428909301758, acc: 0.525, recall: 0.525, precision: 0.5255754475703325, f_beta: 0.5223130106851037
train: step: 73, loss: 0.6932330131530762, acc: 0.45, recall: 0.45, precision: 0.23684210526315788, f_beta: 0.3103448275862069
train: step: 74, loss: 0.6931455731391907, acc: 0.5, recall: 0.5, precision: 0.25, f_beta: 0.3333333333333333
train: step: 75, loss: 0.6930807828903198, acc: 0.5, recall: 0.5, precision: 0.25, f_beta: 0.3333333333333333
train: step: 76, loss: 0.6931120753288269, acc: 0.475, recall: 0.475, precision: 0.24358974358974358, f_beta: 0.3220338983050847
train: step: 77, loss: 0.6931418180465698, acc: 0.525, recall: 0.525, precision: 0.5255754475703325, f_beta: 0.5223130106851037
train: step: 78, loss: 0.6931377649307251, acc: 0.675, recall: 0.675, precision: 0.6754385964912281, f_beta: 0.6747967479674797
train: step: 79, loss: 0.6931904554367065, acc: 0.475, recall: 0.475, precision: 0.24358974358974358, f_beta: 0.3220338983050847
train: step: 80, loss: 0.6932150721549988, acc: 0.45, recall: 0.45, precision: 0.23684210526315788, f_beta: 0.3103448275862069
train: step: 81, loss: 0.6929176449775696, acc: 0.525, recall: 0.525, precision: 0.7564102564102564, f_beta: 0.38660209846650523
train: step: 82, loss: 0.6931428909301758, acc: 0.525, recall: 0.525, precision: 0.5266666666666666, f_beta: 0.5174603174603175
train: step: 83, loss: 0.693133533000946, acc: 0.575, recall: 0.575, precision: 0.575187969924812, f_beta: 0.5747342088805503
train: step: 84, loss: 0.6931506395339966, acc: 0.475, recall: 0.475, precision: 0.47333333333333333, f_beta: 0.4666666666666666
train: step: 85, loss: 0.6931427717208862, acc: 0.5, recall: 0.5, precision: 0.5, f_beta: 0.4949494949494949
train: step: 86, loss: 0.6931586265563965, acc: 0.5, recall: 0.5, precision: 0.5, f_beta: 0.5
train: step: 87, loss: 0.6931565999984741, acc: 0.6, recall: 0.6, precision: 0.6098901098901099, f_beta: 0.5907928388746804
train: step: 88, loss: 0.6931486129760742, acc: 0.45, recall: 0.45, precision: 0.44791666666666663, f_beta: 0.4444444444444444
train: step: 89, loss: 0.6931487321853638, acc: 0.55, recall: 0.55, precision: 0.554945054945055, f_beta: 0.5396419437340153
train: step: 90, loss: 0.6931495070457458, acc: 0.625, recall: 0.625, precision: 0.6424501424501424, f_beta: 0.6131528046421664
train: step: 91, loss: 0.6931638717651367, acc: 0.375, recall: 0.375, precision: 0.3721227621483376, f_beta: 0.3714644877435575
train: step: 92, loss: 0.6930795907974243, acc: 0.625, recall: 0.625, precision: 0.7857142857142857, f_beta: 0.5636363636363637
train: step: 93, loss: 0.6931443810462952, acc: 0.475, recall: 0.47500000000000003, precision: 0.4744245524296675, f_beta: 0.47203016970458833
train: step: 94, loss: 0.6928423047065735, acc: 0.5, recall: 0.5, precision: 0.25, f_beta: 0.3333333333333333
train: step: 95, loss: 0.6929129958152771, acc: 0.55, recall: 0.55, precision: 0.763157894736842, f_beta: 0.4357366771159875
train: step: 96, loss: 0.6931484937667847, acc: 0.525, recall: 0.525, precision: 0.5255754475703325, f_beta: 0.5223130106851037
train: step: 97, loss: 0.6932889223098755, acc: 0.45, recall: 0.45, precision: 0.23684210526315788, f_beta: 0.3103448275862069
train: step: 98, loss: 0.6932210922241211, acc: 0.5, recall: 0.5, precision: 0.25, f_beta: 0.3333333333333333
train: step: 99, loss: 0.6931292414665222, acc: 0.6, recall: 0.6, precision: 0.6041666666666667, f_beta: 0.595959595959596
train: step: 100, loss: 0.6931220889091492, acc: 0.575, recall: 0.575, precision: 0.5767263427109974, f_beta: 0.5725958516656191
train: step: 101, loss: 0.6931256651878357, acc: 0.5, recall: 0.5, precision: 0.25, f_beta: 0.3333333333333333
train: step: 102, loss: 0.6931626796722412, acc: 0.5, recall: 0.5, precision: 0.5, f_beta: 0.47916666666666663
train: step: 103, loss: 0.6931615471839905, acc: 0.5, recall: 0.5, precision: 0.5, f_beta: 0.40476190476190477
train: step: 104, loss: 0.6931325793266296, acc: 0.6, recall: 0.6000000000000001, precision: 0.601010101010101, f_beta: 0.5989974937343359
train: step: 105, loss: 0.6931456327438354, acc: 0.5, recall: 0.5, precision: 0.5, f_beta: 0.4949494949494949
train: step: 106, loss: 0.692949116230011, acc: 0.525, recall: 0.525, precision: 0.5571428571428572, f_beta: 0.44727272727272727
train: step: 107, loss: 0.6932626366615295, acc: 0.5, recall: 0.5, precision: 0.5, f_beta: 0.43019943019943013
train: step: 108, loss: 0.6926752328872681, acc: 0.55, recall: 0.5499999999999999, precision: 0.6388888888888888, f_beta: 0.46428571428571425
train: step: 109, loss: 0.6926148533821106, acc: 0.575, recall: 0.575, precision: 0.7702702702702703, f_beta: 0.4813119755911518
train: step: 110, loss: 0.692389190196991, acc: 0.55, recall: 0.55, precision: 0.5666666666666667, f_beta: 0.52
train: step: 111, loss: 0.6931115984916687, acc: 0.5, recall: 0.5, precision: 0.5, f_beta: 0.40476190476190477
train: step: 112, loss: 0.6926825046539307, acc: 0.6, recall: 0.6000000000000001, precision: 0.601010101010101, f_beta: 0.5989974937343359
train: step: 113, loss: 0.6925287246704102, acc: 0.525, recall: 0.525, precision: 0.525062656641604, f_beta: 0.5247029393370857
train: step: 114, loss: 0.6937311291694641, acc: 0.475, recall: 0.475, precision: 0.44285714285714284, f_beta: 0.38909090909090915
train: step: 115, loss: 0.693247377872467, acc: 0.475, recall: 0.47500000000000003, precision: 0.45670995670995673, f_beta: 0.41299790356394134
train: step: 116, loss: 0.6927452087402344, acc: 0.525, recall: 0.525, precision: 0.5313479623824451, f_beta: 0.4996708360763661
train: step: 117, loss: 0.6931933164596558, acc: 0.5, recall: 0.5, precision: 0.5, f_beta: 0.4949494949494949
train: step: 118, loss: 0.693222165107727, acc: 0.5, recall: 0.5, precision: 0.5, f_beta: 0.47916666666666663
train: step: 119, loss: 0.6928629875183105, acc: 0.55, recall: 0.55, precision: 0.763157894736842, f_beta: 0.4357366771159875
train: step: 120, loss: 0.6933561563491821, acc: 0.4, recall: 0.4, precision: 0.38095238095238093, f_beta: 0.375
train: step: 121, loss: 0.6933015584945679, acc: 0.375, recall: 0.375, precision: 0.3666666666666667, f_beta: 0.3650793650793651
train: step: 122, loss: 0.6932650208473206, acc: 0.425, recall: 0.425, precision: 0.22972972972972974, f_beta: 0.2982456140350877
train: step: 123, loss: 0.6934888958930969, acc: 0.475, recall: 0.475, precision: 0.46415770609318996, f_beta: 0.43204868154158216
train: step: 124, loss: 0.693169116973877, acc: 0.5, recall: 0.5, precision: 0.5, f_beta: 0.4505494505494506
train: step: 125, loss: 0.693293571472168, acc: 0.425, recall: 0.42500000000000004, precision: 0.32857142857142857, f_beta: 0.33090909090909093
train: step: 126, loss: 0.6931621432304382, acc: 0.525, recall: 0.5249999999999999, precision: 0.5284900284900285, f_beta: 0.5099935525467441
train: step: 127, loss: 0.6938660740852356, acc: 0.35, recall: 0.35, precision: 0.30000000000000004, f_beta: 0.30666666666666664
train: step: 128, loss: 0.6931236386299133, acc: 0.625, recall: 0.625, precision: 0.6333333333333333, f_beta: 0.6190476190476191
train: step: 129, loss: 0.6931358575820923, acc: 0.5, recall: 0.5, precision: 0.5, f_beta: 0.5
train: step: 130, loss: 0.6930236220359802, acc: 0.525, recall: 0.525, precision: 0.5571428571428572, f_beta: 0.44727272727272727
train: step: 131, loss: 0.6931666135787964, acc: 0.5, recall: 0.5, precision: 0.5, f_beta: 0.4949494949494949
train: step: 132, loss: 0.693193793296814, acc: 0.525, recall: 0.525, precision: 0.7564102564102564, f_beta: 0.38660209846650523
train: step: 133, loss: 0.6925354599952698, acc: 0.575, recall: 0.575, precision: 0.6714285714285715, f_beta: 0.5054545454545454
train: step: 134, loss: 0.6931101679801941, acc: 0.575, recall: 0.575, precision: 0.5767263427109974, f_beta: 0.5725958516656191
train: step: 135, loss: 0.6931706070899963, acc: 0.575, recall: 0.575, precision: 0.5940438871473355, f_beta: 0.5523370638578011
train: step: 136, loss: 0.6931933164596558, acc: 0.5, recall: 0.5, precision: 0.5, f_beta: 0.3730407523510972
train: step: 137, loss: 0.6931537985801697, acc: 0.45, recall: 0.45, precision: 0.4494949494949495, f_beta: 0.4486215538847118
train: step: 138, loss: 0.693595826625824, acc: 0.45, recall: 0.45, precision: 0.3611111111111111, f_beta: 0.34523809523809523
train: step: 139, loss: 0.693173885345459, acc: 0.475, recall: 0.475, precision: 0.24358974358974358, f_beta: 0.3220338983050847
train: step: 140, loss: 0.6928099989891052, acc: 0.55, recall: 0.55, precision: 0.763157894736842, f_beta: 0.4357366771159875
train: step: 141, loss: 0.6930888295173645, acc: 0.625, recall: 0.625, precision: 0.6278772378516624, f_beta: 0.6228786926461345
train: step: 142, loss: 0.6930400133132935, acc: 0.575, recall: 0.575, precision: 0.7702702702702703, f_beta: 0.4813119755911518
train: step: 143, loss: 0.693123996257782, acc: 0.55, recall: 0.55, precision: 0.55, f_beta: 0.55
train: step: 144, loss: 0.6927250623703003, acc: 0.575, recall: 0.575, precision: 0.7702702702702703, f_beta: 0.4813119755911518
train: step: 145, loss: 0.6931942105293274, acc: 0.475, recall: 0.475, precision: 0.47333333333333333, f_beta: 0.4666666666666666
train: step: 146, loss: 0.6935147047042847, acc: 0.45, recall: 0.45, precision: 0.3611111111111111, f_beta: 0.34523809523809523
train: step: 147, loss: 0.6931504011154175, acc: 0.5, recall: 0.5, precision: 0.25, f_beta: 0.3333333333333333
train: step: 148, loss: 0.6931298971176147, acc: 0.475, recall: 0.475, precision: 0.474937343358396, f_beta: 0.474671669793621
train: step: 149, loss: 0.6931825876235962, acc: 0.5, recall: 0.5, precision: 0.5, f_beta: 0.40476190476190477
train: step: 150, loss: 0.6931943893432617, acc: 0.475, recall: 0.475, precision: 0.24358974358974358, f_beta: 0.3220338983050847
train: step: 151, loss: 0.6932188272476196, acc: 0.5, recall: 0.5, precision: 0.25, f_beta: 0.3333333333333333
train: step: 152, loss: 0.6931315660476685, acc: 0.525, recall: 0.525, precision: 0.5255754475703325, f_beta: 0.5223130106851037
train: step: 153, loss: 0.6931527256965637, acc: 0.4, recall: 0.4, precision: 0.398989898989899, f_beta: 0.39849624060150374
train: step: 154, loss: 0.6934475898742676, acc: 0.5, recall: 0.5, precision: 0.5, f_beta: 0.4505494505494506
train: step: 155, loss: 0.6931799054145813, acc: 0.45, recall: 0.45, precision: 0.44791666666666663, f_beta: 0.4444444444444444
train: step: 156, loss: 0.6931411623954773, acc: 0.525, recall: 0.525, precision: 0.7564102564102564, f_beta: 0.38660209846650523
train: step: 157, loss: 0.6926711201667786, acc: 0.525, recall: 0.525, precision: 0.7564102564102564, f_beta: 0.38660209846650523
train: step: 158, loss: 0.6931630969047546, acc: 0.35, recall: 0.35, precision: 0.34375, f_beta: 0.3434343434343434
train: step: 159, loss: 0.6931599378585815, acc: 0.6, recall: 0.6, precision: 0.6041666666666667, f_beta: 0.595959595959596
train: step: 160, loss: 0.6926673054695129, acc: 0.55, recall: 0.55, precision: 0.5980392156862745, f_beta: 0.4871794871794872
train: step: 161, loss: 0.6935802698135376, acc: 0.5, recall: 0.5, precision: 0.5, f_beta: 0.4505494505494506
train: step: 162, loss: 0.693185031414032, acc: 0.4, recall: 0.4, precision: 0.3666666666666667, f_beta: 0.36
train: step: 163, loss: 0.6930674314498901, acc: 0.525, recall: 0.525, precision: 0.5432900432900433, f_beta: 0.46890286512928026
train: step: 164, loss: 0.6932121515274048, acc: 0.5, recall: 0.5, precision: 0.25, f_beta: 0.3333333333333333
train: step: 165, loss: 0.6929912567138672, acc: 0.55, recall: 0.5499999999999999, precision: 0.6388888888888888, f_beta: 0.46428571428571425
train: step: 166, loss: 0.6931033134460449, acc: 0.6, recall: 0.6000000000000001, precision: 0.601010101010101, f_beta: 0.5989974937343359
train: step: 167, loss: 0.69330894947052, acc: 0.45, recall: 0.45, precision: 0.421875, f_beta: 0.39560439560439553
train: step: 168, loss: 0.6931734085083008, acc: 0.5, recall: 0.5, precision: 0.5, f_beta: 0.5
train: step: 169, loss: 0.6931309103965759, acc: 0.525, recall: 0.525, precision: 0.5432900432900433, f_beta: 0.46890286512928026
train: step: 170, loss: 0.692082405090332, acc: 0.525, recall: 0.525, precision: 0.7564102564102564, f_beta: 0.38660209846650523
train: step: 171, loss: 0.692769467830658, acc: 0.55, recall: 0.55, precision: 0.763157894736842, f_beta: 0.4357366771159875
train: step: 172, loss: 0.6931489706039429, acc: 0.475, recall: 0.47500000000000003, precision: 0.4744245524296675, f_beta: 0.47203016970458833
train: step: 173, loss: 0.693227231502533, acc: 0.5, recall: 0.5, precision: 0.25, f_beta: 0.3333333333333333
train: step: 174, loss: 0.6931381821632385, acc: 0.6, recall: 0.6000000000000001, precision: 0.601010101010101, f_beta: 0.5989974937343359
train: step: 175, loss: 0.6932754516601562, acc: 0.45, recall: 0.45, precision: 0.4019607843137255, f_beta: 0.37321937321937326
train: step: 176, loss: 0.6934777498245239, acc: 0.4, recall: 0.4, precision: 0.3666666666666667, f_beta: 0.36
train: step: 177, loss: 0.6930888295173645, acc: 0.55, recall: 0.55, precision: 0.5520833333333333, f_beta: 0.5454545454545454
train: step: 178, loss: 0.6934785842895508, acc: 0.45, recall: 0.45, precision: 0.3611111111111111, f_beta: 0.34523809523809523
train: step: 179, loss: 0.6931098103523254, acc: 0.5, recall: 0.5, precision: 0.5, f_beta: 0.3730407523510972
train: step: 180, loss: 0.6931601762771606, acc: 0.5, recall: 0.5, precision: 0.5, f_beta: 0.4884910485933504
train: step: 181, loss: 0.6931840181350708, acc: 0.4, recall: 0.4, precision: 0.398989898989899, f_beta: 0.39849624060150374
train: step: 182, loss: 0.6931393146514893, acc: 0.5, recall: 0.5, precision: 0.5, f_beta: 0.4987468671679198
train: step: 183, loss: 0.6931168437004089, acc: 0.525, recall: 0.525, precision: 0.5266666666666666, f_beta: 0.5174603174603175
train: step: 184, loss: 0.6930867433547974, acc: 0.5, recall: 0.5, precision: 0.5, f_beta: 0.4505494505494506
train: step: 185, loss: 0.6933644413948059, acc: 0.45, recall: 0.45, precision: 0.23684210526315788, f_beta: 0.3103448275862069
train: step: 186, loss: 0.6931964755058289, acc: 0.5, recall: 0.5, precision: 0.5, f_beta: 0.3730407523510972
train: step: 187, loss: 0.6931315660476685, acc: 0.425, recall: 0.42500000000000004, precision: 0.424812030075188, f_beta: 0.4246404002501563
train: step: 188, loss: 0.6930357217788696, acc: 0.575, recall: 0.575, precision: 0.575187969924812, f_beta: 0.5747342088805503
train: step: 189, loss: 0.6933569312095642, acc: 0.45, recall: 0.45, precision: 0.3611111111111111, f_beta: 0.34523809523809523
train: step: 190, loss: 0.6932588219642639, acc: 0.475, recall: 0.475, precision: 0.474937343358396, f_beta: 0.474671669793621
train: step: 191, loss: 0.6931870579719543, acc: 0.425, recall: 0.42500000000000004, precision: 0.42000000000000004, f_beta: 0.4158730158730159
train: step: 192, loss: 0.693072497844696, acc: 0.525, recall: 0.525, precision: 0.5900900900900901, f_beta: 0.4202898550724638
train: step: 193, loss: 0.6931754350662231, acc: 0.525, recall: 0.525, precision: 0.525062656641604, f_beta: 0.5247029393370857
train: step: 194, loss: 0.6931082010269165, acc: 0.45, recall: 0.45, precision: 0.23684210526315788, f_beta: 0.3103448275862069
train: step: 195, loss: 0.6931552290916443, acc: 0.45, recall: 0.44999999999999996, precision: 0.44505494505494503, f_beta: 0.4373401534526854
train: step: 196, loss: 0.693482518196106, acc: 0.425, recall: 0.425, precision: 0.22972972972972974, f_beta: 0.2982456140350877
train: step: 197, loss: 0.6932603716850281, acc: 0.5, recall: 0.5, precision: 0.5, f_beta: 0.4987468671679198
train: step: 198, loss: 0.6931326389312744, acc: 0.45, recall: 0.45, precision: 0.4494949494949495, f_beta: 0.4486215538847118
train: step: 199, loss: 0.6930365562438965, acc: 0.525, recall: 0.525, precision: 0.5900900900900901, f_beta: 0.4202898550724638
train: step: 200, loss: 0.6931447386741638, acc: 0.55, recall: 0.55, precision: 0.5505050505050505, f_beta: 0.5488721804511277
train: step: 201, loss: 0.6932970285415649, acc: 0.375, recall: 0.375, precision: 0.3666666666666667, f_beta: 0.3650793650793651
train: step: 202, loss: 0.6932523846626282, acc: 0.425, recall: 0.425, precision: 0.42327365728900257, f_beta: 0.4217473287240729
train: step: 203, loss: 0.6931830644607544, acc: 0.425, recall: 0.42500000000000004, precision: 0.4059561128526646, f_beta: 0.39433838051349573
train: step: 204, loss: 0.693253755569458, acc: 0.45, recall: 0.45, precision: 0.23684210526315788, f_beta: 0.3103448275862069
train: step: 205, loss: 0.6928998231887817, acc: 0.55, recall: 0.55, precision: 0.763157894736842, f_beta: 0.4357366771159875
train: step: 206, loss: 0.6935005187988281, acc: 0.475, recall: 0.475, precision: 0.24358974358974358, f_beta: 0.3220338983050847
train: step: 207, loss: 0.6932113170623779, acc: 0.475, recall: 0.475, precision: 0.46865203761755486, f_beta: 0.4470046082949308
train: step: 208, loss: 0.6931137442588806, acc: 0.575, recall: 0.575, precision: 0.5800000000000001, f_beta: 0.5682539682539682
train: step: 209, loss: 0.6933850049972534, acc: 0.425, recall: 0.425, precision: 0.3924731182795699, f_beta: 0.37795807978363755
train: step: 210, loss: 0.6931504011154175, acc: 0.5, recall: 0.5, precision: 0.5, f_beta: 0.5
train: step: 211, loss: 0.6931129693984985, acc: 0.675, recall: 0.675, precision: 0.6866666666666666, f_beta: 0.6698412698412699
train: step: 212, loss: 0.6917155981063843, acc: 0.625, recall: 0.625, precision: 0.7857142857142857, f_beta: 0.5636363636363637
train: step: 213, loss: 0.6931756138801575, acc: 0.475, recall: 0.47500000000000003, precision: 0.4744245524296675, f_beta: 0.47203016970458833
train: step: 214, loss: 0.6931384205818176, acc: 0.525, recall: 0.525, precision: 0.525062656641604, f_beta: 0.5247029393370857
train: step: 215, loss: 0.6931532621383667, acc: 0.425, recall: 0.425, precision: 0.42327365728900257, f_beta: 0.4217473287240729
train: step: 216, loss: 0.6932480931282043, acc: 0.475, recall: 0.475, precision: 0.24358974358974358, f_beta: 0.3220338983050847
train: step: 217, loss: 0.6927961111068726, acc: 0.55, recall: 0.55, precision: 0.5980392156862745, f_beta: 0.4871794871794872
train: step: 218, loss: 0.6928587555885315, acc: 0.525, recall: 0.525, precision: 0.5900900900900901, f_beta: 0.4202898550724638
train: step: 219, loss: 0.6931446194648743, acc: 0.55, recall: 0.55, precision: 0.55, f_beta: 0.55
train: step: 220, loss: 0.6931377649307251, acc: 0.6, recall: 0.6, precision: 0.65625, f_beta: 0.5604395604395604
train: step: 221, loss: 0.6931527853012085, acc: 0.575, recall: 0.575, precision: 0.5767263427109974, f_beta: 0.5725958516656191
train: step: 222, loss: 0.6932417750358582, acc: 0.325, recall: 0.32499999999999996, precision: 0.32456140350877194, f_beta: 0.324577861163227
train: step: 223, loss: 0.6929264068603516, acc: 0.525, recall: 0.525, precision: 0.5571428571428572, f_beta: 0.44727272727272727
train: step: 224, loss: 0.6931692957878113, acc: 0.425, recall: 0.425, precision: 0.42327365728900257, f_beta: 0.4217473287240729
train: step: 225, loss: 0.692410945892334, acc: 0.575, recall: 0.575, precision: 0.7702702702702703, f_beta: 0.4813119755911518
train: step: 226, loss: 0.6932409405708313, acc: 0.475, recall: 0.475, precision: 0.47150997150997154, f_beta: 0.4584139264990329
train: step: 227, loss: 0.693282961845398, acc: 0.475, recall: 0.475, precision: 0.24358974358974358, f_beta: 0.3220338983050847
train: step: 228, loss: 0.6931682825088501, acc: 0.425, recall: 0.425, precision: 0.4145299145299145, f_beta: 0.40683430045132174
train: step: 229, loss: 0.6930370926856995, acc: 0.5, recall: 0.5, precision: 0.5, f_beta: 0.3730407523510972
train: step: 230, loss: 0.6931759119033813, acc: 0.45, recall: 0.45, precision: 0.4019607843137255, f_beta: 0.37321937321937326
train: step: 231, loss: 0.693282425403595, acc: 0.5, recall: 0.5, precision: 0.5, f_beta: 0.3730407523510972
train: step: 232, loss: 0.6929010152816772, acc: 0.525, recall: 0.525, precision: 0.7564102564102564, f_beta: 0.38660209846650523
train: step: 233, loss: 0.6929491758346558, acc: 0.6, recall: 0.6, precision: 0.65625, f_beta: 0.5604395604395604
train: step: 234, loss: 0.6888734698295593, acc: 0.675, recall: 0.675, precision: 0.803030303030303, f_beta: 0.636617749825297
train: step: 235, loss: 0.6939101219177246, acc: 0.45, recall: 0.44999999999999996, precision: 0.44505494505494503, f_beta: 0.4373401534526854
train: step: 236, loss: 0.6933392286300659, acc: 0.5, recall: 0.5, precision: 0.5, f_beta: 0.40476190476190477
train: step: 237, loss: 0.6935015916824341, acc: 0.5, recall: 0.5, precision: 0.5, f_beta: 0.43019943019943013
train: step: 238, loss: 0.6931167840957642, acc: 0.525, recall: 0.525, precision: 0.53584229390681, f_beta: 0.4861392832995267
train: step: 239, loss: 0.6929248571395874, acc: 0.675, recall: 0.675, precision: 0.6754385964912281, f_beta: 0.6747967479674797
train: step: 240, loss: 0.6933839917182922, acc: 0.475, recall: 0.475, precision: 0.47333333333333333, f_beta: 0.4666666666666666
train: step: 241, loss: 0.6928590536117554, acc: 0.5, recall: 0.5, precision: 0.5, f_beta: 0.4987468671679198
train: step: 242, loss: 0.6931626200675964, acc: 0.5, recall: 0.5, precision: 0.5, f_beta: 0.3730407523510972
train: step: 243, loss: 0.6931166648864746, acc: 0.5, recall: 0.5, precision: 0.5, f_beta: 0.4884910485933504
train: step: 244, loss: 0.692571759223938, acc: 0.45, recall: 0.45, precision: 0.4019607843137255, f_beta: 0.37321937321937326
train: step: 245, loss: 0.6930413246154785, acc: 0.525, recall: 0.525, precision: 0.525062656641604, f_beta: 0.5247029393370857
train: step: 246, loss: 0.6934592127799988, acc: 0.525, recall: 0.525, precision: 0.5571428571428572, f_beta: 0.44727272727272727
train: step: 247, loss: 0.6929799318313599, acc: 0.55, recall: 0.55, precision: 0.763157894736842, f_beta: 0.4357366771159875
train: step: 248, loss: 0.6927942037582397, acc: 0.525, recall: 0.525, precision: 0.5266666666666666, f_beta: 0.5174603174603175
train: step: 249, loss: 0.6932300329208374, acc: 0.55, recall: 0.55, precision: 0.55, f_beta: 0.55
train: step: 250, loss: 0.6933068037033081, acc: 0.4, recall: 0.4, precision: 0.398989898989899, f_beta: 0.39849624060150374
train: step: 251, loss: 0.6933896541595459, acc: 0.5, recall: 0.5, precision: 0.5, f_beta: 0.4884910485933504
train: step: 252, loss: 0.6928521394729614, acc: 0.525, recall: 0.5249999999999999, precision: 0.5284900284900285, f_beta: 0.5099935525467441
train: step: 253, loss: 0.6929730176925659, acc: 0.525, recall: 0.525, precision: 0.5255754475703325, f_beta: 0.5223130106851037
train: step: 254, loss: 0.6929442286491394, acc: 0.525, recall: 0.525, precision: 0.5266666666666666, f_beta: 0.5174603174603175
train: step: 255, loss: 0.6925418972969055, acc: 0.525, recall: 0.525, precision: 0.5900900900900901, f_beta: 0.4202898550724638
train: step: 256, loss: 0.6931971311569214, acc: 0.425, recall: 0.42500000000000004, precision: 0.32857142857142857, f_beta: 0.33090909090909093
train: step: 257, loss: 0.689278244972229, acc: 0.575, recall: 0.575, precision: 0.6714285714285715, f_beta: 0.5054545454545454
train: step: 258, loss: 0.6938959956169128, acc: 0.5, recall: 0.5, precision: 0.5, f_beta: 0.43019943019943013
train: step: 259, loss: 0.693157970905304, acc: 0.625, recall: 0.625, precision: 0.6333333333333333, f_beta: 0.6190476190476191
train: step: 260, loss: 0.6920295357704163, acc: 0.525, recall: 0.525, precision: 0.7564102564102564, f_beta: 0.38660209846650523
train: step: 261, loss: 0.69221031665802, acc: 0.525, recall: 0.525, precision: 0.5900900900900901, f_beta: 0.4202898550724638
train: step: 262, loss: 0.6930993795394897, acc: 0.5, recall: 0.5, precision: 0.5, f_beta: 0.4987468671679198
train: step: 263, loss: 0.6923836469650269, acc: 0.525, recall: 0.525, precision: 0.5266666666666666, f_beta: 0.5174603174603175
train: step: 264, loss: 0.6934121251106262, acc: 0.475, recall: 0.475, precision: 0.47333333333333333, f_beta: 0.4666666666666666
train: step: 265, loss: 0.69340580701828, acc: 0.35, recall: 0.35, precision: 0.34375, f_beta: 0.3434343434343434
train: step: 266, loss: 0.697257936000824, acc: 0.475, recall: 0.47500000000000003, precision: 0.40990990990990994, f_beta: 0.35926773455377575
train: step: 267, loss: 0.6925479769706726, acc: 0.5, recall: 0.5, precision: 0.25, f_beta: 0.3333333333333333
train: step: 268, loss: 0.6931139230728149, acc: 0.5, recall: 0.5, precision: 0.5, f_beta: 0.4884910485933504
train: step: 269, loss: 0.6933083534240723, acc: 0.5, recall: 0.5, precision: 0.5, f_beta: 0.4987468671679198
train: step: 270, loss: 0.6931017637252808, acc: 0.6, recall: 0.6, precision: 0.6098901098901099, f_beta: 0.5907928388746804
train: step: 271, loss: 0.6929572820663452, acc: 0.5, recall: 0.5, precision: 0.5, f_beta: 0.4949494949494949
train: step: 272, loss: 0.6941112279891968, acc: 0.375, recall: 0.375, precision: 0.3666666666666667, f_beta: 0.3650793650793651
train: step: 273, loss: 0.6944457292556763, acc: 0.525, recall: 0.525, precision: 0.5266666666666666, f_beta: 0.5174603174603175
train: step: 274, loss: 0.6937082409858704, acc: 0.5, recall: 0.5, precision: 0.5, f_beta: 0.47916666666666663
train: step: 275, loss: 0.6944893002510071, acc: 0.325, recall: 0.325, precision: 0.31333333333333335, f_beta: 0.3142857142857143
train: step: 276, loss: 0.6938174962997437, acc: 0.5, recall: 0.5, precision: 0.5, f_beta: 0.4666666666666667
train: step: 277, loss: 0.6925727128982544, acc: 0.45, recall: 0.45, precision: 0.4494949494949495, f_beta: 0.4486215538847118
train: step: 278, loss: 0.6923148036003113, acc: 0.6, recall: 0.6, precision: 0.6, f_beta: 0.6
train: step: 279, loss: 0.6930344104766846, acc: 0.425, recall: 0.425, precision: 0.4145299145299145, f_beta: 0.40683430045132174
train: step: 280, loss: 0.6923596858978271, acc: 0.625, recall: 0.625, precision: 0.62531328320802, f_beta: 0.6247654784240151
train: step: 281, loss: 0.6932341456413269, acc: 0.45, recall: 0.44999999999999996, precision: 0.43333333333333335, f_beta: 0.41333333333333333
train: step: 282, loss: 0.6955627202987671, acc: 0.5, recall: 0.5, precision: 0.5, f_beta: 0.4505494505494506
train: step: 283, loss: 0.6925419569015503, acc: 0.525, recall: 0.525, precision: 0.5313479623824451, f_beta: 0.4996708360763661
train: step: 284, loss: 0.694150447845459, acc: 0.45, recall: 0.45, precision: 0.421875, f_beta: 0.39560439560439553
train: step: 285, loss: 0.6930190920829773, acc: 0.5, recall: 0.5, precision: 0.5, f_beta: 0.4505494505494506
train: step: 286, loss: 0.6921588778495789, acc: 0.525, recall: 0.525, precision: 0.5313479623824451, f_beta: 0.4996708360763661
train: step: 287, loss: 0.6925775408744812, acc: 0.575, recall: 0.575, precision: 0.5767263427109974, f_beta: 0.5725958516656191
train: step: 288, loss: 0.6941061019897461, acc: 0.35, recall: 0.35, precision: 0.35, f_beta: 0.35
train: step: 289, loss: 0.6932186484336853, acc: 0.5, recall: 0.5, precision: 0.5, f_beta: 0.4949494949494949
train: step: 290, loss: 0.6913930177688599, acc: 0.55, recall: 0.55, precision: 0.5520833333333333, f_beta: 0.5454545454545454
train: step: 291, loss: 0.6929923295974731, acc: 0.5, recall: 0.5, precision: 0.5, f_beta: 0.4949494949494949
train: step: 292, loss: 0.69261234998703, acc: 0.45, recall: 0.45, precision: 0.44791666666666663, f_beta: 0.4444444444444444
train: step: 293, loss: 0.6918706297874451, acc: 0.65, recall: 0.65, precision: 0.6648351648351649, f_beta: 0.6419437340153452
train: step: 294, loss: 0.6937776803970337, acc: 0.475, recall: 0.475, precision: 0.24358974358974358, f_beta: 0.3220338983050847
train: step: 295, loss: 0.6957062482833862, acc: 0.475, recall: 0.47500000000000003, precision: 0.45670995670995673, f_beta: 0.41299790356394134
train: step: 296, loss: 0.6919234991073608, acc: 0.525, recall: 0.525, precision: 0.5571428571428572, f_beta: 0.44727272727272727
train: step: 297, loss: 0.6927498579025269, acc: 0.5, recall: 0.5, precision: 0.5, f_beta: 0.43019943019943013
train: step: 298, loss: 0.6933682560920715, acc: 0.5, recall: 0.5, precision: 0.5, f_beta: 0.4949494949494949
train: step: 299, loss: 0.6937350034713745, acc: 0.45, recall: 0.45, precision: 0.45, f_beta: 0.45
train: step: 300, loss: 0.6887381672859192, acc: 0.55, recall: 0.55, precision: 0.5980392156862745, f_beta: 0.4871794871794872
train: step: 301, loss: 0.6941283345222473, acc: 0.425, recall: 0.42500000000000004, precision: 0.424812030075188, f_beta: 0.4246404002501563
train: step: 302, loss: 0.693189799785614, acc: 0.575, recall: 0.575, precision: 0.60752688172043, f_beta: 0.5402298850574712
train: step: 303, loss: 0.6928806304931641, acc: 0.5, recall: 0.5, precision: 0.5, f_beta: 0.4949494949494949
train: step: 304, loss: 0.6934525966644287, acc: 0.4, recall: 0.4, precision: 0.39583333333333337, f_beta: 0.3939393939393939
train: step: 305, loss: 0.6935940980911255, acc: 0.55, recall: 0.55, precision: 0.578125, f_beta: 0.5054945054945055
train: step: 306, loss: 0.6973860859870911, acc: 0.375, recall: 0.375, precision: 0.3666666666666667, f_beta: 0.3650793650793651
train: step: 307, loss: 0.6930779218673706, acc: 0.45, recall: 0.45, precision: 0.44047619047619047, f_beta: 0.42708333333333337
train: step: 308, loss: 0.6928189992904663, acc: 0.5, recall: 0.5, precision: 0.5, f_beta: 0.4884910485933504
train: step: 309, loss: 0.6942602396011353, acc: 0.5, recall: 0.5, precision: 0.5, f_beta: 0.4666666666666667
train: step: 310, loss: 0.6932654976844788, acc: 0.55, recall: 0.55, precision: 0.5666666666666667, f_beta: 0.52
train: step: 311, loss: 0.6938667297363281, acc: 0.425, recall: 0.425, precision: 0.3701298701298701, f_beta: 0.35709294199860236
train: step: 312, loss: 0.6937001943588257, acc: 0.375, recall: 0.375, precision: 0.37468671679197996, f_beta: 0.37460913070669166
train: step: 313, loss: 0.6922217607498169, acc: 0.45, recall: 0.45, precision: 0.44047619047619047, f_beta: 0.42708333333333337
train: step: 314, loss: 0.6956740617752075, acc: 0.425, recall: 0.425, precision: 0.4145299145299145, f_beta: 0.40683430045132174
train: step: 315, loss: 0.6922105550765991, acc: 0.475, recall: 0.47500000000000003, precision: 0.40990990990990994, f_beta: 0.35926773455377575
train: step: 316, loss: 0.6919589042663574, acc: 0.475, recall: 0.475, precision: 0.46415770609318996, f_beta: 0.43204868154158216
train: step: 317, loss: 0.6930630803108215, acc: 0.5, recall: 0.5, precision: 0.5, f_beta: 0.4987468671679198
train: step: 318, loss: 0.6954892873764038, acc: 0.425, recall: 0.42500000000000004, precision: 0.4059561128526646, f_beta: 0.39433838051349573
train: step: 319, loss: 0.6936238408088684, acc: 0.475, recall: 0.475, precision: 0.47333333333333333, f_beta: 0.4666666666666666
train: step: 320, loss: 0.693855345249176, acc: 0.425, recall: 0.42500000000000004, precision: 0.424812030075188, f_beta: 0.4246404002501563
train: step: 321, loss: 0.6923942565917969, acc: 0.475, recall: 0.47500000000000003, precision: 0.4744245524296675, f_beta: 0.47203016970458833
train: step: 322, loss: 0.6919399499893188, acc: 0.55, recall: 0.55, precision: 0.5666666666666667, f_beta: 0.52
train: step: 323, loss: 0.6925536394119263, acc: 0.525, recall: 0.525, precision: 0.5266666666666666, f_beta: 0.5174603174603175
train: step: 324, loss: 0.6932779550552368, acc: 0.55, recall: 0.55, precision: 0.5505050505050505, f_beta: 0.5488721804511277
train: step: 325, loss: 0.6926568746566772, acc: 0.55, recall: 0.55, precision: 0.554945054945055, f_beta: 0.5396419437340153
train: step: 326, loss: 0.694090723991394, acc: 0.45, recall: 0.45, precision: 0.44791666666666663, f_beta: 0.4444444444444444
train: step: 327, loss: 0.6926300525665283, acc: 0.55, recall: 0.55, precision: 0.5520833333333333, f_beta: 0.5454545454545454
train: step: 328, loss: 0.6928530931472778, acc: 0.525, recall: 0.525, precision: 0.5266666666666666, f_beta: 0.5174603174603175
train: step: 329, loss: 0.6961199045181274, acc: 0.425, recall: 0.42500000000000004, precision: 0.32857142857142857, f_beta: 0.33090909090909093
train: step: 330, loss: 0.6929892301559448, acc: 0.5, recall: 0.5, precision: 0.5, f_beta: 0.4987468671679198
train: step: 331, loss: 0.693071186542511, acc: 0.475, recall: 0.475, precision: 0.474937343358396, f_beta: 0.474671669793621
train: step: 332, loss: 0.6922893524169922, acc: 0.65, recall: 0.6499999999999999, precision: 0.6515151515151515, f_beta: 0.6491228070175439
train: step: 333, loss: 0.6926366090774536, acc: 0.525, recall: 0.525, precision: 0.53584229390681, f_beta: 0.4861392832995267
train: step: 334, loss: 0.6935052275657654, acc: 0.55, recall: 0.55, precision: 0.5666666666666667, f_beta: 0.52
train: step: 335, loss: 0.6930674314498901, acc: 0.475, recall: 0.475, precision: 0.47333333333333333, f_beta: 0.4666666666666666
train: step: 336, loss: 0.6943587064743042, acc: 0.35, recall: 0.35, precision: 0.35, f_beta: 0.35
train: step: 337, loss: 0.6924567222595215, acc: 0.575, recall: 0.575, precision: 0.5940438871473355, f_beta: 0.5523370638578011
train: step: 338, loss: 0.6932874917984009, acc: 0.5, recall: 0.5, precision: 0.5, f_beta: 0.4987468671679198
train: step: 339, loss: 0.6931902766227722, acc: 0.525, recall: 0.525, precision: 0.5313479623824451, f_beta: 0.4996708360763661
train: step: 340, loss: 0.6935406923294067, acc: 0.4, recall: 0.39999999999999997, precision: 0.34375, f_beta: 0.34065934065934067
train: step: 341, loss: 0.6930490732192993, acc: 0.525, recall: 0.525, precision: 0.5266666666666666, f_beta: 0.5174603174603175
train: step: 342, loss: 0.6945278644561768, acc: 0.5, recall: 0.5, precision: 0.25, f_beta: 0.3333333333333333
train: step: 343, loss: 0.6947841048240662, acc: 0.375, recall: 0.375, precision: 0.37468671679197996, f_beta: 0.37460913070669166
train: step: 344, loss: 0.6935479640960693, acc: 0.45, recall: 0.45, precision: 0.4494949494949495, f_beta: 0.4486215538847118
train: step: 345, loss: 0.6937354207038879, acc: 0.425, recall: 0.425, precision: 0.42327365728900257, f_beta: 0.4217473287240729
train: step: 346, loss: 0.6926938891410828, acc: 0.625, recall: 0.625, precision: 0.6424501424501424, f_beta: 0.6131528046421664
train: step: 347, loss: 0.6932168006896973, acc: 0.4, recall: 0.4, precision: 0.398989898989899, f_beta: 0.39849624060150374
train: step: 348, loss: 0.693069577217102, acc: 0.55, recall: 0.55, precision: 0.55, f_beta: 0.55
train: step: 349, loss: 0.6913683414459229, acc: 0.525, recall: 0.525, precision: 0.5571428571428572, f_beta: 0.44727272727272727
train: step: 350, loss: 0.6930611729621887, acc: 0.5, recall: 0.5, precision: 0.5, f_beta: 0.4987468671679198
train: step: 351, loss: 0.6926434636116028, acc: 0.525, recall: 0.525, precision: 0.5571428571428572, f_beta: 0.44727272727272727
train: step: 352, loss: 0.6927438378334045, acc: 0.5, recall: 0.5, precision: 0.5, f_beta: 0.4949494949494949
train: step: 353, loss: 0.6921939849853516, acc: 0.575, recall: 0.575, precision: 0.5767263427109974, f_beta: 0.5725958516656191
train: step: 354, loss: 0.6932902336120605, acc: 0.525, recall: 0.525, precision: 0.5266666666666666, f_beta: 0.5174603174603175
train: step: 355, loss: 0.6928569674491882, acc: 0.5, recall: 0.5, precision: 0.5, f_beta: 0.5
train: step: 356, loss: 0.6934586763381958, acc: 0.5, recall: 0.5, precision: 0.5, f_beta: 0.47916666666666663
train: step: 357, loss: 0.6927213668823242, acc: 0.65, recall: 0.65, precision: 0.6648351648351649, f_beta: 0.6419437340153452
train: step: 358, loss: 0.693615734577179, acc: 0.325, recall: 0.325, precision: 0.30056980056980054, f_beta: 0.3036750483558994
train: step: 359, loss: 0.6920862197875977, acc: 0.55, recall: 0.55, precision: 0.5980392156862745, f_beta: 0.4871794871794872
train: step: 360, loss: 0.6949681043624878, acc: 0.375, recall: 0.375, precision: 0.3666666666666667, f_beta: 0.3650793650793651
train: step: 361, loss: 0.6903008222579956, acc: 0.575, recall: 0.575, precision: 0.6714285714285715, f_beta: 0.5054545454545454
train: step: 362, loss: 0.6937694549560547, acc: 0.425, recall: 0.425, precision: 0.4145299145299145, f_beta: 0.40683430045132174
train: step: 363, loss: 0.6922811269760132, acc: 0.5, recall: 0.5, precision: 0.5, f_beta: 0.4987468671679198
train: step: 364, loss: 0.6929313540458679, acc: 0.6, recall: 0.6, precision: 0.6, f_beta: 0.6
train: step: 365, loss: 0.6913037896156311, acc: 0.6, recall: 0.6, precision: 0.696078431372549, f_beta: 0.5441595441595442
train: step: 366, loss: 0.6841508746147156, acc: 0.625, recall: 0.625, precision: 0.6792114695340502, f_beta: 0.5943204868154157
train: step: 367, loss: 0.6920017004013062, acc: 0.475, recall: 0.475, precision: 0.46865203761755486, f_beta: 0.4470046082949308
train: step: 368, loss: 0.6903349757194519, acc: 0.65, recall: 0.65, precision: 0.7, f_beta: 0.6266666666666667
train: step: 369, loss: 0.6933628916740417, acc: 0.5, recall: 0.5, precision: 0.5, f_beta: 0.4949494949494949
train: step: 370, loss: 0.6941930055618286, acc: 0.5, recall: 0.5, precision: 0.5, f_beta: 0.47916666666666663
train: step: 371, loss: 0.691606879234314, acc: 0.675, recall: 0.675, precision: 0.6866666666666666, f_beta: 0.6698412698412699
train: step: 372, loss: 0.6927610039710999, acc: 0.45, recall: 0.44999999999999996, precision: 0.43333333333333335, f_beta: 0.41333333333333333
train: step: 373, loss: 0.6871824264526367, acc: 0.575, recall: 0.575, precision: 0.7702702702702703, f_beta: 0.4813119755911518
train: step: 374, loss: 0.6929911375045776, acc: 0.55, recall: 0.55, precision: 0.5595238095238095, f_beta: 0.53125
train: step: 375, loss: 0.6930314898490906, acc: 0.55, recall: 0.55, precision: 0.5595238095238095, f_beta: 0.53125
train: step: 376, loss: 0.6943849325180054, acc: 0.375, recall: 0.375, precision: 0.3721227621483376, f_beta: 0.3714644877435575
train: step: 377, loss: 0.6950047016143799, acc: 0.475, recall: 0.475, precision: 0.47150997150997154, f_beta: 0.4584139264990329
train: step: 378, loss: 0.6925204396247864, acc: 0.5, recall: 0.5, precision: 0.5, f_beta: 0.43019943019943013
train: step: 379, loss: 0.6968938708305359, acc: 0.475, recall: 0.475, precision: 0.44285714285714284, f_beta: 0.38909090909090915
train: step: 380, loss: 0.6927908658981323, acc: 0.5, recall: 0.5, precision: 0.5, f_beta: 0.47916666666666663
train: step: 381, loss: 0.6934927701950073, acc: 0.525, recall: 0.5249999999999999, precision: 0.5284900284900285, f_beta: 0.5099935525467441
train: step: 382, loss: 0.6936588287353516, acc: 0.475, recall: 0.475, precision: 0.47150997150997154, f_beta: 0.4584139264990329
train: step: 383, loss: 0.6937934160232544, acc: 0.45, recall: 0.44999999999999996, precision: 0.43333333333333335, f_beta: 0.41333333333333333
train: step: 384, loss: 0.6934873461723328, acc: 0.45, recall: 0.44999999999999996, precision: 0.44505494505494503, f_beta: 0.4373401534526854
train: step: 385, loss: 0.6925666928291321, acc: 0.575, recall: 0.575, precision: 0.5854700854700855, f_beta: 0.5615731785944552
train: step: 386, loss: 0.6926060318946838, acc: 0.6, recall: 0.6, precision: 0.6041666666666667, f_beta: 0.595959595959596
train: step: 387, loss: 0.6920768618583679, acc: 0.575, recall: 0.575, precision: 0.575187969924812, f_beta: 0.5747342088805503
train: step: 388, loss: 0.695356547832489, acc: 0.4, recall: 0.4, precision: 0.38095238095238093, f_beta: 0.375
train: step: 389, loss: 0.6895036101341248, acc: 0.625, recall: 0.625, precision: 0.62531328320802, f_beta: 0.6247654784240151
train: step: 390, loss: 0.6933977007865906, acc: 0.575, recall: 0.575, precision: 0.5767263427109974, f_beta: 0.5725958516656191
train: step: 391, loss: 0.6934751868247986, acc: 0.5, recall: 0.5, precision: 0.5, f_beta: 0.4949494949494949
train: step: 392, loss: 0.6863309144973755, acc: 0.6, recall: 0.6, precision: 0.696078431372549, f_beta: 0.5441595441595442
train: step: 393, loss: 0.6923624277114868, acc: 0.5, recall: 0.5, precision: 0.5, f_beta: 0.3730407523510972
train: step: 394, loss: 0.6894830465316772, acc: 0.55, recall: 0.55, precision: 0.763157894736842, f_beta: 0.4357366771159875
train: step: 395, loss: 0.6932876706123352, acc: 0.5, recall: 0.5, precision: 0.5, f_beta: 0.4987468671679198
train: step: 396, loss: 0.6945104598999023, acc: 0.4, recall: 0.4, precision: 0.3901098901098901, f_beta: 0.38618925831202044
train: step: 397, loss: 0.6943010091781616, acc: 0.525, recall: 0.525, precision: 0.5432900432900433, f_beta: 0.46890286512928026
train: step: 398, loss: 0.6933388113975525, acc: 0.45, recall: 0.45, precision: 0.45, f_beta: 0.45
train: step: 399, loss: 0.6927198171615601, acc: 0.5, recall: 0.5, precision: 0.5, f_beta: 0.5
train: step: 400, loss: 0.6939125061035156, acc: 0.475, recall: 0.475, precision: 0.47333333333333333, f_beta: 0.4666666666666666
train: step: 401, loss: 0.6926105618476868, acc: 0.55, recall: 0.55, precision: 0.5520833333333333, f_beta: 0.5454545454545454
train: step: 402, loss: 0.6935350298881531, acc: 0.4, recall: 0.4, precision: 0.398989898989899, f_beta: 0.39849624060150374
train: step: 403, loss: 0.6920534372329712, acc: 0.625, recall: 0.625, precision: 0.6278772378516624, f_beta: 0.6228786926461345
train: step: 404, loss: 0.6926102638244629, acc: 0.5, recall: 0.5, precision: 0.5, f_beta: 0.40476190476190477
train: step: 405, loss: 0.6931813955307007, acc: 0.5, recall: 0.5, precision: 0.5, f_beta: 0.4987468671679198
train: step: 406, loss: 0.6916897892951965, acc: 0.525, recall: 0.5249999999999999, precision: 0.5284900284900285, f_beta: 0.5099935525467441
train: step: 407, loss: 0.694209098815918, acc: 0.45, recall: 0.45, precision: 0.44047619047619047, f_beta: 0.42708333333333337
train: step: 408, loss: 0.6925557851791382, acc: 0.575, recall: 0.575, precision: 0.575187969924812, f_beta: 0.5747342088805503
train: step: 409, loss: 0.6918617486953735, acc: 0.6, recall: 0.6, precision: 0.6041666666666667, f_beta: 0.595959595959596
train: step: 410, loss: 0.6860928535461426, acc: 0.6, recall: 0.6, precision: 0.65625, f_beta: 0.5604395604395604
train: step: 411, loss: 0.692138671875, acc: 0.55, recall: 0.55, precision: 0.5520833333333333, f_beta: 0.5454545454545454
train: step: 412, loss: 0.6935647130012512, acc: 0.45, recall: 0.45, precision: 0.44791666666666663, f_beta: 0.4444444444444444
train: step: 413, loss: 0.6936294436454773, acc: 0.475, recall: 0.475, precision: 0.474937343358396, f_beta: 0.474671669793621
train: step: 414, loss: 0.6928434371948242, acc: 0.475, recall: 0.475, precision: 0.474937343358396, f_beta: 0.474671669793621
train: step: 415, loss: 0.6932551264762878, acc: 0.375, recall: 0.375, precision: 0.3721227621483376, f_beta: 0.3714644877435575
train: step: 416, loss: 0.6953300833702087, acc: 0.325, recall: 0.325, precision: 0.30056980056980054, f_beta: 0.3036750483558994
train: step: 417, loss: 0.6949355006217957, acc: 0.5, recall: 0.5, precision: 0.5, f_beta: 0.4949494949494949
train: step: 418, loss: 0.6921675801277161, acc: 0.55, recall: 0.55, precision: 0.5520833333333333, f_beta: 0.5454545454545454
train: step: 419, loss: 0.6877897381782532, acc: 0.525, recall: 0.525, precision: 0.5266666666666666, f_beta: 0.5174603174603175
train: step: 420, loss: 0.6939327120780945, acc: 0.5, recall: 0.5, precision: 0.5, f_beta: 0.4505494505494506
train: step: 421, loss: 0.692410945892334, acc: 0.525, recall: 0.5249999999999999, precision: 0.5284900284900285, f_beta: 0.5099935525467441
train: step: 422, loss: 0.6922133564949036, acc: 0.525, recall: 0.525, precision: 0.53584229390681, f_beta: 0.4861392832995267
train: step: 423, loss: 0.6934428215026855, acc: 0.475, recall: 0.47500000000000003, precision: 0.4744245524296675, f_beta: 0.47203016970458833
train: step: 424, loss: 0.6946640014648438, acc: 0.525, recall: 0.5249999999999999, precision: 0.5284900284900285, f_beta: 0.5099935525467441
train: step: 425, loss: 0.6932567358016968, acc: 0.5, recall: 0.5, precision: 0.5, f_beta: 0.47916666666666663
train: step: 426, loss: 0.6931287050247192, acc: 0.525, recall: 0.5249999999999999, precision: 0.5284900284900285, f_beta: 0.5099935525467441
train: step: 427, loss: 0.6936105489730835, acc: 0.475, recall: 0.475, precision: 0.46865203761755486, f_beta: 0.4470046082949308
train: step: 428, loss: 0.6923028230667114, acc: 0.525, recall: 0.525, precision: 0.53584229390681, f_beta: 0.4861392832995267
train: step: 429, loss: 0.6934653520584106, acc: 0.475, recall: 0.475, precision: 0.474937343358396, f_beta: 0.474671669793621
train: step: 430, loss: 0.6931971907615662, acc: 0.45, recall: 0.44999999999999996, precision: 0.44505494505494503, f_beta: 0.4373401534526854
train: step: 431, loss: 0.6949309706687927, acc: 0.425, recall: 0.42500000000000004, precision: 0.424812030075188, f_beta: 0.4246404002501563
train: step: 432, loss: 0.6926227807998657, acc: 0.625, recall: 0.625, precision: 0.62531328320802, f_beta: 0.6247654784240151
train: step: 433, loss: 0.6950286030769348, acc: 0.425, recall: 0.42500000000000004, precision: 0.424812030075188, f_beta: 0.4246404002501563
train: step: 434, loss: 0.6937946081161499, acc: 0.475, recall: 0.475, precision: 0.24358974358974358, f_beta: 0.3220338983050847
train: step: 435, loss: 0.6936496496200562, acc: 0.525, recall: 0.525, precision: 0.525062656641604, f_beta: 0.5247029393370857
train: step: 436, loss: 0.6933996081352234, acc: 0.5, recall: 0.5, precision: 0.5, f_beta: 0.4666666666666667
train: step: 437, loss: 0.6970452666282654, acc: 0.45, recall: 0.45, precision: 0.3611111111111111, f_beta: 0.34523809523809523
train: step: 438, loss: 0.6919369697570801, acc: 0.55, recall: 0.55, precision: 0.5595238095238095, f_beta: 0.53125
train: step: 439, loss: 0.6919986009597778, acc: 0.6, recall: 0.6000000000000001, precision: 0.601010101010101, f_beta: 0.5989974937343359
train: step: 440, loss: 0.6899093389511108, acc: 0.475, recall: 0.475, precision: 0.46865203761755486, f_beta: 0.4470046082949308
train: step: 441, loss: 0.6961771249771118, acc: 0.425, recall: 0.42500000000000004, precision: 0.424812030075188, f_beta: 0.4246404002501563
train: step: 442, loss: 0.6927959322929382, acc: 0.55, recall: 0.55, precision: 0.55, f_beta: 0.55
train: step: 443, loss: 0.6944533586502075, acc: 0.375, recall: 0.375, precision: 0.3575498575498576, f_beta: 0.3552546744036106
train: step: 444, loss: 0.6947340965270996, acc: 0.425, recall: 0.425, precision: 0.4145299145299145, f_beta: 0.40683430045132174
train: step: 445, loss: 0.6949433088302612, acc: 0.325, recall: 0.325, precision: 0.31333333333333335, f_beta: 0.3142857142857143
train: step: 446, loss: 0.6931701898574829, acc: 0.625, recall: 0.625, precision: 0.6424501424501424, f_beta: 0.6131528046421664
train: step: 447, loss: 0.6935110092163086, acc: 0.525, recall: 0.525, precision: 0.525062656641604, f_beta: 0.5247029393370857
train: step: 448, loss: 0.6939573287963867, acc: 0.5, recall: 0.5, precision: 0.5, f_beta: 0.5
train: step: 449, loss: 0.6948026418685913, acc: 0.375, recall: 0.375, precision: 0.3721227621483376, f_beta: 0.3714644877435575
train: step: 450, loss: 0.6938748359680176, acc: 0.4, recall: 0.4, precision: 0.398989898989899, f_beta: 0.39849624060150374
train: step: 451, loss: 0.6924928426742554, acc: 0.525, recall: 0.525, precision: 0.53584229390681, f_beta: 0.4861392832995267
train: step: 452, loss: 0.6934919953346252, acc: 0.475, recall: 0.475, precision: 0.47150997150997154, f_beta: 0.4584139264990329
train: step: 453, loss: 0.692756175994873, acc: 0.475, recall: 0.47500000000000003, precision: 0.4744245524296675, f_beta: 0.47203016970458833
train: step: 454, loss: 0.6946927309036255, acc: 0.425, recall: 0.42500000000000004, precision: 0.42000000000000004, f_beta: 0.4158730158730159
train: step: 455, loss: 0.6842684745788574, acc: 0.575, recall: 0.575, precision: 0.6298701298701299, f_beta: 0.5248078266946192
train: step: 456, loss: 0.6919710040092468, acc: 0.5, recall: 0.5, precision: 0.5, f_beta: 0.47916666666666663
train: step: 457, loss: 0.6945807337760925, acc: 0.5, recall: 0.5, precision: 0.5, f_beta: 0.40476190476190477
train: step: 458, loss: 0.692510724067688, acc: 0.475, recall: 0.47500000000000003, precision: 0.4744245524296675, f_beta: 0.47203016970458833
train: step: 459, loss: 0.6892237663269043, acc: 0.6, recall: 0.6000000000000001, precision: 0.601010101010101, f_beta: 0.5989974937343359
train: step: 460, loss: 0.672715961933136, acc: 0.65, recall: 0.65, precision: 0.7, f_beta: 0.6266666666666667
train: step: 461, loss: 0.6950143575668335, acc: 0.425, recall: 0.425, precision: 0.22972972972972974, f_beta: 0.2982456140350877
train: step: 462, loss: 0.693576455116272, acc: 0.4, recall: 0.4, precision: 0.38095238095238093, f_beta: 0.375
train: step: 463, loss: 0.6894168853759766, acc: 0.525, recall: 0.525, precision: 0.5571428571428572, f_beta: 0.44727272727272727
train: step: 464, loss: 0.692330539226532, acc: 0.475, recall: 0.475, precision: 0.47333333333333333, f_beta: 0.4666666666666666
train: step: 465, loss: 0.6931391954421997, acc: 0.45, recall: 0.45, precision: 0.421875, f_beta: 0.39560439560439553
train: step: 466, loss: 0.6928272247314453, acc: 0.55, recall: 0.55, precision: 0.5666666666666667, f_beta: 0.52
train: step: 467, loss: 0.6738805770874023, acc: 0.675, recall: 0.675, precision: 0.7508960573476702, f_beta: 0.6484110885733605
train: step: 468, loss: 0.6945022344589233, acc: 0.4, recall: 0.4, precision: 0.398989898989899, f_beta: 0.39849624060150374
train: step: 469, loss: 0.6939443349838257, acc: 0.425, recall: 0.42500000000000004, precision: 0.42000000000000004, f_beta: 0.4158730158730159
train: step: 470, loss: 0.6931081414222717, acc: 0.6, recall: 0.6, precision: 0.6, f_beta: 0.6
train: step: 471, loss: 0.6945384740829468, acc: 0.325, recall: 0.325, precision: 0.280564263322884, f_beta: 0.2890059249506254
train: step: 472, loss: 0.6927883625030518, acc: 0.6, recall: 0.6000000000000001, precision: 0.601010101010101, f_beta: 0.5989974937343359
train: step: 473, loss: 0.6944893002510071, acc: 0.4, recall: 0.4, precision: 0.3666666666666667, f_beta: 0.36
train: step: 474, loss: 0.6918441653251648, acc: 0.575, recall: 0.575, precision: 0.5800000000000001, f_beta: 0.5682539682539682
train: step: 475, loss: 0.6980666518211365, acc: 0.475, recall: 0.47500000000000003, precision: 0.4744245524296675, f_beta: 0.47203016970458833
train: step: 476, loss: 0.6926234364509583, acc: 0.575, recall: 0.575, precision: 0.5854700854700855, f_beta: 0.5615731785944552
train: step: 477, loss: 0.6930270195007324, acc: 0.6, recall: 0.6000000000000001, precision: 0.601010101010101, f_beta: 0.5989974937343359
train: step: 478, loss: 0.6920713186264038, acc: 0.55, recall: 0.55, precision: 0.554945054945055, f_beta: 0.5396419437340153
train: step: 479, loss: 0.696578860282898, acc: 0.425, recall: 0.425, precision: 0.42327365728900257, f_beta: 0.4217473287240729
train: step: 480, loss: 0.6950098276138306, acc: 0.425, recall: 0.42500000000000004, precision: 0.42000000000000004, f_beta: 0.4158730158730159
train: step: 481, loss: 0.693845272064209, acc: 0.55, recall: 0.55, precision: 0.578125, f_beta: 0.5054945054945055
train: step: 482, loss: 0.6902970671653748, acc: 0.525, recall: 0.525, precision: 0.5900900900900901, f_beta: 0.4202898550724638
train: step: 483, loss: 0.6925557851791382, acc: 0.575, recall: 0.575, precision: 0.575187969924812, f_beta: 0.5747342088805503
train: step: 484, loss: 0.6909326910972595, acc: 0.5, recall: 0.5, precision: 0.5, f_beta: 0.4949494949494949
train: step: 485, loss: 0.6964324712753296, acc: 0.475, recall: 0.47500000000000003, precision: 0.40990990990990994, f_beta: 0.35926773455377575
train: step: 486, loss: 0.694501519203186, acc: 0.4, recall: 0.4, precision: 0.398989898989899, f_beta: 0.39849624060150374
train: step: 487, loss: 0.6936534643173218, acc: 0.45, recall: 0.45, precision: 0.3611111111111111, f_beta: 0.34523809523809523
train: step: 488, loss: 0.6925755739212036, acc: 0.625, recall: 0.625, precision: 0.6278772378516624, f_beta: 0.6228786926461345
train: step: 489, loss: 0.6924343109130859, acc: 0.525, recall: 0.525, precision: 0.525062656641604, f_beta: 0.5247029393370857
train: step: 490, loss: 0.6935631036758423, acc: 0.45, recall: 0.45, precision: 0.44791666666666663, f_beta: 0.4444444444444444
train: step: 491, loss: 0.6934260129928589, acc: 0.475, recall: 0.47500000000000003, precision: 0.4744245524296675, f_beta: 0.47203016970458833
train: step: 492, loss: 0.6937564611434937, acc: 0.375, recall: 0.375, precision: 0.3666666666666667, f_beta: 0.3650793650793651
train: step: 493, loss: 0.6924294233322144, acc: 0.425, recall: 0.42500000000000004, precision: 0.42000000000000004, f_beta: 0.4158730158730159
train: step: 494, loss: 0.6948188543319702, acc: 0.4, recall: 0.4, precision: 0.39583333333333337, f_beta: 0.3939393939393939
train: step: 495, loss: 0.6906825304031372, acc: 0.575, recall: 0.575, precision: 0.60752688172043, f_beta: 0.5402298850574712
train: step: 496, loss: 0.6969539523124695, acc: 0.425, recall: 0.425, precision: 0.42327365728900257, f_beta: 0.4217473287240729
train: step: 497, loss: 0.6932999491691589, acc: 0.525, recall: 0.525, precision: 0.5255754475703325, f_beta: 0.5223130106851037
train: step: 498, loss: 0.6924448013305664, acc: 0.475, recall: 0.475, precision: 0.474937343358396, f_beta: 0.474671669793621
train: step: 499, loss: 0.691928505897522, acc: 0.625, recall: 0.625, precision: 0.6278772378516624, f_beta: 0.6228786926461345
train: step: 500, loss: 0.6996980309486389, acc: 0.45, recall: 0.44999999999999996, precision: 0.44505494505494503, f_beta: 0.4373401534526854
train: step: 501, loss: 0.6943858861923218, acc: 0.475, recall: 0.475, precision: 0.44285714285714284, f_beta: 0.38909090909090915
train: step: 502, loss: 0.694545567035675, acc: 0.375, recall: 0.375, precision: 0.3666666666666667, f_beta: 0.3650793650793651
train: step: 503, loss: 0.6933369636535645, acc: 0.45, recall: 0.45, precision: 0.421875, f_beta: 0.39560439560439553
train: step: 504, loss: 0.6941438317298889, acc: 0.45, recall: 0.45, precision: 0.44047619047619047, f_beta: 0.42708333333333337
train: step: 505, loss: 0.6974858045578003, acc: 0.35, recall: 0.35, precision: 0.35, f_beta: 0.35
train: step: 506, loss: 0.6881858110427856, acc: 0.55, recall: 0.55, precision: 0.5666666666666667, f_beta: 0.52
train: step: 507, loss: 0.6978369951248169, acc: 0.45, recall: 0.44999999999999996, precision: 0.43333333333333335, f_beta: 0.41333333333333333
train: step: 508, loss: 0.6927775144577026, acc: 0.525, recall: 0.5249999999999999, precision: 0.5284900284900285, f_beta: 0.5099935525467441
train: step: 509, loss: 0.6927570104598999, acc: 0.525, recall: 0.525, precision: 0.5266666666666666, f_beta: 0.5174603174603175
train: step: 510, loss: 0.691228985786438, acc: 0.575, recall: 0.575, precision: 0.575187969924812, f_beta: 0.5747342088805503
train: step: 511, loss: 0.6901217699050903, acc: 0.525, recall: 0.525, precision: 0.53584229390681, f_beta: 0.4861392832995267
train: step: 512, loss: 0.6937088370323181, acc: 0.425, recall: 0.425, precision: 0.3701298701298701, f_beta: 0.35709294199860236
train: step: 513, loss: 0.6940943002700806, acc: 0.45, recall: 0.44999999999999996, precision: 0.43333333333333335, f_beta: 0.41333333333333333
train: step: 514, loss: 0.6925281286239624, acc: 0.625, recall: 0.625, precision: 0.6792114695340502, f_beta: 0.5943204868154157
train: step: 515, loss: 0.6923274397850037, acc: 0.525, recall: 0.5249999999999999, precision: 0.5284900284900285, f_beta: 0.5099935525467441
train: step: 516, loss: 0.6921815276145935, acc: 0.5, recall: 0.5, precision: 0.5, f_beta: 0.43019943019943013
train: step: 517, loss: 0.6944236755371094, acc: 0.45, recall: 0.45, precision: 0.45, f_beta: 0.45
train: step: 518, loss: 0.6932141184806824, acc: 0.5, recall: 0.5, precision: 0.5, f_beta: 0.47916666666666663
train: step: 519, loss: 0.6954008936882019, acc: 0.35, recall: 0.35, precision: 0.3351648351648352, f_beta: 0.3350383631713555
train: step: 520, loss: 0.6952053308486938, acc: 0.425, recall: 0.42500000000000004, precision: 0.424812030075188, f_beta: 0.4246404002501563
train: step: 521, loss: 0.6934962272644043, acc: 0.575, recall: 0.575, precision: 0.5767263427109974, f_beta: 0.5725958516656191
train: step: 522, loss: 0.6928458213806152, acc: 0.55, recall: 0.55, precision: 0.5520833333333333, f_beta: 0.5454545454545454
train: step: 523, loss: 0.6939606666564941, acc: 0.525, recall: 0.525, precision: 0.5313479623824451, f_beta: 0.4996708360763661
train: step: 524, loss: 0.6942564845085144, acc: 0.425, recall: 0.425, precision: 0.42327365728900257, f_beta: 0.4217473287240729
train: step: 525, loss: 0.6891775727272034, acc: 0.65, recall: 0.6499999999999999, precision: 0.734375, f_beta: 0.6153846153846154
train: step: 526, loss: 0.6940954327583313, acc: 0.425, recall: 0.42500000000000004, precision: 0.4059561128526646, f_beta: 0.39433838051349573
train: step: 527, loss: 0.6878848075866699, acc: 0.625, recall: 0.625, precision: 0.62531328320802, f_beta: 0.6247654784240151
train: step: 528, loss: 0.7012723684310913, acc: 0.35, recall: 0.35, precision: 0.3484848484848485, f_beta: 0.3483709273182958
train: step: 529, loss: 0.6942813992500305, acc: 0.525, recall: 0.525, precision: 0.53584229390681, f_beta: 0.4861392832995267
train: step: 530, loss: 0.6935387253761292, acc: 0.45, recall: 0.45, precision: 0.44791666666666663, f_beta: 0.4444444444444444
train: step: 531, loss: 0.6945969462394714, acc: 0.35, recall: 0.35000000000000003, precision: 0.3214285714285714, f_beta: 0.3229166666666667
train: step: 532, loss: 0.6941254138946533, acc: 0.525, recall: 0.525, precision: 0.525062656641604, f_beta: 0.5247029393370857
train: step: 533, loss: 0.6924682855606079, acc: 0.525, recall: 0.525, precision: 0.525062656641604, f_beta: 0.5247029393370857
train: step: 534, loss: 0.6889775395393372, acc: 0.55, recall: 0.5499999999999999, precision: 0.6388888888888888, f_beta: 0.46428571428571425
train: step: 535, loss: 0.6934669613838196, acc: 0.475, recall: 0.475, precision: 0.474937343358396, f_beta: 0.474671669793621
train: step: 536, loss: 0.6935875415802002, acc: 0.6, recall: 0.6, precision: 0.6098901098901099, f_beta: 0.5907928388746804
train: step: 537, loss: 0.6935046315193176, acc: 0.475, recall: 0.475, precision: 0.474937343358396, f_beta: 0.474671669793621
train: step: 538, loss: 0.692825198173523, acc: 0.5, recall: 0.5, precision: 0.5, f_beta: 0.4666666666666667
train: step: 539, loss: 0.6929729580879211, acc: 0.525, recall: 0.525, precision: 0.5313479623824451, f_beta: 0.4996708360763661
train: step: 540, loss: 0.6928507089614868, acc: 0.55, recall: 0.55, precision: 0.5505050505050505, f_beta: 0.5488721804511277
train: step: 541, loss: 0.693056583404541, acc: 0.525, recall: 0.525, precision: 0.5255754475703325, f_beta: 0.5223130106851037
train: step: 542, loss: 0.6925239562988281, acc: 0.5, recall: 0.5, precision: 0.5, f_beta: 0.40476190476190477
train: step: 543, loss: 0.6909448504447937, acc: 0.55, recall: 0.55, precision: 0.5666666666666667, f_beta: 0.52
train: step: 544, loss: 0.6929911375045776, acc: 0.45, recall: 0.45, precision: 0.44791666666666663, f_beta: 0.4444444444444444
train: step: 545, loss: 0.6941835284233093, acc: 0.4, recall: 0.4, precision: 0.3901098901098901, f_beta: 0.38618925831202044
train: step: 546, loss: 0.6912809014320374, acc: 0.575, recall: 0.575, precision: 0.60752688172043, f_beta: 0.5402298850574712
train: step: 547, loss: 0.6920499205589294, acc: 0.625, recall: 0.625, precision: 0.6278772378516624, f_beta: 0.6228786926461345
train: step: 548, loss: 0.6934095621109009, acc: 0.425, recall: 0.425, precision: 0.42327365728900257, f_beta: 0.4217473287240729
train: step: 549, loss: 0.6951766014099121, acc: 0.45, recall: 0.45, precision: 0.3611111111111111, f_beta: 0.34523809523809523
train: step: 550, loss: 0.6924017667770386, acc: 0.525, recall: 0.5249999999999999, precision: 0.5284900284900285, f_beta: 0.5099935525467441
train: step: 551, loss: 0.6927710771560669, acc: 0.5, recall: 0.5, precision: 0.5, f_beta: 0.4949494949494949
train: step: 552, loss: 0.6911935210227966, acc: 0.65, recall: 0.65, precision: 0.65625, f_beta: 0.6464646464646464
train: step: 553, loss: 0.6931249499320984, acc: 0.525, recall: 0.525, precision: 0.5255754475703325, f_beta: 0.5223130106851037
train: step: 554, loss: 0.694124698638916, acc: 0.425, recall: 0.425, precision: 0.42327365728900257, f_beta: 0.4217473287240729
train: step: 555, loss: 0.6922195553779602, acc: 0.5, recall: 0.5, precision: 0.5, f_beta: 0.3730407523510972
train: step: 556, loss: 0.693985641002655, acc: 0.45, recall: 0.45, precision: 0.421875, f_beta: 0.39560439560439553
train: step: 557, loss: 0.6931508779525757, acc: 0.575, recall: 0.575, precision: 0.5854700854700855, f_beta: 0.5615731785944552
train: step: 558, loss: 0.6964871287345886, acc: 0.375, recall: 0.375, precision: 0.37468671679197996, f_beta: 0.37460913070669166
train: step: 559, loss: 0.692838191986084, acc: 0.575, recall: 0.575, precision: 0.5940438871473355, f_beta: 0.5523370638578011
train: step: 560, loss: 0.6926485300064087, acc: 0.6, recall: 0.6, precision: 0.65625, f_beta: 0.5604395604395604
train: step: 561, loss: 0.690007209777832, acc: 0.575, recall: 0.575, precision: 0.5767263427109974, f_beta: 0.5725958516656191
train: step: 562, loss: 0.6939790844917297, acc: 0.475, recall: 0.47500000000000003, precision: 0.4744245524296675, f_beta: 0.47203016970458833
train: step: 563, loss: 0.6942657232284546, acc: 0.475, recall: 0.47500000000000003, precision: 0.45670995670995673, f_beta: 0.41299790356394134
train: step: 564, loss: 0.6923729777336121, acc: 0.6, recall: 0.6, precision: 0.65625, f_beta: 0.5604395604395604
train: step: 565, loss: 0.6928004622459412, acc: 0.475, recall: 0.47500000000000003, precision: 0.40990990990990994, f_beta: 0.35926773455377575
train: step: 566, loss: 0.6939589381217957, acc: 0.45, recall: 0.45, precision: 0.44047619047619047, f_beta: 0.42708333333333337
train: step: 567, loss: 0.6931668519973755, acc: 0.525, recall: 0.525, precision: 0.5266666666666666, f_beta: 0.5174603174603175
train: step: 568, loss: 0.6950674057006836, acc: 0.425, recall: 0.425, precision: 0.42327365728900257, f_beta: 0.4217473287240729
train: step: 569, loss: 0.6925481557846069, acc: 0.55, recall: 0.55, precision: 0.5505050505050505, f_beta: 0.5488721804511277
train: step: 570, loss: 0.6972627639770508, acc: 0.45, recall: 0.44999999999999996, precision: 0.44505494505494503, f_beta: 0.4373401534526854
train: step: 571, loss: 0.6929997801780701, acc: 0.45, recall: 0.44999999999999996, precision: 0.43333333333333335, f_beta: 0.41333333333333333
train: step: 572, loss: 0.7011626958847046, acc: 0.475, recall: 0.475, precision: 0.46415770609318996, f_beta: 0.43204868154158216
train: step: 573, loss: 0.695722222328186, acc: 0.45, recall: 0.44999999999999996, precision: 0.43333333333333335, f_beta: 0.41333333333333333
train: step: 574, loss: 0.6902719736099243, acc: 0.65, recall: 0.65, precision: 0.7, f_beta: 0.6266666666666667
train: step: 575, loss: 0.6917957067489624, acc: 0.55, recall: 0.55, precision: 0.554945054945055, f_beta: 0.5396419437340153
train: step: 576, loss: 0.6933751702308655, acc: 0.475, recall: 0.47500000000000003, precision: 0.4744245524296675, f_beta: 0.47203016970458833
train: step: 577, loss: 0.6958562135696411, acc: 0.45, recall: 0.45, precision: 0.4019607843137255, f_beta: 0.37321937321937326
train: step: 578, loss: 0.6961639523506165, acc: 0.5, recall: 0.5, precision: 0.5, f_beta: 0.3730407523510972
train: step: 579, loss: 0.6975151896476746, acc: 0.45, recall: 0.45, precision: 0.421875, f_beta: 0.39560439560439553
train: step: 580, loss: 0.6924325227737427, acc: 0.525, recall: 0.525, precision: 0.5900900900900901, f_beta: 0.4202898550724638
train: step: 581, loss: 0.6928923726081848, acc: 0.475, recall: 0.475, precision: 0.46865203761755486, f_beta: 0.4470046082949308
train: step: 582, loss: 0.6918253898620605, acc: 0.6, recall: 0.6, precision: 0.6333333333333333, f_beta: 0.5733333333333334
train: step: 583, loss: 0.6929513216018677, acc: 0.425, recall: 0.42500000000000004, precision: 0.42000000000000004, f_beta: 0.4158730158730159
train: step: 584, loss: 0.6919926404953003, acc: 0.6, recall: 0.6000000000000001, precision: 0.601010101010101, f_beta: 0.5989974937343359
train: step: 585, loss: 0.6932412385940552, acc: 0.475, recall: 0.475, precision: 0.46865203761755486, f_beta: 0.4470046082949308
train: step: 586, loss: 0.6919442415237427, acc: 0.5, recall: 0.5, precision: 0.5, f_beta: 0.43019943019943013
train: step: 587, loss: 0.6931460499763489, acc: 0.525, recall: 0.525, precision: 0.525062656641604, f_beta: 0.5247029393370857
train: step: 588, loss: 0.6935893297195435, acc: 0.475, recall: 0.47500000000000003, precision: 0.4744245524296675, f_beta: 0.47203016970458833
train: step: 589, loss: 0.6917555928230286, acc: 0.6, recall: 0.6, precision: 0.696078431372549, f_beta: 0.5441595441595442
train: step: 590, loss: 0.6928209066390991, acc: 0.425, recall: 0.425, precision: 0.42327365728900257, f_beta: 0.4217473287240729
train: step: 591, loss: 0.6923872232437134, acc: 0.5, recall: 0.5, precision: 0.5, f_beta: 0.5
train: step: 592, loss: 0.6922036409378052, acc: 0.525, recall: 0.525, precision: 0.525062656641604, f_beta: 0.5247029393370857
train: step: 593, loss: 0.6932103037834167, acc: 0.475, recall: 0.47500000000000003, precision: 0.4744245524296675, f_beta: 0.47203016970458833
train: step: 594, loss: 0.6925088763237, acc: 0.675, recall: 0.675, precision: 0.6866666666666666, f_beta: 0.6698412698412699
train: step: 595, loss: 0.6929327845573425, acc: 0.525, recall: 0.5249999999999999, precision: 0.5284900284900285, f_beta: 0.5099935525467441
train: step: 596, loss: 0.6937472820281982, acc: 0.4, recall: 0.4, precision: 0.4, f_beta: 0.4000000000000001
train: step: 597, loss: 0.6934203505516052, acc: 0.525, recall: 0.525, precision: 0.53584229390681, f_beta: 0.4861392832995267
train: step: 598, loss: 0.6949346661567688, acc: 0.425, recall: 0.42500000000000004, precision: 0.424812030075188, f_beta: 0.4246404002501563
train: step: 599, loss: 0.6900971531867981, acc: 0.7, recall: 0.7, precision: 0.8125, f_beta: 0.6703296703296704
train: step: 600, loss: 0.6932315826416016, acc: 0.525, recall: 0.5249999999999999, precision: 0.5284900284900285, f_beta: 0.5099935525467441
train: step: 601, loss: 0.695330023765564, acc: 0.375, recall: 0.375, precision: 0.34326018808777425, f_beta: 0.34167215273206053
train: step: 602, loss: 0.6921663284301758, acc: 0.65, recall: 0.6499999999999999, precision: 0.6515151515151515, f_beta: 0.6491228070175439
train: step: 603, loss: 0.6956769227981567, acc: 0.4, recall: 0.39999999999999997, precision: 0.34375, f_beta: 0.34065934065934067
train: step: 604, loss: 0.6937311291694641, acc: 0.4, recall: 0.4, precision: 0.3901098901098901, f_beta: 0.38618925831202044
train: step: 605, loss: 0.693282961845398, acc: 0.4, recall: 0.39999999999999997, precision: 0.34375, f_beta: 0.34065934065934067
train: step: 606, loss: 0.6898711919784546, acc: 0.55, recall: 0.55, precision: 0.578125, f_beta: 0.5054945054945055
train: step: 607, loss: 0.6951961517333984, acc: 0.475, recall: 0.475, precision: 0.46415770609318996, f_beta: 0.43204868154158216
train: step: 608, loss: 0.6930434107780457, acc: 0.575, recall: 0.575, precision: 0.5854700854700855, f_beta: 0.5615731785944552
train: step: 609, loss: 0.6931138038635254, acc: 0.475, recall: 0.475, precision: 0.47333333333333333, f_beta: 0.4666666666666666
train: step: 610, loss: 0.6924705505371094, acc: 0.575, recall: 0.575, precision: 0.575187969924812, f_beta: 0.5747342088805503
train: step: 611, loss: 0.694182276725769, acc: 0.35, recall: 0.35, precision: 0.3351648351648352, f_beta: 0.3350383631713555
train: step: 612, loss: 0.6972624659538269, acc: 0.425, recall: 0.425, precision: 0.42327365728900257, f_beta: 0.4217473287240729
train: step: 613, loss: 0.692589282989502, acc: 0.475, recall: 0.47500000000000003, precision: 0.45670995670995673, f_beta: 0.41299790356394134
train: step: 614, loss: 0.6934796571731567, acc: 0.4, recall: 0.4, precision: 0.3666666666666667, f_beta: 0.36
train: step: 615, loss: 0.6921944618225098, acc: 0.575, recall: 0.575, precision: 0.575187969924812, f_beta: 0.5747342088805503
train: step: 616, loss: 0.6935111284255981, acc: 0.475, recall: 0.475, precision: 0.474937343358396, f_beta: 0.474671669793621
train: step: 617, loss: 0.6972363591194153, acc: 0.3, recall: 0.3, precision: 0.297979797979798, f_beta: 0.2982456140350877
train: step: 618, loss: 0.6927046775817871, acc: 0.55, recall: 0.55, precision: 0.5520833333333333, f_beta: 0.5454545454545454
train: step: 619, loss: 0.6931154131889343, acc: 0.525, recall: 0.5249999999999999, precision: 0.5284900284900285, f_beta: 0.5099935525467441
train: step: 620, loss: 0.6939292550086975, acc: 0.45, recall: 0.44999999999999996, precision: 0.43333333333333335, f_beta: 0.41333333333333333
train: step: 621, loss: 0.6944604516029358, acc: 0.375, recall: 0.375, precision: 0.28354978354978355, f_beta: 0.3011879804332634
train: step: 622, loss: 0.6913886070251465, acc: 0.65, recall: 0.65, precision: 0.65625, f_beta: 0.6464646464646464
train: step: 623, loss: 0.6940183043479919, acc: 0.475, recall: 0.475, precision: 0.47333333333333333, f_beta: 0.4666666666666666
train: step: 624, loss: 0.692569375038147, acc: 0.5, recall: 0.5, precision: 0.5, f_beta: 0.4666666666666667
train: step: 625, loss: 0.6939300298690796, acc: 0.525, recall: 0.525, precision: 0.53584229390681, f_beta: 0.4861392832995267
train: step: 626, loss: 0.6945549249649048, acc: 0.45, recall: 0.44999999999999996, precision: 0.44505494505494503, f_beta: 0.4373401534526854
train: step: 627, loss: 0.6926614046096802, acc: 0.55, recall: 0.55, precision: 0.5505050505050505, f_beta: 0.5488721804511277
train: step: 628, loss: 0.6930427551269531, acc: 0.525, recall: 0.5249999999999999, precision: 0.5284900284900285, f_beta: 0.5099935525467441
train: step: 629, loss: 0.6929196119308472, acc: 0.525, recall: 0.525, precision: 0.5266666666666666, f_beta: 0.5174603174603175
train: step: 630, loss: 0.6926180720329285, acc: 0.55, recall: 0.55, precision: 0.5595238095238095, f_beta: 0.53125
train: step: 631, loss: 0.691437840461731, acc: 0.6, recall: 0.6, precision: 0.6098901098901099, f_beta: 0.5907928388746804
train: step: 632, loss: 0.6812914609909058, acc: 0.675, recall: 0.675, precision: 0.7508960573476702, f_beta: 0.6484110885733605
train: step: 633, loss: 0.6919395923614502, acc: 0.625, recall: 0.625, precision: 0.62531328320802, f_beta: 0.6247654784240151
train: step: 634, loss: 0.6901100873947144, acc: 0.575, recall: 0.575, precision: 0.6298701298701299, f_beta: 0.5248078266946192
train: step: 635, loss: 0.6917954683303833, acc: 0.7, recall: 0.7, precision: 0.702020202020202, f_beta: 0.6992481203007519
train: step: 636, loss: 0.692966103553772, acc: 0.525, recall: 0.525, precision: 0.525062656641604, f_beta: 0.5247029393370857
train: step: 637, loss: 0.6898856163024902, acc: 0.6, recall: 0.6, precision: 0.65625, f_beta: 0.5604395604395604
train: step: 638, loss: 0.6935121417045593, acc: 0.45, recall: 0.45, precision: 0.44791666666666663, f_beta: 0.4444444444444444
train: step: 639, loss: 0.6932567358016968, acc: 0.45, recall: 0.45, precision: 0.4019607843137255, f_beta: 0.37321937321937326
train: step: 640, loss: 0.6909841299057007, acc: 0.625, recall: 0.625, precision: 0.6278772378516624, f_beta: 0.6228786926461345
train: step: 641, loss: 0.6894977688789368, acc: 0.65, recall: 0.6499999999999999, precision: 0.734375, f_beta: 0.6153846153846154
train: step: 642, loss: 0.6923577189445496, acc: 0.55, recall: 0.55, precision: 0.5666666666666667, f_beta: 0.52
train: step: 643, loss: 0.6934890747070312, acc: 0.45, recall: 0.45, precision: 0.45, f_beta: 0.45
train: step: 644, loss: 0.6896457672119141, acc: 0.55, recall: 0.55, precision: 0.578125, f_beta: 0.5054945054945055
train: step: 645, loss: 0.6944277882575989, acc: 0.475, recall: 0.475, precision: 0.47150997150997154, f_beta: 0.4584139264990329
train: step: 646, loss: 0.6918612718582153, acc: 0.625, recall: 0.625, precision: 0.6278772378516624, f_beta: 0.6228786926461345
train: step: 647, loss: 0.6917604207992554, acc: 0.55, recall: 0.55, precision: 0.5520833333333333, f_beta: 0.5454545454545454
train: step: 648, loss: 0.690829336643219, acc: 0.675, recall: 0.675, precision: 0.6790281329923273, f_beta: 0.6731615336266499
train: step: 649, loss: 0.6933492422103882, acc: 0.475, recall: 0.475, precision: 0.474937343358396, f_beta: 0.474671669793621
train: step: 650, loss: 0.691016674041748, acc: 0.525, recall: 0.525, precision: 0.525062656641604, f_beta: 0.5247029393370857
train: step: 651, loss: 0.6928722262382507, acc: 0.525, recall: 0.525, precision: 0.5900900900900901, f_beta: 0.4202898550724638
train: step: 652, loss: 0.6938856244087219, acc: 0.375, recall: 0.375, precision: 0.34326018808777425, f_beta: 0.34167215273206053
train: step: 653, loss: 0.693583607673645, acc: 0.45, recall: 0.45, precision: 0.45, f_beta: 0.45
train: step: 654, loss: 0.6929256319999695, acc: 0.55, recall: 0.55, precision: 0.5505050505050505, f_beta: 0.5488721804511277
train: step: 655, loss: 0.6946203708648682, acc: 0.475, recall: 0.475, precision: 0.46415770609318996, f_beta: 0.43204868154158216
train: step: 656, loss: 0.6925765872001648, acc: 0.475, recall: 0.47500000000000003, precision: 0.4744245524296675, f_beta: 0.47203016970458833
train: step: 657, loss: 0.6943899989128113, acc: 0.35, recall: 0.35000000000000003, precision: 0.3214285714285714, f_beta: 0.3229166666666667
train: step: 658, loss: 0.6926266551017761, acc: 0.575, recall: 0.575, precision: 0.575187969924812, f_beta: 0.5747342088805503
train: step: 659, loss: 0.6921407580375671, acc: 0.575, recall: 0.575, precision: 0.575187969924812, f_beta: 0.5747342088805503
train: step: 660, loss: 0.6849341988563538, acc: 0.55, recall: 0.55, precision: 0.763157894736842, f_beta: 0.4357366771159875
train: step: 661, loss: 0.6937470436096191, acc: 0.475, recall: 0.47500000000000003, precision: 0.4744245524296675, f_beta: 0.47203016970458833
train: step: 662, loss: 0.6932100057601929, acc: 0.5, recall: 0.5, precision: 0.5, f_beta: 0.4949494949494949
train: step: 663, loss: 0.6957873702049255, acc: 0.45, recall: 0.45, precision: 0.44047619047619047, f_beta: 0.42708333333333337
train: step: 664, loss: 0.6933363676071167, acc: 0.5, recall: 0.5, precision: 0.5, f_beta: 0.40476190476190477
train: step: 665, loss: 0.6946197748184204, acc: 0.375, recall: 0.375, precision: 0.3575498575498576, f_beta: 0.3552546744036106
train: step: 666, loss: 0.6948977708816528, acc: 0.5, recall: 0.5, precision: 0.5, f_beta: 0.4666666666666667
train: step: 667, loss: 0.6913646459579468, acc: 0.525, recall: 0.525, precision: 0.5255754475703325, f_beta: 0.5223130106851037
train: step: 668, loss: 0.6942469477653503, acc: 0.475, recall: 0.47500000000000003, precision: 0.45670995670995673, f_beta: 0.41299790356394134
train: step: 669, loss: 0.6926875114440918, acc: 0.575, recall: 0.575, precision: 0.60752688172043, f_beta: 0.5402298850574712
train: step: 670, loss: 0.6968574523925781, acc: 0.325, recall: 0.325, precision: 0.280564263322884, f_beta: 0.2890059249506254
train: step: 671, loss: 0.6948688626289368, acc: 0.5, recall: 0.5, precision: 0.5, f_beta: 0.43019943019943013
train: step: 672, loss: 0.6954848170280457, acc: 0.375, recall: 0.375, precision: 0.34326018808777425, f_beta: 0.34167215273206053
train: step: 673, loss: 0.6917160153388977, acc: 0.575, recall: 0.575, precision: 0.575187969924812, f_beta: 0.5747342088805503
train: step: 674, loss: 0.6941317915916443, acc: 0.35, recall: 0.35, precision: 0.34375, f_beta: 0.3434343434343434
train: step: 675, loss: 0.693947970867157, acc: 0.475, recall: 0.475, precision: 0.474937343358396, f_beta: 0.474671669793621
train: step: 676, loss: 0.6929361820220947, acc: 0.475, recall: 0.475, precision: 0.474937343358396, f_beta: 0.474671669793621
train: step: 677, loss: 0.6927655935287476, acc: 0.5, recall: 0.5, precision: 0.5, f_beta: 0.47916666666666663
train: step: 678, loss: 0.693160891532898, acc: 0.475, recall: 0.475, precision: 0.46865203761755486, f_beta: 0.4470046082949308
train: step: 679, loss: 0.7036043405532837, acc: 0.425, recall: 0.425, precision: 0.22972972972972974, f_beta: 0.2982456140350877
train: step: 680, loss: 0.6945787668228149, acc: 0.45, recall: 0.45, precision: 0.44047619047619047, f_beta: 0.42708333333333337
train: step: 681, loss: 0.6932541131973267, acc: 0.475, recall: 0.475, precision: 0.47333333333333333, f_beta: 0.4666666666666666
train: step: 682, loss: 0.6985920667648315, acc: 0.45, recall: 0.44999999999999996, precision: 0.44505494505494503, f_beta: 0.4373401534526854
train: step: 683, loss: 0.692833423614502, acc: 0.525, recall: 0.5249999999999999, precision: 0.5284900284900285, f_beta: 0.5099935525467441
train: step: 684, loss: 0.6922596096992493, acc: 0.55, recall: 0.55, precision: 0.554945054945055, f_beta: 0.5396419437340153
train: step: 685, loss: 0.6931818723678589, acc: 0.5, recall: 0.5, precision: 0.5, f_beta: 0.5
train: step: 686, loss: 0.6918940544128418, acc: 0.5, recall: 0.5, precision: 0.5, f_beta: 0.4666666666666667
train: step: 687, loss: 0.6931643486022949, acc: 0.55, recall: 0.55, precision: 0.5595238095238095, f_beta: 0.53125
train: step: 688, loss: 0.6929473876953125, acc: 0.525, recall: 0.525, precision: 0.5266666666666666, f_beta: 0.5174603174603175
train: step: 689, loss: 0.6940802931785583, acc: 0.475, recall: 0.475, precision: 0.46415770609318996, f_beta: 0.43204868154158216
train: step: 690, loss: 0.6927489042282104, acc: 0.575, recall: 0.575, precision: 0.5767263427109974, f_beta: 0.5725958516656191
train: step: 691, loss: 0.6925765872001648, acc: 0.475, recall: 0.475, precision: 0.47150997150997154, f_beta: 0.4584139264990329
train: step: 692, loss: 0.6945042610168457, acc: 0.475, recall: 0.475, precision: 0.47150997150997154, f_beta: 0.4584139264990329
train: step: 693, loss: 0.6926918029785156, acc: 0.45, recall: 0.45, precision: 0.44791666666666663, f_beta: 0.4444444444444444
train: step: 694, loss: 0.6915677785873413, acc: 0.55, recall: 0.55, precision: 0.554945054945055, f_beta: 0.5396419437340153
train: step: 695, loss: 0.6928965449333191, acc: 0.475, recall: 0.47500000000000003, precision: 0.4744245524296675, f_beta: 0.47203016970458833
train: step: 696, loss: 0.6924829483032227, acc: 0.575, recall: 0.575, precision: 0.5800000000000001, f_beta: 0.5682539682539682
train: step: 697, loss: 0.6928582191467285, acc: 0.55, recall: 0.55, precision: 0.578125, f_beta: 0.5054945054945055
train: step: 698, loss: 0.6913219690322876, acc: 0.55, recall: 0.55, precision: 0.554945054945055, f_beta: 0.5396419437340153
train: step: 699, loss: 0.6916443109512329, acc: 0.625, recall: 0.625, precision: 0.6333333333333333, f_beta: 0.6190476190476191
train: step: 700, loss: 0.6931970715522766, acc: 0.6, recall: 0.6000000000000001, precision: 0.601010101010101, f_beta: 0.5989974937343359
train: step: 701, loss: 0.692999005317688, acc: 0.475, recall: 0.47500000000000003, precision: 0.45670995670995673, f_beta: 0.41299790356394134
train: step: 702, loss: 0.6926473379135132, acc: 0.55, recall: 0.55, precision: 0.554945054945055, f_beta: 0.5396419437340153
train: step: 703, loss: 0.6928387880325317, acc: 0.475, recall: 0.475, precision: 0.47150997150997154, f_beta: 0.4584139264990329
train: step: 704, loss: 0.6933887600898743, acc: 0.55, recall: 0.55, precision: 0.55, f_beta: 0.55
train: step: 705, loss: 0.6930509805679321, acc: 0.5, recall: 0.5, precision: 0.5, f_beta: 0.47916666666666663
train: step: 706, loss: 0.6917822957038879, acc: 0.725, recall: 0.7250000000000001, precision: 0.7301790281329923, f_beta: 0.7234443746071653
train: step: 707, loss: 0.6937000751495361, acc: 0.5, recall: 0.5, precision: 0.5, f_beta: 0.47916666666666663
train: step: 708, loss: 0.6941572427749634, acc: 0.375, recall: 0.375, precision: 0.37468671679197996, f_beta: 0.37460913070669166
train: step: 709, loss: 0.6928005814552307, acc: 0.55, recall: 0.55, precision: 0.5505050505050505, f_beta: 0.5488721804511277
train: step: 710, loss: 0.6922064423561096, acc: 0.525, recall: 0.525, precision: 0.525062656641604, f_beta: 0.5247029393370857
train: step: 711, loss: 0.6929420232772827, acc: 0.45, recall: 0.45, precision: 0.4494949494949495, f_beta: 0.4486215538847118
train: step: 712, loss: 0.6919285655021667, acc: 0.55, recall: 0.55, precision: 0.554945054945055, f_beta: 0.5396419437340153
train: step: 713, loss: 0.6927129626274109, acc: 0.575, recall: 0.575, precision: 0.5854700854700855, f_beta: 0.5615731785944552
train: step: 714, loss: 0.6928966045379639, acc: 0.525, recall: 0.525, precision: 0.525062656641604, f_beta: 0.5247029393370857
train: step: 715, loss: 0.694667637348175, acc: 0.375, recall: 0.375, precision: 0.37468671679197996, f_beta: 0.37460913070669166
train: step: 716, loss: 0.6944774389266968, acc: 0.5, recall: 0.5, precision: 0.5, f_beta: 0.4949494949494949
train: step: 717, loss: 0.692231297492981, acc: 0.6, recall: 0.6, precision: 0.6333333333333333, f_beta: 0.5733333333333334
train: step: 718, loss: 0.6929813027381897, acc: 0.5, recall: 0.5, precision: 0.5, f_beta: 0.4987468671679198
train: step: 719, loss: 0.6928340792655945, acc: 0.525, recall: 0.525, precision: 0.5266666666666666, f_beta: 0.5174603174603175
train: step: 720, loss: 0.6935914754867554, acc: 0.525, recall: 0.525, precision: 0.5255754475703325, f_beta: 0.5223130106851037
train: step: 721, loss: 0.6931848526000977, acc: 0.45, recall: 0.45, precision: 0.4494949494949495, f_beta: 0.4486215538847118
train: step: 722, loss: 0.6972149610519409, acc: 0.475, recall: 0.475, precision: 0.24358974358974358, f_beta: 0.3220338983050847
train: step: 723, loss: 0.691210150718689, acc: 0.5, recall: 0.5, precision: 0.5, f_beta: 0.4505494505494506
train: step: 724, loss: 0.692722737789154, acc: 0.5, recall: 0.5, precision: 0.5, f_beta: 0.4505494505494506
train: step: 725, loss: 0.6912585496902466, acc: 0.625, recall: 0.625, precision: 0.6567398119122256, f_beta: 0.6050032916392363
train: step: 726, loss: 0.692958652973175, acc: 0.55, recall: 0.55, precision: 0.5520833333333333, f_beta: 0.5454545454545454
train: step: 727, loss: 0.6918098330497742, acc: 0.575, recall: 0.575, precision: 0.60752688172043, f_beta: 0.5402298850574712
train: step: 728, loss: 0.6919208765029907, acc: 0.5, recall: 0.5, precision: 0.5, f_beta: 0.4987468671679198
train: step: 729, loss: 0.6927371621131897, acc: 0.525, recall: 0.525, precision: 0.525062656641604, f_beta: 0.5247029393370857
train: step: 730, loss: 0.6869268417358398, acc: 0.75, recall: 0.75, precision: 0.7976190476190477, f_beta: 0.7395833333333333
train: step: 731, loss: 0.694096565246582, acc: 0.475, recall: 0.475, precision: 0.46415770609318996, f_beta: 0.43204868154158216
train: step: 732, loss: 0.6930462121963501, acc: 0.675, recall: 0.675, precision: 0.6754385964912281, f_beta: 0.6747967479674797
train: step: 733, loss: 0.6952254176139832, acc: 0.475, recall: 0.475, precision: 0.474937343358396, f_beta: 0.474671669793621
train: step: 734, loss: 0.6928807497024536, acc: 0.475, recall: 0.475, precision: 0.474937343358396, f_beta: 0.474671669793621
train: step: 735, loss: 0.6923516392707825, acc: 0.575, recall: 0.575, precision: 0.5854700854700855, f_beta: 0.5615731785944552
train: step: 736, loss: 0.6916043758392334, acc: 0.6, recall: 0.6000000000000001, precision: 0.601010101010101, f_beta: 0.5989974937343359
train: step: 737, loss: 0.7006014585494995, acc: 0.4, recall: 0.39999999999999997, precision: 0.34375, f_beta: 0.34065934065934067
train: step: 738, loss: 0.6928244829177856, acc: 0.5, recall: 0.5, precision: 0.5, f_beta: 0.5
train: step: 739, loss: 0.6941367983818054, acc: 0.475, recall: 0.475, precision: 0.474937343358396, f_beta: 0.474671669793621
train: step: 740, loss: 0.6925970911979675, acc: 0.5, recall: 0.5, precision: 0.5, f_beta: 0.4949494949494949
train: step: 741, loss: 0.6940481066703796, acc: 0.45, recall: 0.45, precision: 0.4494949494949495, f_beta: 0.4486215538847118
train: step: 742, loss: 0.693263828754425, acc: 0.5, recall: 0.5, precision: 0.5, f_beta: 0.47916666666666663
train: step: 743, loss: 0.6931570172309875, acc: 0.525, recall: 0.5249999999999999, precision: 0.5284900284900285, f_beta: 0.5099935525467441
train: step: 744, loss: 0.6938708424568176, acc: 0.475, recall: 0.475, precision: 0.474937343358396, f_beta: 0.474671669793621
train: step: 745, loss: 0.6923054456710815, acc: 0.475, recall: 0.47500000000000003, precision: 0.4744245524296675, f_beta: 0.47203016970458833
train: step: 746, loss: 0.6962366104125977, acc: 0.35, recall: 0.35, precision: 0.35, f_beta: 0.35
train: step: 747, loss: 0.6751309633255005, acc: 0.675, recall: 0.675, precision: 0.7508960573476702, f_beta: 0.6484110885733605
train: step: 748, loss: 0.6899255514144897, acc: 0.575, recall: 0.575, precision: 0.6298701298701299, f_beta: 0.5248078266946192
train: step: 749, loss: 0.6935253143310547, acc: 0.55, recall: 0.55, precision: 0.55, f_beta: 0.55
train: step: 750, loss: 0.6908926963806152, acc: 0.6, recall: 0.6, precision: 0.6333333333333333, f_beta: 0.5733333333333334
train: step: 751, loss: 0.6926224231719971, acc: 0.525, recall: 0.525, precision: 0.5432900432900433, f_beta: 0.46890286512928026
train: step: 752, loss: 0.6931662559509277, acc: 0.525, recall: 0.525, precision: 0.5255754475703325, f_beta: 0.5223130106851037
train: step: 753, loss: 0.6943915486335754, acc: 0.325, recall: 0.325, precision: 0.31333333333333335, f_beta: 0.3142857142857143
train: step: 754, loss: 0.6914531588554382, acc: 0.625, recall: 0.625, precision: 0.6333333333333333, f_beta: 0.6190476190476191
train: step: 755, loss: 0.6935660243034363, acc: 0.5, recall: 0.5, precision: 0.5, f_beta: 0.40476190476190477
train: step: 756, loss: 0.6930233240127563, acc: 0.5, recall: 0.5, precision: 0.5, f_beta: 0.4949494949494949
train: step: 757, loss: 0.6927515268325806, acc: 0.5, recall: 0.5, precision: 0.5, f_beta: 0.4949494949494949
train: step: 758, loss: 0.694317102432251, acc: 0.4, recall: 0.4, precision: 0.4, f_beta: 0.4000000000000001
train: step: 759, loss: 0.6890062093734741, acc: 0.575, recall: 0.575, precision: 0.5940438871473355, f_beta: 0.5523370638578011
train: step: 760, loss: 0.6948723793029785, acc: 0.45, recall: 0.45, precision: 0.421875, f_beta: 0.39560439560439553
train: step: 761, loss: 0.6946010589599609, acc: 0.375, recall: 0.375, precision: 0.37468671679197996, f_beta: 0.37460913070669166
train: step: 762, loss: 0.6934372782707214, acc: 0.4, recall: 0.4, precision: 0.3666666666666667, f_beta: 0.36
train: step: 763, loss: 0.6925980448722839, acc: 0.55, recall: 0.55, precision: 0.554945054945055, f_beta: 0.5396419437340153
train: step: 764, loss: 0.6932007670402527, acc: 0.45, recall: 0.45, precision: 0.44791666666666663, f_beta: 0.4444444444444444
train: step: 765, loss: 0.6959342360496521, acc: 0.375, recall: 0.375, precision: 0.3207885304659498, f_beta: 0.32386747802569305
train: step: 766, loss: 0.6952216029167175, acc: 0.45, recall: 0.45, precision: 0.3611111111111111, f_beta: 0.34523809523809523
train: step: 767, loss: 0.6950525045394897, acc: 0.375, recall: 0.375, precision: 0.37468671679197996, f_beta: 0.37460913070669166
train: step: 768, loss: 0.6934161186218262, acc: 0.5, recall: 0.5, precision: 0.5, f_beta: 0.4949494949494949
train: step: 769, loss: 0.6873261332511902, acc: 0.675, recall: 0.675, precision: 0.803030303030303, f_beta: 0.636617749825297
train: step: 770, loss: 0.6902247071266174, acc: 0.575, recall: 0.575, precision: 0.60752688172043, f_beta: 0.5402298850574712
train: step: 771, loss: 0.6927552223205566, acc: 0.575, recall: 0.575, precision: 0.5767263427109974, f_beta: 0.5725958516656191
train: step: 772, loss: 0.6786271333694458, acc: 0.625, recall: 0.625, precision: 0.6792114695340502, f_beta: 0.5943204868154157
train: step: 773, loss: 0.6883586645126343, acc: 0.525, recall: 0.5249999999999999, precision: 0.5284900284900285, f_beta: 0.5099935525467441
train: step: 774, loss: 0.6940357089042664, acc: 0.475, recall: 0.47500000000000003, precision: 0.4744245524296675, f_beta: 0.47203016970458833
train: step: 775, loss: 0.6971551775932312, acc: 0.375, recall: 0.375, precision: 0.3666666666666667, f_beta: 0.3650793650793651
train: step: 776, loss: 0.6947404146194458, acc: 0.425, recall: 0.42500000000000004, precision: 0.4059561128526646, f_beta: 0.39433838051349573
train: step: 777, loss: 0.6922000646591187, acc: 0.65, recall: 0.6499999999999999, precision: 0.6515151515151515, f_beta: 0.6491228070175439
train: step: 778, loss: 0.6925161480903625, acc: 0.525, recall: 0.525, precision: 0.525062656641604, f_beta: 0.5247029393370857
train: step: 779, loss: 0.6928075551986694, acc: 0.475, recall: 0.475, precision: 0.47150997150997154, f_beta: 0.4584139264990329
train: step: 780, loss: 0.69350266456604, acc: 0.35, recall: 0.35, precision: 0.34375, f_beta: 0.3434343434343434
train: step: 781, loss: 0.6924256086349487, acc: 0.5, recall: 0.5, precision: 0.5, f_beta: 0.4884910485933504
train: step: 782, loss: 0.6936626434326172, acc: 0.45, recall: 0.44999999999999996, precision: 0.43333333333333335, f_beta: 0.41333333333333333
train: step: 783, loss: 0.6925655007362366, acc: 0.5, recall: 0.5, precision: 0.5, f_beta: 0.4949494949494949
train: step: 784, loss: 0.691831111907959, acc: 0.55, recall: 0.55, precision: 0.5505050505050505, f_beta: 0.5488721804511277
train: step: 785, loss: 0.6910762190818787, acc: 0.625, recall: 0.625, precision: 0.6333333333333333, f_beta: 0.6190476190476191
train: step: 786, loss: 0.6908717155456543, acc: 0.575, recall: 0.575, precision: 0.5940438871473355, f_beta: 0.5523370638578011
train: step: 787, loss: 0.6890514492988586, acc: 0.65, recall: 0.6499999999999999, precision: 0.6515151515151515, f_beta: 0.6491228070175439
train: step: 788, loss: 0.6930529475212097, acc: 0.425, recall: 0.42500000000000004, precision: 0.42000000000000004, f_beta: 0.4158730158730159
train: step: 789, loss: 0.6920208930969238, acc: 0.6, recall: 0.6, precision: 0.6333333333333333, f_beta: 0.5733333333333334
train: step: 790, loss: 0.6897256970405579, acc: 0.575, recall: 0.575, precision: 0.6714285714285715, f_beta: 0.5054545454545454
train: step: 791, loss: 0.691361129283905, acc: 0.575, recall: 0.575, precision: 0.60752688172043, f_beta: 0.5402298850574712
train: step: 792, loss: 0.6974298357963562, acc: 0.35, recall: 0.35, precision: 0.3484848484848485, f_beta: 0.3483709273182958
train: step: 793, loss: 0.6930133700370789, acc: 0.475, recall: 0.475, precision: 0.47150997150997154, f_beta: 0.4584139264990329
train: step: 794, loss: 0.6931265592575073, acc: 0.475, recall: 0.475, precision: 0.47150997150997154, f_beta: 0.4584139264990329
train: step: 795, loss: 0.6917809247970581, acc: 0.525, recall: 0.525, precision: 0.525062656641604, f_beta: 0.5247029393370857
train: step: 796, loss: 0.6931785345077515, acc: 0.625, recall: 0.625, precision: 0.62531328320802, f_beta: 0.6247654784240151
train: step: 797, loss: 0.6918083429336548, acc: 0.6, recall: 0.6, precision: 0.696078431372549, f_beta: 0.5441595441595442
train: step: 798, loss: 0.6924781799316406, acc: 0.525, recall: 0.525, precision: 0.5266666666666666, f_beta: 0.5174603174603175
train: step: 799, loss: 0.6890662312507629, acc: 0.55, recall: 0.55, precision: 0.5980392156862745, f_beta: 0.4871794871794872
train: step: 800, loss: 0.6925057172775269, acc: 0.6, recall: 0.6000000000000001, precision: 0.6190476190476191, f_beta: 0.5833333333333333
train: step: 801, loss: 0.6952641606330872, acc: 0.45, recall: 0.44999999999999996, precision: 0.43333333333333335, f_beta: 0.41333333333333333
train: step: 802, loss: 0.6922062635421753, acc: 0.55, recall: 0.55, precision: 0.554945054945055, f_beta: 0.5396419437340153
train: step: 803, loss: 0.6928552389144897, acc: 0.55, recall: 0.55, precision: 0.5595238095238095, f_beta: 0.53125
train: step: 804, loss: 0.6927627325057983, acc: 0.525, recall: 0.525, precision: 0.5255754475703325, f_beta: 0.5223130106851037
train: step: 805, loss: 0.6943135261535645, acc: 0.425, recall: 0.425, precision: 0.3924731182795699, f_beta: 0.37795807978363755
train: step: 806, loss: 0.6933320760726929, acc: 0.525, recall: 0.525, precision: 0.5313479623824451, f_beta: 0.4996708360763661
train: step: 807, loss: 0.6918633580207825, acc: 0.55, recall: 0.55, precision: 0.5505050505050505, f_beta: 0.5488721804511277
train: step: 808, loss: 0.69378662109375, acc: 0.525, recall: 0.525, precision: 0.5313479623824451, f_beta: 0.4996708360763661
train: step: 809, loss: 0.6937527060508728, acc: 0.475, recall: 0.475, precision: 0.47150997150997154, f_beta: 0.4584139264990329
train: step: 810, loss: 0.6938561201095581, acc: 0.525, recall: 0.525, precision: 0.5313479623824451, f_beta: 0.4996708360763661
train: step: 811, loss: 0.6926562786102295, acc: 0.575, recall: 0.575, precision: 0.575187969924812, f_beta: 0.5747342088805503
train: step: 812, loss: 0.69380122423172, acc: 0.375, recall: 0.375, precision: 0.3721227621483376, f_beta: 0.3714644877435575
train: step: 813, loss: 0.6946333050727844, acc: 0.45, recall: 0.45, precision: 0.44047619047619047, f_beta: 0.42708333333333337
train: step: 814, loss: 0.6889699697494507, acc: 0.65, recall: 0.65, precision: 0.7, f_beta: 0.6266666666666667
train: step: 815, loss: 0.6954578161239624, acc: 0.425, recall: 0.425, precision: 0.3924731182795699, f_beta: 0.37795807978363755
train: step: 816, loss: 0.6941085457801819, acc: 0.475, recall: 0.475, precision: 0.46415770609318996, f_beta: 0.43204868154158216
train: step: 817, loss: 0.6923630833625793, acc: 0.55, recall: 0.55, precision: 0.55, f_beta: 0.55
train: step: 818, loss: 0.6853062510490417, acc: 0.6, recall: 0.6000000000000001, precision: 0.6190476190476191, f_beta: 0.5833333333333333
train: step: 819, loss: 0.6934865713119507, acc: 0.575, recall: 0.575, precision: 0.5854700854700855, f_beta: 0.5615731785944552
train: step: 820, loss: 0.6924881935119629, acc: 0.5, recall: 0.5, precision: 0.5, f_beta: 0.4949494949494949
train: step: 821, loss: 0.6959410905838013, acc: 0.35, recall: 0.35, precision: 0.3351648351648352, f_beta: 0.3350383631713555
train: step: 822, loss: 0.691887378692627, acc: 0.525, recall: 0.525, precision: 0.5266666666666666, f_beta: 0.5174603174603175
train: step: 823, loss: 0.6940520405769348, acc: 0.35, recall: 0.35, precision: 0.34375, f_beta: 0.3434343434343434
train: step: 824, loss: 0.6922289133071899, acc: 0.575, recall: 0.575, precision: 0.5800000000000001, f_beta: 0.5682539682539682
train: step: 825, loss: 0.6942787170410156, acc: 0.4, recall: 0.4, precision: 0.4, f_beta: 0.4000000000000001
train: step: 826, loss: 0.6933776140213013, acc: 0.55, recall: 0.55, precision: 0.5520833333333333, f_beta: 0.5454545454545454
train: step: 827, loss: 0.6975530385971069, acc: 0.375, recall: 0.375, precision: 0.3666666666666667, f_beta: 0.3650793650793651
train: step: 828, loss: 0.6929945945739746, acc: 0.4, recall: 0.4, precision: 0.398989898989899, f_beta: 0.39849624060150374
train: step: 829, loss: 0.6936774253845215, acc: 0.4, recall: 0.4, precision: 0.3901098901098901, f_beta: 0.38618925831202044
train: step: 830, loss: 0.6930588483810425, acc: 0.475, recall: 0.475, precision: 0.474937343358396, f_beta: 0.474671669793621
train: step: 831, loss: 0.692676842212677, acc: 0.525, recall: 0.5249999999999999, precision: 0.5284900284900285, f_beta: 0.5099935525467441
train: step: 832, loss: 0.6923485994338989, acc: 0.5, recall: 0.5, precision: 0.5, f_beta: 0.4949494949494949
train: step: 833, loss: 0.6975871920585632, acc: 0.4, recall: 0.4, precision: 0.38095238095238093, f_beta: 0.375
train: step: 834, loss: 0.6929796934127808, acc: 0.6, recall: 0.6, precision: 0.6098901098901099, f_beta: 0.5907928388746804
train: step: 835, loss: 0.6920472383499146, acc: 0.425, recall: 0.42500000000000004, precision: 0.424812030075188, f_beta: 0.4246404002501563
train: step: 836, loss: 0.6782839298248291, acc: 0.65, recall: 0.65, precision: 0.7941176470588236, f_beta: 0.6011396011396011
train: step: 837, loss: 0.6946444511413574, acc: 0.575, recall: 0.575, precision: 0.60752688172043, f_beta: 0.5402298850574712
train: step: 838, loss: 0.69291752576828, acc: 0.5, recall: 0.5, precision: 0.5, f_beta: 0.4949494949494949
train: step: 839, loss: 0.6942609548568726, acc: 0.4, recall: 0.4, precision: 0.39583333333333337, f_beta: 0.3939393939393939
train: step: 840, loss: 0.6917085647583008, acc: 0.475, recall: 0.475, precision: 0.46865203761755486, f_beta: 0.4470046082949308
train: step: 841, loss: 0.6933378577232361, acc: 0.525, recall: 0.525, precision: 0.5313479623824451, f_beta: 0.4996708360763661
train: step: 842, loss: 0.6923035383224487, acc: 0.525, recall: 0.525, precision: 0.5255754475703325, f_beta: 0.5223130106851037
train: step: 843, loss: 0.6939188838005066, acc: 0.45, recall: 0.44999999999999996, precision: 0.44505494505494503, f_beta: 0.4373401534526854
train: step: 844, loss: 0.6907274127006531, acc: 0.55, recall: 0.55, precision: 0.55, f_beta: 0.55
train: step: 845, loss: 0.6952964663505554, acc: 0.5, recall: 0.5, precision: 0.5, f_beta: 0.43019943019943013
train: step: 846, loss: 0.693466067314148, acc: 0.5, recall: 0.5, precision: 0.5, f_beta: 0.40476190476190477
train: step: 847, loss: 0.6929842233657837, acc: 0.5, recall: 0.5, precision: 0.5, f_beta: 0.47916666666666663
train: step: 848, loss: 0.694046676158905, acc: 0.425, recall: 0.42500000000000004, precision: 0.42000000000000004, f_beta: 0.4158730158730159
train: step: 849, loss: 0.6922820806503296, acc: 0.5, recall: 0.5, precision: 0.5, f_beta: 0.4666666666666667
train: step: 850, loss: 0.6937578916549683, acc: 0.375, recall: 0.375, precision: 0.3575498575498576, f_beta: 0.3552546744036106
train: step: 851, loss: 0.6941463351249695, acc: 0.525, recall: 0.525, precision: 0.5266666666666666, f_beta: 0.5174603174603175
train: step: 852, loss: 0.6938999891281128, acc: 0.475, recall: 0.475, precision: 0.474937343358396, f_beta: 0.474671669793621
train: step: 853, loss: 0.692541241645813, acc: 0.6, recall: 0.6000000000000001, precision: 0.601010101010101, f_beta: 0.5989974937343359
train: step: 854, loss: 0.6935015916824341, acc: 0.4, recall: 0.4, precision: 0.398989898989899, f_beta: 0.39849624060150374
train: step: 855, loss: 0.693494975566864, acc: 0.5, recall: 0.5, precision: 0.5, f_beta: 0.4949494949494949
train: step: 856, loss: 0.6944109201431274, acc: 0.5, recall: 0.5, precision: 0.5, f_beta: 0.5
train: step: 857, loss: 0.6941736340522766, acc: 0.45, recall: 0.45, precision: 0.421875, f_beta: 0.39560439560439553
train: step: 858, loss: 0.6912089586257935, acc: 0.575, recall: 0.575, precision: 0.5800000000000001, f_beta: 0.5682539682539682
train: step: 859, loss: 0.6946913003921509, acc: 0.35, recall: 0.35, precision: 0.3484848484848485, f_beta: 0.3483709273182958
train: step: 860, loss: 0.6944757699966431, acc: 0.45, recall: 0.45, precision: 0.44791666666666663, f_beta: 0.4444444444444444
train: step: 861, loss: 0.6928369402885437, acc: 0.525, recall: 0.525, precision: 0.5255754475703325, f_beta: 0.5223130106851037
train: step: 862, loss: 0.6957979202270508, acc: 0.25, recall: 0.25, precision: 0.24747474747474746, f_beta: 0.24812030075187969
train: step: 863, loss: 0.6876932978630066, acc: 0.575, recall: 0.575, precision: 0.6298701298701299, f_beta: 0.5248078266946192
train: step: 864, loss: 0.6933383941650391, acc: 0.5, recall: 0.5, precision: 0.5, f_beta: 0.5
train: step: 865, loss: 0.6944102048873901, acc: 0.5, recall: 0.5, precision: 0.5, f_beta: 0.40476190476190477
train: step: 866, loss: 0.6933531165122986, acc: 0.5, recall: 0.5, precision: 0.5, f_beta: 0.4949494949494949
train: step: 867, loss: 0.6939318180084229, acc: 0.55, recall: 0.55, precision: 0.5595238095238095, f_beta: 0.53125
train: step: 868, loss: 0.6924262046813965, acc: 0.575, recall: 0.575, precision: 0.575187969924812, f_beta: 0.5747342088805503
train: step: 869, loss: 0.6940725445747375, acc: 0.4, recall: 0.4, precision: 0.4, f_beta: 0.4000000000000001
train: step: 870, loss: 0.6918531060218811, acc: 0.45, recall: 0.45, precision: 0.44047619047619047, f_beta: 0.42708333333333337
train: step: 871, loss: 0.693221926689148, acc: 0.55, recall: 0.55, precision: 0.5666666666666667, f_beta: 0.52
train: step: 872, loss: 0.693858802318573, acc: 0.475, recall: 0.475, precision: 0.47333333333333333, f_beta: 0.4666666666666666
train: step: 873, loss: 0.6975638270378113, acc: 0.475, recall: 0.475, precision: 0.44285714285714284, f_beta: 0.38909090909090915
train: step: 874, loss: 0.693167507648468, acc: 0.475, recall: 0.47500000000000003, precision: 0.4744245524296675, f_beta: 0.47203016970458833
train: step: 875, loss: 0.6918246746063232, acc: 0.55, recall: 0.55, precision: 0.5595238095238095, f_beta: 0.53125
train: step: 876, loss: 0.693024754524231, acc: 0.475, recall: 0.475, precision: 0.47150997150997154, f_beta: 0.4584139264990329
train: step: 877, loss: 0.6927044987678528, acc: 0.45, recall: 0.44999999999999996, precision: 0.44505494505494503, f_beta: 0.4373401534526854
train: step: 878, loss: 0.6932096481323242, acc: 0.45, recall: 0.45, precision: 0.4494949494949495, f_beta: 0.4486215538847118
train: step: 879, loss: 0.6939113736152649, acc: 0.5, recall: 0.5, precision: 0.5, f_beta: 0.40476190476190477
train: step: 880, loss: 0.693220317363739, acc: 0.475, recall: 0.47500000000000003, precision: 0.4744245524296675, f_beta: 0.47203016970458833
train: step: 881, loss: 0.6921335458755493, acc: 0.525, recall: 0.525, precision: 0.5255754475703325, f_beta: 0.5223130106851037
train: step: 882, loss: 0.6936579942703247, acc: 0.4, recall: 0.4, precision: 0.38095238095238093, f_beta: 0.375
train: step: 883, loss: 0.6922422647476196, acc: 0.55, recall: 0.5499999999999999, precision: 0.6388888888888888, f_beta: 0.46428571428571425
train: step: 884, loss: 0.694236159324646, acc: 0.425, recall: 0.42500000000000004, precision: 0.42000000000000004, f_beta: 0.4158730158730159
train: step: 885, loss: 0.694538950920105, acc: 0.325, recall: 0.32499999999999996, precision: 0.32456140350877194, f_beta: 0.324577861163227
train: step: 886, loss: 0.6924372315406799, acc: 0.5, recall: 0.5, precision: 0.5, f_beta: 0.47916666666666663
train: step: 887, loss: 0.6933894157409668, acc: 0.5, recall: 0.5, precision: 0.5, f_beta: 0.4949494949494949
train: step: 888, loss: 0.6933214068412781, acc: 0.5, recall: 0.5, precision: 0.5, f_beta: 0.4505494505494506
train: step: 889, loss: 0.6932170987129211, acc: 0.475, recall: 0.475, precision: 0.474937343358396, f_beta: 0.474671669793621
train: step: 890, loss: 0.6946845054626465, acc: 0.35, recall: 0.35, precision: 0.35, f_beta: 0.35
train: step: 891, loss: 0.6931678652763367, acc: 0.45, recall: 0.45, precision: 0.4494949494949495, f_beta: 0.4486215538847118
train: step: 892, loss: 0.6852885484695435, acc: 0.575, recall: 0.575, precision: 0.6298701298701299, f_beta: 0.5248078266946192
train: step: 893, loss: 0.6942204833030701, acc: 0.425, recall: 0.42500000000000004, precision: 0.424812030075188, f_beta: 0.4246404002501563
train: step: 894, loss: 0.6931768655776978, acc: 0.475, recall: 0.475, precision: 0.47333333333333333, f_beta: 0.4666666666666666
train: step: 895, loss: 0.6939555406570435, acc: 0.525, recall: 0.525, precision: 0.53584229390681, f_beta: 0.4861392832995267
train: step: 896, loss: 0.6920729875564575, acc: 0.625, recall: 0.625, precision: 0.62531328320802, f_beta: 0.6247654784240151
train: step: 897, loss: 0.6946563124656677, acc: 0.45, recall: 0.45, precision: 0.44047619047619047, f_beta: 0.42708333333333337
train: step: 898, loss: 0.6873762011528015, acc: 0.55, recall: 0.55, precision: 0.5980392156862745, f_beta: 0.4871794871794872
train: step: 899, loss: 0.6911773085594177, acc: 0.475, recall: 0.47500000000000003, precision: 0.45670995670995673, f_beta: 0.41299790356394134
train: step: 900, loss: 0.6920442581176758, acc: 0.55, recall: 0.55, precision: 0.5666666666666667, f_beta: 0.52
train: step: 901, loss: 0.6923447847366333, acc: 0.5, recall: 0.5, precision: 0.5, f_beta: 0.4949494949494949
train: step: 902, loss: 0.6940029263496399, acc: 0.5, recall: 0.5, precision: 0.5, f_beta: 0.4505494505494506
train: step: 903, loss: 0.6931792497634888, acc: 0.525, recall: 0.525, precision: 0.5266666666666666, f_beta: 0.5174603174603175
train: step: 904, loss: 0.6919422149658203, acc: 0.525, recall: 0.525, precision: 0.5900900900900901, f_beta: 0.4202898550724638
train: step: 905, loss: 0.6920604705810547, acc: 0.5, recall: 0.5, precision: 0.5, f_beta: 0.4666666666666667
train: step: 906, loss: 0.695575475692749, acc: 0.475, recall: 0.475, precision: 0.46415770609318996, f_beta: 0.43204868154158216
train: step: 907, loss: 0.6934399604797363, acc: 0.55, recall: 0.55, precision: 0.5520833333333333, f_beta: 0.5454545454545454
train: step: 908, loss: 0.6913577318191528, acc: 0.475, recall: 0.475, precision: 0.44285714285714284, f_beta: 0.38909090909090915
train: step: 909, loss: 0.6930952668190002, acc: 0.575, recall: 0.575, precision: 0.5767263427109974, f_beta: 0.5725958516656191
train: step: 910, loss: 0.6947131156921387, acc: 0.425, recall: 0.42500000000000004, precision: 0.42000000000000004, f_beta: 0.4158730158730159
train: step: 911, loss: 0.6962336897850037, acc: 0.425, recall: 0.42500000000000004, precision: 0.424812030075188, f_beta: 0.4246404002501563
train: step: 912, loss: 0.6937805414199829, acc: 0.475, recall: 0.475, precision: 0.46865203761755486, f_beta: 0.4470046082949308
train: step: 913, loss: 0.6920986175537109, acc: 0.525, recall: 0.525, precision: 0.5432900432900433, f_beta: 0.46890286512928026
train: step: 914, loss: 0.6931897401809692, acc: 0.475, recall: 0.475, precision: 0.47333333333333333, f_beta: 0.4666666666666666
train: step: 915, loss: 0.6874351501464844, acc: 0.575, recall: 0.575, precision: 0.6298701298701299, f_beta: 0.5248078266946192
train: step: 916, loss: 0.6935306191444397, acc: 0.45, recall: 0.45, precision: 0.44791666666666663, f_beta: 0.4444444444444444
train: step: 917, loss: 0.692871630191803, acc: 0.6, recall: 0.6, precision: 0.6, f_beta: 0.6
train: step: 918, loss: 0.6936644911766052, acc: 0.4, recall: 0.4, precision: 0.398989898989899, f_beta: 0.39849624060150374
train: step: 919, loss: 0.6919358968734741, acc: 0.525, recall: 0.525, precision: 0.5432900432900433, f_beta: 0.46890286512928026
train: step: 920, loss: 0.6954882740974426, acc: 0.325, recall: 0.325, precision: 0.32097186700767266, f_beta: 0.32118164676304206
train: step: 921, loss: 0.6928297281265259, acc: 0.55, recall: 0.55, precision: 0.5595238095238095, f_beta: 0.53125
train: step: 922, loss: 0.6815009117126465, acc: 0.6, recall: 0.6, precision: 0.696078431372549, f_beta: 0.5441595441595442
train: step: 923, loss: 0.6928079724311829, acc: 0.5, recall: 0.5, precision: 0.5, f_beta: 0.4949494949494949
train: step: 924, loss: 0.6944907307624817, acc: 0.5, recall: 0.5, precision: 0.5, f_beta: 0.47916666666666663
train: step: 925, loss: 0.6887814402580261, acc: 0.525, recall: 0.525, precision: 0.5266666666666666, f_beta: 0.5174603174603175
train: step: 926, loss: 0.6925023794174194, acc: 0.55, recall: 0.55, precision: 0.5505050505050505, f_beta: 0.5488721804511277
train: step: 927, loss: 0.6938470005989075, acc: 0.45, recall: 0.45, precision: 0.3611111111111111, f_beta: 0.34523809523809523
train: step: 928, loss: 0.6945756673812866, acc: 0.35, recall: 0.35, precision: 0.3351648351648352, f_beta: 0.3350383631713555
train: step: 929, loss: 0.6944369077682495, acc: 0.4, recall: 0.4, precision: 0.3901098901098901, f_beta: 0.38618925831202044
train: step: 930, loss: 0.6929534673690796, acc: 0.575, recall: 0.575, precision: 0.5767263427109974, f_beta: 0.5725958516656191
train: step: 931, loss: 0.6943454742431641, acc: 0.45, recall: 0.45, precision: 0.4494949494949495, f_beta: 0.4486215538847118
train: step: 932, loss: 0.6900890469551086, acc: 0.525, recall: 0.525, precision: 0.5900900900900901, f_beta: 0.4202898550724638
train: step: 933, loss: 0.693159282207489, acc: 0.375, recall: 0.375, precision: 0.3721227621483376, f_beta: 0.3714644877435575
train: step: 934, loss: 0.6952468156814575, acc: 0.425, recall: 0.425, precision: 0.42327365728900257, f_beta: 0.4217473287240729
train: step: 935, loss: 0.692948043346405, acc: 0.525, recall: 0.525, precision: 0.525062656641604, f_beta: 0.5247029393370857
train: step: 936, loss: 0.6940304040908813, acc: 0.5, recall: 0.5, precision: 0.5, f_beta: 0.4949494949494949
train: step: 937, loss: 0.6933164596557617, acc: 0.45, recall: 0.45, precision: 0.45, f_beta: 0.45
train: step: 938, loss: 0.6930023431777954, acc: 0.575, recall: 0.575, precision: 0.5854700854700855, f_beta: 0.5615731785944552
train: step: 939, loss: 0.6920037269592285, acc: 0.55, recall: 0.55, precision: 0.5595238095238095, f_beta: 0.53125
train: step: 940, loss: 0.6954379081726074, acc: 0.5, recall: 0.5, precision: 0.5, f_beta: 0.4505494505494506
train: step: 941, loss: 0.6942418217658997, acc: 0.325, recall: 0.325, precision: 0.32097186700767266, f_beta: 0.32118164676304206
train: step: 942, loss: 0.6943209171295166, acc: 0.425, recall: 0.42500000000000004, precision: 0.4059561128526646, f_beta: 0.39433838051349573
train: step: 943, loss: 0.6908718943595886, acc: 0.65, recall: 0.65, precision: 0.65, f_beta: 0.65
train: step: 944, loss: 0.692594051361084, acc: 0.5, recall: 0.5, precision: 0.5, f_beta: 0.43019943019943013
train: step: 945, loss: 0.6929682493209839, acc: 0.375, recall: 0.375, precision: 0.3666666666666667, f_beta: 0.3650793650793651
train: step: 946, loss: 0.7054165005683899, acc: 0.375, recall: 0.375, precision: 0.21428571428571427, f_beta: 0.2727272727272727
train: step: 947, loss: 0.6948655247688293, acc: 0.475, recall: 0.475, precision: 0.44285714285714284, f_beta: 0.38909090909090915
train: step: 948, loss: 0.6930615305900574, acc: 0.5, recall: 0.5, precision: 0.5, f_beta: 0.43019943019943013
train: step: 949, loss: 0.6923257112503052, acc: 0.6, recall: 0.6000000000000001, precision: 0.6190476190476191, f_beta: 0.5833333333333333
train: step: 950, loss: 0.6950691342353821, acc: 0.5, recall: 0.5, precision: 0.5, f_beta: 0.40476190476190477
train: step: 951, loss: 0.6933966875076294, acc: 0.525, recall: 0.525, precision: 0.5313479623824451, f_beta: 0.4996708360763661
train: step: 952, loss: 0.691328763961792, acc: 0.55, recall: 0.55, precision: 0.578125, f_beta: 0.5054945054945055
train: step: 953, loss: 0.691565215587616, acc: 0.525, recall: 0.525, precision: 0.5432900432900433, f_beta: 0.46890286512928026
train: step: 954, loss: 0.6936153173446655, acc: 0.475, recall: 0.475, precision: 0.46415770609318996, f_beta: 0.43204868154158216
train: step: 955, loss: 0.6963024139404297, acc: 0.475, recall: 0.475, precision: 0.44285714285714284, f_beta: 0.38909090909090915
train: step: 956, loss: 0.6917833089828491, acc: 0.6, recall: 0.6000000000000001, precision: 0.601010101010101, f_beta: 0.5989974937343359
train: step: 957, loss: 0.7065582275390625, acc: 0.375, recall: 0.375, precision: 0.3207885304659498, f_beta: 0.32386747802569305
train: step: 958, loss: 0.6964321136474609, acc: 0.325, recall: 0.325, precision: 0.31333333333333335, f_beta: 0.3142857142857143
train: step: 959, loss: 0.693375289440155, acc: 0.55, recall: 0.55, precision: 0.5595238095238095, f_beta: 0.53125
train: step: 960, loss: 0.6957036256790161, acc: 0.5, recall: 0.5, precision: 0.5, f_beta: 0.43019943019943013
train: step: 961, loss: 0.6948099136352539, acc: 0.45, recall: 0.45, precision: 0.44047619047619047, f_beta: 0.42708333333333337
train: step: 962, loss: 0.6930962800979614, acc: 0.475, recall: 0.475, precision: 0.47150997150997154, f_beta: 0.4584139264990329
train: step: 963, loss: 0.6934061646461487, acc: 0.425, recall: 0.425, precision: 0.4145299145299145, f_beta: 0.40683430045132174
train: step: 964, loss: 0.6879881620407104, acc: 0.575, recall: 0.575, precision: 0.6298701298701299, f_beta: 0.5248078266946192
train: step: 965, loss: 0.693445086479187, acc: 0.525, recall: 0.525, precision: 0.5313479623824451, f_beta: 0.4996708360763661
train: step: 966, loss: 0.688149631023407, acc: 0.55, recall: 0.5499999999999999, precision: 0.6388888888888888, f_beta: 0.46428571428571425
train: step: 967, loss: 0.6939173936843872, acc: 0.5, recall: 0.5, precision: 0.5, f_beta: 0.4987468671679198
train: step: 968, loss: 0.6938556432723999, acc: 0.5, recall: 0.5, precision: 0.5, f_beta: 0.4884910485933504
train: step: 969, loss: 0.6939544081687927, acc: 0.45, recall: 0.44999999999999996, precision: 0.44505494505494503, f_beta: 0.4373401534526854
train: step: 970, loss: 0.6944102048873901, acc: 0.475, recall: 0.47500000000000003, precision: 0.40990990990990994, f_beta: 0.35926773455377575
train: step: 971, loss: 0.6939907670021057, acc: 0.525, recall: 0.525, precision: 0.5313479623824451, f_beta: 0.4996708360763661
train: step: 972, loss: 0.6925754547119141, acc: 0.55, recall: 0.55, precision: 0.578125, f_beta: 0.5054945054945055
train: step: 973, loss: 0.6942740082740784, acc: 0.475, recall: 0.475, precision: 0.46415770609318996, f_beta: 0.43204868154158216
train: step: 974, loss: 0.6934539675712585, acc: 0.425, recall: 0.425, precision: 0.4145299145299145, f_beta: 0.40683430045132174
train: step: 975, loss: 0.6940463781356812, acc: 0.45, recall: 0.45, precision: 0.4494949494949495, f_beta: 0.4486215538847118
train: step: 976, loss: 0.6929596066474915, acc: 0.475, recall: 0.475, precision: 0.474937343358396, f_beta: 0.474671669793621
train: step: 977, loss: 0.6927862167358398, acc: 0.575, recall: 0.575, precision: 0.575187969924812, f_beta: 0.5747342088805503
train: step: 978, loss: 0.691509485244751, acc: 0.55, recall: 0.55, precision: 0.5595238095238095, f_beta: 0.53125
train: step: 979, loss: 0.6931701302528381, acc: 0.475, recall: 0.475, precision: 0.47333333333333333, f_beta: 0.4666666666666666
train: step: 980, loss: 0.6896976232528687, acc: 0.6, recall: 0.6, precision: 0.696078431372549, f_beta: 0.5441595441595442
train: step: 981, loss: 0.6913892030715942, acc: 0.575, recall: 0.575, precision: 0.5767263427109974, f_beta: 0.5725958516656191
train: step: 982, loss: 0.6930232048034668, acc: 0.575, recall: 0.575, precision: 0.5800000000000001, f_beta: 0.5682539682539682
train: step: 983, loss: 0.6930146217346191, acc: 0.475, recall: 0.47500000000000003, precision: 0.4744245524296675, f_beta: 0.47203016970458833
train: step: 984, loss: 0.6937509775161743, acc: 0.5, recall: 0.5, precision: 0.5, f_beta: 0.5
train: step: 985, loss: 0.6925835013389587, acc: 0.6, recall: 0.6000000000000001, precision: 0.601010101010101, f_beta: 0.5989974937343359
train: step: 986, loss: 0.6939476728439331, acc: 0.425, recall: 0.425, precision: 0.4145299145299145, f_beta: 0.40683430045132174
train: step: 987, loss: 0.6910587549209595, acc: 0.575, recall: 0.575, precision: 0.7702702702702703, f_beta: 0.4813119755911518
train: step: 988, loss: 0.6921426057815552, acc: 0.55, recall: 0.55, precision: 0.5505050505050505, f_beta: 0.5488721804511277
train: step: 989, loss: 0.6937955617904663, acc: 0.45, recall: 0.45, precision: 0.44791666666666663, f_beta: 0.4444444444444444
train: step: 990, loss: 0.6937240958213806, acc: 0.35, recall: 0.35000000000000003, precision: 0.265625, f_beta: 0.28571428571428575
train: step: 991, loss: 0.6925373673439026, acc: 0.55, recall: 0.55, precision: 0.5505050505050505, f_beta: 0.5488721804511277
train: step: 992, loss: 0.6926243901252747, acc: 0.475, recall: 0.475, precision: 0.474937343358396, f_beta: 0.474671669793621
train: step: 993, loss: 0.6934424042701721, acc: 0.5, recall: 0.5, precision: 0.5, f_beta: 0.4987468671679198
train: step: 994, loss: 0.6932345628738403, acc: 0.55, recall: 0.55, precision: 0.5520833333333333, f_beta: 0.5454545454545454
train: step: 995, loss: 0.6921729445457458, acc: 0.5, recall: 0.5, precision: 0.5, f_beta: 0.40476190476190477
train: step: 996, loss: 0.6937034726142883, acc: 0.525, recall: 0.525, precision: 0.5266666666666666, f_beta: 0.5174603174603175
train: step: 997, loss: 0.6898185014724731, acc: 0.55, recall: 0.55, precision: 0.5505050505050505, f_beta: 0.5488721804511277
train: step: 998, loss: 0.693281352519989, acc: 0.575, recall: 0.575, precision: 0.575187969924812, f_beta: 0.5747342088805503
train: step: 999, loss: 0.6944373846054077, acc: 0.475, recall: 0.47500000000000003, precision: 0.40990990990990994, f_beta: 0.35926773455377575
train: step: 1000, loss: 0.6924039125442505, acc: 0.55, recall: 0.55, precision: 0.5505050505050505, f_beta: 0.5488721804511277
train: step: 1001, loss: 0.6932600736618042, acc: 0.525, recall: 0.525, precision: 0.525062656641604, f_beta: 0.5247029393370857
train: step: 1002, loss: 0.6927244663238525, acc: 0.6, recall: 0.6000000000000001, precision: 0.6190476190476191, f_beta: 0.5833333333333333
train: step: 1003, loss: 0.7010912895202637, acc: 0.4, recall: 0.4, precision: 0.30392156862745096, f_beta: 0.3162393162393162
train: step: 1004, loss: 0.6944589018821716, acc: 0.475, recall: 0.47500000000000003, precision: 0.45670995670995673, f_beta: 0.41299790356394134
train: step: 1005, loss: 0.6922799348831177, acc: 0.525, recall: 0.5249999999999999, precision: 0.5284900284900285, f_beta: 0.5099935525467441
train: step: 1006, loss: 0.6921640634536743, acc: 0.525, recall: 0.525, precision: 0.525062656641604, f_beta: 0.5247029393370857
train: step: 1007, loss: 0.6927071809768677, acc: 0.5, recall: 0.5, precision: 0.5, f_beta: 0.4505494505494506
train: step: 1008, loss: 0.6941556930541992, acc: 0.35, recall: 0.35, precision: 0.34375, f_beta: 0.3434343434343434
train: step: 1009, loss: 0.6891652345657349, acc: 0.575, recall: 0.575, precision: 0.6714285714285715, f_beta: 0.5054545454545454
train: step: 1010, loss: 0.6937025189399719, acc: 0.475, recall: 0.475, precision: 0.46865203761755486, f_beta: 0.4470046082949308
train: step: 1011, loss: 0.6969243288040161, acc: 0.45, recall: 0.45, precision: 0.3611111111111111, f_beta: 0.34523809523809523
train: step: 1012, loss: 0.6916338205337524, acc: 0.55, recall: 0.55, precision: 0.55, f_beta: 0.55
train: step: 1013, loss: 0.6930787563323975, acc: 0.475, recall: 0.475, precision: 0.47333333333333333, f_beta: 0.4666666666666666
train: step: 1014, loss: 0.6942728161811829, acc: 0.45, recall: 0.45, precision: 0.4494949494949495, f_beta: 0.4486215538847118
train: step: 1015, loss: 0.6990681886672974, acc: 0.325, recall: 0.325, precision: 0.280564263322884, f_beta: 0.2890059249506254
train: step: 1016, loss: 0.6929558515548706, acc: 0.6, recall: 0.6, precision: 0.6098901098901099, f_beta: 0.5907928388746804
train: step: 1017, loss: 0.6929590106010437, acc: 0.475, recall: 0.475, precision: 0.474937343358396, f_beta: 0.474671669793621
train: step: 1018, loss: 0.6952842473983765, acc: 0.475, recall: 0.475, precision: 0.474937343358396, f_beta: 0.474671669793621
train: step: 1019, loss: 0.695094108581543, acc: 0.525, recall: 0.525, precision: 0.5255754475703325, f_beta: 0.5223130106851037
train: step: 1020, loss: 0.6937723159790039, acc: 0.425, recall: 0.42500000000000004, precision: 0.424812030075188, f_beta: 0.4246404002501563
train: step: 1021, loss: 0.6941756010055542, acc: 0.5, recall: 0.5, precision: 0.5, f_beta: 0.4949494949494949
train: step: 1022, loss: 0.6926543712615967, acc: 0.5, recall: 0.5, precision: 0.5, f_beta: 0.40476190476190477
train: step: 1023, loss: 0.6933807134628296, acc: 0.45, recall: 0.44999999999999996, precision: 0.43333333333333335, f_beta: 0.41333333333333333
train: step: 1024, loss: 0.691030740737915, acc: 0.475, recall: 0.475, precision: 0.44285714285714284, f_beta: 0.38909090909090915
train: step: 1025, loss: 0.6932519674301147, acc: 0.5, recall: 0.5, precision: 0.5, f_beta: 0.4884910485933504
train: step: 1026, loss: 0.6922763586044312, acc: 0.675, recall: 0.675, precision: 0.6866666666666666, f_beta: 0.6698412698412699
train: step: 1027, loss: 0.6925832629203796, acc: 0.6, recall: 0.6000000000000001, precision: 0.6190476190476191, f_beta: 0.5833333333333333
train: step: 1028, loss: 0.6948641538619995, acc: 0.375, recall: 0.375, precision: 0.3721227621483376, f_beta: 0.3714644877435575
train: step: 1029, loss: 0.6923157572746277, acc: 0.5, recall: 0.5, precision: 0.5, f_beta: 0.4884910485933504
train: step: 1030, loss: 0.6927396059036255, acc: 0.5, recall: 0.5, precision: 0.5, f_beta: 0.4505494505494506
train: step: 1031, loss: 0.6933236718177795, acc: 0.475, recall: 0.47500000000000003, precision: 0.45670995670995673, f_beta: 0.41299790356394134
train: step: 1032, loss: 0.6944934725761414, acc: 0.375, recall: 0.375, precision: 0.3575498575498576, f_beta: 0.3552546744036106
train: step: 1033, loss: 0.6891657710075378, acc: 0.55, recall: 0.55, precision: 0.5666666666666667, f_beta: 0.52
train: step: 1034, loss: 0.6941828727722168, acc: 0.425, recall: 0.42500000000000004, precision: 0.4059561128526646, f_beta: 0.39433838051349573
train: step: 1035, loss: 0.6922749280929565, acc: 0.6, recall: 0.6, precision: 0.6098901098901099, f_beta: 0.5907928388746804
train: step: 1036, loss: 0.7060198783874512, acc: 0.325, recall: 0.325, precision: 0.30056980056980054, f_beta: 0.3036750483558994
train: step: 1037, loss: 0.6903342008590698, acc: 0.7, recall: 0.7, precision: 0.7, f_beta: 0.7
train: step: 1038, loss: 0.6940155625343323, acc: 0.375, recall: 0.375, precision: 0.3666666666666667, f_beta: 0.3650793650793651
train: step: 1039, loss: 0.6958390474319458, acc: 0.475, recall: 0.475, precision: 0.474937343358396, f_beta: 0.474671669793621
train: step: 1040, loss: 0.692976176738739, acc: 0.425, recall: 0.42500000000000004, precision: 0.424812030075188, f_beta: 0.4246404002501563
train: step: 1041, loss: 0.6930103302001953, acc: 0.5, recall: 0.5, precision: 0.5, f_beta: 0.4987468671679198
train: step: 1042, loss: 0.6915489435195923, acc: 0.575, recall: 0.575, precision: 0.5800000000000001, f_beta: 0.5682539682539682
train: step: 1043, loss: 0.6950511932373047, acc: 0.375, recall: 0.375, precision: 0.3666666666666667, f_beta: 0.3650793650793651
train: step: 1044, loss: 0.6937076449394226, acc: 0.45, recall: 0.45, precision: 0.44047619047619047, f_beta: 0.42708333333333337
train: step: 1045, loss: 0.6919319033622742, acc: 0.55, recall: 0.55, precision: 0.5520833333333333, f_beta: 0.5454545454545454
train: step: 1046, loss: 0.6930395364761353, acc: 0.65, recall: 0.65, precision: 0.6785714285714286, f_beta: 0.6354166666666667
train: step: 1047, loss: 0.6912297010421753, acc: 0.6, recall: 0.6, precision: 0.6333333333333333, f_beta: 0.5733333333333334
train: step: 1048, loss: 0.6945167183876038, acc: 0.325, recall: 0.325, precision: 0.31333333333333335, f_beta: 0.3142857142857143
train: step: 1049, loss: 0.6936296224594116, acc: 0.5, recall: 0.5, precision: 0.5, f_beta: 0.40476190476190477
train: step: 1050, loss: 0.6926363110542297, acc: 0.6, recall: 0.6000000000000001, precision: 0.601010101010101, f_beta: 0.5989974937343359
train: step: 1051, loss: 0.6929730176925659, acc: 0.5, recall: 0.5, precision: 0.5, f_beta: 0.47916666666666663
train: step: 1052, loss: 0.6932046413421631, acc: 0.5, recall: 0.5, precision: 0.5, f_beta: 0.4949494949494949
train: step: 1053, loss: 0.6939924359321594, acc: 0.475, recall: 0.475, precision: 0.46865203761755486, f_beta: 0.4470046082949308
train: step: 1054, loss: 0.6927064061164856, acc: 0.525, recall: 0.525, precision: 0.5255754475703325, f_beta: 0.5223130106851037
train: step: 1055, loss: 0.6927902102470398, acc: 0.575, recall: 0.575, precision: 0.575187969924812, f_beta: 0.5747342088805503
train: step: 1056, loss: 0.6929247975349426, acc: 0.45, recall: 0.45, precision: 0.4494949494949495, f_beta: 0.4486215538847118
train: step: 1057, loss: 0.6917754411697388, acc: 0.625, recall: 0.625, precision: 0.7164502164502164, f_beta: 0.5807127882599581
train: step: 1058, loss: 0.6936953663825989, acc: 0.45, recall: 0.44999999999999996, precision: 0.44505494505494503, f_beta: 0.4373401534526854
train: step: 1059, loss: 0.6921192407608032, acc: 0.575, recall: 0.575, precision: 0.60752688172043, f_beta: 0.5402298850574712
train: step: 1060, loss: 0.6937323808670044, acc: 0.45, recall: 0.45, precision: 0.4494949494949495, f_beta: 0.4486215538847118
train: step: 1061, loss: 0.698760449886322, acc: 0.4, recall: 0.4, precision: 0.30392156862745096, f_beta: 0.3162393162393162
train: step: 1062, loss: 0.6934086680412292, acc: 0.475, recall: 0.475, precision: 0.474937343358396, f_beta: 0.474671669793621
train: step: 1063, loss: 0.6915019750595093, acc: 0.525, recall: 0.525, precision: 0.5571428571428572, f_beta: 0.44727272727272727
train: step: 1064, loss: 0.6940611600875854, acc: 0.525, recall: 0.5249999999999999, precision: 0.5284900284900285, f_beta: 0.5099935525467441
train: step: 1065, loss: 0.6947449445724487, acc: 0.5, recall: 0.5, precision: 0.5, f_beta: 0.4884910485933504
train: step: 1066, loss: 0.6939670443534851, acc: 0.45, recall: 0.45, precision: 0.45, f_beta: 0.45
train: step: 1067, loss: 0.6928507685661316, acc: 0.55, recall: 0.55, precision: 0.554945054945055, f_beta: 0.5396419437340153
train: step: 1068, loss: 0.6940423250198364, acc: 0.425, recall: 0.425, precision: 0.3924731182795699, f_beta: 0.37795807978363755
train: step: 1069, loss: 0.6925206184387207, acc: 0.625, recall: 0.625, precision: 0.6278772378516624, f_beta: 0.6228786926461345
train: step: 1070, loss: 0.692036509513855, acc: 0.55, recall: 0.55, precision: 0.5980392156862745, f_beta: 0.4871794871794872
train: step: 1071, loss: 0.6930094957351685, acc: 0.575, recall: 0.575, precision: 0.5800000000000001, f_beta: 0.5682539682539682
train: step: 1072, loss: 0.6930946111679077, acc: 0.525, recall: 0.525, precision: 0.5255754475703325, f_beta: 0.5223130106851037
train: step: 1073, loss: 0.6932393312454224, acc: 0.525, recall: 0.525, precision: 0.5266666666666666, f_beta: 0.5174603174603175
train: step: 1074, loss: 0.6930525898933411, acc: 0.4, recall: 0.4, precision: 0.4, f_beta: 0.4000000000000001
train: step: 1075, loss: 0.6945158839225769, acc: 0.4, recall: 0.4, precision: 0.38095238095238093, f_beta: 0.375
train: step: 1076, loss: 0.6932648420333862, acc: 0.6, recall: 0.6000000000000001, precision: 0.601010101010101, f_beta: 0.5989974937343359
train: step: 1077, loss: 0.6935139298439026, acc: 0.5, recall: 0.5, precision: 0.5, f_beta: 0.47916666666666663
train: step: 1078, loss: 0.6925312280654907, acc: 0.525, recall: 0.525, precision: 0.5255754475703325, f_beta: 0.5223130106851037
train: step: 1079, loss: 0.6930955648422241, acc: 0.525, recall: 0.525, precision: 0.5266666666666666, f_beta: 0.5174603174603175
train: step: 1080, loss: 0.691832959651947, acc: 0.625, recall: 0.625, precision: 0.6278772378516624, f_beta: 0.6228786926461345
train: step: 1081, loss: 0.6926676034927368, acc: 0.5, recall: 0.5, precision: 0.5, f_beta: 0.47916666666666663
train: step: 1082, loss: 0.6939905285835266, acc: 0.475, recall: 0.475, precision: 0.46415770609318996, f_beta: 0.43204868154158216
train: step: 1083, loss: 0.6934089064598083, acc: 0.525, recall: 0.525, precision: 0.525062656641604, f_beta: 0.5247029393370857
train: step: 1084, loss: 0.6933112144470215, acc: 0.45, recall: 0.44999999999999996, precision: 0.43333333333333335, f_beta: 0.41333333333333333
train: step: 1085, loss: 0.6927320957183838, acc: 0.525, recall: 0.525, precision: 0.53584229390681, f_beta: 0.4861392832995267
train: step: 1086, loss: 0.6911183595657349, acc: 0.6, recall: 0.6000000000000001, precision: 0.6190476190476191, f_beta: 0.5833333333333333
train: step: 1087, loss: 0.6921588182449341, acc: 0.6, recall: 0.6, precision: 0.6098901098901099, f_beta: 0.5907928388746804
train: step: 1088, loss: 0.6913369297981262, acc: 0.675, recall: 0.675, precision: 0.6790281329923273, f_beta: 0.6731615336266499
train: step: 1089, loss: 0.6937853693962097, acc: 0.45, recall: 0.45, precision: 0.45, f_beta: 0.45
train: step: 1090, loss: 0.6932490468025208, acc: 0.475, recall: 0.47500000000000003, precision: 0.4744245524296675, f_beta: 0.47203016970458833
train: step: 1091, loss: 0.6930943727493286, acc: 0.525, recall: 0.525, precision: 0.53584229390681, f_beta: 0.4861392832995267
train: step: 1092, loss: 0.695426344871521, acc: 0.525, recall: 0.525, precision: 0.5571428571428572, f_beta: 0.44727272727272727
train: step: 1093, loss: 0.6930798292160034, acc: 0.525, recall: 0.525, precision: 0.5255754475703325, f_beta: 0.5223130106851037
train: step: 1094, loss: 0.6922590136528015, acc: 0.55, recall: 0.5499999999999999, precision: 0.6388888888888888, f_beta: 0.46428571428571425
train: step: 1095, loss: 0.6910403370857239, acc: 0.475, recall: 0.47500000000000003, precision: 0.4744245524296675, f_beta: 0.47203016970458833
train: step: 1096, loss: 0.6930798888206482, acc: 0.425, recall: 0.42500000000000004, precision: 0.424812030075188, f_beta: 0.4246404002501563
train: step: 1097, loss: 0.6898479461669922, acc: 0.575, recall: 0.575, precision: 0.6714285714285715, f_beta: 0.5054545454545454
train: step: 1098, loss: 0.6935461759567261, acc: 0.475, recall: 0.475, precision: 0.47150997150997154, f_beta: 0.4584139264990329
train: step: 1099, loss: 0.6919793486595154, acc: 0.525, recall: 0.525, precision: 0.5900900900900901, f_beta: 0.4202898550724638
train: step: 1100, loss: 0.693622887134552, acc: 0.45, recall: 0.45, precision: 0.4494949494949495, f_beta: 0.4486215538847118
train: step: 1101, loss: 0.6927782893180847, acc: 0.525, recall: 0.5249999999999999, precision: 0.5284900284900285, f_beta: 0.5099935525467441
train: step: 1102, loss: 0.6924740076065063, acc: 0.55, recall: 0.55, precision: 0.5520833333333333, f_beta: 0.5454545454545454
train: step: 1103, loss: 0.6949704885482788, acc: 0.35, recall: 0.35, precision: 0.30000000000000004, f_beta: 0.30666666666666664
train: step: 1104, loss: 0.6925832033157349, acc: 0.425, recall: 0.425, precision: 0.3701298701298701, f_beta: 0.35709294199860236
train: step: 1105, loss: 0.6925820112228394, acc: 0.5, recall: 0.5, precision: 0.5, f_beta: 0.4666666666666667
train: step: 1106, loss: 0.6932754516601562, acc: 0.55, recall: 0.5499999999999999, precision: 0.6388888888888888, f_beta: 0.46428571428571425
train: step: 1107, loss: 0.6939706802368164, acc: 0.475, recall: 0.475, precision: 0.46415770609318996, f_beta: 0.43204868154158216
train: step: 1108, loss: 0.693087100982666, acc: 0.475, recall: 0.475, precision: 0.47333333333333333, f_beta: 0.4666666666666666
train: step: 1109, loss: 0.6932493448257446, acc: 0.475, recall: 0.475, precision: 0.474937343358396, f_beta: 0.474671669793621
train: step: 1110, loss: 0.694214940071106, acc: 0.35, recall: 0.35, precision: 0.3351648351648352, f_beta: 0.3350383631713555
train: step: 1111, loss: 0.6939336657524109, acc: 0.325, recall: 0.32499999999999996, precision: 0.32456140350877194, f_beta: 0.324577861163227
train: step: 1112, loss: 0.6933490037918091, acc: 0.55, recall: 0.55, precision: 0.5505050505050505, f_beta: 0.5488721804511277
train: step: 1113, loss: 0.6933594942092896, acc: 0.4, recall: 0.4, precision: 0.39583333333333337, f_beta: 0.3939393939393939
train: step: 1114, loss: 0.6930965185165405, acc: 0.475, recall: 0.475, precision: 0.47150997150997154, f_beta: 0.4584139264990329
train: step: 1115, loss: 0.6928068399429321, acc: 0.5, recall: 0.5, precision: 0.5, f_beta: 0.4666666666666667
train: step: 1116, loss: 0.6938191652297974, acc: 0.5, recall: 0.5, precision: 0.5, f_beta: 0.47916666666666663
train: step: 1117, loss: 0.6932271718978882, acc: 0.475, recall: 0.475, precision: 0.474937343358396, f_beta: 0.474671669793621
train: step: 1118, loss: 0.6936410665512085, acc: 0.45, recall: 0.44999999999999996, precision: 0.44505494505494503, f_beta: 0.4373401534526854
train: step: 1119, loss: 0.6915314793586731, acc: 0.55, recall: 0.55, precision: 0.763157894736842, f_beta: 0.4357366771159875
train: step: 1120, loss: 0.6859639883041382, acc: 0.625, recall: 0.625, precision: 0.7857142857142857, f_beta: 0.5636363636363637
train: step: 1121, loss: 0.694009006023407, acc: 0.45, recall: 0.45, precision: 0.3611111111111111, f_beta: 0.34523809523809523
train: step: 1122, loss: 0.693625271320343, acc: 0.425, recall: 0.425, precision: 0.42327365728900257, f_beta: 0.4217473287240729
train: step: 1123, loss: 0.6924976110458374, acc: 0.575, recall: 0.575, precision: 0.5767263427109974, f_beta: 0.5725958516656191
train: step: 1124, loss: 0.6923873424530029, acc: 0.625, recall: 0.625, precision: 0.6333333333333333, f_beta: 0.6190476190476191
train: step: 1125, loss: 0.6905883550643921, acc: 0.725, recall: 0.725, precision: 0.7255639097744361, f_beta: 0.7248280175109443
train: step: 1126, loss: 0.6938400268554688, acc: 0.375, recall: 0.375, precision: 0.37468671679197996, f_beta: 0.37460913070669166
train: step: 1127, loss: 0.6941708326339722, acc: 0.45, recall: 0.45, precision: 0.421875, f_beta: 0.39560439560439553
train: step: 1128, loss: 0.6922460794448853, acc: 0.55, recall: 0.55, precision: 0.5666666666666667, f_beta: 0.52
train: step: 1129, loss: 0.6942151784896851, acc: 0.375, recall: 0.375, precision: 0.37468671679197996, f_beta: 0.37460913070669166
train: step: 1130, loss: 0.6922472715377808, acc: 0.525, recall: 0.525, precision: 0.525062656641604, f_beta: 0.5247029393370857
train: step: 1131, loss: 0.6937704086303711, acc: 0.425, recall: 0.42500000000000004, precision: 0.42000000000000004, f_beta: 0.4158730158730159
train: step: 1132, loss: 0.6922720670700073, acc: 0.525, recall: 0.525, precision: 0.5900900900900901, f_beta: 0.4202898550724638
train: step: 1133, loss: 0.6914054155349731, acc: 0.575, recall: 0.575, precision: 0.6298701298701299, f_beta: 0.5248078266946192
train: step: 1134, loss: 0.6923227310180664, acc: 0.525, recall: 0.525, precision: 0.5266666666666666, f_beta: 0.5174603174603175
train: step: 1135, loss: 0.6926690936088562, acc: 0.475, recall: 0.475, precision: 0.47333333333333333, f_beta: 0.4666666666666666
train: step: 1136, loss: 0.6921547651290894, acc: 0.525, recall: 0.5249999999999999, precision: 0.5284900284900285, f_beta: 0.5099935525467441
train: step: 1137, loss: 0.6945897936820984, acc: 0.35, recall: 0.35, precision: 0.35, f_beta: 0.35
train: step: 1138, loss: 0.69403475522995, acc: 0.45, recall: 0.45, precision: 0.45, f_beta: 0.45
train: step: 1139, loss: 0.6925078630447388, acc: 0.6, recall: 0.6, precision: 0.6041666666666667, f_beta: 0.595959595959596
train: step: 1140, loss: 0.6937697529792786, acc: 0.3, recall: 0.3, precision: 0.297979797979798, f_beta: 0.2982456140350877
train: step: 1141, loss: 0.6934673190116882, acc: 0.45, recall: 0.45, precision: 0.44791666666666663, f_beta: 0.4444444444444444
train: step: 1142, loss: 0.6932455897331238, acc: 0.45, recall: 0.45, precision: 0.45, f_beta: 0.45
train: step: 1143, loss: 0.6973468065261841, acc: 0.45, recall: 0.45, precision: 0.4019607843137255, f_beta: 0.37321937321937326
train: step: 1144, loss: 0.6942278742790222, acc: 0.425, recall: 0.425, precision: 0.42327365728900257, f_beta: 0.4217473287240729
train: step: 1145, loss: 0.6902390718460083, acc: 0.55, recall: 0.5499999999999999, precision: 0.6388888888888888, f_beta: 0.46428571428571425
train: step: 1146, loss: 0.692869246006012, acc: 0.425, recall: 0.42500000000000004, precision: 0.42000000000000004, f_beta: 0.4158730158730159
train: step: 1147, loss: 0.6927099823951721, acc: 0.525, recall: 0.525, precision: 0.5255754475703325, f_beta: 0.5223130106851037
train: step: 1148, loss: 0.6670211553573608, acc: 0.725, recall: 0.725, precision: 0.8225806451612903, f_beta: 0.7025016903313049
train: step: 1149, loss: 0.6931874752044678, acc: 0.525, recall: 0.525, precision: 0.525062656641604, f_beta: 0.5247029393370857
train: step: 1150, loss: 0.6917697787284851, acc: 0.5, recall: 0.5, precision: 0.5, f_beta: 0.47916666666666663
train: step: 1151, loss: 0.6931725740432739, acc: 0.475, recall: 0.47500000000000003, precision: 0.4744245524296675, f_beta: 0.47203016970458833
train: step: 1152, loss: 0.6920426487922668, acc: 0.55, recall: 0.55, precision: 0.5520833333333333, f_beta: 0.5454545454545454
train: step: 1153, loss: 0.6789841651916504, acc: 0.65, recall: 0.65, precision: 0.7941176470588236, f_beta: 0.6011396011396011
train: step: 1154, loss: 0.6930740475654602, acc: 0.475, recall: 0.47500000000000003, precision: 0.4744245524296675, f_beta: 0.47203016970458833
train: step: 1155, loss: 0.692348837852478, acc: 0.625, recall: 0.625, precision: 0.6333333333333333, f_beta: 0.6190476190476191
train: step: 1156, loss: 0.6932929754257202, acc: 0.5, recall: 0.5, precision: 0.5, f_beta: 0.5
train: step: 1157, loss: 0.6925612688064575, acc: 0.525, recall: 0.5249999999999999, precision: 0.5284900284900285, f_beta: 0.5099935525467441
train: step: 1158, loss: 0.6930220723152161, acc: 0.525, recall: 0.525, precision: 0.5255754475703325, f_beta: 0.5223130106851037
train: step: 1159, loss: 0.6929492950439453, acc: 0.45, recall: 0.45, precision: 0.44047619047619047, f_beta: 0.42708333333333337
train: step: 1160, loss: 0.6942149996757507, acc: 0.425, recall: 0.425, precision: 0.42327365728900257, f_beta: 0.4217473287240729
train: step: 1161, loss: 0.6942634582519531, acc: 0.425, recall: 0.42500000000000004, precision: 0.4059561128526646, f_beta: 0.39433838051349573
train: step: 1162, loss: 0.6947841644287109, acc: 0.5, recall: 0.5, precision: 0.5, f_beta: 0.47916666666666663
train: step: 1163, loss: 0.6893181204795837, acc: 0.625, recall: 0.625, precision: 0.6792114695340502, f_beta: 0.5943204868154157
train: step: 1164, loss: 0.6972360610961914, acc: 0.45, recall: 0.45, precision: 0.23684210526315788, f_beta: 0.3103448275862069
train: step: 1165, loss: 0.6913789510726929, acc: 0.625, recall: 0.625, precision: 0.62531328320802, f_beta: 0.6247654784240151
train: step: 1166, loss: 0.6949511170387268, acc: 0.525, recall: 0.525, precision: 0.5255754475703325, f_beta: 0.5223130106851037
train: step: 1167, loss: 0.7107905149459839, acc: 0.4, recall: 0.4, precision: 0.2222222222222222, f_beta: 0.2857142857142857
train: step: 1168, loss: 0.6955584287643433, acc: 0.4, recall: 0.4, precision: 0.3666666666666667, f_beta: 0.36
train: step: 1169, loss: 0.6865261793136597, acc: 0.65, recall: 0.6499999999999999, precision: 0.734375, f_beta: 0.6153846153846154
train: step: 1170, loss: 0.6955791711807251, acc: 0.475, recall: 0.47500000000000003, precision: 0.45670995670995673, f_beta: 0.41299790356394134
train: step: 1171, loss: 0.6926172375679016, acc: 0.525, recall: 0.525, precision: 0.5313479623824451, f_beta: 0.4996708360763661
train: step: 1172, loss: 0.6927465200424194, acc: 0.525, recall: 0.525, precision: 0.5313479623824451, f_beta: 0.4996708360763661
train: step: 1173, loss: 0.6925759315490723, acc: 0.575, recall: 0.575, precision: 0.5767263427109974, f_beta: 0.5725958516656191
train: step: 1174, loss: 0.6937805414199829, acc: 0.5, recall: 0.5, precision: 0.5, f_beta: 0.40476190476190477
train: step: 1175, loss: 0.6954141855239868, acc: 0.325, recall: 0.325, precision: 0.32097186700767266, f_beta: 0.32118164676304206
train: step: 1176, loss: 0.691296398639679, acc: 0.55, recall: 0.55, precision: 0.5520833333333333, f_beta: 0.5454545454545454
train: step: 1177, loss: 0.6893243789672852, acc: 0.55, recall: 0.55, precision: 0.5595238095238095, f_beta: 0.53125
train: step: 1178, loss: 0.6952170133590698, acc: 0.4, recall: 0.4, precision: 0.3901098901098901, f_beta: 0.38618925831202044
train: step: 1179, loss: 0.6941800117492676, acc: 0.45, recall: 0.45, precision: 0.4494949494949495, f_beta: 0.4486215538847118
train: step: 1180, loss: 0.6943324208259583, acc: 0.525, recall: 0.525, precision: 0.5432900432900433, f_beta: 0.46890286512928026
train: step: 1181, loss: 0.6931141018867493, acc: 0.575, recall: 0.575, precision: 0.5800000000000001, f_beta: 0.5682539682539682
train: step: 1182, loss: 0.68858802318573, acc: 0.575, recall: 0.575, precision: 0.7702702702702703, f_beta: 0.4813119755911518
train: step: 1183, loss: 0.6926823854446411, acc: 0.5, recall: 0.5, precision: 0.5, f_beta: 0.47916666666666663
train: step: 1184, loss: 0.6946696043014526, acc: 0.45, recall: 0.45, precision: 0.44047619047619047, f_beta: 0.42708333333333337
train: step: 1185, loss: 0.6901439428329468, acc: 0.575, recall: 0.575, precision: 0.60752688172043, f_beta: 0.5402298850574712
train: step: 1186, loss: 0.6945091485977173, acc: 0.45, recall: 0.45, precision: 0.44791666666666663, f_beta: 0.4444444444444444
train: step: 1187, loss: 0.69453364610672, acc: 0.4, recall: 0.4, precision: 0.398989898989899, f_beta: 0.39849624060150374
train: step: 1188, loss: 0.6934772729873657, acc: 0.525, recall: 0.525, precision: 0.7564102564102564, f_beta: 0.38660209846650523
train: step: 1189, loss: 0.6937091946601868, acc: 0.4, recall: 0.4, precision: 0.39583333333333337, f_beta: 0.3939393939393939
train: step: 1190, loss: 0.6934472322463989, acc: 0.425, recall: 0.425, precision: 0.3701298701298701, f_beta: 0.35709294199860236
train: step: 1191, loss: 0.6919443011283875, acc: 0.55, recall: 0.55, precision: 0.5505050505050505, f_beta: 0.5488721804511277
train: step: 1192, loss: 0.6931471824645996, acc: 0.525, recall: 0.525, precision: 0.5571428571428572, f_beta: 0.44727272727272727
train: step: 1193, loss: 0.6923667192459106, acc: 0.5, recall: 0.5, precision: 0.5, f_beta: 0.4949494949494949
train: step: 1194, loss: 0.6939014196395874, acc: 0.45, recall: 0.45, precision: 0.44791666666666663, f_beta: 0.4444444444444444
train: step: 1195, loss: 0.6927446126937866, acc: 0.525, recall: 0.525, precision: 0.525062656641604, f_beta: 0.5247029393370857
train: step: 1196, loss: 0.6914842128753662, acc: 0.675, recall: 0.675, precision: 0.6866666666666666, f_beta: 0.6698412698412699
train: step: 1197, loss: 0.6924842596054077, acc: 0.475, recall: 0.47500000000000003, precision: 0.45670995670995673, f_beta: 0.41299790356394134
train: step: 1198, loss: 0.6944664716720581, acc: 0.425, recall: 0.425, precision: 0.22972972972972974, f_beta: 0.2982456140350877
train: step: 1199, loss: 0.692862868309021, acc: 0.475, recall: 0.47500000000000003, precision: 0.4744245524296675, f_beta: 0.47203016970458833
train: step: 1200, loss: 0.6961128115653992, acc: 0.325, recall: 0.325, precision: 0.31333333333333335, f_beta: 0.3142857142857143
train: step: 1201, loss: 0.6924461126327515, acc: 0.525, recall: 0.525, precision: 0.5313479623824451, f_beta: 0.4996708360763661
train: step: 1202, loss: 0.6920029520988464, acc: 0.5, recall: 0.5, precision: 0.5, f_beta: 0.4505494505494506
train: step: 1203, loss: 0.6945574879646301, acc: 0.4, recall: 0.4, precision: 0.398989898989899, f_beta: 0.39849624060150374
train: step: 1204, loss: 0.6929543614387512, acc: 0.575, recall: 0.575, precision: 0.5800000000000001, f_beta: 0.5682539682539682
train: step: 1205, loss: 0.6939279437065125, acc: 0.4, recall: 0.4, precision: 0.3901098901098901, f_beta: 0.38618925831202044
train: step: 1206, loss: 0.6909023523330688, acc: 0.525, recall: 0.525, precision: 0.7564102564102564, f_beta: 0.38660209846650523
train: step: 1207, loss: 0.690329909324646, acc: 0.55, recall: 0.5499999999999999, precision: 0.6388888888888888, f_beta: 0.46428571428571425
train: step: 1208, loss: 0.6850169897079468, acc: 0.6, recall: 0.6, precision: 0.696078431372549, f_beta: 0.5441595441595442
train: step: 1209, loss: 0.6834194660186768, acc: 0.7, recall: 0.7, precision: 0.7380952380952381, f_beta: 0.6875
train: step: 1210, loss: 0.6967359781265259, acc: 0.475, recall: 0.475, precision: 0.46415770609318996, f_beta: 0.43204868154158216
train: step: 1211, loss: 0.6924964189529419, acc: 0.525, recall: 0.525, precision: 0.5313479623824451, f_beta: 0.4996708360763661
train: step: 1212, loss: 0.6928259134292603, acc: 0.475, recall: 0.47500000000000003, precision: 0.45670995670995673, f_beta: 0.41299790356394134
train: step: 1213, loss: 0.6917856931686401, acc: 0.625, recall: 0.625, precision: 0.6792114695340502, f_beta: 0.5943204868154157
train: step: 1214, loss: 0.6967656016349792, acc: 0.375, recall: 0.375, precision: 0.37468671679197996, f_beta: 0.37460913070669166
train: step: 1215, loss: 0.6924214363098145, acc: 0.625, recall: 0.625, precision: 0.7164502164502164, f_beta: 0.5807127882599581
train: step: 1216, loss: 0.6929882764816284, acc: 0.525, recall: 0.525, precision: 0.525062656641604, f_beta: 0.5247029393370857
train: step: 1217, loss: 0.6939343214035034, acc: 0.525, recall: 0.5249999999999999, precision: 0.5284900284900285, f_beta: 0.5099935525467441
train: step: 1218, loss: 0.6945449113845825, acc: 0.4, recall: 0.4, precision: 0.39583333333333337, f_beta: 0.3939393939393939
train: step: 1219, loss: 0.6932511329650879, acc: 0.45, recall: 0.45, precision: 0.4494949494949495, f_beta: 0.4486215538847118
train: step: 1220, loss: 0.6917641758918762, acc: 0.5, recall: 0.5, precision: 0.5, f_beta: 0.4884910485933504
train: step: 1221, loss: 0.6973945498466492, acc: 0.45, recall: 0.44999999999999996, precision: 0.43333333333333335, f_beta: 0.41333333333333333
train: step: 1222, loss: 0.6936804056167603, acc: 0.5, recall: 0.5, precision: 0.5, f_beta: 0.4949494949494949
train: step: 1223, loss: 0.6929191946983337, acc: 0.55, recall: 0.55, precision: 0.5520833333333333, f_beta: 0.5454545454545454
train: step: 1224, loss: 0.6920768022537231, acc: 0.5, recall: 0.5, precision: 0.5, f_beta: 0.4884910485933504
train: step: 1225, loss: 0.6953325867652893, acc: 0.35, recall: 0.35, precision: 0.3351648351648352, f_beta: 0.3350383631713555
train: step: 1226, loss: 0.6933034658432007, acc: 0.45, recall: 0.45, precision: 0.421875, f_beta: 0.39560439560439553
train: step: 1227, loss: 0.6892699003219604, acc: 0.55, recall: 0.55, precision: 0.578125, f_beta: 0.5054945054945055
train: step: 1228, loss: 0.6954382658004761, acc: 0.375, recall: 0.375, precision: 0.3666666666666667, f_beta: 0.3650793650793651
train: step: 1229, loss: 0.6917542219161987, acc: 0.5, recall: 0.5, precision: 0.5, f_beta: 0.4505494505494506
train: step: 1230, loss: 0.6922634243965149, acc: 0.6, recall: 0.6000000000000001, precision: 0.601010101010101, f_beta: 0.5989974937343359
train: step: 1231, loss: 0.6935220956802368, acc: 0.45, recall: 0.45, precision: 0.44791666666666663, f_beta: 0.4444444444444444
train: step: 1232, loss: 0.6932557225227356, acc: 0.575, recall: 0.575, precision: 0.5767263427109974, f_beta: 0.5725958516656191
train: step: 1233, loss: 0.6934112310409546, acc: 0.55, recall: 0.55, precision: 0.554945054945055, f_beta: 0.5396419437340153
train: step: 1234, loss: 0.6936308741569519, acc: 0.475, recall: 0.475, precision: 0.44285714285714284, f_beta: 0.38909090909090915
train: step: 1235, loss: 0.6886851191520691, acc: 0.55, recall: 0.55, precision: 0.763157894736842, f_beta: 0.4357366771159875
train: step: 1236, loss: 0.6942402720451355, acc: 0.4, recall: 0.4, precision: 0.38095238095238093, f_beta: 0.375
train: step: 1237, loss: 0.6949596405029297, acc: 0.425, recall: 0.425, precision: 0.3701298701298701, f_beta: 0.35709294199860236
train: step: 1238, loss: 0.6906771659851074, acc: 0.7, recall: 0.7, precision: 0.702020202020202, f_beta: 0.6992481203007519
train: step: 1239, loss: 0.6928504705429077, acc: 0.55, recall: 0.55, precision: 0.5520833333333333, f_beta: 0.5454545454545454
train: step: 1240, loss: 0.6925622820854187, acc: 0.525, recall: 0.525, precision: 0.5900900900900901, f_beta: 0.4202898550724638
train: step: 1241, loss: 0.6923069953918457, acc: 0.5, recall: 0.5, precision: 0.5, f_beta: 0.5
train: step: 1242, loss: 0.6963058114051819, acc: 0.45, recall: 0.44999999999999996, precision: 0.43333333333333335, f_beta: 0.41333333333333333
train: step: 1243, loss: 0.6904460787773132, acc: 0.6, recall: 0.6, precision: 0.696078431372549, f_beta: 0.5441595441595442
train: step: 1244, loss: 0.6924689412117004, acc: 0.525, recall: 0.525, precision: 0.53584229390681, f_beta: 0.4861392832995267
train: step: 1245, loss: 0.6938716173171997, acc: 0.4, recall: 0.4, precision: 0.39583333333333337, f_beta: 0.3939393939393939
train: step: 1246, loss: 0.687696099281311, acc: 0.55, recall: 0.55, precision: 0.763157894736842, f_beta: 0.4357366771159875
train: step: 1247, loss: 0.6935070753097534, acc: 0.475, recall: 0.475, precision: 0.474937343358396, f_beta: 0.474671669793621
train: step: 1248, loss: 0.6937915086746216, acc: 0.375, recall: 0.375, precision: 0.3575498575498576, f_beta: 0.3552546744036106
train: step: 1249, loss: 0.6950639486312866, acc: 0.325, recall: 0.325, precision: 0.32097186700767266, f_beta: 0.32118164676304206
train: step: 1250, loss: 0.6954076290130615, acc: 0.5, recall: 0.5, precision: 0.5, f_beta: 0.43019943019943013
train: step: 1251, loss: 0.6920496225357056, acc: 0.6, recall: 0.6000000000000001, precision: 0.601010101010101, f_beta: 0.5989974937343359
train: step: 1252, loss: 0.6920862197875977, acc: 0.5, recall: 0.5, precision: 0.5, f_beta: 0.4949494949494949
train: step: 1253, loss: 0.6927553415298462, acc: 0.5, recall: 0.5, precision: 0.5, f_beta: 0.47916666666666663
train: step: 1254, loss: 0.6922159194946289, acc: 0.575, recall: 0.575, precision: 0.5854700854700855, f_beta: 0.5615731785944552
train: step: 1255, loss: 0.6917413473129272, acc: 0.6, recall: 0.6, precision: 0.6333333333333333, f_beta: 0.5733333333333334
train: step: 1256, loss: 0.6924850940704346, acc: 0.5, recall: 0.5, precision: 0.5, f_beta: 0.47916666666666663
train: step: 1257, loss: 0.6932369470596313, acc: 0.425, recall: 0.42500000000000004, precision: 0.424812030075188, f_beta: 0.4246404002501563
train: step: 1258, loss: 0.6935220956802368, acc: 0.525, recall: 0.525, precision: 0.5266666666666666, f_beta: 0.5174603174603175
train: step: 1259, loss: 0.6911827325820923, acc: 0.525, recall: 0.525, precision: 0.5571428571428572, f_beta: 0.44727272727272727
train: step: 1260, loss: 0.6949838399887085, acc: 0.45, recall: 0.45, precision: 0.3611111111111111, f_beta: 0.34523809523809523
train: step: 1261, loss: 0.6940695643424988, acc: 0.4, recall: 0.4, precision: 0.39583333333333337, f_beta: 0.3939393939393939
train: step: 1262, loss: 0.6905032992362976, acc: 0.5, recall: 0.5, precision: 0.5, f_beta: 0.4505494505494506
train: step: 1263, loss: 0.696661651134491, acc: 0.425, recall: 0.425, precision: 0.3924731182795699, f_beta: 0.37795807978363755
train: step: 1264, loss: 0.6937545537948608, acc: 0.5, recall: 0.5, precision: 0.5, f_beta: 0.4505494505494506
train: step: 1265, loss: 0.6899259686470032, acc: 0.475, recall: 0.47500000000000003, precision: 0.4744245524296675, f_beta: 0.47203016970458833
train: step: 1266, loss: 0.6927154660224915, acc: 0.55, recall: 0.55, precision: 0.5520833333333333, f_beta: 0.5454545454545454
train: step: 1267, loss: 0.692162275314331, acc: 0.475, recall: 0.475, precision: 0.474937343358396, f_beta: 0.474671669793621
train: step: 1268, loss: 0.6943972706794739, acc: 0.4, recall: 0.4, precision: 0.398989898989899, f_beta: 0.39849624060150374
train: step: 1269, loss: 0.6943074464797974, acc: 0.425, recall: 0.425, precision: 0.42327365728900257, f_beta: 0.4217473287240729
train: step: 1270, loss: 0.6937551498413086, acc: 0.425, recall: 0.42500000000000004, precision: 0.424812030075188, f_beta: 0.4246404002501563
train: step: 1271, loss: 0.6944411993026733, acc: 0.425, recall: 0.42500000000000004, precision: 0.42000000000000004, f_beta: 0.4158730158730159
train: step: 1272, loss: 0.6929208040237427, acc: 0.525, recall: 0.525, precision: 0.525062656641604, f_beta: 0.5247029393370857
train: step: 1273, loss: 0.6921616792678833, acc: 0.525, recall: 0.525, precision: 0.5313479623824451, f_beta: 0.4996708360763661
train: step: 1274, loss: 0.6925358772277832, acc: 0.55, recall: 0.55, precision: 0.5520833333333333, f_beta: 0.5454545454545454
train: step: 1275, loss: 0.6896221041679382, acc: 0.55, recall: 0.55, precision: 0.763157894736842, f_beta: 0.4357366771159875
train: step: 1276, loss: 0.692920446395874, acc: 0.575, recall: 0.575, precision: 0.5800000000000001, f_beta: 0.5682539682539682
train: step: 1277, loss: 0.6923874020576477, acc: 0.575, recall: 0.575, precision: 0.575187969924812, f_beta: 0.5747342088805503
train: step: 1278, loss: 0.6922074556350708, acc: 0.6, recall: 0.6000000000000001, precision: 0.601010101010101, f_beta: 0.5989974937343359
train: step: 1279, loss: 0.6938084363937378, acc: 0.475, recall: 0.475, precision: 0.47150997150997154, f_beta: 0.4584139264990329
train: step: 1280, loss: 0.6960572004318237, acc: 0.375, recall: 0.375, precision: 0.3666666666666667, f_beta: 0.3650793650793651
train: step: 1281, loss: 0.6943058371543884, acc: 0.475, recall: 0.475, precision: 0.24358974358974358, f_beta: 0.3220338983050847
train: step: 1282, loss: 0.6943261623382568, acc: 0.4, recall: 0.4, precision: 0.3901098901098901, f_beta: 0.38618925831202044
train: step: 1283, loss: 0.6935378313064575, acc: 0.5, recall: 0.5, precision: 0.5, f_beta: 0.4987468671679198
train: step: 1284, loss: 0.6946641802787781, acc: 0.4, recall: 0.4, precision: 0.3666666666666667, f_beta: 0.36
train: step: 1285, loss: 0.6918172240257263, acc: 0.525, recall: 0.525, precision: 0.5255754475703325, f_beta: 0.5223130106851037
train: step: 1286, loss: 0.6912644505500793, acc: 0.625, recall: 0.625, precision: 0.6278772378516624, f_beta: 0.6228786926461345
train: step: 1287, loss: 0.6902934312820435, acc: 0.5, recall: 0.5, precision: 0.5, f_beta: 0.40476190476190477
train: step: 1288, loss: 0.691038966178894, acc: 0.5, recall: 0.5, precision: 0.5, f_beta: 0.4884910485933504
train: step: 1289, loss: 0.6927938461303711, acc: 0.525, recall: 0.5249999999999999, precision: 0.5284900284900285, f_beta: 0.5099935525467441
train: step: 1290, loss: 0.6920090913772583, acc: 0.525, recall: 0.525, precision: 0.5255754475703325, f_beta: 0.5223130106851037
train: step: 1291, loss: 0.6935124397277832, acc: 0.55, recall: 0.55, precision: 0.554945054945055, f_beta: 0.5396419437340153
train: step: 1292, loss: 0.6916048526763916, acc: 0.525, recall: 0.525, precision: 0.5255754475703325, f_beta: 0.5223130106851037
train: step: 1293, loss: 0.6854289174079895, acc: 0.6, recall: 0.6, precision: 0.696078431372549, f_beta: 0.5441595441595442
train: step: 1294, loss: 0.6925435066223145, acc: 0.55, recall: 0.55, precision: 0.5520833333333333, f_beta: 0.5454545454545454
train: step: 1295, loss: 0.6927763223648071, acc: 0.525, recall: 0.5249999999999999, precision: 0.5284900284900285, f_beta: 0.5099935525467441
train: step: 1296, loss: 0.6920952796936035, acc: 0.55, recall: 0.55, precision: 0.5505050505050505, f_beta: 0.5488721804511277
train: step: 1297, loss: 0.6961168050765991, acc: 0.475, recall: 0.47500000000000003, precision: 0.45670995670995673, f_beta: 0.41299790356394134
train: step: 1298, loss: 0.6930612325668335, acc: 0.525, recall: 0.525, precision: 0.525062656641604, f_beta: 0.5247029393370857
train: step: 1299, loss: 0.6921136379241943, acc: 0.575, recall: 0.575, precision: 0.575187969924812, f_beta: 0.5747342088805503
train: step: 1300, loss: 0.6930449604988098, acc: 0.6, recall: 0.6000000000000001, precision: 0.601010101010101, f_beta: 0.5989974937343359
train: step: 1301, loss: 0.6963238716125488, acc: 0.425, recall: 0.425, precision: 0.3924731182795699, f_beta: 0.37795807978363755
train: step: 1302, loss: 0.6922814846038818, acc: 0.6, recall: 0.6, precision: 0.6333333333333333, f_beta: 0.5733333333333334
train: step: 1303, loss: 0.6884649395942688, acc: 0.55, recall: 0.55, precision: 0.763157894736842, f_beta: 0.4357366771159875
train: step: 1304, loss: 0.6947128176689148, acc: 0.45, recall: 0.45, precision: 0.4494949494949495, f_beta: 0.4486215538847118
train: step: 1305, loss: 0.6928085088729858, acc: 0.525, recall: 0.525, precision: 0.525062656641604, f_beta: 0.5247029393370857
train: step: 1306, loss: 0.6939748525619507, acc: 0.4, recall: 0.4, precision: 0.4, f_beta: 0.4000000000000001
train: step: 1307, loss: 0.6814461946487427, acc: 0.725, recall: 0.725, precision: 0.8225806451612903, f_beta: 0.7025016903313049
train: step: 1308, loss: 0.6932567358016968, acc: 0.425, recall: 0.425, precision: 0.4145299145299145, f_beta: 0.40683430045132174
train: step: 1309, loss: 0.6931977272033691, acc: 0.475, recall: 0.47500000000000003, precision: 0.4744245524296675, f_beta: 0.47203016970458833
train: step: 1310, loss: 0.6928814649581909, acc: 0.55, recall: 0.55, precision: 0.5520833333333333, f_beta: 0.5454545454545454
train: step: 1311, loss: 0.6915460824966431, acc: 0.55, recall: 0.55, precision: 0.578125, f_beta: 0.5054945054945055
train: step: 1312, loss: 0.692358136177063, acc: 0.55, recall: 0.55, precision: 0.5505050505050505, f_beta: 0.5488721804511277
train: step: 1313, loss: 0.6934150457382202, acc: 0.475, recall: 0.475, precision: 0.474937343358396, f_beta: 0.474671669793621
train: step: 1314, loss: 0.6950403451919556, acc: 0.35, recall: 0.35, precision: 0.3484848484848485, f_beta: 0.3483709273182958
train: step: 1315, loss: 0.6945222020149231, acc: 0.55, recall: 0.55, precision: 0.5595238095238095, f_beta: 0.53125
train: step: 1316, loss: 0.6912043690681458, acc: 0.45, recall: 0.45, precision: 0.421875, f_beta: 0.39560439560439553
train: step: 1317, loss: 0.6890780329704285, acc: 0.55, recall: 0.55, precision: 0.5595238095238095, f_beta: 0.53125
train: step: 1318, loss: 0.6924468278884888, acc: 0.525, recall: 0.525, precision: 0.5266666666666666, f_beta: 0.5174603174603175
train: step: 1319, loss: 0.6806133985519409, acc: 0.625, recall: 0.625, precision: 0.7857142857142857, f_beta: 0.5636363636363637
train: step: 1320, loss: 0.6935403943061829, acc: 0.425, recall: 0.42500000000000004, precision: 0.424812030075188, f_beta: 0.4246404002501563
train: step: 1321, loss: 0.6961716413497925, acc: 0.325, recall: 0.325, precision: 0.31333333333333335, f_beta: 0.3142857142857143
train: step: 1322, loss: 0.6831902265548706, acc: 0.55, recall: 0.55, precision: 0.5980392156862745, f_beta: 0.4871794871794872
train: step: 1323, loss: 0.6942551732063293, acc: 0.45, recall: 0.45, precision: 0.4494949494949495, f_beta: 0.4486215538847118
train: step: 1324, loss: 0.6907978057861328, acc: 0.575, recall: 0.575, precision: 0.5940438871473355, f_beta: 0.5523370638578011
train: step: 1325, loss: 0.6930940747261047, acc: 0.5, recall: 0.5, precision: 0.5, f_beta: 0.40476190476190477
train: step: 1326, loss: 0.6885925531387329, acc: 0.65, recall: 0.65, precision: 0.65625, f_beta: 0.6464646464646464
train: step: 1327, loss: 0.6932843327522278, acc: 0.55, recall: 0.55, precision: 0.554945054945055, f_beta: 0.5396419437340153
train: step: 1328, loss: 0.6929044723510742, acc: 0.575, recall: 0.575, precision: 0.6714285714285715, f_beta: 0.5054545454545454
train: step: 1329, loss: 0.6859591007232666, acc: 0.575, recall: 0.575, precision: 0.60752688172043, f_beta: 0.5402298850574712
train: step: 1330, loss: 0.6936448812484741, acc: 0.575, recall: 0.575, precision: 0.5767263427109974, f_beta: 0.5725958516656191
train: step: 1331, loss: 0.6959512829780579, acc: 0.45, recall: 0.45, precision: 0.44047619047619047, f_beta: 0.42708333333333337
train: step: 1332, loss: 0.692353367805481, acc: 0.525, recall: 0.525, precision: 0.525062656641604, f_beta: 0.5247029393370857
train: step: 1333, loss: 0.6930795907974243, acc: 0.575, recall: 0.575, precision: 0.5767263427109974, f_beta: 0.5725958516656191
train: step: 1334, loss: 0.6888860464096069, acc: 0.575, recall: 0.575, precision: 0.6714285714285715, f_beta: 0.5054545454545454
train: step: 1335, loss: 0.6958044767379761, acc: 0.425, recall: 0.42500000000000004, precision: 0.4059561128526646, f_beta: 0.39433838051349573
train: step: 1336, loss: 0.6935020089149475, acc: 0.45, recall: 0.45, precision: 0.45, f_beta: 0.45
train: step: 1337, loss: 0.6942065954208374, acc: 0.5, recall: 0.5, precision: 0.5, f_beta: 0.4884910485933504
train: step: 1338, loss: 0.6940613389015198, acc: 0.425, recall: 0.425, precision: 0.4145299145299145, f_beta: 0.40683430045132174
train: step: 1339, loss: 0.692961573600769, acc: 0.575, recall: 0.575, precision: 0.5800000000000001, f_beta: 0.5682539682539682
train: step: 1340, loss: 0.6940330862998962, acc: 0.4, recall: 0.4, precision: 0.39583333333333337, f_beta: 0.3939393939393939
train: step: 1341, loss: 0.6928836703300476, acc: 0.525, recall: 0.525, precision: 0.525062656641604, f_beta: 0.5247029393370857
train: step: 1342, loss: 0.6953204870223999, acc: 0.45, recall: 0.45, precision: 0.421875, f_beta: 0.39560439560439553
train: step: 1343, loss: 0.6942749619483948, acc: 0.45, recall: 0.45, precision: 0.45, f_beta: 0.45
train: step: 1344, loss: 0.6930416822433472, acc: 0.575, recall: 0.575, precision: 0.5767263427109974, f_beta: 0.5725958516656191
train: step: 1345, loss: 0.6926724314689636, acc: 0.525, recall: 0.525, precision: 0.5432900432900433, f_beta: 0.46890286512928026
train: step: 1346, loss: 0.6941373944282532, acc: 0.4, recall: 0.4, precision: 0.4, f_beta: 0.4000000000000001
train: step: 1347, loss: 0.6909093856811523, acc: 0.525, recall: 0.5249999999999999, precision: 0.5284900284900285, f_beta: 0.5099935525467441
train: step: 1348, loss: 0.6944342851638794, acc: 0.4, recall: 0.4, precision: 0.398989898989899, f_beta: 0.39849624060150374
train: step: 1349, loss: 0.696043848991394, acc: 0.525, recall: 0.525, precision: 0.525062656641604, f_beta: 0.5247029393370857
train: step: 1350, loss: 0.6935046315193176, acc: 0.525, recall: 0.525, precision: 0.525062656641604, f_beta: 0.5247029393370857
train: step: 1351, loss: 0.691778302192688, acc: 0.475, recall: 0.475, precision: 0.47333333333333333, f_beta: 0.4666666666666666
train: step: 1352, loss: 0.6937503814697266, acc: 0.4, recall: 0.4, precision: 0.39583333333333337, f_beta: 0.3939393939393939
train: step: 1353, loss: 0.6948912739753723, acc: 0.45, recall: 0.45, precision: 0.4494949494949495, f_beta: 0.4486215538847118
train: step: 1354, loss: 0.6959525942802429, acc: 0.45, recall: 0.45, precision: 0.44791666666666663, f_beta: 0.4444444444444444
train: step: 1355, loss: 0.6939433813095093, acc: 0.5, recall: 0.5, precision: 0.5, f_beta: 0.4884910485933504
train: step: 1356, loss: 0.7004157900810242, acc: 0.475, recall: 0.475, precision: 0.47333333333333333, f_beta: 0.4666666666666666
train: step: 1357, loss: 0.6927669644355774, acc: 0.55, recall: 0.55, precision: 0.5595238095238095, f_beta: 0.53125
train: step: 1358, loss: 0.6932920217514038, acc: 0.525, recall: 0.525, precision: 0.53584229390681, f_beta: 0.4861392832995267
train: step: 1359, loss: 0.6906591057777405, acc: 0.5, recall: 0.5, precision: 0.5, f_beta: 0.43019943019943013
train: step: 1360, loss: 0.6935206651687622, acc: 0.425, recall: 0.42500000000000004, precision: 0.42000000000000004, f_beta: 0.4158730158730159
train: step: 1361, loss: 0.6957752108573914, acc: 0.475, recall: 0.47500000000000003, precision: 0.45670995670995673, f_beta: 0.41299790356394134
train: step: 1362, loss: 0.693834125995636, acc: 0.5, recall: 0.5, precision: 0.5, f_beta: 0.4884910485933504
train: step: 1363, loss: 0.6933493614196777, acc: 0.5, recall: 0.5, precision: 0.5, f_beta: 0.4987468671679198
train: step: 1364, loss: 0.6930359601974487, acc: 0.5, recall: 0.5, precision: 0.5, f_beta: 0.40476190476190477
train: step: 1365, loss: 0.6921899914741516, acc: 0.625, recall: 0.625, precision: 0.62531328320802, f_beta: 0.6247654784240151
train: step: 1366, loss: 0.6915891766548157, acc: 0.5, recall: 0.5, precision: 0.5, f_beta: 0.4505494505494506
train: step: 1367, loss: 0.6926940083503723, acc: 0.5, recall: 0.5, precision: 0.5, f_beta: 0.4949494949494949
train: step: 1368, loss: 0.6955629587173462, acc: 0.425, recall: 0.42500000000000004, precision: 0.32857142857142857, f_beta: 0.33090909090909093
train: step: 1369, loss: 0.6908520460128784, acc: 0.6, recall: 0.6, precision: 0.6041666666666667, f_beta: 0.595959595959596
train: step: 1370, loss: 0.6928523182868958, acc: 0.55, recall: 0.55, precision: 0.5595238095238095, f_beta: 0.53125
train: step: 1371, loss: 0.692878246307373, acc: 0.55, recall: 0.55, precision: 0.5980392156862745, f_beta: 0.4871794871794872
train: step: 1372, loss: 0.6923243999481201, acc: 0.65, recall: 0.65, precision: 0.6648351648351649, f_beta: 0.6419437340153452
train: step: 1373, loss: 0.6949063539505005, acc: 0.35, recall: 0.35000000000000003, precision: 0.3214285714285714, f_beta: 0.3229166666666667
train: step: 1374, loss: 0.6921380758285522, acc: 0.575, recall: 0.575, precision: 0.5767263427109974, f_beta: 0.5725958516656191
train: step: 1375, loss: 0.693727970123291, acc: 0.475, recall: 0.475, precision: 0.44285714285714284, f_beta: 0.38909090909090915
train: step: 1376, loss: 0.6944165229797363, acc: 0.425, recall: 0.425, precision: 0.42327365728900257, f_beta: 0.4217473287240729
train: step: 1377, loss: 0.688888430595398, acc: 0.6, recall: 0.6, precision: 0.65625, f_beta: 0.5604395604395604
train: step: 1378, loss: 0.692934513092041, acc: 0.475, recall: 0.475, precision: 0.47333333333333333, f_beta: 0.4666666666666666
train: step: 1379, loss: 0.6900820732116699, acc: 0.525, recall: 0.525, precision: 0.5900900900900901, f_beta: 0.4202898550724638
train: step: 1380, loss: 0.6935750246047974, acc: 0.425, recall: 0.425, precision: 0.42327365728900257, f_beta: 0.4217473287240729
train: step: 1381, loss: 0.6922107934951782, acc: 0.625, recall: 0.625, precision: 0.6567398119122256, f_beta: 0.6050032916392363
train: step: 1382, loss: 0.6930612325668335, acc: 0.525, recall: 0.525, precision: 0.53584229390681, f_beta: 0.4861392832995267
train: step: 1383, loss: 0.6883614659309387, acc: 0.575, recall: 0.575, precision: 0.5800000000000001, f_beta: 0.5682539682539682
train: step: 1384, loss: 0.6889569163322449, acc: 0.575, recall: 0.575, precision: 0.7702702702702703, f_beta: 0.4813119755911518
train: step: 1385, loss: 0.6914031505584717, acc: 0.625, recall: 0.625, precision: 0.6333333333333333, f_beta: 0.6190476190476191
train: step: 1386, loss: 0.6920279264450073, acc: 0.525, recall: 0.525, precision: 0.5255754475703325, f_beta: 0.5223130106851037
train: step: 1387, loss: 0.6927695274353027, acc: 0.525, recall: 0.525, precision: 0.5432900432900433, f_beta: 0.46890286512928026
train: step: 1388, loss: 0.6890547275543213, acc: 0.65, recall: 0.6499999999999999, precision: 0.734375, f_beta: 0.6153846153846154
train: step: 1389, loss: 0.6939135789871216, acc: 0.425, recall: 0.42500000000000004, precision: 0.4059561128526646, f_beta: 0.39433838051349573
train: step: 1390, loss: 0.6811776757240295, acc: 0.575, recall: 0.575, precision: 0.60752688172043, f_beta: 0.5402298850574712
train: step: 1391, loss: 0.6954618692398071, acc: 0.5, recall: 0.5, precision: 0.5, f_beta: 0.47916666666666663
train: step: 1392, loss: 0.6952556371688843, acc: 0.425, recall: 0.42500000000000004, precision: 0.4059561128526646, f_beta: 0.39433838051349573
train: step: 1393, loss: 0.6959096789360046, acc: 0.425, recall: 0.425, precision: 0.22972972972972974, f_beta: 0.2982456140350877
train: step: 1394, loss: 0.6930304765701294, acc: 0.45, recall: 0.45, precision: 0.44047619047619047, f_beta: 0.42708333333333337
train: step: 1395, loss: 0.686661422252655, acc: 0.55, recall: 0.5499999999999999, precision: 0.6388888888888888, f_beta: 0.46428571428571425
train: step: 1396, loss: 0.6923937797546387, acc: 0.55, recall: 0.55, precision: 0.5520833333333333, f_beta: 0.5454545454545454
train: step: 1397, loss: 0.6951261758804321, acc: 0.5, recall: 0.5, precision: 0.25, f_beta: 0.3333333333333333
train: step: 1398, loss: 0.6928439736366272, acc: 0.475, recall: 0.475, precision: 0.474937343358396, f_beta: 0.474671669793621
train: step: 1399, loss: 0.6936084032058716, acc: 0.55, recall: 0.55, precision: 0.5666666666666667, f_beta: 0.52
train: step: 1400, loss: 0.694418728351593, acc: 0.425, recall: 0.425, precision: 0.42327365728900257, f_beta: 0.4217473287240729
train: step: 1401, loss: 0.6922356486320496, acc: 0.5, recall: 0.5, precision: 0.5, f_beta: 0.4666666666666667
train: step: 1402, loss: 0.6936523914337158, acc: 0.55, recall: 0.55, precision: 0.5520833333333333, f_beta: 0.5454545454545454
train: step: 1403, loss: 0.6929577589035034, acc: 0.525, recall: 0.525, precision: 0.525062656641604, f_beta: 0.5247029393370857
train: step: 1404, loss: 0.6896395683288574, acc: 0.525, recall: 0.525, precision: 0.5432900432900433, f_beta: 0.46890286512928026
train: step: 1405, loss: 0.699921727180481, acc: 0.5, recall: 0.5, precision: 0.5, f_beta: 0.47916666666666663
train: step: 1406, loss: 0.6916383504867554, acc: 0.45, recall: 0.45, precision: 0.4494949494949495, f_beta: 0.4486215538847118
train: step: 1407, loss: 0.689307689666748, acc: 0.65, recall: 0.6499999999999999, precision: 0.6515151515151515, f_beta: 0.6491228070175439
train: step: 1408, loss: 0.6928592920303345, acc: 0.525, recall: 0.525, precision: 0.53584229390681, f_beta: 0.4861392832995267
train: step: 1409, loss: 0.6931028962135315, acc: 0.55, recall: 0.55, precision: 0.5505050505050505, f_beta: 0.5488721804511277
train: step: 1410, loss: 0.6936231851577759, acc: 0.475, recall: 0.475, precision: 0.47333333333333333, f_beta: 0.4666666666666666
train: step: 1411, loss: 0.6971611976623535, acc: 0.5, recall: 0.5, precision: 0.5, f_beta: 0.40476190476190477
train: step: 1412, loss: 0.6925050020217896, acc: 0.525, recall: 0.525, precision: 0.5255754475703325, f_beta: 0.5223130106851037
train: step: 1413, loss: 0.6929439306259155, acc: 0.475, recall: 0.475, precision: 0.47333333333333333, f_beta: 0.4666666666666666
train: step: 1414, loss: 0.6948105096817017, acc: 0.425, recall: 0.425, precision: 0.42327365728900257, f_beta: 0.4217473287240729
train: step: 1415, loss: 0.6910282969474792, acc: 0.7, recall: 0.7, precision: 0.7197802197802198, f_beta: 0.6930946291560103
train: step: 1416, loss: 0.6937052011489868, acc: 0.5, recall: 0.5, precision: 0.5, f_beta: 0.4949494949494949
train: step: 1417, loss: 0.695695698261261, acc: 0.475, recall: 0.475, precision: 0.44285714285714284, f_beta: 0.38909090909090915
train: step: 1418, loss: 0.6916872262954712, acc: 0.525, recall: 0.525, precision: 0.5313479623824451, f_beta: 0.4996708360763661
train: step: 1419, loss: 0.6923723220825195, acc: 0.5, recall: 0.5, precision: 0.5, f_beta: 0.4949494949494949
train: step: 1420, loss: 0.68760085105896, acc: 0.55, recall: 0.55, precision: 0.5595238095238095, f_beta: 0.53125
train: step: 1421, loss: 0.6934070587158203, acc: 0.4, recall: 0.4, precision: 0.39583333333333337, f_beta: 0.3939393939393939
train: step: 1422, loss: 0.6926197409629822, acc: 0.55, recall: 0.55, precision: 0.763157894736842, f_beta: 0.4357366771159875
train: step: 1423, loss: 0.6934153437614441, acc: 0.45, recall: 0.45, precision: 0.4494949494949495, f_beta: 0.4486215538847118
train: step: 1424, loss: 0.6918599605560303, acc: 0.525, recall: 0.5249999999999999, precision: 0.5284900284900285, f_beta: 0.5099935525467441
train: step: 1425, loss: 0.6948047876358032, acc: 0.475, recall: 0.475, precision: 0.47333333333333333, f_beta: 0.4666666666666666
train: step: 1426, loss: 0.7008123397827148, acc: 0.375, recall: 0.375, precision: 0.28354978354978355, f_beta: 0.3011879804332634
train: step: 1427, loss: 0.6948252320289612, acc: 0.425, recall: 0.42500000000000004, precision: 0.424812030075188, f_beta: 0.4246404002501563
train: step: 1428, loss: 0.6929464340209961, acc: 0.525, recall: 0.525, precision: 0.5255754475703325, f_beta: 0.5223130106851037
train: step: 1429, loss: 0.6924082040786743, acc: 0.5, recall: 0.5, precision: 0.5, f_beta: 0.40476190476190477
train: step: 1430, loss: 0.697045087814331, acc: 0.475, recall: 0.47500000000000003, precision: 0.45670995670995673, f_beta: 0.41299790356394134
train: step: 1431, loss: 0.6945602297782898, acc: 0.475, recall: 0.475, precision: 0.474937343358396, f_beta: 0.474671669793621
train: step: 1432, loss: 0.6934170722961426, acc: 0.5, recall: 0.5, precision: 0.5, f_beta: 0.4666666666666667
train: step: 1433, loss: 0.6935771703720093, acc: 0.475, recall: 0.475, precision: 0.47150997150997154, f_beta: 0.4584139264990329
train: step: 1434, loss: 0.6928924322128296, acc: 0.575, recall: 0.575, precision: 0.575187969924812, f_beta: 0.5747342088805503
train: step: 1435, loss: 0.6960523724555969, acc: 0.425, recall: 0.42500000000000004, precision: 0.4059561128526646, f_beta: 0.39433838051349573
train: step: 1436, loss: 0.6937289237976074, acc: 0.575, recall: 0.575, precision: 0.5800000000000001, f_beta: 0.5682539682539682
train: step: 1437, loss: 0.6961362957954407, acc: 0.45, recall: 0.44999999999999996, precision: 0.43333333333333335, f_beta: 0.41333333333333333
train: step: 1438, loss: 0.6852496266365051, acc: 0.575, recall: 0.575, precision: 0.5800000000000001, f_beta: 0.5682539682539682
train: step: 1439, loss: 0.6922359466552734, acc: 0.6, recall: 0.6, precision: 0.6041666666666667, f_beta: 0.595959595959596
train: step: 1440, loss: 0.6948594450950623, acc: 0.4, recall: 0.4, precision: 0.398989898989899, f_beta: 0.39849624060150374
train: step: 1441, loss: 0.6958628296852112, acc: 0.45, recall: 0.45, precision: 0.421875, f_beta: 0.39560439560439553
train: step: 1442, loss: 0.6941058039665222, acc: 0.425, recall: 0.42500000000000004, precision: 0.424812030075188, f_beta: 0.4246404002501563
train: step: 1443, loss: 0.6933759450912476, acc: 0.5, recall: 0.5, precision: 0.5, f_beta: 0.40476190476190477
train: step: 1444, loss: 0.6951324343681335, acc: 0.375, recall: 0.375, precision: 0.34326018808777425, f_beta: 0.34167215273206053
train: step: 1445, loss: 0.6936975717544556, acc: 0.5, recall: 0.5, precision: 0.5, f_beta: 0.5
train: step: 1446, loss: 0.6908434629440308, acc: 0.675, recall: 0.675, precision: 0.6866666666666666, f_beta: 0.6698412698412699
train: step: 1447, loss: 0.6894737482070923, acc: 0.625, recall: 0.625, precision: 0.7857142857142857, f_beta: 0.5636363636363637
train: step: 1448, loss: 0.6920285224914551, acc: 0.575, recall: 0.575, precision: 0.5767263427109974, f_beta: 0.5725958516656191
train: step: 1449, loss: 0.6931262016296387, acc: 0.5, recall: 0.5, precision: 0.5, f_beta: 0.4987468671679198
train: step: 1450, loss: 0.6938918828964233, acc: 0.5, recall: 0.5, precision: 0.5, f_beta: 0.4987468671679198
train: step: 1451, loss: 0.6938597559928894, acc: 0.475, recall: 0.475, precision: 0.44285714285714284, f_beta: 0.38909090909090915
train: step: 1452, loss: 0.6932911276817322, acc: 0.525, recall: 0.525, precision: 0.5266666666666666, f_beta: 0.5174603174603175
train: step: 1453, loss: 0.6927803754806519, acc: 0.55, recall: 0.55, precision: 0.5595238095238095, f_beta: 0.53125
train: step: 1454, loss: 0.69187992811203, acc: 0.6, recall: 0.6, precision: 0.6041666666666667, f_beta: 0.595959595959596
train: step: 1455, loss: 0.690907895565033, acc: 0.5, recall: 0.5, precision: 0.5, f_beta: 0.4949494949494949
train: step: 1456, loss: 0.6925321817398071, acc: 0.525, recall: 0.525, precision: 0.5313479623824451, f_beta: 0.4996708360763661
train: step: 1457, loss: 0.6907321214675903, acc: 0.65, recall: 0.6499999999999999, precision: 0.6515151515151515, f_beta: 0.6491228070175439
train: step: 1458, loss: 0.6947768330574036, acc: 0.375, recall: 0.375, precision: 0.3666666666666667, f_beta: 0.3650793650793651
train: step: 1459, loss: 0.6925279498100281, acc: 0.475, recall: 0.475, precision: 0.474937343358396, f_beta: 0.474671669793621
train: step: 1460, loss: 0.693760871887207, acc: 0.45, recall: 0.44999999999999996, precision: 0.43333333333333335, f_beta: 0.41333333333333333
train: step: 1461, loss: 0.6977468729019165, acc: 0.5, recall: 0.5, precision: 0.5, f_beta: 0.40476190476190477
train: step: 1462, loss: 0.6937991380691528, acc: 0.575, recall: 0.575, precision: 0.60752688172043, f_beta: 0.5402298850574712
train: step: 1463, loss: 0.6924446821212769, acc: 0.475, recall: 0.475, precision: 0.47150997150997154, f_beta: 0.4584139264990329
train: step: 1464, loss: 0.6918756365776062, acc: 0.625, recall: 0.625, precision: 0.62531328320802, f_beta: 0.6247654784240151
train: step: 1465, loss: 0.6955057978630066, acc: 0.375, recall: 0.375, precision: 0.3721227621483376, f_beta: 0.3714644877435575
train: step: 1466, loss: 0.6924359202384949, acc: 0.5, recall: 0.5, precision: 0.5, f_beta: 0.40476190476190477
train: step: 1467, loss: 0.6926683187484741, acc: 0.575, recall: 0.575, precision: 0.575187969924812, f_beta: 0.5747342088805503
train: step: 1468, loss: 0.692787766456604, acc: 0.55, recall: 0.55, precision: 0.5666666666666667, f_beta: 0.52
train: step: 1469, loss: 0.6933647394180298, acc: 0.4, recall: 0.4, precision: 0.38095238095238093, f_beta: 0.375
train: step: 1470, loss: 0.6947320103645325, acc: 0.425, recall: 0.42500000000000004, precision: 0.424812030075188, f_beta: 0.4246404002501563
train: step: 1471, loss: 0.6926869750022888, acc: 0.525, recall: 0.525, precision: 0.5255754475703325, f_beta: 0.5223130106851037
train: step: 1472, loss: 0.6930667161941528, acc: 0.525, recall: 0.525, precision: 0.53584229390681, f_beta: 0.4861392832995267
train: step: 1473, loss: 0.6926549673080444, acc: 0.525, recall: 0.525, precision: 0.525062656641604, f_beta: 0.5247029393370857
train: step: 1474, loss: 0.6761341094970703, acc: 0.625, recall: 0.625, precision: 0.6567398119122256, f_beta: 0.6050032916392363
train: step: 1475, loss: 0.6911578178405762, acc: 0.5, recall: 0.5, precision: 0.5, f_beta: 0.47916666666666663
train: step: 1476, loss: 0.6938968896865845, acc: 0.5, recall: 0.5, precision: 0.5, f_beta: 0.47916666666666663
train: step: 1477, loss: 0.6887437105178833, acc: 0.575, recall: 0.575, precision: 0.6298701298701299, f_beta: 0.5248078266946192
train: step: 1478, loss: 0.6924933195114136, acc: 0.55, recall: 0.55, precision: 0.5520833333333333, f_beta: 0.5454545454545454
train: step: 1479, loss: 0.6918745636940002, acc: 0.55, recall: 0.55, precision: 0.5520833333333333, f_beta: 0.5454545454545454
train: step: 1480, loss: 0.6928483843803406, acc: 0.525, recall: 0.525, precision: 0.5571428571428572, f_beta: 0.44727272727272727
train: step: 1481, loss: 0.6942028403282166, acc: 0.475, recall: 0.47500000000000003, precision: 0.4744245524296675, f_beta: 0.47203016970458833
train: step: 1482, loss: 0.6939778923988342, acc: 0.45, recall: 0.44999999999999996, precision: 0.43333333333333335, f_beta: 0.41333333333333333
train: step: 1483, loss: 0.6906968951225281, acc: 0.625, recall: 0.625, precision: 0.6333333333333333, f_beta: 0.6190476190476191
train: step: 1484, loss: 0.6926644444465637, acc: 0.55, recall: 0.55, precision: 0.5520833333333333, f_beta: 0.5454545454545454
train: step: 1485, loss: 0.6948065757751465, acc: 0.45, recall: 0.45, precision: 0.4494949494949495, f_beta: 0.4486215538847118
train: step: 1486, loss: 0.6938700079917908, acc: 0.5, recall: 0.5, precision: 0.5, f_beta: 0.4949494949494949
train: step: 1487, loss: 0.6936456561088562, acc: 0.35, recall: 0.35, precision: 0.3351648351648352, f_beta: 0.3350383631713555
train: step: 1488, loss: 0.6908539533615112, acc: 0.6, recall: 0.6, precision: 0.6041666666666667, f_beta: 0.595959595959596
train: step: 1489, loss: 0.6904386281967163, acc: 0.525, recall: 0.525, precision: 0.5571428571428572, f_beta: 0.44727272727272727
train: step: 1490, loss: 0.6923570036888123, acc: 0.525, recall: 0.525, precision: 0.5266666666666666, f_beta: 0.5174603174603175
train: step: 1491, loss: 0.6936750411987305, acc: 0.425, recall: 0.425, precision: 0.42327365728900257, f_beta: 0.4217473287240729
train: step: 1492, loss: 0.6895374059677124, acc: 0.55, recall: 0.5499999999999999, precision: 0.6388888888888888, f_beta: 0.46428571428571425
train: step: 1493, loss: 0.6937037706375122, acc: 0.4, recall: 0.4, precision: 0.39583333333333337, f_beta: 0.3939393939393939
train: step: 1494, loss: 0.6940664052963257, acc: 0.425, recall: 0.425, precision: 0.22972972972972974, f_beta: 0.2982456140350877
train: step: 1495, loss: 0.6906805038452148, acc: 0.45, recall: 0.45, precision: 0.44047619047619047, f_beta: 0.42708333333333337
train: step: 1496, loss: 0.6932320594787598, acc: 0.4, recall: 0.4, precision: 0.3901098901098901, f_beta: 0.38618925831202044
train: step: 1497, loss: 0.692375898361206, acc: 0.525, recall: 0.525, precision: 0.525062656641604, f_beta: 0.5247029393370857
train: step: 1498, loss: 0.6922725439071655, acc: 0.575, recall: 0.575, precision: 0.5854700854700855, f_beta: 0.5615731785944552
train: step: 1499, loss: 0.6952434778213501, acc: 0.5, recall: 0.5, precision: 0.5, f_beta: 0.4987468671679198
train: step: 1500, loss: 0.6893631815910339, acc: 0.575, recall: 0.575, precision: 0.6714285714285715, f_beta: 0.5054545454545454
train: step: 1501, loss: 0.6939624547958374, acc: 0.4, recall: 0.4, precision: 0.398989898989899, f_beta: 0.39849624060150374
train: step: 1502, loss: 0.6913859248161316, acc: 0.6, recall: 0.6, precision: 0.6041666666666667, f_beta: 0.595959595959596
train: step: 1503, loss: 0.6921704411506653, acc: 0.625, recall: 0.625, precision: 0.6278772378516624, f_beta: 0.6228786926461345
train: step: 1504, loss: 0.6915062665939331, acc: 0.6, recall: 0.6000000000000001, precision: 0.6190476190476191, f_beta: 0.5833333333333333
train: step: 1505, loss: 0.6945606470108032, acc: 0.475, recall: 0.475, precision: 0.47333333333333333, f_beta: 0.4666666666666666
train: step: 1506, loss: 0.6951199769973755, acc: 0.4, recall: 0.4, precision: 0.38095238095238093, f_beta: 0.375
train: step: 1507, loss: 0.690199077129364, acc: 0.55, recall: 0.55, precision: 0.5505050505050505, f_beta: 0.5488721804511277
train: step: 1508, loss: 0.6949609518051147, acc: 0.375, recall: 0.375, precision: 0.28354978354978355, f_beta: 0.3011879804332634
train: step: 1509, loss: 0.6933759450912476, acc: 0.525, recall: 0.525, precision: 0.5266666666666666, f_beta: 0.5174603174603175
train: step: 1510, loss: 0.6939879655838013, acc: 0.55, recall: 0.55, precision: 0.5505050505050505, f_beta: 0.5488721804511277
train: step: 1511, loss: 0.6906970739364624, acc: 0.525, recall: 0.525, precision: 0.525062656641604, f_beta: 0.5247029393370857
train: step: 1512, loss: 0.6938194036483765, acc: 0.425, recall: 0.42500000000000004, precision: 0.42000000000000004, f_beta: 0.4158730158730159
train: step: 1513, loss: 0.6939862966537476, acc: 0.55, recall: 0.55, precision: 0.554945054945055, f_beta: 0.5396419437340153
train: step: 1514, loss: 0.6931490898132324, acc: 0.55, recall: 0.55, precision: 0.5505050505050505, f_beta: 0.5488721804511277
train: step: 1515, loss: 0.6942300200462341, acc: 0.425, recall: 0.425, precision: 0.42327365728900257, f_beta: 0.4217473287240729
train: step: 1516, loss: 0.6923734545707703, acc: 0.5, recall: 0.5, precision: 0.5, f_beta: 0.47916666666666663
train: step: 1517, loss: 0.6897542476654053, acc: 0.5, recall: 0.5, precision: 0.5, f_beta: 0.4884910485933504
train: step: 1518, loss: 0.6954642534255981, acc: 0.425, recall: 0.425, precision: 0.3924731182795699, f_beta: 0.37795807978363755
train: step: 1519, loss: 0.6950045824050903, acc: 0.425, recall: 0.42500000000000004, precision: 0.42000000000000004, f_beta: 0.4158730158730159
train: step: 1520, loss: 0.6931924223899841, acc: 0.45, recall: 0.44999999999999996, precision: 0.43333333333333335, f_beta: 0.41333333333333333
train: step: 1521, loss: 0.6923906207084656, acc: 0.55, recall: 0.55, precision: 0.554945054945055, f_beta: 0.5396419437340153
train: step: 1522, loss: 0.6924179196357727, acc: 0.475, recall: 0.47500000000000003, precision: 0.4744245524296675, f_beta: 0.47203016970458833
train: step: 1523, loss: 0.6939261555671692, acc: 0.55, recall: 0.55, precision: 0.5666666666666667, f_beta: 0.52
train: step: 1524, loss: 0.6931861639022827, acc: 0.525, recall: 0.525, precision: 0.5266666666666666, f_beta: 0.5174603174603175
train: step: 1525, loss: 0.6946638226509094, acc: 0.525, recall: 0.525, precision: 0.5255754475703325, f_beta: 0.5223130106851037
train: step: 1526, loss: 0.6939932107925415, acc: 0.45, recall: 0.45, precision: 0.4019607843137255, f_beta: 0.37321937321937326
train: step: 1527, loss: 0.6935769319534302, acc: 0.475, recall: 0.47500000000000003, precision: 0.45670995670995673, f_beta: 0.41299790356394134
train: step: 1528, loss: 0.6809739470481873, acc: 0.625, recall: 0.625, precision: 0.7164502164502164, f_beta: 0.5807127882599581
train: step: 1529, loss: 0.6931098699569702, acc: 0.45, recall: 0.44999999999999996, precision: 0.44505494505494503, f_beta: 0.4373401534526854
train: step: 1530, loss: 0.6944136619567871, acc: 0.5, recall: 0.5, precision: 0.5, f_beta: 0.4949494949494949
train: step: 1531, loss: 0.6935478448867798, acc: 0.475, recall: 0.475, precision: 0.474937343358396, f_beta: 0.474671669793621
train: step: 1532, loss: 0.6947375535964966, acc: 0.375, recall: 0.375, precision: 0.34326018808777425, f_beta: 0.34167215273206053
train: step: 1533, loss: 0.6917277574539185, acc: 0.55, recall: 0.55, precision: 0.5980392156862745, f_beta: 0.4871794871794872
train: step: 1534, loss: 0.6926536560058594, acc: 0.55, recall: 0.55, precision: 0.5980392156862745, f_beta: 0.4871794871794872
train: step: 1535, loss: 0.6917357444763184, acc: 0.5, recall: 0.5, precision: 0.5, f_beta: 0.3730407523510972
train: step: 1536, loss: 0.6927057504653931, acc: 0.55, recall: 0.55, precision: 0.5520833333333333, f_beta: 0.5454545454545454
train: step: 1537, loss: 0.6933211088180542, acc: 0.475, recall: 0.475, precision: 0.47333333333333333, f_beta: 0.4666666666666666
train: step: 1538, loss: 0.6895475387573242, acc: 0.5, recall: 0.5, precision: 0.5, f_beta: 0.4884910485933504
train: step: 1539, loss: 0.6939713954925537, acc: 0.525, recall: 0.525, precision: 0.5900900900900901, f_beta: 0.4202898550724638
train: step: 1540, loss: 0.6932158470153809, acc: 0.425, recall: 0.425, precision: 0.22972972972972974, f_beta: 0.2982456140350877
train: step: 1541, loss: 0.6917261481285095, acc: 0.575, recall: 0.575, precision: 0.5940438871473355, f_beta: 0.5523370638578011
train: step: 1542, loss: 0.6929160952568054, acc: 0.525, recall: 0.525, precision: 0.5313479623824451, f_beta: 0.4996708360763661
train: step: 1543, loss: 0.6927509307861328, acc: 0.525, recall: 0.525, precision: 0.5266666666666666, f_beta: 0.5174603174603175
train: step: 1544, loss: 0.6917441487312317, acc: 0.5, recall: 0.5, precision: 0.5, f_beta: 0.47916666666666663
train: step: 1545, loss: 0.6871402263641357, acc: 0.55, recall: 0.55, precision: 0.5595238095238095, f_beta: 0.53125
train: step: 1546, loss: 0.6934782266616821, acc: 0.475, recall: 0.475, precision: 0.47333333333333333, f_beta: 0.4666666666666666
train: step: 1547, loss: 0.6945943832397461, acc: 0.4, recall: 0.4, precision: 0.4, f_beta: 0.4000000000000001
train: step: 1548, loss: 0.6766231656074524, acc: 0.625, recall: 0.625, precision: 0.7857142857142857, f_beta: 0.5636363636363637
train: step: 1549, loss: 0.6970334053039551, acc: 0.5, recall: 0.5, precision: 0.5, f_beta: 0.47916666666666663
train: step: 1550, loss: 0.6930515170097351, acc: 0.5, recall: 0.5, precision: 0.5, f_beta: 0.3730407523510972
train: step: 1551, loss: 0.6931084990501404, acc: 0.575, recall: 0.575, precision: 0.575187969924812, f_beta: 0.5747342088805503
train: step: 1552, loss: 0.6929359436035156, acc: 0.45, recall: 0.45, precision: 0.44047619047619047, f_beta: 0.42708333333333337
train: step: 1553, loss: 0.6965571641921997, acc: 0.4, recall: 0.4, precision: 0.3901098901098901, f_beta: 0.38618925831202044
train: step: 1554, loss: 0.693509042263031, acc: 0.475, recall: 0.475, precision: 0.474937343358396, f_beta: 0.474671669793621
train: step: 1555, loss: 0.6918001174926758, acc: 0.5, recall: 0.5, precision: 0.5, f_beta: 0.4884910485933504
train: step: 1556, loss: 0.693311333656311, acc: 0.525, recall: 0.525, precision: 0.5255754475703325, f_beta: 0.5223130106851037
train: step: 1557, loss: 0.6931912302970886, acc: 0.5, recall: 0.5, precision: 0.5, f_beta: 0.4987468671679198
train: step: 1558, loss: 0.6976513266563416, acc: 0.375, recall: 0.375, precision: 0.3575498575498576, f_beta: 0.3552546744036106
train: step: 1559, loss: 0.6883773803710938, acc: 0.55, recall: 0.55, precision: 0.5595238095238095, f_beta: 0.53125
train: step: 1560, loss: 0.6943855285644531, acc: 0.5, recall: 0.5, precision: 0.5, f_beta: 0.4505494505494506
train: step: 1561, loss: 0.6932332515716553, acc: 0.525, recall: 0.525, precision: 0.5266666666666666, f_beta: 0.5174603174603175
train: step: 1562, loss: 0.6933010220527649, acc: 0.625, recall: 0.625, precision: 0.6424501424501424, f_beta: 0.6131528046421664
train: step: 1563, loss: 0.6925588846206665, acc: 0.575, recall: 0.575, precision: 0.5940438871473355, f_beta: 0.5523370638578011
train: step: 1564, loss: 0.6923293471336365, acc: 0.55, recall: 0.55, precision: 0.5595238095238095, f_beta: 0.53125
train: step: 1565, loss: 0.6923412084579468, acc: 0.575, recall: 0.575, precision: 0.6298701298701299, f_beta: 0.5248078266946192
train: step: 1566, loss: 0.6848435401916504, acc: 0.625, recall: 0.625, precision: 0.7857142857142857, f_beta: 0.5636363636363637
train: step: 1567, loss: 0.6939055323600769, acc: 0.425, recall: 0.42500000000000004, precision: 0.424812030075188, f_beta: 0.4246404002501563
train: step: 1568, loss: 0.691186249256134, acc: 0.6, recall: 0.6, precision: 0.65625, f_beta: 0.5604395604395604
train: step: 1569, loss: 0.6946014165878296, acc: 0.45, recall: 0.44999999999999996, precision: 0.43333333333333335, f_beta: 0.41333333333333333
train: step: 1570, loss: 0.6945158243179321, acc: 0.425, recall: 0.42500000000000004, precision: 0.424812030075188, f_beta: 0.4246404002501563
train: step: 1571, loss: 0.6946298480033875, acc: 0.45, recall: 0.45, precision: 0.421875, f_beta: 0.39560439560439553
train: step: 1572, loss: 0.6925799250602722, acc: 0.475, recall: 0.475, precision: 0.47150997150997154, f_beta: 0.4584139264990329
train: step: 1573, loss: 0.6914786100387573, acc: 0.575, recall: 0.575, precision: 0.5800000000000001, f_beta: 0.5682539682539682
train: step: 1574, loss: 0.6909874081611633, acc: 0.575, recall: 0.575, precision: 0.5854700854700855, f_beta: 0.5615731785944552
train: step: 1575, loss: 0.6930232048034668, acc: 0.575, recall: 0.575, precision: 0.575187969924812, f_beta: 0.5747342088805503
train: step: 1576, loss: 0.6961369514465332, acc: 0.5, recall: 0.5, precision: 0.5, f_beta: 0.4949494949494949
train: step: 1577, loss: 0.6905752420425415, acc: 0.625, recall: 0.625, precision: 0.6567398119122256, f_beta: 0.6050032916392363
train: step: 1578, loss: 0.6940909624099731, acc: 0.45, recall: 0.44999999999999996, precision: 0.43333333333333335, f_beta: 0.41333333333333333
train: step: 1579, loss: 0.6843987703323364, acc: 0.65, recall: 0.6499999999999999, precision: 0.734375, f_beta: 0.6153846153846154
train: step: 1580, loss: 0.6924859881401062, acc: 0.5, recall: 0.5, precision: 0.5, f_beta: 0.43019943019943013
train: step: 1581, loss: 0.6934735178947449, acc: 0.45, recall: 0.45, precision: 0.421875, f_beta: 0.39560439560439553
train: step: 1582, loss: 0.6926954984664917, acc: 0.475, recall: 0.47500000000000003, precision: 0.45670995670995673, f_beta: 0.41299790356394134
train: step: 1583, loss: 0.6932281255722046, acc: 0.5, recall: 0.5, precision: 0.5, f_beta: 0.4987468671679198
train: step: 1584, loss: 0.6950796842575073, acc: 0.45, recall: 0.45, precision: 0.44047619047619047, f_beta: 0.42708333333333337
train: step: 1585, loss: 0.689784049987793, acc: 0.525, recall: 0.525, precision: 0.7564102564102564, f_beta: 0.38660209846650523
train: step: 1586, loss: 0.6926113963127136, acc: 0.525, recall: 0.525, precision: 0.5255754475703325, f_beta: 0.5223130106851037
train: step: 1587, loss: 0.693861186504364, acc: 0.525, recall: 0.525, precision: 0.53584229390681, f_beta: 0.4861392832995267
train: step: 1588, loss: 0.6954191327095032, acc: 0.525, recall: 0.525, precision: 0.53584229390681, f_beta: 0.4861392832995267
train: step: 1589, loss: 0.6970510482788086, acc: 0.5, recall: 0.5, precision: 0.25, f_beta: 0.3333333333333333
train: step: 1590, loss: 0.6930490732192993, acc: 0.525, recall: 0.525, precision: 0.5266666666666666, f_beta: 0.5174603174603175
train: step: 1591, loss: 0.6936763525009155, acc: 0.525, recall: 0.525, precision: 0.525062656641604, f_beta: 0.5247029393370857
train: step: 1592, loss: 0.6938658952713013, acc: 0.425, recall: 0.42500000000000004, precision: 0.424812030075188, f_beta: 0.4246404002501563
train: step: 1593, loss: 0.6964956521987915, acc: 0.45, recall: 0.45, precision: 0.3611111111111111, f_beta: 0.34523809523809523
train: step: 1594, loss: 0.6940314173698425, acc: 0.425, recall: 0.425, precision: 0.4145299145299145, f_beta: 0.40683430045132174
train: step: 1595, loss: 0.692866861820221, acc: 0.525, recall: 0.525, precision: 0.5313479623824451, f_beta: 0.4996708360763661
train: step: 1596, loss: 0.6971050500869751, acc: 0.5, recall: 0.5, precision: 0.5, f_beta: 0.4949494949494949
train: step: 1597, loss: 0.6929279565811157, acc: 0.35, recall: 0.35, precision: 0.3351648351648352, f_beta: 0.3350383631713555
train: step: 1598, loss: 0.693322479724884, acc: 0.525, recall: 0.525, precision: 0.5255754475703325, f_beta: 0.5223130106851037
train: step: 1599, loss: 0.6867307424545288, acc: 0.575, recall: 0.575, precision: 0.5940438871473355, f_beta: 0.5523370638578011
train: step: 1600, loss: 0.6922381520271301, acc: 0.525, recall: 0.525, precision: 0.5266666666666666, f_beta: 0.5174603174603175
train: step: 1601, loss: 0.6937378644943237, acc: 0.425, recall: 0.42500000000000004, precision: 0.424812030075188, f_beta: 0.4246404002501563
train: step: 1602, loss: 0.6957704424858093, acc: 0.35, recall: 0.35, precision: 0.3351648351648352, f_beta: 0.3350383631713555
train: step: 1603, loss: 0.6934593915939331, acc: 0.5, recall: 0.5, precision: 0.5, f_beta: 0.4987468671679198
train: step: 1604, loss: 0.6928378343582153, acc: 0.5, recall: 0.5, precision: 0.5, f_beta: 0.4949494949494949
train: step: 1605, loss: 0.6972366571426392, acc: 0.475, recall: 0.47500000000000003, precision: 0.40990990990990994, f_beta: 0.35926773455377575
train: step: 1606, loss: 0.6928403973579407, acc: 0.45, recall: 0.44999999999999996, precision: 0.43333333333333335, f_beta: 0.41333333333333333
train: step: 1607, loss: 0.6936752200126648, acc: 0.45, recall: 0.44999999999999996, precision: 0.44505494505494503, f_beta: 0.4373401534526854
train: step: 1608, loss: 0.6932355761528015, acc: 0.525, recall: 0.5249999999999999, precision: 0.5284900284900285, f_beta: 0.5099935525467441
train: step: 1609, loss: 0.6941791772842407, acc: 0.475, recall: 0.475, precision: 0.44285714285714284, f_beta: 0.38909090909090915
train: step: 1610, loss: 0.6924174427986145, acc: 0.525, recall: 0.525, precision: 0.5571428571428572, f_beta: 0.44727272727272727
train: step: 1611, loss: 0.6951907873153687, acc: 0.5, recall: 0.5, precision: 0.5, f_beta: 0.40476190476190477
train: step: 1612, loss: 0.6899056434631348, acc: 0.55, recall: 0.5499999999999999, precision: 0.6388888888888888, f_beta: 0.46428571428571425
train: step: 1613, loss: 0.6937919855117798, acc: 0.375, recall: 0.375, precision: 0.37468671679197996, f_beta: 0.37460913070669166
train: step: 1614, loss: 0.6896907091140747, acc: 0.475, recall: 0.475, precision: 0.47333333333333333, f_beta: 0.4666666666666666
train: step: 1615, loss: 0.6956838369369507, acc: 0.45, recall: 0.44999999999999996, precision: 0.43333333333333335, f_beta: 0.41333333333333333
train: step: 1616, loss: 0.6934469938278198, acc: 0.525, recall: 0.525, precision: 0.5571428571428572, f_beta: 0.44727272727272727
train: step: 1617, loss: 0.6917203068733215, acc: 0.6, recall: 0.6000000000000001, precision: 0.601010101010101, f_beta: 0.5989974937343359
train: step: 1618, loss: 0.6658119559288025, acc: 0.65, recall: 0.65, precision: 0.7941176470588236, f_beta: 0.6011396011396011
train: step: 1619, loss: 0.6980727314949036, acc: 0.475, recall: 0.475, precision: 0.44285714285714284, f_beta: 0.38909090909090915
train: step: 1620, loss: 0.6955941319465637, acc: 0.425, recall: 0.42500000000000004, precision: 0.32857142857142857, f_beta: 0.33090909090909093
train: step: 1621, loss: 0.6928824782371521, acc: 0.5, recall: 0.5, precision: 0.5, f_beta: 0.4987468671679198
train: step: 1622, loss: 0.6933086514472961, acc: 0.45, recall: 0.45, precision: 0.421875, f_beta: 0.39560439560439553
train: step: 1623, loss: 0.6955932378768921, acc: 0.475, recall: 0.47500000000000003, precision: 0.40990990990990994, f_beta: 0.35926773455377575
train: step: 1624, loss: 0.6914603114128113, acc: 0.575, recall: 0.575, precision: 0.60752688172043, f_beta: 0.5402298850574712
train: step: 1625, loss: 0.6915777325630188, acc: 0.55, recall: 0.55, precision: 0.5595238095238095, f_beta: 0.53125
train: step: 1626, loss: 0.6931165456771851, acc: 0.525, recall: 0.525, precision: 0.5266666666666666, f_beta: 0.5174603174603175
train: step: 1627, loss: 0.6937064528465271, acc: 0.45, recall: 0.45, precision: 0.45, f_beta: 0.45
train: step: 1628, loss: 0.6933476328849792, acc: 0.475, recall: 0.47500000000000003, precision: 0.4744245524296675, f_beta: 0.47203016970458833
train: step: 1629, loss: 0.6937057375907898, acc: 0.4, recall: 0.4, precision: 0.39583333333333337, f_beta: 0.3939393939393939
train: step: 1630, loss: 0.6926971673965454, acc: 0.575, recall: 0.575, precision: 0.5854700854700855, f_beta: 0.5615731785944552
train: step: 1631, loss: 0.6942999958992004, acc: 0.475, recall: 0.475, precision: 0.474937343358396, f_beta: 0.474671669793621
train: step: 1632, loss: 0.6929198503494263, acc: 0.5, recall: 0.5, precision: 0.5, f_beta: 0.4949494949494949
train: step: 1633, loss: 0.6917654275894165, acc: 0.5, recall: 0.5, precision: 0.5, f_beta: 0.4666666666666667
train: step: 1634, loss: 0.6946563720703125, acc: 0.5, recall: 0.5, precision: 0.25, f_beta: 0.3333333333333333
train: step: 1635, loss: 0.6930059194564819, acc: 0.5, recall: 0.5, precision: 0.5, f_beta: 0.4949494949494949
train: step: 1636, loss: 0.6935083270072937, acc: 0.45, recall: 0.45, precision: 0.4494949494949495, f_beta: 0.4486215538847118
train: step: 1637, loss: 0.6939083933830261, acc: 0.45, recall: 0.45, precision: 0.4494949494949495, f_beta: 0.4486215538847118
train: step: 1638, loss: 0.6973153352737427, acc: 0.475, recall: 0.47500000000000003, precision: 0.45670995670995673, f_beta: 0.41299790356394134
train: step: 1639, loss: 0.6914017200469971, acc: 0.575, recall: 0.575, precision: 0.60752688172043, f_beta: 0.5402298850574712
train: step: 1640, loss: 0.6961542367935181, acc: 0.45, recall: 0.44999999999999996, precision: 0.43333333333333335, f_beta: 0.41333333333333333
train: step: 1641, loss: 0.7175500988960266, acc: 0.325, recall: 0.325, precision: 0.19696969696969696, f_beta: 0.2452830188679245
train: step: 1642, loss: 0.6916003227233887, acc: 0.6, recall: 0.6, precision: 0.6333333333333333, f_beta: 0.5733333333333334
train: step: 1643, loss: 0.68939608335495, acc: 0.625, recall: 0.625, precision: 0.6278772378516624, f_beta: 0.6228786926461345
train: step: 1644, loss: 0.693475067615509, acc: 0.45, recall: 0.44999999999999996, precision: 0.44505494505494503, f_beta: 0.4373401534526854
train: step: 1645, loss: 0.6929982900619507, acc: 0.6, recall: 0.6, precision: 0.6, f_beta: 0.6
train: step: 1646, loss: 0.6924406886100769, acc: 0.5, recall: 0.5, precision: 0.5, f_beta: 0.4987468671679198
train: step: 1647, loss: 0.6927216649055481, acc: 0.475, recall: 0.475, precision: 0.46865203761755486, f_beta: 0.4470046082949308
train: step: 1648, loss: 0.6911695599555969, acc: 0.425, recall: 0.42500000000000004, precision: 0.4059561128526646, f_beta: 0.39433838051349573
train: step: 1649, loss: 0.6921099424362183, acc: 0.625, recall: 0.625, precision: 0.6278772378516624, f_beta: 0.6228786926461345
train: step: 1650, loss: 0.6946529150009155, acc: 0.5, recall: 0.5, precision: 0.5, f_beta: 0.40476190476190477
train: step: 1651, loss: 0.6954582929611206, acc: 0.375, recall: 0.375, precision: 0.34326018808777425, f_beta: 0.34167215273206053
train: step: 1652, loss: 0.6940978169441223, acc: 0.45, recall: 0.45, precision: 0.4494949494949495, f_beta: 0.4486215538847118
train: step: 1653, loss: 0.6908900141716003, acc: 0.475, recall: 0.475, precision: 0.46415770609318996, f_beta: 0.43204868154158216
train: step: 1654, loss: 0.6944409608840942, acc: 0.375, recall: 0.375, precision: 0.3666666666666667, f_beta: 0.3650793650793651
train: step: 1655, loss: 0.693738579750061, acc: 0.55, recall: 0.55, precision: 0.5666666666666667, f_beta: 0.52
train: step: 1656, loss: 0.6895896792411804, acc: 0.55, recall: 0.55, precision: 0.763157894736842, f_beta: 0.4357366771159875
train: step: 1657, loss: 0.6930112838745117, acc: 0.475, recall: 0.47500000000000003, precision: 0.4744245524296675, f_beta: 0.47203016970458833
train: step: 1658, loss: 0.6940903663635254, acc: 0.475, recall: 0.475, precision: 0.46415770609318996, f_beta: 0.43204868154158216
train: step: 1659, loss: 0.69133061170578, acc: 0.575, recall: 0.575, precision: 0.5767263427109974, f_beta: 0.5725958516656191
train: step: 1660, loss: 0.6947113871574402, acc: 0.5, recall: 0.5, precision: 0.5, f_beta: 0.47916666666666663
train: step: 1661, loss: 0.6884708404541016, acc: 0.55, recall: 0.55, precision: 0.5666666666666667, f_beta: 0.52
train: step: 1662, loss: 0.6925069093704224, acc: 0.65, recall: 0.6499999999999999, precision: 0.6515151515151515, f_beta: 0.6491228070175439
train: step: 1663, loss: 0.6917098760604858, acc: 0.425, recall: 0.42500000000000004, precision: 0.4059561128526646, f_beta: 0.39433838051349573
train: step: 1664, loss: 0.6929956674575806, acc: 0.5, recall: 0.5, precision: 0.5, f_beta: 0.4987468671679198
train: step: 1665, loss: 0.6939043998718262, acc: 0.475, recall: 0.475, precision: 0.47333333333333333, f_beta: 0.4666666666666666
train: step: 1666, loss: 0.6930167078971863, acc: 0.55, recall: 0.55, precision: 0.5505050505050505, f_beta: 0.5488721804511277
train: step: 1667, loss: 0.6916930675506592, acc: 0.525, recall: 0.525, precision: 0.5313479623824451, f_beta: 0.4996708360763661
train: step: 1668, loss: 0.6921305656433105, acc: 0.55, recall: 0.55, precision: 0.554945054945055, f_beta: 0.5396419437340153
train: step: 1669, loss: 0.6917278170585632, acc: 0.575, recall: 0.575, precision: 0.6298701298701299, f_beta: 0.5248078266946192
train: step: 1670, loss: 0.6931933164596558, acc: 0.525, recall: 0.525, precision: 0.5255754475703325, f_beta: 0.5223130106851037
train: step: 1671, loss: 0.693069338798523, acc: 0.5, recall: 0.5, precision: 0.5, f_beta: 0.5
train: step: 1672, loss: 0.6952505111694336, acc: 0.45, recall: 0.44999999999999996, precision: 0.44505494505494503, f_beta: 0.4373401534526854
train: step: 1673, loss: 0.6926807165145874, acc: 0.5, recall: 0.5, precision: 0.5, f_beta: 0.4949494949494949
train: step: 1674, loss: 0.6921560168266296, acc: 0.575, recall: 0.575, precision: 0.5854700854700855, f_beta: 0.5615731785944552
train: step: 1675, loss: 0.6923013925552368, acc: 0.6, recall: 0.6000000000000001, precision: 0.601010101010101, f_beta: 0.5989974937343359
train: step: 1676, loss: 0.6945034861564636, acc: 0.425, recall: 0.425, precision: 0.42327365728900257, f_beta: 0.4217473287240729
train: step: 1677, loss: 0.6941002011299133, acc: 0.525, recall: 0.5249999999999999, precision: 0.5284900284900285, f_beta: 0.5099935525467441
train: step: 1678, loss: 0.6940154433250427, acc: 0.45, recall: 0.45, precision: 0.44047619047619047, f_beta: 0.42708333333333337
train: step: 1679, loss: 0.6968998312950134, acc: 0.45, recall: 0.45, precision: 0.3611111111111111, f_beta: 0.34523809523809523
train: step: 1680, loss: 0.6949336528778076, acc: 0.5, recall: 0.5, precision: 0.5, f_beta: 0.4505494505494506
train: step: 1681, loss: 0.6905795931816101, acc: 0.625, recall: 0.625, precision: 0.6333333333333333, f_beta: 0.6190476190476191
train: step: 1682, loss: 0.691857635974884, acc: 0.55, recall: 0.55, precision: 0.5980392156862745, f_beta: 0.4871794871794872
train: step: 1683, loss: 0.693581759929657, acc: 0.5, recall: 0.5, precision: 0.5, f_beta: 0.47916666666666663
train: step: 1684, loss: 0.6943284273147583, acc: 0.475, recall: 0.47500000000000003, precision: 0.4744245524296675, f_beta: 0.47203016970458833
train: step: 1685, loss: 0.6935743093490601, acc: 0.525, recall: 0.525, precision: 0.525062656641604, f_beta: 0.5247029393370857
train: step: 1686, loss: 0.6956833600997925, acc: 0.425, recall: 0.425, precision: 0.3701298701298701, f_beta: 0.35709294199860236
train: step: 1687, loss: 0.6917157173156738, acc: 0.575, recall: 0.575, precision: 0.575187969924812, f_beta: 0.5747342088805503
train: step: 1688, loss: 0.690486490726471, acc: 0.55, recall: 0.55, precision: 0.5980392156862745, f_beta: 0.4871794871794872
train: step: 1689, loss: 0.6913601756095886, acc: 0.65, recall: 0.65, precision: 0.65, f_beta: 0.65
train: step: 1690, loss: 0.69379723072052, acc: 0.425, recall: 0.425, precision: 0.42327365728900257, f_beta: 0.4217473287240729
train: step: 1691, loss: 0.6919554471969604, acc: 0.55, recall: 0.55, precision: 0.554945054945055, f_beta: 0.5396419437340153
train: step: 1692, loss: 0.6896756291389465, acc: 0.6, recall: 0.6, precision: 0.696078431372549, f_beta: 0.5441595441595442
train: step: 1693, loss: 0.6939605474472046, acc: 0.45, recall: 0.45, precision: 0.45, f_beta: 0.45
train: step: 1694, loss: 0.6890151500701904, acc: 0.6, recall: 0.6, precision: 0.6333333333333333, f_beta: 0.5733333333333334
train: step: 1695, loss: 0.6925050020217896, acc: 0.6, recall: 0.6, precision: 0.6, f_beta: 0.6
train: step: 1696, loss: 0.6949650645256042, acc: 0.325, recall: 0.325, precision: 0.24910394265232974, f_beta: 0.2697768762677485
train: step: 1697, loss: 0.6939141750335693, acc: 0.425, recall: 0.42500000000000004, precision: 0.42000000000000004, f_beta: 0.4158730158730159
train: step: 1698, loss: 0.6936672329902649, acc: 0.325, recall: 0.32499999999999996, precision: 0.32456140350877194, f_beta: 0.324577861163227
train: step: 1699, loss: 0.6941420435905457, acc: 0.475, recall: 0.47500000000000003, precision: 0.4744245524296675, f_beta: 0.47203016970458833
train: step: 1700, loss: 0.6936472654342651, acc: 0.45, recall: 0.45, precision: 0.44047619047619047, f_beta: 0.42708333333333337
train: step: 1701, loss: 0.693773090839386, acc: 0.475, recall: 0.475, precision: 0.474937343358396, f_beta: 0.474671669793621
train: step: 1702, loss: 0.6961244344711304, acc: 0.475, recall: 0.47500000000000003, precision: 0.40990990990990994, f_beta: 0.35926773455377575
train: step: 1703, loss: 0.6982512474060059, acc: 0.475, recall: 0.475, precision: 0.46865203761755486, f_beta: 0.4470046082949308
train: step: 1704, loss: 0.6898437738418579, acc: 0.55, recall: 0.55, precision: 0.763157894736842, f_beta: 0.4357366771159875
train: step: 1705, loss: 0.6922893524169922, acc: 0.55, recall: 0.55, precision: 0.5505050505050505, f_beta: 0.5488721804511277
train: step: 1706, loss: 0.6923542022705078, acc: 0.575, recall: 0.575, precision: 0.6298701298701299, f_beta: 0.5248078266946192
train: step: 1707, loss: 0.6936744451522827, acc: 0.5, recall: 0.5, precision: 0.5, f_beta: 0.4987468671679198
train: step: 1708, loss: 0.6939775943756104, acc: 0.475, recall: 0.475, precision: 0.47333333333333333, f_beta: 0.4666666666666666
train: step: 1709, loss: 0.6929270625114441, acc: 0.525, recall: 0.525, precision: 0.5266666666666666, f_beta: 0.5174603174603175
train: step: 1710, loss: 0.691441535949707, acc: 0.475, recall: 0.475, precision: 0.44285714285714284, f_beta: 0.38909090909090915
train: step: 1711, loss: 0.6928139925003052, acc: 0.45, recall: 0.45, precision: 0.45, f_beta: 0.45
train: step: 1712, loss: 0.6946743726730347, acc: 0.475, recall: 0.475, precision: 0.474937343358396, f_beta: 0.474671669793621
train: step: 1713, loss: 0.6991381645202637, acc: 0.375, recall: 0.375, precision: 0.3207885304659498, f_beta: 0.32386747802569305
train: step: 1714, loss: 0.6923721432685852, acc: 0.55, recall: 0.55, precision: 0.554945054945055, f_beta: 0.5396419437340153
train: step: 1715, loss: 0.6949132084846497, acc: 0.35, recall: 0.35000000000000003, precision: 0.3214285714285714, f_beta: 0.3229166666666667
train: step: 1716, loss: 0.6923447847366333, acc: 0.5, recall: 0.5, precision: 0.5, f_beta: 0.4987468671679198
train: step: 1717, loss: 0.6911503076553345, acc: 0.525, recall: 0.525, precision: 0.5313479623824451, f_beta: 0.4996708360763661
train: step: 1718, loss: 0.6936190724372864, acc: 0.5, recall: 0.5, precision: 0.5, f_beta: 0.47916666666666663
train: step: 1719, loss: 0.6937035322189331, acc: 0.45, recall: 0.45, precision: 0.4494949494949495, f_beta: 0.4486215538847118
train: step: 1720, loss: 0.6929980516433716, acc: 0.5, recall: 0.5, precision: 0.5, f_beta: 0.47916666666666663
train: step: 1721, loss: 0.6921096444129944, acc: 0.625, recall: 0.625, precision: 0.62531328320802, f_beta: 0.6247654784240151
train: step: 1722, loss: 0.6947404742240906, acc: 0.475, recall: 0.475, precision: 0.46865203761755486, f_beta: 0.4470046082949308
train: step: 1723, loss: 0.69366854429245, acc: 0.45, recall: 0.45, precision: 0.44791666666666663, f_beta: 0.4444444444444444
train: step: 1724, loss: 0.692069947719574, acc: 0.575, recall: 0.575, precision: 0.5767263427109974, f_beta: 0.5725958516656191
train: step: 1725, loss: 0.6957730054855347, acc: 0.425, recall: 0.425, precision: 0.4145299145299145, f_beta: 0.40683430045132174
train: step: 1726, loss: 0.6911629438400269, acc: 0.525, recall: 0.525, precision: 0.5571428571428572, f_beta: 0.44727272727272727
train: step: 1727, loss: 0.6937568187713623, acc: 0.35, recall: 0.35, precision: 0.34375, f_beta: 0.3434343434343434
train: step: 1728, loss: 0.6934173703193665, acc: 0.475, recall: 0.47500000000000003, precision: 0.45670995670995673, f_beta: 0.41299790356394134
train: step: 1729, loss: 0.6932559013366699, acc: 0.475, recall: 0.475, precision: 0.474937343358396, f_beta: 0.474671669793621
train: step: 1730, loss: 0.6945504546165466, acc: 0.35, recall: 0.35, precision: 0.34375, f_beta: 0.3434343434343434
train: step: 1731, loss: 0.6918296217918396, acc: 0.65, recall: 0.65, precision: 0.6785714285714286, f_beta: 0.6354166666666667
train: step: 1732, loss: 0.6937508583068848, acc: 0.5, recall: 0.5, precision: 0.5, f_beta: 0.4884910485933504
train: step: 1733, loss: 0.6936244964599609, acc: 0.5, recall: 0.5, precision: 0.5, f_beta: 0.4987468671679198
train: step: 1734, loss: 0.6924924850463867, acc: 0.55, recall: 0.55, precision: 0.55, f_beta: 0.55
train: step: 1735, loss: 0.6935359835624695, acc: 0.575, recall: 0.575, precision: 0.5767263427109974, f_beta: 0.5725958516656191
train: step: 1736, loss: 0.6943026781082153, acc: 0.525, recall: 0.525, precision: 0.5266666666666666, f_beta: 0.5174603174603175
train: step: 1737, loss: 0.6936742067337036, acc: 0.375, recall: 0.375, precision: 0.37468671679197996, f_beta: 0.37460913070669166
train: step: 1738, loss: 0.6961044073104858, acc: 0.35, recall: 0.35, precision: 0.34375, f_beta: 0.3434343434343434
train: step: 1739, loss: 0.6939190626144409, acc: 0.475, recall: 0.47500000000000003, precision: 0.45670995670995673, f_beta: 0.41299790356394134
train: step: 1740, loss: 0.6936365962028503, acc: 0.5, recall: 0.5, precision: 0.5, f_beta: 0.4884910485933504
train: step: 1741, loss: 0.6923900842666626, acc: 0.55, recall: 0.55, precision: 0.5666666666666667, f_beta: 0.52
train: step: 1742, loss: 0.6937360167503357, acc: 0.475, recall: 0.47500000000000003, precision: 0.4744245524296675, f_beta: 0.47203016970458833
train: step: 1743, loss: 0.6934013366699219, acc: 0.45, recall: 0.44999999999999996, precision: 0.44505494505494503, f_beta: 0.4373401534526854
train: step: 1744, loss: 0.695770263671875, acc: 0.425, recall: 0.42500000000000004, precision: 0.4059561128526646, f_beta: 0.39433838051349573
train: step: 1745, loss: 0.6924539804458618, acc: 0.65, recall: 0.65, precision: 0.65, f_beta: 0.65
train: step: 1746, loss: 0.6944901943206787, acc: 0.425, recall: 0.425, precision: 0.4145299145299145, f_beta: 0.40683430045132174
train: step: 1747, loss: 0.6911373734474182, acc: 0.55, recall: 0.55, precision: 0.578125, f_beta: 0.5054945054945055
train: step: 1748, loss: 0.6955865025520325, acc: 0.425, recall: 0.425, precision: 0.42327365728900257, f_beta: 0.4217473287240729
train: step: 1749, loss: 0.6944330930709839, acc: 0.425, recall: 0.42500000000000004, precision: 0.424812030075188, f_beta: 0.4246404002501563
train: step: 1750, loss: 0.6926125288009644, acc: 0.575, recall: 0.575, precision: 0.5800000000000001, f_beta: 0.5682539682539682
train: step: 1751, loss: 0.6933717131614685, acc: 0.5, recall: 0.5, precision: 0.5, f_beta: 0.4949494949494949
train: step: 1752, loss: 0.6943591237068176, acc: 0.425, recall: 0.425, precision: 0.3701298701298701, f_beta: 0.35709294199860236
train: step: 1753, loss: 0.6903413534164429, acc: 0.6, recall: 0.6, precision: 0.65625, f_beta: 0.5604395604395604
train: step: 1754, loss: 0.692219078540802, acc: 0.5, recall: 0.5, precision: 0.5, f_beta: 0.4666666666666667
train: step: 1755, loss: 0.6916487812995911, acc: 0.525, recall: 0.525, precision: 0.5255754475703325, f_beta: 0.5223130106851037
train: step: 1756, loss: 0.692973792552948, acc: 0.55, recall: 0.55, precision: 0.5595238095238095, f_beta: 0.53125
train: step: 1757, loss: 0.6957893371582031, acc: 0.35, recall: 0.35, precision: 0.35, f_beta: 0.35
train: step: 1758, loss: 0.6970646977424622, acc: 0.4, recall: 0.4, precision: 0.3666666666666667, f_beta: 0.36
train: step: 1759, loss: 0.6925934553146362, acc: 0.45, recall: 0.45, precision: 0.45, f_beta: 0.45
train: step: 1760, loss: 0.6922715902328491, acc: 0.525, recall: 0.5249999999999999, precision: 0.5284900284900285, f_beta: 0.5099935525467441
train: step: 1761, loss: 0.6936984062194824, acc: 0.425, recall: 0.42500000000000004, precision: 0.42000000000000004, f_beta: 0.4158730158730159
train: step: 1762, loss: 0.694896399974823, acc: 0.575, recall: 0.575, precision: 0.5767263427109974, f_beta: 0.5725958516656191
train: step: 1763, loss: 0.694124162197113, acc: 0.55, recall: 0.55, precision: 0.578125, f_beta: 0.5054945054945055
train: step: 1764, loss: 0.6972596049308777, acc: 0.45, recall: 0.44999999999999996, precision: 0.44505494505494503, f_beta: 0.4373401534526854
train: step: 1765, loss: 0.6918684244155884, acc: 0.675, recall: 0.675, precision: 0.6754385964912281, f_beta: 0.6747967479674797
train: step: 1766, loss: 0.6929675340652466, acc: 0.525, recall: 0.5249999999999999, precision: 0.5284900284900285, f_beta: 0.5099935525467441
train: step: 1767, loss: 0.6917427182197571, acc: 0.675, recall: 0.675, precision: 0.6866666666666666, f_beta: 0.6698412698412699
train: step: 1768, loss: 0.6864489912986755, acc: 0.625, recall: 0.625, precision: 0.6792114695340502, f_beta: 0.5943204868154157
train: step: 1769, loss: 0.694019615650177, acc: 0.5, recall: 0.5, precision: 0.5, f_beta: 0.43019943019943013
train: step: 1770, loss: 0.6965368390083313, acc: 0.425, recall: 0.42500000000000004, precision: 0.4059561128526646, f_beta: 0.39433838051349573
train: step: 1771, loss: 0.6953262090682983, acc: 0.525, recall: 0.525, precision: 0.5266666666666666, f_beta: 0.5174603174603175
train: step: 1772, loss: 0.6956024169921875, acc: 0.475, recall: 0.475, precision: 0.24358974358974358, f_beta: 0.3220338983050847
train: step: 1773, loss: 0.6914902925491333, acc: 0.525, recall: 0.525, precision: 0.7564102564102564, f_beta: 0.38660209846650523
train: step: 1774, loss: 0.6898192167282104, acc: 0.55, recall: 0.55, precision: 0.578125, f_beta: 0.5054945054945055
train: step: 1775, loss: 0.6953364610671997, acc: 0.375, recall: 0.375, precision: 0.3207885304659498, f_beta: 0.32386747802569305
train: step: 1776, loss: 0.6932255029678345, acc: 0.475, recall: 0.475, precision: 0.46415770609318996, f_beta: 0.43204868154158216
train: step: 1777, loss: 0.6927425861358643, acc: 0.6, recall: 0.6000000000000001, precision: 0.601010101010101, f_beta: 0.5989974937343359
train: step: 1778, loss: 0.6921406984329224, acc: 0.6, recall: 0.6, precision: 0.6098901098901099, f_beta: 0.5907928388746804
train: step: 1779, loss: 0.6925944685935974, acc: 0.55, recall: 0.55, precision: 0.55, f_beta: 0.55
train: step: 1780, loss: 0.6928671598434448, acc: 0.55, recall: 0.55, precision: 0.5595238095238095, f_beta: 0.53125
train: step: 1781, loss: 0.6937651634216309, acc: 0.425, recall: 0.42500000000000004, precision: 0.424812030075188, f_beta: 0.4246404002501563
train: step: 1782, loss: 0.6912344098091125, acc: 0.55, recall: 0.55, precision: 0.5520833333333333, f_beta: 0.5454545454545454
train: step: 1783, loss: 0.6938193440437317, acc: 0.5, recall: 0.5, precision: 0.5, f_beta: 0.4987468671679198
train: step: 1784, loss: 0.6926562190055847, acc: 0.525, recall: 0.525, precision: 0.525062656641604, f_beta: 0.5247029393370857
train: step: 1785, loss: 0.6910907030105591, acc: 0.55, recall: 0.55, precision: 0.5980392156862745, f_beta: 0.4871794871794872
train: step: 1786, loss: 0.6937211751937866, acc: 0.525, recall: 0.525, precision: 0.525062656641604, f_beta: 0.5247029393370857
train: step: 1787, loss: 0.6932626962661743, acc: 0.525, recall: 0.525, precision: 0.53584229390681, f_beta: 0.4861392832995267
train: step: 1788, loss: 0.6929180026054382, acc: 0.525, recall: 0.525, precision: 0.525062656641604, f_beta: 0.5247029393370857
train: step: 1789, loss: 0.6951245069503784, acc: 0.375, recall: 0.375, precision: 0.34326018808777425, f_beta: 0.34167215273206053
train: step: 1790, loss: 0.6923105120658875, acc: 0.525, recall: 0.525, precision: 0.525062656641604, f_beta: 0.5247029393370857
train: step: 1791, loss: 0.693744957447052, acc: 0.475, recall: 0.475, precision: 0.46865203761755486, f_beta: 0.4470046082949308
train: step: 1792, loss: 0.6943726539611816, acc: 0.45, recall: 0.44999999999999996, precision: 0.43333333333333335, f_beta: 0.41333333333333333
train: step: 1793, loss: 0.6914457082748413, acc: 0.45, recall: 0.44999999999999996, precision: 0.44505494505494503, f_beta: 0.4373401534526854
train: step: 1794, loss: 0.692909836769104, acc: 0.5, recall: 0.5, precision: 0.5, f_beta: 0.40476190476190477
train: step: 1795, loss: 0.6927762031555176, acc: 0.575, recall: 0.575, precision: 0.6298701298701299, f_beta: 0.5248078266946192
train: step: 1796, loss: 0.6930739283561707, acc: 0.45, recall: 0.44999999999999996, precision: 0.43333333333333335, f_beta: 0.41333333333333333
train: step: 1797, loss: 0.6931994557380676, acc: 0.5, recall: 0.5, precision: 0.5, f_beta: 0.4949494949494949
train: step: 1798, loss: 0.6937323808670044, acc: 0.475, recall: 0.47500000000000003, precision: 0.4744245524296675, f_beta: 0.47203016970458833
train: step: 1799, loss: 0.6934318542480469, acc: 0.45, recall: 0.45, precision: 0.4494949494949495, f_beta: 0.4486215538847118
train: step: 1800, loss: 0.694725513458252, acc: 0.475, recall: 0.47500000000000003, precision: 0.40990990990990994, f_beta: 0.35926773455377575
train: step: 1801, loss: 0.69259113073349, acc: 0.475, recall: 0.475, precision: 0.474937343358396, f_beta: 0.474671669793621
train: step: 1802, loss: 0.6944550275802612, acc: 0.45, recall: 0.45, precision: 0.4494949494949495, f_beta: 0.4486215538847118
train: step: 1803, loss: 0.6904504299163818, acc: 0.7, recall: 0.7, precision: 0.7, f_beta: 0.7
train: step: 1804, loss: 0.693926215171814, acc: 0.5, recall: 0.5, precision: 0.5, f_beta: 0.4987468671679198
train: step: 1805, loss: 0.6984976530075073, acc: 0.475, recall: 0.47500000000000003, precision: 0.40990990990990994, f_beta: 0.35926773455377575
train: step: 1806, loss: 0.693199098110199, acc: 0.475, recall: 0.475, precision: 0.47333333333333333, f_beta: 0.4666666666666666
train: step: 1807, loss: 0.6912325620651245, acc: 0.625, recall: 0.625, precision: 0.6333333333333333, f_beta: 0.6190476190476191
train: step: 1808, loss: 0.6926873326301575, acc: 0.575, recall: 0.575, precision: 0.60752688172043, f_beta: 0.5402298850574712
train: step: 1809, loss: 0.6929638981819153, acc: 0.45, recall: 0.45, precision: 0.44047619047619047, f_beta: 0.42708333333333337
train: step: 1810, loss: 0.6938691139221191, acc: 0.4, recall: 0.4, precision: 0.38095238095238093, f_beta: 0.375
train: step: 1811, loss: 0.694510281085968, acc: 0.375, recall: 0.375, precision: 0.3575498575498576, f_beta: 0.3552546744036106
train: step: 1812, loss: 0.6935971975326538, acc: 0.425, recall: 0.425, precision: 0.42327365728900257, f_beta: 0.4217473287240729
train: step: 1813, loss: 0.6934751272201538, acc: 0.4, recall: 0.4, precision: 0.38095238095238093, f_beta: 0.375
train: step: 1814, loss: 0.6941741108894348, acc: 0.425, recall: 0.42500000000000004, precision: 0.4059561128526646, f_beta: 0.39433838051349573
train: step: 1815, loss: 0.6928378939628601, acc: 0.575, recall: 0.575, precision: 0.60752688172043, f_beta: 0.5402298850574712
train: step: 1816, loss: 0.6932638883590698, acc: 0.525, recall: 0.525, precision: 0.525062656641604, f_beta: 0.5247029393370857
train: step: 1817, loss: 0.6915932893753052, acc: 0.675, recall: 0.675, precision: 0.6866666666666666, f_beta: 0.6698412698412699
train: step: 1818, loss: 0.6854761242866516, acc: 0.625, recall: 0.625, precision: 0.6792114695340502, f_beta: 0.5943204868154157
train: step: 1819, loss: 0.6934095621109009, acc: 0.425, recall: 0.425, precision: 0.4145299145299145, f_beta: 0.40683430045132174
train: step: 1820, loss: 0.6928712129592896, acc: 0.6, recall: 0.6000000000000001, precision: 0.6190476190476191, f_beta: 0.5833333333333333
train: step: 1821, loss: 0.6928526163101196, acc: 0.425, recall: 0.425, precision: 0.42327365728900257, f_beta: 0.4217473287240729
train: step: 1822, loss: 0.694172203540802, acc: 0.35, recall: 0.35, precision: 0.3484848484848485, f_beta: 0.3483709273182958
train: step: 1823, loss: 0.6922776103019714, acc: 0.525, recall: 0.525, precision: 0.5900900900900901, f_beta: 0.4202898550724638
train: step: 1824, loss: 0.6938989162445068, acc: 0.5, recall: 0.5, precision: 0.5, f_beta: 0.5
train: step: 1825, loss: 0.6990793943405151, acc: 0.475, recall: 0.47500000000000003, precision: 0.45670995670995673, f_beta: 0.41299790356394134
train: step: 1826, loss: 0.6929275393486023, acc: 0.525, recall: 0.525, precision: 0.5255754475703325, f_beta: 0.5223130106851037
train: step: 1827, loss: 0.693334698677063, acc: 0.475, recall: 0.475, precision: 0.46865203761755486, f_beta: 0.4470046082949308
train: step: 1828, loss: 0.6927782893180847, acc: 0.5, recall: 0.5, precision: 0.5, f_beta: 0.40476190476190477
train: step: 1829, loss: 0.6922656297683716, acc: 0.525, recall: 0.525, precision: 0.525062656641604, f_beta: 0.5247029393370857
train: step: 1830, loss: 0.693696141242981, acc: 0.475, recall: 0.47500000000000003, precision: 0.4744245524296675, f_beta: 0.47203016970458833
train: step: 1831, loss: 0.6921716928482056, acc: 0.575, recall: 0.575, precision: 0.575187969924812, f_beta: 0.5747342088805503
train: step: 1832, loss: 0.6922296285629272, acc: 0.575, recall: 0.575, precision: 0.5854700854700855, f_beta: 0.5615731785944552
train: step: 1833, loss: 0.6910707354545593, acc: 0.575, recall: 0.575, precision: 0.60752688172043, f_beta: 0.5402298850574712
train: step: 1834, loss: 0.6925328373908997, acc: 0.475, recall: 0.475, precision: 0.47333333333333333, f_beta: 0.4666666666666666
train: step: 1835, loss: 0.6853145360946655, acc: 0.525, recall: 0.525, precision: 0.5313479623824451, f_beta: 0.4996708360763661
train: step: 1836, loss: 0.6922715902328491, acc: 0.625, recall: 0.625, precision: 0.6278772378516624, f_beta: 0.6228786926461345
train: step: 1837, loss: 0.6930009722709656, acc: 0.45, recall: 0.45, precision: 0.421875, f_beta: 0.39560439560439553
train: step: 1838, loss: 0.6933597922325134, acc: 0.5, recall: 0.5, precision: 0.5, f_beta: 0.47916666666666663
train: step: 1839, loss: 0.692301869392395, acc: 0.625, recall: 0.625, precision: 0.6424501424501424, f_beta: 0.6131528046421664
train: step: 1840, loss: 0.6931471228599548, acc: 0.475, recall: 0.47500000000000003, precision: 0.45670995670995673, f_beta: 0.41299790356394134
train: step: 1841, loss: 0.6880958676338196, acc: 0.65, recall: 0.65, precision: 0.7, f_beta: 0.6266666666666667
train: step: 1842, loss: 0.6943375468254089, acc: 0.4, recall: 0.4, precision: 0.398989898989899, f_beta: 0.39849624060150374
train: step: 1843, loss: 0.6927927732467651, acc: 0.5, recall: 0.5, precision: 0.5, f_beta: 0.4884910485933504
train: step: 1844, loss: 0.6926685571670532, acc: 0.5, recall: 0.5, precision: 0.5, f_beta: 0.4666666666666667
train: step: 1845, loss: 0.693080723285675, acc: 0.575, recall: 0.575, precision: 0.5800000000000001, f_beta: 0.5682539682539682
train: step: 1846, loss: 0.6923147439956665, acc: 0.6, recall: 0.6000000000000001, precision: 0.6190476190476191, f_beta: 0.5833333333333333
train: step: 1847, loss: 0.6938571929931641, acc: 0.55, recall: 0.55, precision: 0.554945054945055, f_beta: 0.5396419437340153
train: step: 1848, loss: 0.6936802864074707, acc: 0.475, recall: 0.475, precision: 0.47333333333333333, f_beta: 0.4666666666666666
train: step: 1849, loss: 0.6933701634407043, acc: 0.475, recall: 0.475, precision: 0.474937343358396, f_beta: 0.474671669793621
train: step: 1850, loss: 0.6938530206680298, acc: 0.5, recall: 0.5, precision: 0.5, f_beta: 0.4949494949494949
train: step: 1851, loss: 0.6937838196754456, acc: 0.475, recall: 0.47500000000000003, precision: 0.40990990990990994, f_beta: 0.35926773455377575
train: step: 1852, loss: 0.6937177777290344, acc: 0.475, recall: 0.475, precision: 0.44285714285714284, f_beta: 0.38909090909090915
train: step: 1853, loss: 0.6917089819908142, acc: 0.475, recall: 0.47500000000000003, precision: 0.4744245524296675, f_beta: 0.47203016970458833
train: step: 1854, loss: 0.6906148195266724, acc: 0.575, recall: 0.575, precision: 0.575187969924812, f_beta: 0.5747342088805503
train: step: 1855, loss: 0.6928454637527466, acc: 0.525, recall: 0.525, precision: 0.5255754475703325, f_beta: 0.5223130106851037
train: step: 1856, loss: 0.6920049786567688, acc: 0.525, recall: 0.525, precision: 0.5255754475703325, f_beta: 0.5223130106851037
train: step: 1857, loss: 0.6951921582221985, acc: 0.425, recall: 0.42500000000000004, precision: 0.424812030075188, f_beta: 0.4246404002501563
train: step: 1858, loss: 0.6921831369400024, acc: 0.525, recall: 0.525, precision: 0.5266666666666666, f_beta: 0.5174603174603175
train: step: 1859, loss: 0.6935974359512329, acc: 0.475, recall: 0.475, precision: 0.46865203761755486, f_beta: 0.4470046082949308
train: step: 1860, loss: 0.6944137811660767, acc: 0.5, recall: 0.5, precision: 0.5, f_beta: 0.47916666666666663
train: step: 1861, loss: 0.6912497282028198, acc: 0.575, recall: 0.575, precision: 0.5940438871473355, f_beta: 0.5523370638578011
train: step: 1862, loss: 0.691108763217926, acc: 0.5, recall: 0.5, precision: 0.5, f_beta: 0.4505494505494506
train: step: 1863, loss: 0.6883929967880249, acc: 0.575, recall: 0.575, precision: 0.7702702702702703, f_beta: 0.4813119755911518
train: step: 1864, loss: 0.6934613585472107, acc: 0.475, recall: 0.475, precision: 0.46415770609318996, f_beta: 0.43204868154158216
train: step: 1865, loss: 0.6947717070579529, acc: 0.525, recall: 0.525, precision: 0.53584229390681, f_beta: 0.4861392832995267
train: step: 1866, loss: 0.6921374797821045, acc: 0.625, recall: 0.625, precision: 0.6424501424501424, f_beta: 0.6131528046421664
train: step: 1867, loss: 0.6926172375679016, acc: 0.45, recall: 0.44999999999999996, precision: 0.43333333333333335, f_beta: 0.41333333333333333
train: step: 1868, loss: 0.6915335655212402, acc: 0.675, recall: 0.675, precision: 0.6790281329923273, f_beta: 0.6731615336266499
train: step: 1869, loss: 0.6952322721481323, acc: 0.45, recall: 0.45, precision: 0.4494949494949495, f_beta: 0.4486215538847118
train: step: 1870, loss: 0.6904786825180054, acc: 0.6, recall: 0.6000000000000001, precision: 0.601010101010101, f_beta: 0.5989974937343359
train: step: 1871, loss: 0.693849503993988, acc: 0.45, recall: 0.45, precision: 0.44791666666666663, f_beta: 0.4444444444444444
train: step: 1872, loss: 0.7042512893676758, acc: 0.475, recall: 0.475, precision: 0.46865203761755486, f_beta: 0.4470046082949308
train: step: 1873, loss: 0.6917335391044617, acc: 0.525, recall: 0.525, precision: 0.5571428571428572, f_beta: 0.44727272727272727
train: step: 1874, loss: 0.6924716830253601, acc: 0.55, recall: 0.55, precision: 0.5520833333333333, f_beta: 0.5454545454545454
train: step: 1875, loss: 0.6918414831161499, acc: 0.525, recall: 0.525, precision: 0.5900900900900901, f_beta: 0.4202898550724638
train: step: 1876, loss: 0.6929280161857605, acc: 0.525, recall: 0.525, precision: 0.5255754475703325, f_beta: 0.5223130106851037
train: step: 1877, loss: 0.695330023765564, acc: 0.4, recall: 0.4, precision: 0.38095238095238093, f_beta: 0.375
train: step: 1878, loss: 0.6942170858383179, acc: 0.425, recall: 0.425, precision: 0.4145299145299145, f_beta: 0.40683430045132174
train: step: 1879, loss: 0.6921325325965881, acc: 0.5, recall: 0.5, precision: 0.5, f_beta: 0.4987468671679198
train: step: 1880, loss: 0.6923825144767761, acc: 0.575, recall: 0.575, precision: 0.5767263427109974, f_beta: 0.5725958516656191
train: step: 1881, loss: 0.6931664943695068, acc: 0.525, recall: 0.525, precision: 0.5255754475703325, f_beta: 0.5223130106851037
train: step: 1882, loss: 0.69461989402771, acc: 0.425, recall: 0.42500000000000004, precision: 0.424812030075188, f_beta: 0.4246404002501563
train: step: 1883, loss: 0.6939120292663574, acc: 0.5, recall: 0.5, precision: 0.5, f_beta: 0.4987468671679198
train: step: 1884, loss: 0.6921960115432739, acc: 0.575, recall: 0.575, precision: 0.575187969924812, f_beta: 0.5747342088805503
train: step: 1885, loss: 0.6913310885429382, acc: 0.725, recall: 0.7250000000000001, precision: 0.7301790281329923, f_beta: 0.7234443746071653
train: step: 1886, loss: 0.6949813961982727, acc: 0.4, recall: 0.4, precision: 0.4, f_beta: 0.4000000000000001
train: step: 1887, loss: 0.6936426162719727, acc: 0.475, recall: 0.47500000000000003, precision: 0.4744245524296675, f_beta: 0.47203016970458833
train: step: 1888, loss: 0.6918197870254517, acc: 0.65, recall: 0.65, precision: 0.65625, f_beta: 0.6464646464646464
train: step: 1889, loss: 0.6919673681259155, acc: 0.525, recall: 0.525, precision: 0.5900900900900901, f_beta: 0.4202898550724638
train: step: 1890, loss: 0.6910563707351685, acc: 0.55, recall: 0.55, precision: 0.5980392156862745, f_beta: 0.4871794871794872
train: step: 1891, loss: 0.6939042806625366, acc: 0.475, recall: 0.475, precision: 0.474937343358396, f_beta: 0.474671669793621
train: step: 1892, loss: 0.693602979183197, acc: 0.475, recall: 0.475, precision: 0.24358974358974358, f_beta: 0.3220338983050847
train: step: 1893, loss: 0.6929172277450562, acc: 0.55, recall: 0.55, precision: 0.5505050505050505, f_beta: 0.5488721804511277
train: step: 1894, loss: 0.6943982839584351, acc: 0.4, recall: 0.4, precision: 0.398989898989899, f_beta: 0.39849624060150374
train: step: 1895, loss: 0.6904724836349487, acc: 0.55, recall: 0.55, precision: 0.5980392156862745, f_beta: 0.4871794871794872
train: step: 1896, loss: 0.692013144493103, acc: 0.6, recall: 0.6, precision: 0.65625, f_beta: 0.5604395604395604
train: step: 1897, loss: 0.693095326423645, acc: 0.575, recall: 0.575, precision: 0.5854700854700855, f_beta: 0.5615731785944552
train: step: 1898, loss: 0.6953893899917603, acc: 0.35, recall: 0.35, precision: 0.3484848484848485, f_beta: 0.3483709273182958
train: step: 1899, loss: 0.6950052976608276, acc: 0.275, recall: 0.275, precision: 0.2744360902255639, f_beta: 0.27454659161976236
train: step: 1900, loss: 0.6979292631149292, acc: 0.375, recall: 0.375, precision: 0.3575498575498576, f_beta: 0.3552546744036106
train: step: 1901, loss: 0.6918641924858093, acc: 0.55, recall: 0.55, precision: 0.5595238095238095, f_beta: 0.53125
train: step: 1902, loss: 0.6924240589141846, acc: 0.6, recall: 0.6000000000000001, precision: 0.601010101010101, f_beta: 0.5989974937343359
train: step: 1903, loss: 0.6923531293869019, acc: 0.5, recall: 0.5, precision: 0.5, f_beta: 0.4505494505494506
train: step: 1904, loss: 0.6919597387313843, acc: 0.5, recall: 0.5, precision: 0.5, f_beta: 0.4987468671679198
train: step: 1905, loss: 0.6945973634719849, acc: 0.475, recall: 0.475, precision: 0.474937343358396, f_beta: 0.474671669793621
train: step: 1906, loss: 0.6911770105361938, acc: 0.55, recall: 0.55, precision: 0.5980392156862745, f_beta: 0.4871794871794872
train: step: 1907, loss: 0.6937085390090942, acc: 0.475, recall: 0.475, precision: 0.47333333333333333, f_beta: 0.4666666666666666
train: step: 1908, loss: 0.6923704147338867, acc: 0.55, recall: 0.55, precision: 0.55, f_beta: 0.55
train: step: 1909, loss: 0.6919553875923157, acc: 0.6, recall: 0.6, precision: 0.6333333333333333, f_beta: 0.5733333333333334
train: step: 1910, loss: 0.6945300102233887, acc: 0.35, recall: 0.35000000000000003, precision: 0.3214285714285714, f_beta: 0.3229166666666667
train: step: 1911, loss: 0.6928070783615112, acc: 0.575, recall: 0.575, precision: 0.575187969924812, f_beta: 0.5747342088805503
train: step: 1912, loss: 0.692559540271759, acc: 0.55, recall: 0.55, precision: 0.5666666666666667, f_beta: 0.52
train: step: 1913, loss: 0.6937058568000793, acc: 0.375, recall: 0.375, precision: 0.37468671679197996, f_beta: 0.37460913070669166
train: step: 1914, loss: 0.6921050548553467, acc: 0.525, recall: 0.525, precision: 0.525062656641604, f_beta: 0.5247029393370857
train: step: 1915, loss: 0.6935139894485474, acc: 0.4, recall: 0.4, precision: 0.398989898989899, f_beta: 0.39849624060150374
train: step: 1916, loss: 0.6930669546127319, acc: 0.575, recall: 0.575, precision: 0.575187969924812, f_beta: 0.5747342088805503
train: step: 1917, loss: 0.6947935819625854, acc: 0.4, recall: 0.4, precision: 0.3901098901098901, f_beta: 0.38618925831202044
train: step: 1918, loss: 0.6951694488525391, acc: 0.475, recall: 0.475, precision: 0.47150997150997154, f_beta: 0.4584139264990329
train: step: 1919, loss: 0.6924686431884766, acc: 0.6, recall: 0.6, precision: 0.65625, f_beta: 0.5604395604395604
train: step: 1920, loss: 0.6922338604927063, acc: 0.55, recall: 0.55, precision: 0.5520833333333333, f_beta: 0.5454545454545454
train: step: 1921, loss: 0.694223165512085, acc: 0.475, recall: 0.475, precision: 0.47150997150997154, f_beta: 0.4584139264990329
train: step: 1922, loss: 0.6921355128288269, acc: 0.65, recall: 0.65, precision: 0.6785714285714286, f_beta: 0.6354166666666667
train: step: 1923, loss: 0.692997932434082, acc: 0.55, recall: 0.55, precision: 0.5505050505050505, f_beta: 0.5488721804511277
train: step: 1924, loss: 0.6934621334075928, acc: 0.525, recall: 0.525, precision: 0.5313479623824451, f_beta: 0.4996708360763661
train: step: 1925, loss: 0.6926448345184326, acc: 0.475, recall: 0.475, precision: 0.46415770609318996, f_beta: 0.43204868154158216
train: step: 1926, loss: 0.691622793674469, acc: 0.575, recall: 0.575, precision: 0.5767263427109974, f_beta: 0.5725958516656191
train: step: 1927, loss: 0.6967705488204956, acc: 0.375, recall: 0.375, precision: 0.28354978354978355, f_beta: 0.3011879804332634
train: step: 1928, loss: 0.6934665441513062, acc: 0.45, recall: 0.45, precision: 0.44791666666666663, f_beta: 0.4444444444444444
train: step: 1929, loss: 0.6932278871536255, acc: 0.5, recall: 0.5, precision: 0.5, f_beta: 0.40476190476190477
train: step: 1930, loss: 0.6926813125610352, acc: 0.525, recall: 0.525, precision: 0.525062656641604, f_beta: 0.5247029393370857
train: step: 1931, loss: 0.6888442039489746, acc: 0.525, recall: 0.525, precision: 0.5571428571428572, f_beta: 0.44727272727272727
train: step: 1932, loss: 0.6910823583602905, acc: 0.575, recall: 0.575, precision: 0.5940438871473355, f_beta: 0.5523370638578011
train: step: 1933, loss: 0.6960264444351196, acc: 0.425, recall: 0.42500000000000004, precision: 0.424812030075188, f_beta: 0.4246404002501563
train: step: 1934, loss: 0.694545328617096, acc: 0.45, recall: 0.45, precision: 0.44047619047619047, f_beta: 0.42708333333333337
train: step: 1935, loss: 0.6939881443977356, acc: 0.525, recall: 0.525, precision: 0.5255754475703325, f_beta: 0.5223130106851037
train: step: 1936, loss: 0.6968029737472534, acc: 0.45, recall: 0.45, precision: 0.421875, f_beta: 0.39560439560439553
train: step: 1937, loss: 0.691473662853241, acc: 0.5, recall: 0.5, precision: 0.5, f_beta: 0.4884910485933504
train: step: 1938, loss: 0.69282466173172, acc: 0.575, recall: 0.575, precision: 0.575187969924812, f_beta: 0.5747342088805503
train: step: 1939, loss: 0.694671630859375, acc: 0.45, recall: 0.45, precision: 0.4019607843137255, f_beta: 0.37321937321937326
train: step: 1940, loss: 0.6976504921913147, acc: 0.475, recall: 0.475, precision: 0.24358974358974358, f_beta: 0.3220338983050847
train: step: 1941, loss: 0.691912055015564, acc: 0.55, recall: 0.55, precision: 0.5505050505050505, f_beta: 0.5488721804511277
train: step: 1942, loss: 0.6934958696365356, acc: 0.525, recall: 0.525, precision: 0.53584229390681, f_beta: 0.4861392832995267
train: step: 1943, loss: 0.6970996856689453, acc: 0.45, recall: 0.45, precision: 0.421875, f_beta: 0.39560439560439553
train: step: 1944, loss: 0.6925432085990906, acc: 0.525, recall: 0.525, precision: 0.53584229390681, f_beta: 0.4861392832995267
train: step: 1945, loss: 0.691973090171814, acc: 0.5, recall: 0.5, precision: 0.5, f_beta: 0.4987468671679198
train: step: 1946, loss: 0.6928268671035767, acc: 0.5, recall: 0.5, precision: 0.5, f_beta: 0.4666666666666667
train: step: 1947, loss: 0.6920772790908813, acc: 0.55, recall: 0.55, precision: 0.55, f_beta: 0.55
train: step: 1948, loss: 0.690693736076355, acc: 0.65, recall: 0.6499999999999999, precision: 0.734375, f_beta: 0.6153846153846154
train: step: 1949, loss: 0.6938474774360657, acc: 0.4, recall: 0.4, precision: 0.39583333333333337, f_beta: 0.3939393939393939
train: step: 1950, loss: 0.6902146339416504, acc: 0.55, recall: 0.55, precision: 0.5666666666666667, f_beta: 0.52
train: step: 1951, loss: 0.6969944834709167, acc: 0.45, recall: 0.45, precision: 0.4019607843137255, f_beta: 0.37321937321937326
train: step: 1952, loss: 0.6919153928756714, acc: 0.55, recall: 0.55, precision: 0.5520833333333333, f_beta: 0.5454545454545454
train: step: 1953, loss: 0.6925099492073059, acc: 0.45, recall: 0.45, precision: 0.4494949494949495, f_beta: 0.4486215538847118
train: step: 1954, loss: 0.6932517886161804, acc: 0.5, recall: 0.5, precision: 0.5, f_beta: 0.4949494949494949
train: step: 1955, loss: 0.69269198179245, acc: 0.55, recall: 0.55, precision: 0.5520833333333333, f_beta: 0.5454545454545454
train: step: 1956, loss: 0.6927388906478882, acc: 0.575, recall: 0.575, precision: 0.575187969924812, f_beta: 0.5747342088805503
train: step: 1957, loss: 0.6937090158462524, acc: 0.375, recall: 0.375, precision: 0.3666666666666667, f_beta: 0.3650793650793651
train: step: 1958, loss: 0.6951901316642761, acc: 0.425, recall: 0.42500000000000004, precision: 0.424812030075188, f_beta: 0.4246404002501563
train: step: 1959, loss: 0.6923460960388184, acc: 0.575, recall: 0.575, precision: 0.6298701298701299, f_beta: 0.5248078266946192
train: step: 1960, loss: 0.6939194798469543, acc: 0.475, recall: 0.475, precision: 0.44285714285714284, f_beta: 0.38909090909090915
train: step: 1961, loss: 0.6928671598434448, acc: 0.45, recall: 0.45, precision: 0.45, f_beta: 0.45
train: step: 1962, loss: 0.6815769076347351, acc: 0.65, recall: 0.6499999999999999, precision: 0.734375, f_beta: 0.6153846153846154
train: step: 1963, loss: 0.6922469139099121, acc: 0.475, recall: 0.475, precision: 0.47333333333333333, f_beta: 0.4666666666666666
train: step: 1964, loss: 0.6931794285774231, acc: 0.5, recall: 0.5, precision: 0.5, f_beta: 0.4884910485933504
train: step: 1965, loss: 0.6933706402778625, acc: 0.55, recall: 0.55, precision: 0.5666666666666667, f_beta: 0.52
train: step: 1966, loss: 0.6886281967163086, acc: 0.65, recall: 0.65, precision: 0.6785714285714286, f_beta: 0.6354166666666667
train: step: 1967, loss: 0.6932287812232971, acc: 0.475, recall: 0.475, precision: 0.44285714285714284, f_beta: 0.38909090909090915
train: step: 1968, loss: 0.6936377286911011, acc: 0.475, recall: 0.475, precision: 0.44285714285714284, f_beta: 0.38909090909090915
train: step: 1969, loss: 0.6925376653671265, acc: 0.6, recall: 0.6000000000000001, precision: 0.601010101010101, f_beta: 0.5989974937343359
train: step: 1970, loss: 0.6945880651473999, acc: 0.475, recall: 0.47500000000000003, precision: 0.40990990990990994, f_beta: 0.35926773455377575
train: step: 1971, loss: 0.6936342716217041, acc: 0.375, recall: 0.375, precision: 0.37468671679197996, f_beta: 0.37460913070669166
train: step: 1972, loss: 0.6894461512565613, acc: 0.65, recall: 0.65, precision: 0.6785714285714286, f_beta: 0.6354166666666667
train: step: 1973, loss: 0.6913570165634155, acc: 0.65, recall: 0.6499999999999999, precision: 0.6515151515151515, f_beta: 0.6491228070175439
train: step: 1974, loss: 0.6916371583938599, acc: 0.55, recall: 0.55, precision: 0.578125, f_beta: 0.5054945054945055
train: step: 1975, loss: 0.6916488409042358, acc: 0.7, recall: 0.7, precision: 0.7197802197802198, f_beta: 0.6930946291560103
train: step: 1976, loss: 0.6933784484863281, acc: 0.475, recall: 0.475, precision: 0.44285714285714284, f_beta: 0.38909090909090915
train: step: 1977, loss: 0.6950308680534363, acc: 0.425, recall: 0.425, precision: 0.4145299145299145, f_beta: 0.40683430045132174
train: step: 1978, loss: 0.6936579942703247, acc: 0.45, recall: 0.45, precision: 0.421875, f_beta: 0.39560439560439553
train: step: 1979, loss: 0.6943796873092651, acc: 0.525, recall: 0.5249999999999999, precision: 0.5284900284900285, f_beta: 0.5099935525467441
train: step: 1980, loss: 0.6904082298278809, acc: 0.55, recall: 0.55, precision: 0.554945054945055, f_beta: 0.5396419437340153
train: step: 1981, loss: 0.6940776109695435, acc: 0.525, recall: 0.525, precision: 0.5313479623824451, f_beta: 0.4996708360763661
train: step: 1982, loss: 0.695088803768158, acc: 0.45, recall: 0.45, precision: 0.44791666666666663, f_beta: 0.4444444444444444
train: step: 1983, loss: 0.6950863599777222, acc: 0.375, recall: 0.375, precision: 0.34326018808777425, f_beta: 0.34167215273206053
train: step: 1984, loss: 0.6939417123794556, acc: 0.45, recall: 0.45, precision: 0.3611111111111111, f_beta: 0.34523809523809523
train: step: 1985, loss: 0.6929206848144531, acc: 0.575, recall: 0.575, precision: 0.5767263427109974, f_beta: 0.5725958516656191
train: step: 1986, loss: 0.6931865811347961, acc: 0.5, recall: 0.5, precision: 0.5, f_beta: 0.4949494949494949
train: step: 1987, loss: 0.6951351165771484, acc: 0.425, recall: 0.42500000000000004, precision: 0.32857142857142857, f_beta: 0.33090909090909093
train: step: 1988, loss: 0.6942598819732666, acc: 0.375, recall: 0.375, precision: 0.3666666666666667, f_beta: 0.3650793650793651
train: step: 1989, loss: 0.6930767297744751, acc: 0.45, recall: 0.45, precision: 0.44791666666666663, f_beta: 0.4444444444444444
train: step: 1990, loss: 0.6942909955978394, acc: 0.45, recall: 0.45, precision: 0.45, f_beta: 0.45
train: step: 1991, loss: 0.6940086483955383, acc: 0.4, recall: 0.4, precision: 0.3901098901098901, f_beta: 0.38618925831202044
train: step: 1992, loss: 0.6934111714363098, acc: 0.475, recall: 0.475, precision: 0.47333333333333333, f_beta: 0.4666666666666666
train: step: 1993, loss: 0.6926008462905884, acc: 0.475, recall: 0.47500000000000003, precision: 0.45670995670995673, f_beta: 0.41299790356394134
train: step: 1994, loss: 0.6924557089805603, acc: 0.525, recall: 0.525, precision: 0.5266666666666666, f_beta: 0.5174603174603175
train: step: 1995, loss: 0.6940263509750366, acc: 0.375, recall: 0.375, precision: 0.3666666666666667, f_beta: 0.3650793650793651
train: step: 1996, loss: 0.6930545568466187, acc: 0.575, recall: 0.575, precision: 0.575187969924812, f_beta: 0.5747342088805503
train: step: 1997, loss: 0.6931194067001343, acc: 0.525, recall: 0.525, precision: 0.525062656641604, f_beta: 0.5247029393370857
train: step: 1998, loss: 0.6924498081207275, acc: 0.5, recall: 0.5, precision: 0.5, f_beta: 0.4884910485933504
train: step: 1999, loss: 0.6933101415634155, acc: 0.475, recall: 0.475, precision: 0.47333333333333333, f_beta: 0.4666666666666666
train: step: 2000, loss: 0.6932662725448608, acc: 0.525, recall: 0.525, precision: 0.525062656641604, f_beta: 0.5247029393370857
train: step: 2001, loss: 0.6913841962814331, acc: 0.6, recall: 0.6, precision: 0.6, f_beta: 0.6
train: step: 2002, loss: 0.6939128637313843, acc: 0.5, recall: 0.5, precision: 0.5, f_beta: 0.4666666666666667
train: step: 2003, loss: 0.6900932192802429, acc: 0.6, recall: 0.6, precision: 0.6333333333333333, f_beta: 0.5733333333333334
train: step: 2004, loss: 0.6953082084655762, acc: 0.45, recall: 0.45, precision: 0.4494949494949495, f_beta: 0.4486215538847118
train: step: 2005, loss: 0.693311333656311, acc: 0.525, recall: 0.5249999999999999, precision: 0.5284900284900285, f_beta: 0.5099935525467441
train: step: 2006, loss: 0.6886711120605469, acc: 0.575, recall: 0.575, precision: 0.7702702702702703, f_beta: 0.4813119755911518
train: step: 2007, loss: 0.6939650774002075, acc: 0.45, recall: 0.45, precision: 0.45, f_beta: 0.45
train: step: 2008, loss: 0.6928709745407104, acc: 0.525, recall: 0.525, precision: 0.525062656641604, f_beta: 0.5247029393370857
train: step: 2009, loss: 0.6932206153869629, acc: 0.5, recall: 0.5, precision: 0.5, f_beta: 0.4884910485933504
train: step: 2010, loss: 0.6928333044052124, acc: 0.425, recall: 0.42500000000000004, precision: 0.4059561128526646, f_beta: 0.39433838051349573
train: step: 2011, loss: 0.6927210092544556, acc: 0.45, recall: 0.45, precision: 0.4494949494949495, f_beta: 0.4486215538847118
train: step: 2012, loss: 0.6915048360824585, acc: 0.65, recall: 0.6499999999999999, precision: 0.734375, f_beta: 0.6153846153846154
train: step: 2013, loss: 0.6920521855354309, acc: 0.475, recall: 0.475, precision: 0.46865203761755486, f_beta: 0.4470046082949308
train: step: 2014, loss: 0.6917122006416321, acc: 0.625, recall: 0.625, precision: 0.6424501424501424, f_beta: 0.6131528046421664
train: step: 2015, loss: 0.6910889744758606, acc: 0.625, recall: 0.625, precision: 0.6792114695340502, f_beta: 0.5943204868154157
train: step: 2016, loss: 0.6897938847541809, acc: 0.55, recall: 0.55, precision: 0.763157894736842, f_beta: 0.4357366771159875
train: step: 2017, loss: 0.694107174873352, acc: 0.425, recall: 0.425, precision: 0.3924731182795699, f_beta: 0.37795807978363755
train: step: 2018, loss: 0.6956809163093567, acc: 0.35, recall: 0.35, precision: 0.3351648351648352, f_beta: 0.3350383631713555
train: step: 2019, loss: 0.6937907934188843, acc: 0.5, recall: 0.5, precision: 0.5, f_beta: 0.4505494505494506
train: step: 2020, loss: 0.6934946179389954, acc: 0.35, recall: 0.35, precision: 0.30000000000000004, f_beta: 0.30666666666666664
train: step: 2021, loss: 0.6937697529792786, acc: 0.35, recall: 0.35, precision: 0.3351648351648352, f_beta: 0.3350383631713555
train: step: 2022, loss: 0.6962600946426392, acc: 0.35, recall: 0.35, precision: 0.35, f_beta: 0.35
train: step: 2023, loss: 0.6933358311653137, acc: 0.475, recall: 0.475, precision: 0.47150997150997154, f_beta: 0.4584139264990329
train: step: 2024, loss: 0.6903029680252075, acc: 0.525, recall: 0.525, precision: 0.53584229390681, f_beta: 0.4861392832995267
train: step: 2025, loss: 0.6933112740516663, acc: 0.475, recall: 0.475, precision: 0.474937343358396, f_beta: 0.474671669793621
train: step: 2026, loss: 0.6929933428764343, acc: 0.6, recall: 0.6000000000000001, precision: 0.601010101010101, f_beta: 0.5989974937343359
train: step: 2027, loss: 0.6929060816764832, acc: 0.475, recall: 0.47500000000000003, precision: 0.4744245524296675, f_beta: 0.47203016970458833
train: step: 2028, loss: 0.68886798620224, acc: 0.625, recall: 0.625, precision: 0.6567398119122256, f_beta: 0.6050032916392363
train: step: 2029, loss: 0.6928653120994568, acc: 0.55, recall: 0.55, precision: 0.554945054945055, f_beta: 0.5396419437340153
train: step: 2030, loss: 0.6937429308891296, acc: 0.525, recall: 0.525, precision: 0.5432900432900433, f_beta: 0.46890286512928026
train: step: 2031, loss: 0.6937743425369263, acc: 0.425, recall: 0.42500000000000004, precision: 0.4059561128526646, f_beta: 0.39433838051349573
train: step: 2032, loss: 0.6944120526313782, acc: 0.475, recall: 0.47500000000000003, precision: 0.4744245524296675, f_beta: 0.47203016970458833
train: step: 2033, loss: 0.6923530697822571, acc: 0.6, recall: 0.6, precision: 0.6098901098901099, f_beta: 0.5907928388746804
train: step: 2034, loss: 0.6948750615119934, acc: 0.475, recall: 0.475, precision: 0.47333333333333333, f_beta: 0.4666666666666666
train: step: 2035, loss: 0.6908053159713745, acc: 0.675, recall: 0.675, precision: 0.7508960573476702, f_beta: 0.6484110885733605
train: step: 2036, loss: 0.6918586492538452, acc: 0.6, recall: 0.6, precision: 0.6041666666666667, f_beta: 0.595959595959596
train: step: 2037, loss: 0.6945571899414062, acc: 0.325, recall: 0.325, precision: 0.32097186700767266, f_beta: 0.32118164676304206
train: step: 2038, loss: 0.690441906452179, acc: 0.575, recall: 0.575, precision: 0.7702702702702703, f_beta: 0.4813119755911518
train: step: 2039, loss: 0.6924566030502319, acc: 0.55, recall: 0.55, precision: 0.5505050505050505, f_beta: 0.5488721804511277
train: step: 2040, loss: 0.6939195394515991, acc: 0.525, recall: 0.525, precision: 0.5571428571428572, f_beta: 0.44727272727272727
train: step: 2041, loss: 0.6936771273612976, acc: 0.55, recall: 0.55, precision: 0.554945054945055, f_beta: 0.5396419437340153
train: step: 2042, loss: 0.6926456689834595, acc: 0.475, recall: 0.475, precision: 0.46865203761755486, f_beta: 0.4470046082949308
train: step: 2043, loss: 0.692206621170044, acc: 0.55, recall: 0.55, precision: 0.5505050505050505, f_beta: 0.5488721804511277
train: step: 2044, loss: 0.6928273439407349, acc: 0.5, recall: 0.5, precision: 0.5, f_beta: 0.4884910485933504
train: step: 2045, loss: 0.6936939358711243, acc: 0.525, recall: 0.525, precision: 0.5255754475703325, f_beta: 0.5223130106851037
train: step: 2046, loss: 0.6900573968887329, acc: 0.5, recall: 0.5, precision: 0.5, f_beta: 0.40476190476190477
train: step: 2047, loss: 0.6932622194290161, acc: 0.575, recall: 0.575, precision: 0.5767263427109974, f_beta: 0.5725958516656191
train: step: 2048, loss: 0.6876519918441772, acc: 0.525, recall: 0.525, precision: 0.5432900432900433, f_beta: 0.46890286512928026
train: step: 2049, loss: 0.6938475966453552, acc: 0.5, recall: 0.5, precision: 0.5, f_beta: 0.4666666666666667
train: step: 2050, loss: 0.6950885653495789, acc: 0.45, recall: 0.45, precision: 0.23684210526315788, f_beta: 0.3103448275862069
train: step: 2051, loss: 0.6938096284866333, acc: 0.4, recall: 0.4, precision: 0.4, f_beta: 0.4000000000000001
train: step: 2052, loss: 0.6939858794212341, acc: 0.425, recall: 0.425, precision: 0.4145299145299145, f_beta: 0.40683430045132174
train: step: 2053, loss: 0.6923494338989258, acc: 0.6, recall: 0.6, precision: 0.6041666666666667, f_beta: 0.595959595959596
train: step: 2054, loss: 0.6927403211593628, acc: 0.525, recall: 0.5249999999999999, precision: 0.5284900284900285, f_beta: 0.5099935525467441
train: step: 2055, loss: 0.6891578435897827, acc: 0.6, recall: 0.6, precision: 0.6333333333333333, f_beta: 0.5733333333333334
train: step: 2056, loss: 0.6871120929718018, acc: 0.575, recall: 0.575, precision: 0.60752688172043, f_beta: 0.5402298850574712
train: step: 2057, loss: 0.6915668249130249, acc: 0.5, recall: 0.5, precision: 0.5, f_beta: 0.4505494505494506
train: step: 2058, loss: 0.6923586130142212, acc: 0.525, recall: 0.525, precision: 0.5313479623824451, f_beta: 0.4996708360763661
train: step: 2059, loss: 0.6921094655990601, acc: 0.6, recall: 0.6, precision: 0.6, f_beta: 0.6
train: step: 2060, loss: 0.6893597841262817, acc: 0.525, recall: 0.525, precision: 0.5266666666666666, f_beta: 0.5174603174603175
train: step: 2061, loss: 0.6916895508766174, acc: 0.575, recall: 0.575, precision: 0.575187969924812, f_beta: 0.5747342088805503
train: step: 2062, loss: 0.6888409852981567, acc: 0.55, recall: 0.5499999999999999, precision: 0.6388888888888888, f_beta: 0.46428571428571425
train: step: 2063, loss: 0.6956608891487122, acc: 0.5, recall: 0.5, precision: 0.5, f_beta: 0.4884910485933504
train: step: 2064, loss: 0.693274974822998, acc: 0.55, recall: 0.5499999999999999, precision: 0.6388888888888888, f_beta: 0.46428571428571425
train: step: 2065, loss: 0.694645881652832, acc: 0.525, recall: 0.525, precision: 0.5313479623824451, f_beta: 0.4996708360763661
train: step: 2066, loss: 0.6882756948471069, acc: 0.55, recall: 0.55, precision: 0.5595238095238095, f_beta: 0.53125
train: step: 2067, loss: 0.6935788989067078, acc: 0.5, recall: 0.5, precision: 0.5, f_beta: 0.4884910485933504
train: step: 2068, loss: 0.6943680644035339, acc: 0.35, recall: 0.35, precision: 0.35, f_beta: 0.35
train: step: 2069, loss: 0.6927164793014526, acc: 0.575, recall: 0.575, precision: 0.5800000000000001, f_beta: 0.5682539682539682
train: step: 2070, loss: 0.6895455121994019, acc: 0.575, recall: 0.575, precision: 0.60752688172043, f_beta: 0.5402298850574712
train: step: 2071, loss: 0.6945406198501587, acc: 0.45, recall: 0.45, precision: 0.3611111111111111, f_beta: 0.34523809523809523
train: step: 2072, loss: 0.6959074139595032, acc: 0.425, recall: 0.42500000000000004, precision: 0.32857142857142857, f_beta: 0.33090909090909093
train: step: 2073, loss: 0.697567343711853, acc: 0.375, recall: 0.375, precision: 0.3721227621483376, f_beta: 0.3714644877435575
train: step: 2074, loss: 0.693777859210968, acc: 0.5, recall: 0.5, precision: 0.5, f_beta: 0.4987468671679198
train: step: 2075, loss: 0.6924000978469849, acc: 0.625, recall: 0.625, precision: 0.62531328320802, f_beta: 0.6247654784240151
train: step: 2076, loss: 0.6948761940002441, acc: 0.475, recall: 0.475, precision: 0.474937343358396, f_beta: 0.474671669793621
train: step: 2077, loss: 0.7067617774009705, acc: 0.4, recall: 0.4, precision: 0.30392156862745096, f_beta: 0.3162393162393162
train: step: 2078, loss: 0.692339301109314, acc: 0.5, recall: 0.5, precision: 0.5, f_beta: 0.4987468671679198
train: step: 2079, loss: 0.6931742429733276, acc: 0.475, recall: 0.475, precision: 0.47150997150997154, f_beta: 0.4584139264990329
train: step: 2080, loss: 0.6930171847343445, acc: 0.5, recall: 0.5, precision: 0.5, f_beta: 0.4949494949494949
train: step: 2081, loss: 0.6955307126045227, acc: 0.525, recall: 0.525, precision: 0.5432900432900433, f_beta: 0.46890286512928026
train: step: 2082, loss: 0.6836027503013611, acc: 0.625, recall: 0.625, precision: 0.6567398119122256, f_beta: 0.6050032916392363
train: step: 2083, loss: 0.6930211782455444, acc: 0.475, recall: 0.475, precision: 0.47150997150997154, f_beta: 0.4584139264990329
train: step: 2084, loss: 0.6916651725769043, acc: 0.55, recall: 0.55, precision: 0.55, f_beta: 0.55
train: step: 2085, loss: 0.6927410364151001, acc: 0.525, recall: 0.525, precision: 0.5255754475703325, f_beta: 0.5223130106851037
train: step: 2086, loss: 0.6918204426765442, acc: 0.5, recall: 0.5, precision: 0.5, f_beta: 0.4987468671679198
train: step: 2087, loss: 0.6958657503128052, acc: 0.425, recall: 0.425, precision: 0.3924731182795699, f_beta: 0.37795807978363755
train: step: 2088, loss: 0.6906529664993286, acc: 0.525, recall: 0.525, precision: 0.5571428571428572, f_beta: 0.44727272727272727
train: step: 2089, loss: 0.6916002631187439, acc: 0.575, recall: 0.575, precision: 0.575187969924812, f_beta: 0.5747342088805503
train: step: 2090, loss: 0.6923505663871765, acc: 0.525, recall: 0.525, precision: 0.5255754475703325, f_beta: 0.5223130106851037
train: step: 2091, loss: 0.6933766603469849, acc: 0.55, recall: 0.55, precision: 0.5595238095238095, f_beta: 0.53125
train: step: 2092, loss: 0.6897143721580505, acc: 0.575, recall: 0.575, precision: 0.7702702702702703, f_beta: 0.4813119755911518
train: step: 2093, loss: 0.6927298903465271, acc: 0.575, recall: 0.575, precision: 0.5767263427109974, f_beta: 0.5725958516656191
train: step: 2094, loss: 0.6938942670822144, acc: 0.5, recall: 0.5, precision: 0.5, f_beta: 0.4884910485933504
train: step: 2095, loss: 0.6920009851455688, acc: 0.625, recall: 0.625, precision: 0.62531328320802, f_beta: 0.6247654784240151
train: step: 2096, loss: 0.6910563707351685, acc: 0.6, recall: 0.6, precision: 0.6333333333333333, f_beta: 0.5733333333333334
train: step: 2097, loss: 0.6922651529312134, acc: 0.5, recall: 0.5, precision: 0.5, f_beta: 0.4505494505494506
train: step: 2098, loss: 0.6936339139938354, acc: 0.475, recall: 0.475, precision: 0.47333333333333333, f_beta: 0.4666666666666666
train: step: 2099, loss: 0.6951040029525757, acc: 0.4, recall: 0.4, precision: 0.3901098901098901, f_beta: 0.38618925831202044
train: step: 2100, loss: 0.6933307647705078, acc: 0.575, recall: 0.575, precision: 0.5767263427109974, f_beta: 0.5725958516656191
train: step: 2101, loss: 0.6913499236106873, acc: 0.55, recall: 0.55, precision: 0.554945054945055, f_beta: 0.5396419437340153
train: step: 2102, loss: 0.6926572918891907, acc: 0.55, recall: 0.55, precision: 0.554945054945055, f_beta: 0.5396419437340153
train: step: 2103, loss: 0.694643497467041, acc: 0.4, recall: 0.4, precision: 0.38095238095238093, f_beta: 0.375
train: step: 2104, loss: 0.6941903829574585, acc: 0.4, recall: 0.4, precision: 0.4, f_beta: 0.4000000000000001
train: step: 2105, loss: 0.6923853754997253, acc: 0.5, recall: 0.5, precision: 0.5, f_beta: 0.47916666666666663
train: step: 2106, loss: 0.6920866966247559, acc: 0.5, recall: 0.5, precision: 0.5, f_beta: 0.5
train: step: 2107, loss: 0.6916868090629578, acc: 0.6, recall: 0.6, precision: 0.6, f_beta: 0.6
train: step: 2108, loss: 0.6930416822433472, acc: 0.475, recall: 0.47500000000000003, precision: 0.40990990990990994, f_beta: 0.35926773455377575
train: step: 2109, loss: 0.6909762620925903, acc: 0.625, recall: 0.625, precision: 0.6424501424501424, f_beta: 0.6131528046421664
train: step: 2110, loss: 0.692171573638916, acc: 0.5, recall: 0.5, precision: 0.5, f_beta: 0.4949494949494949
train: step: 2111, loss: 0.692943811416626, acc: 0.5, recall: 0.5, precision: 0.5, f_beta: 0.4884910485933504
train: step: 2112, loss: 0.6941174268722534, acc: 0.45, recall: 0.44999999999999996, precision: 0.44505494505494503, f_beta: 0.4373401534526854
train: step: 2113, loss: 0.6927698254585266, acc: 0.475, recall: 0.475, precision: 0.474937343358396, f_beta: 0.474671669793621
train: step: 2114, loss: 0.6948584318161011, acc: 0.4, recall: 0.4, precision: 0.398989898989899, f_beta: 0.39849624060150374
train: step: 2115, loss: 0.6926263570785522, acc: 0.475, recall: 0.47500000000000003, precision: 0.45670995670995673, f_beta: 0.41299790356394134
train: step: 2116, loss: 0.6917086839675903, acc: 0.55, recall: 0.55, precision: 0.5520833333333333, f_beta: 0.5454545454545454
train: step: 2117, loss: 0.6923754215240479, acc: 0.5, recall: 0.5, precision: 0.5, f_beta: 0.4987468671679198
train: step: 2118, loss: 0.6928967237472534, acc: 0.525, recall: 0.525, precision: 0.5266666666666666, f_beta: 0.5174603174603175
train: step: 2119, loss: 0.693133533000946, acc: 0.4, recall: 0.4, precision: 0.3901098901098901, f_beta: 0.38618925831202044
train: step: 2120, loss: 0.6946880221366882, acc: 0.35, recall: 0.35, precision: 0.34375, f_beta: 0.3434343434343434
train: step: 2121, loss: 0.6911633610725403, acc: 0.575, recall: 0.575, precision: 0.60752688172043, f_beta: 0.5402298850574712
train: step: 2122, loss: 0.6940933465957642, acc: 0.45, recall: 0.45, precision: 0.4494949494949495, f_beta: 0.4486215538847118
train: step: 2123, loss: 0.6910330057144165, acc: 0.6, recall: 0.6000000000000001, precision: 0.6190476190476191, f_beta: 0.5833333333333333
train: step: 2124, loss: 0.6877184510231018, acc: 0.6, recall: 0.6, precision: 0.65625, f_beta: 0.5604395604395604
train: step: 2125, loss: 0.6950569152832031, acc: 0.475, recall: 0.475, precision: 0.46415770609318996, f_beta: 0.43204868154158216
train: step: 2126, loss: 0.6932525634765625, acc: 0.45, recall: 0.45, precision: 0.4494949494949495, f_beta: 0.4486215538847118
train: step: 2127, loss: 0.6932408213615417, acc: 0.475, recall: 0.47500000000000003, precision: 0.4744245524296675, f_beta: 0.47203016970458833
train: step: 2128, loss: 0.6922284364700317, acc: 0.575, recall: 0.575, precision: 0.575187969924812, f_beta: 0.5747342088805503
train: step: 2129, loss: 0.6927156448364258, acc: 0.525, recall: 0.525, precision: 0.5255754475703325, f_beta: 0.5223130106851037
train: step: 2130, loss: 0.6940826773643494, acc: 0.45, recall: 0.45, precision: 0.421875, f_beta: 0.39560439560439553
train: step: 2131, loss: 0.6921136975288391, acc: 0.475, recall: 0.47500000000000003, precision: 0.45670995670995673, f_beta: 0.41299790356394134
train: step: 2132, loss: 0.6925358772277832, acc: 0.5, recall: 0.5, precision: 0.5, f_beta: 0.4505494505494506
train: step: 2133, loss: 0.6920152306556702, acc: 0.5, recall: 0.5, precision: 0.5, f_beta: 0.4987468671679198
train: step: 2134, loss: 0.6934940814971924, acc: 0.475, recall: 0.47500000000000003, precision: 0.4744245524296675, f_beta: 0.47203016970458833
train: step: 2135, loss: 0.6909408569335938, acc: 0.675, recall: 0.675, precision: 0.6994301994301995, f_beta: 0.6647324306898775
train: step: 2136, loss: 0.6944347620010376, acc: 0.475, recall: 0.475, precision: 0.46865203761755486, f_beta: 0.4470046082949308
train: step: 2137, loss: 0.694601833820343, acc: 0.45, recall: 0.45, precision: 0.421875, f_beta: 0.39560439560439553
train: step: 2138, loss: 0.69469153881073, acc: 0.425, recall: 0.42500000000000004, precision: 0.42000000000000004, f_beta: 0.4158730158730159
train: step: 2139, loss: 0.6943963170051575, acc: 0.45, recall: 0.44999999999999996, precision: 0.44505494505494503, f_beta: 0.4373401534526854
train: step: 2140, loss: 0.6940234899520874, acc: 0.45, recall: 0.45, precision: 0.4494949494949495, f_beta: 0.4486215538847118
train: step: 2141, loss: 0.6941893696784973, acc: 0.375, recall: 0.375, precision: 0.37468671679197996, f_beta: 0.37460913070669166
train: step: 2142, loss: 0.6920045018196106, acc: 0.55, recall: 0.55, precision: 0.578125, f_beta: 0.5054945054945055
train: step: 2143, loss: 0.6932007670402527, acc: 0.475, recall: 0.475, precision: 0.474937343358396, f_beta: 0.474671669793621
train: step: 2144, loss: 0.6938890218734741, acc: 0.425, recall: 0.42500000000000004, precision: 0.424812030075188, f_beta: 0.4246404002501563
train: step: 2145, loss: 0.692836344242096, acc: 0.6, recall: 0.6000000000000001, precision: 0.6190476190476191, f_beta: 0.5833333333333333
train: step: 2146, loss: 0.6911797523498535, acc: 0.575, recall: 0.575, precision: 0.5854700854700855, f_beta: 0.5615731785944552
train: step: 2147, loss: 0.6913453936576843, acc: 0.5, recall: 0.5, precision: 0.5, f_beta: 0.47916666666666663
train: step: 2148, loss: 0.6943203806877136, acc: 0.475, recall: 0.475, precision: 0.47150997150997154, f_beta: 0.4584139264990329
train: step: 2149, loss: 0.6941242814064026, acc: 0.4, recall: 0.4, precision: 0.30392156862745096, f_beta: 0.3162393162393162
train: step: 2150, loss: 0.6929343938827515, acc: 0.525, recall: 0.5249999999999999, precision: 0.5284900284900285, f_beta: 0.5099935525467441
train: step: 2151, loss: 0.692264199256897, acc: 0.475, recall: 0.475, precision: 0.44285714285714284, f_beta: 0.38909090909090915
train: step: 2152, loss: 0.6938535571098328, acc: 0.525, recall: 0.525, precision: 0.525062656641604, f_beta: 0.5247029393370857
train: step: 2153, loss: 0.6940589547157288, acc: 0.45, recall: 0.45, precision: 0.421875, f_beta: 0.39560439560439553
train: step: 2154, loss: 0.6926344633102417, acc: 0.6, recall: 0.6, precision: 0.6098901098901099, f_beta: 0.5907928388746804
train: step: 2155, loss: 0.691890299320221, acc: 0.575, recall: 0.575, precision: 0.60752688172043, f_beta: 0.5402298850574712
train: step: 2156, loss: 0.694299042224884, acc: 0.45, recall: 0.44999999999999996, precision: 0.43333333333333335, f_beta: 0.41333333333333333
train: step: 2157, loss: 0.6927391290664673, acc: 0.5, recall: 0.5, precision: 0.5, f_beta: 0.4666666666666667
train: step: 2158, loss: 0.6933472156524658, acc: 0.525, recall: 0.525, precision: 0.5255754475703325, f_beta: 0.5223130106851037
train: step: 2159, loss: 0.6945632100105286, acc: 0.375, recall: 0.375, precision: 0.3666666666666667, f_beta: 0.3650793650793651
train: step: 2160, loss: 0.6928377151489258, acc: 0.575, recall: 0.575, precision: 0.575187969924812, f_beta: 0.5747342088805503
train: step: 2161, loss: 0.6926144957542419, acc: 0.525, recall: 0.525, precision: 0.525062656641604, f_beta: 0.5247029393370857
train: step: 2162, loss: 0.6917327046394348, acc: 0.575, recall: 0.575, precision: 0.5854700854700855, f_beta: 0.5615731785944552
train: step: 2163, loss: 0.6932579278945923, acc: 0.4, recall: 0.4, precision: 0.39583333333333337, f_beta: 0.3939393939393939
train: step: 2164, loss: 0.6925758123397827, acc: 0.525, recall: 0.525, precision: 0.5266666666666666, f_beta: 0.5174603174603175
train: step: 2165, loss: 0.6924377679824829, acc: 0.55, recall: 0.55, precision: 0.5980392156862745, f_beta: 0.4871794871794872
train: step: 2166, loss: 0.6940287351608276, acc: 0.5, recall: 0.5, precision: 0.5, f_beta: 0.4949494949494949
train: step: 2167, loss: 0.6934652924537659, acc: 0.5, recall: 0.5, precision: 0.5, f_beta: 0.40476190476190477
train: step: 2168, loss: 0.6960009336471558, acc: 0.35, recall: 0.35, precision: 0.34375, f_beta: 0.3434343434343434
train: step: 2169, loss: 0.6909297704696655, acc: 0.65, recall: 0.65, precision: 0.65, f_beta: 0.65
train: step: 2170, loss: 0.693381130695343, acc: 0.475, recall: 0.475, precision: 0.44285714285714284, f_beta: 0.38909090909090915
train: step: 2171, loss: 0.6933883428573608, acc: 0.525, recall: 0.525, precision: 0.5313479623824451, f_beta: 0.4996708360763661
train: step: 2172, loss: 0.6919220685958862, acc: 0.525, recall: 0.525, precision: 0.5571428571428572, f_beta: 0.44727272727272727
train: step: 2173, loss: 0.6913530230522156, acc: 0.575, recall: 0.575, precision: 0.6714285714285715, f_beta: 0.5054545454545454
train: step: 2174, loss: 0.6924323439598083, acc: 0.55, recall: 0.55, precision: 0.5505050505050505, f_beta: 0.5488721804511277
train: step: 2175, loss: 0.6923872828483582, acc: 0.575, recall: 0.575, precision: 0.5800000000000001, f_beta: 0.5682539682539682
train: step: 2176, loss: 0.6907888054847717, acc: 0.575, recall: 0.575, precision: 0.5767263427109974, f_beta: 0.5725958516656191
train: step: 2177, loss: 0.6912556886672974, acc: 0.625, recall: 0.625, precision: 0.6333333333333333, f_beta: 0.6190476190476191
train: step: 2178, loss: 0.6929377317428589, acc: 0.575, recall: 0.575, precision: 0.575187969924812, f_beta: 0.5747342088805503
train: step: 2179, loss: 0.6934458017349243, acc: 0.55, recall: 0.55, precision: 0.5666666666666667, f_beta: 0.52
train: step: 2180, loss: 0.688130259513855, acc: 0.6, recall: 0.6000000000000001, precision: 0.6190476190476191, f_beta: 0.5833333333333333
train: step: 2181, loss: 0.6931997537612915, acc: 0.55, recall: 0.55, precision: 0.5595238095238095, f_beta: 0.53125
train: step: 2182, loss: 0.691695511341095, acc: 0.55, recall: 0.55, precision: 0.5505050505050505, f_beta: 0.5488721804511277
train: step: 2183, loss: 0.6929581761360168, acc: 0.525, recall: 0.525, precision: 0.5255754475703325, f_beta: 0.5223130106851037
train: step: 2184, loss: 0.6918953061103821, acc: 0.55, recall: 0.55, precision: 0.55, f_beta: 0.55
train: step: 2185, loss: 0.6909626126289368, acc: 0.575, recall: 0.575, precision: 0.575187969924812, f_beta: 0.5747342088805503
train: step: 2186, loss: 0.6928023099899292, acc: 0.5, recall: 0.5, precision: 0.5, f_beta: 0.47916666666666663
train: step: 2187, loss: 0.6921594738960266, acc: 0.5, recall: 0.5, precision: 0.5, f_beta: 0.4987468671679198
train: step: 2188, loss: 0.6927912831306458, acc: 0.475, recall: 0.475, precision: 0.46415770609318996, f_beta: 0.43204868154158216
train: step: 2189, loss: 0.6947972774505615, acc: 0.425, recall: 0.42500000000000004, precision: 0.42000000000000004, f_beta: 0.4158730158730159
train: step: 2190, loss: 0.6927305459976196, acc: 0.5, recall: 0.5, precision: 0.5, f_beta: 0.4987468671679198
train: step: 2191, loss: 0.6923142671585083, acc: 0.55, recall: 0.55, precision: 0.554945054945055, f_beta: 0.5396419437340153
train: step: 2192, loss: 0.6945474743843079, acc: 0.4, recall: 0.4, precision: 0.3666666666666667, f_beta: 0.36
train: step: 2193, loss: 0.6929870247840881, acc: 0.475, recall: 0.475, precision: 0.46415770609318996, f_beta: 0.43204868154158216
train: step: 2194, loss: 0.693785548210144, acc: 0.375, recall: 0.375, precision: 0.3666666666666667, f_beta: 0.3650793650793651
train: step: 2195, loss: 0.6937822103500366, acc: 0.475, recall: 0.475, precision: 0.47150997150997154, f_beta: 0.4584139264990329
train: step: 2196, loss: 0.693078875541687, acc: 0.55, recall: 0.55, precision: 0.554945054945055, f_beta: 0.5396419437340153
train: step: 2197, loss: 0.6947207450866699, acc: 0.4, recall: 0.4, precision: 0.38095238095238093, f_beta: 0.375
train: step: 2198, loss: 0.6917697787284851, acc: 0.55, recall: 0.55, precision: 0.554945054945055, f_beta: 0.5396419437340153
train: step: 2199, loss: 0.6944662928581238, acc: 0.5, recall: 0.5, precision: 0.5, f_beta: 0.43019943019943013
train: step: 2200, loss: 0.6945828199386597, acc: 0.5, recall: 0.5, precision: 0.5, f_beta: 0.5
train: step: 2201, loss: 0.694708526134491, acc: 0.475, recall: 0.475, precision: 0.44285714285714284, f_beta: 0.38909090909090915
train: step: 2202, loss: 0.692109227180481, acc: 0.575, recall: 0.575, precision: 0.60752688172043, f_beta: 0.5402298850574712
train: step: 2203, loss: 0.6955752968788147, acc: 0.3, recall: 0.30000000000000004, precision: 0.29166666666666663, f_beta: 0.29292929292929293
train: step: 2204, loss: 0.6963917016983032, acc: 0.475, recall: 0.475, precision: 0.44285714285714284, f_beta: 0.38909090909090915
train: step: 2205, loss: 0.6911917924880981, acc: 0.525, recall: 0.525, precision: 0.53584229390681, f_beta: 0.4861392832995267
train: step: 2206, loss: 0.6942591667175293, acc: 0.425, recall: 0.425, precision: 0.42327365728900257, f_beta: 0.4217473287240729
train: step: 2207, loss: 0.6924454569816589, acc: 0.45, recall: 0.44999999999999996, precision: 0.43333333333333335, f_beta: 0.41333333333333333
train: step: 2208, loss: 0.69377201795578, acc: 0.425, recall: 0.42500000000000004, precision: 0.4059561128526646, f_beta: 0.39433838051349573
train: step: 2209, loss: 0.6924557685852051, acc: 0.525, recall: 0.525, precision: 0.525062656641604, f_beta: 0.5247029393370857
train: step: 2210, loss: 0.6951082944869995, acc: 0.425, recall: 0.42500000000000004, precision: 0.424812030075188, f_beta: 0.4246404002501563
train: step: 2211, loss: 0.6945063471794128, acc: 0.475, recall: 0.47500000000000003, precision: 0.4744245524296675, f_beta: 0.47203016970458833
train: step: 2212, loss: 0.694287896156311, acc: 0.5, recall: 0.5, precision: 0.5, f_beta: 0.4987468671679198
train: step: 2213, loss: 0.6923689246177673, acc: 0.575, recall: 0.575, precision: 0.5854700854700855, f_beta: 0.5615731785944552
train: step: 2214, loss: 0.6942938566207886, acc: 0.375, recall: 0.375, precision: 0.3721227621483376, f_beta: 0.3714644877435575
train: step: 2215, loss: 0.6933369636535645, acc: 0.45, recall: 0.44999999999999996, precision: 0.44505494505494503, f_beta: 0.4373401534526854
train: step: 2216, loss: 0.6936022043228149, acc: 0.525, recall: 0.5249999999999999, precision: 0.5284900284900285, f_beta: 0.5099935525467441
train: step: 2217, loss: 0.6946426033973694, acc: 0.525, recall: 0.5249999999999999, precision: 0.5284900284900285, f_beta: 0.5099935525467441
train: step: 2218, loss: 0.6943502426147461, acc: 0.425, recall: 0.425, precision: 0.42327365728900257, f_beta: 0.4217473287240729
train: step: 2219, loss: 0.6938901543617249, acc: 0.475, recall: 0.475, precision: 0.474937343358396, f_beta: 0.474671669793621
train: step: 2220, loss: 0.6907952427864075, acc: 0.675, recall: 0.675, precision: 0.6994301994301995, f_beta: 0.6647324306898775
train: step: 2221, loss: 0.6953004598617554, acc: 0.5, recall: 0.5, precision: 0.5, f_beta: 0.4949494949494949
train: step: 2222, loss: 0.6916691660881042, acc: 0.65, recall: 0.65, precision: 0.6785714285714286, f_beta: 0.6354166666666667
train: step: 2223, loss: 0.6917120218276978, acc: 0.55, recall: 0.55, precision: 0.5505050505050505, f_beta: 0.5488721804511277
train: step: 2224, loss: 0.6934199333190918, acc: 0.5, recall: 0.5, precision: 0.5, f_beta: 0.4666666666666667
train: step: 2225, loss: 0.6948860883712769, acc: 0.35, recall: 0.35, precision: 0.34375, f_beta: 0.3434343434343434
train: step: 2226, loss: 0.6922990679740906, acc: 0.625, recall: 0.625, precision: 0.6333333333333333, f_beta: 0.6190476190476191
train: step: 2227, loss: 0.6923900842666626, acc: 0.575, recall: 0.575, precision: 0.5940438871473355, f_beta: 0.5523370638578011
train: step: 2228, loss: 0.6930626034736633, acc: 0.45, recall: 0.45, precision: 0.421875, f_beta: 0.39560439560439553
train: step: 2229, loss: 0.6931694746017456, acc: 0.525, recall: 0.525, precision: 0.5432900432900433, f_beta: 0.46890286512928026
train: step: 2230, loss: 0.6927970051765442, acc: 0.45, recall: 0.44999999999999996, precision: 0.44505494505494503, f_beta: 0.4373401534526854
train: step: 2231, loss: 0.692751944065094, acc: 0.5, recall: 0.5, precision: 0.5, f_beta: 0.4884910485933504
train: step: 2232, loss: 0.6933509111404419, acc: 0.45, recall: 0.45, precision: 0.44791666666666663, f_beta: 0.4444444444444444
train: step: 2233, loss: 0.6923578977584839, acc: 0.475, recall: 0.475, precision: 0.47150997150997154, f_beta: 0.4584139264990329
train: step: 2234, loss: 0.6930433511734009, acc: 0.55, recall: 0.55, precision: 0.5520833333333333, f_beta: 0.5454545454545454
train: step: 2235, loss: 0.6933754086494446, acc: 0.5, recall: 0.5, precision: 0.5, f_beta: 0.4884910485933504
train: step: 2236, loss: 0.6921507120132446, acc: 0.575, recall: 0.575, precision: 0.5940438871473355, f_beta: 0.5523370638578011
train: step: 2237, loss: 0.6908484697341919, acc: 0.525, recall: 0.525, precision: 0.5432900432900433, f_beta: 0.46890286512928026
train: step: 2238, loss: 0.6920229196548462, acc: 0.5, recall: 0.5, precision: 0.5, f_beta: 0.43019943019943013
train: step: 2239, loss: 0.6939785480499268, acc: 0.4, recall: 0.4, precision: 0.398989898989899, f_beta: 0.39849624060150374
train: step: 2240, loss: 0.6929084658622742, acc: 0.45, recall: 0.45, precision: 0.44791666666666663, f_beta: 0.4444444444444444
train: step: 2241, loss: 0.6915773153305054, acc: 0.475, recall: 0.47500000000000003, precision: 0.45670995670995673, f_beta: 0.41299790356394134
train: step: 2242, loss: 0.6912738084793091, acc: 0.625, recall: 0.625, precision: 0.6567398119122256, f_beta: 0.6050032916392363
train: step: 2243, loss: 0.6926857233047485, acc: 0.425, recall: 0.42500000000000004, precision: 0.42000000000000004, f_beta: 0.4158730158730159
train: step: 2244, loss: 0.6932279467582703, acc: 0.475, recall: 0.475, precision: 0.47333333333333333, f_beta: 0.4666666666666666
train: step: 2245, loss: 0.6933912038803101, acc: 0.5, recall: 0.5, precision: 0.5, f_beta: 0.4949494949494949
train: step: 2246, loss: 0.6922491788864136, acc: 0.5, recall: 0.5, precision: 0.5, f_beta: 0.47916666666666663
train: step: 2247, loss: 0.6950730085372925, acc: 0.425, recall: 0.42500000000000004, precision: 0.42000000000000004, f_beta: 0.4158730158730159
train: step: 2248, loss: 0.6936893463134766, acc: 0.5, recall: 0.5, precision: 0.5, f_beta: 0.4884910485933504
train: step: 2249, loss: 0.6918150186538696, acc: 0.575, recall: 0.575, precision: 0.60752688172043, f_beta: 0.5402298850574712
train: step: 2250, loss: 0.6922438740730286, acc: 0.55, recall: 0.55, precision: 0.5520833333333333, f_beta: 0.5454545454545454
train: step: 2251, loss: 0.6895986199378967, acc: 0.65, recall: 0.65, precision: 0.65, f_beta: 0.65
train: step: 2252, loss: 0.6919978857040405, acc: 0.5, recall: 0.5, precision: 0.5, f_beta: 0.5
train: step: 2253, loss: 0.6943272352218628, acc: 0.475, recall: 0.475, precision: 0.46415770609318996, f_beta: 0.43204868154158216
train: step: 2254, loss: 0.693859875202179, acc: 0.55, recall: 0.55, precision: 0.5505050505050505, f_beta: 0.5488721804511277
train: step: 2255, loss: 0.6936867833137512, acc: 0.475, recall: 0.475, precision: 0.46865203761755486, f_beta: 0.4470046082949308
train: step: 2256, loss: 0.6944871544837952, acc: 0.475, recall: 0.475, precision: 0.44285714285714284, f_beta: 0.38909090909090915
train: step: 2257, loss: 0.6907752156257629, acc: 0.525, recall: 0.525, precision: 0.5571428571428572, f_beta: 0.44727272727272727
train: step: 2258, loss: 0.6931968927383423, acc: 0.5, recall: 0.5, precision: 0.5, f_beta: 0.4884910485933504
train: step: 2259, loss: 0.6993046998977661, acc: 0.5, recall: 0.5, precision: 0.5, f_beta: 0.43019943019943013
train: step: 2260, loss: 0.692787766456604, acc: 0.55, recall: 0.55, precision: 0.5505050505050505, f_beta: 0.5488721804511277
train: step: 2261, loss: 0.6920042037963867, acc: 0.6, recall: 0.6000000000000001, precision: 0.601010101010101, f_beta: 0.5989974937343359
train: step: 2262, loss: 0.6926096677780151, acc: 0.5, recall: 0.5, precision: 0.5, f_beta: 0.4949494949494949
train: step: 2263, loss: 0.6932235956192017, acc: 0.525, recall: 0.525, precision: 0.5266666666666666, f_beta: 0.5174603174603175
train: step: 2264, loss: 0.6926838755607605, acc: 0.5, recall: 0.5, precision: 0.5, f_beta: 0.47916666666666663
train: step: 2265, loss: 0.6931459307670593, acc: 0.5, recall: 0.5, precision: 0.5, f_beta: 0.4987468671679198
train: step: 2266, loss: 0.6921960711479187, acc: 0.5, recall: 0.5, precision: 0.5, f_beta: 0.3730407523510972
train: step: 2267, loss: 0.6902164220809937, acc: 0.55, recall: 0.5499999999999999, precision: 0.6388888888888888, f_beta: 0.46428571428571425
train: step: 2268, loss: 0.6963487863540649, acc: 0.4, recall: 0.39999999999999997, precision: 0.34375, f_beta: 0.34065934065934067
train: step: 2269, loss: 0.6919658780097961, acc: 0.575, recall: 0.575, precision: 0.575187969924812, f_beta: 0.5747342088805503
train: step: 2270, loss: 0.6811416149139404, acc: 0.6, recall: 0.6, precision: 0.6, f_beta: 0.6
train: step: 2271, loss: 0.6951277256011963, acc: 0.35, recall: 0.35, precision: 0.35, f_beta: 0.35
train: step: 2272, loss: 0.6933091282844543, acc: 0.525, recall: 0.525, precision: 0.5432900432900433, f_beta: 0.46890286512928026
train: step: 2273, loss: 0.6883314847946167, acc: 0.525, recall: 0.525, precision: 0.7564102564102564, f_beta: 0.38660209846650523
train: step: 2274, loss: 0.6920161247253418, acc: 0.6, recall: 0.6000000000000001, precision: 0.601010101010101, f_beta: 0.5989974937343359
train: step: 2275, loss: 0.6896228790283203, acc: 0.625, recall: 0.625, precision: 0.7164502164502164, f_beta: 0.5807127882599581
train: step: 2276, loss: 0.6948181390762329, acc: 0.425, recall: 0.42500000000000004, precision: 0.42000000000000004, f_beta: 0.4158730158730159
train: step: 2277, loss: 0.6950114965438843, acc: 0.375, recall: 0.375, precision: 0.3666666666666667, f_beta: 0.3650793650793651
train: step: 2278, loss: 0.6621122360229492, acc: 0.65, recall: 0.65, precision: 0.6648351648351649, f_beta: 0.6419437340153452
train: step: 2279, loss: 0.6959908604621887, acc: 0.425, recall: 0.42500000000000004, precision: 0.42000000000000004, f_beta: 0.4158730158730159
train: step: 2280, loss: 0.6895415186882019, acc: 0.725, recall: 0.7250000000000001, precision: 0.7301790281329923, f_beta: 0.7234443746071653
train: step: 2281, loss: 0.6906062960624695, acc: 0.525, recall: 0.525, precision: 0.53584229390681, f_beta: 0.4861392832995267
train: step: 2282, loss: 0.6906269192695618, acc: 0.525, recall: 0.525, precision: 0.5432900432900433, f_beta: 0.46890286512928026
train: step: 2283, loss: 0.6836898922920227, acc: 0.575, recall: 0.575, precision: 0.60752688172043, f_beta: 0.5402298850574712
train: step: 2284, loss: 0.6934715509414673, acc: 0.5, recall: 0.5, precision: 0.5, f_beta: 0.4505494505494506
train: step: 2285, loss: 0.6924874186515808, acc: 0.45, recall: 0.45, precision: 0.45, f_beta: 0.45
train: step: 2286, loss: 0.695330023765564, acc: 0.45, recall: 0.44999999999999996, precision: 0.43333333333333335, f_beta: 0.41333333333333333
train: step: 2287, loss: 0.7023977041244507, acc: 0.475, recall: 0.475, precision: 0.44285714285714284, f_beta: 0.38909090909090915
train: step: 2288, loss: 0.6941437721252441, acc: 0.45, recall: 0.45, precision: 0.421875, f_beta: 0.39560439560439553
train: step: 2289, loss: 0.6929615139961243, acc: 0.55, recall: 0.55, precision: 0.5520833333333333, f_beta: 0.5454545454545454
train: step: 2290, loss: 0.6915712356567383, acc: 0.6, recall: 0.6, precision: 0.6098901098901099, f_beta: 0.5907928388746804
train: step: 2291, loss: 0.6936074495315552, acc: 0.425, recall: 0.425, precision: 0.4145299145299145, f_beta: 0.40683430045132174
train: step: 2292, loss: 0.692848801612854, acc: 0.425, recall: 0.42500000000000004, precision: 0.424812030075188, f_beta: 0.4246404002501563
train: step: 2293, loss: 0.6866251230239868, acc: 0.5, recall: 0.5, precision: 0.5, f_beta: 0.40476190476190477
train: step: 2294, loss: 0.6943682432174683, acc: 0.475, recall: 0.475, precision: 0.46415770609318996, f_beta: 0.43204868154158216
train: step: 2295, loss: 0.6922725439071655, acc: 0.525, recall: 0.525, precision: 0.5255754475703325, f_beta: 0.5223130106851037
train: step: 2296, loss: 0.6918002963066101, acc: 0.6, recall: 0.6, precision: 0.6, f_beta: 0.6
train: step: 2297, loss: 0.6957005262374878, acc: 0.375, recall: 0.375, precision: 0.37468671679197996, f_beta: 0.37460913070669166
train: step: 2298, loss: 0.6775606274604797, acc: 0.575, recall: 0.575, precision: 0.7702702702702703, f_beta: 0.4813119755911518
train: step: 2299, loss: 0.6935513615608215, acc: 0.45, recall: 0.45, precision: 0.45, f_beta: 0.45
train: step: 2300, loss: 0.6933547854423523, acc: 0.525, recall: 0.525, precision: 0.5313479623824451, f_beta: 0.4996708360763661
train: step: 2301, loss: 0.6738629937171936, acc: 0.6, recall: 0.6000000000000001, precision: 0.601010101010101, f_beta: 0.5989974937343359
train: step: 2302, loss: 0.6937607526779175, acc: 0.525, recall: 0.525, precision: 0.5255754475703325, f_beta: 0.5223130106851037
train: step: 2303, loss: 0.6692940592765808, acc: 0.65, recall: 0.6499999999999999, precision: 0.734375, f_beta: 0.6153846153846154
train: step: 2304, loss: 0.6919858455657959, acc: 0.55, recall: 0.55, precision: 0.5520833333333333, f_beta: 0.5454545454545454
train: step: 2305, loss: 0.6972475051879883, acc: 0.325, recall: 0.32499999999999996, precision: 0.32456140350877194, f_beta: 0.324577861163227
train: step: 2306, loss: 0.6917295455932617, acc: 0.475, recall: 0.475, precision: 0.47150997150997154, f_beta: 0.4584139264990329
train: step: 2307, loss: 0.6928919553756714, acc: 0.55, recall: 0.55, precision: 0.5505050505050505, f_beta: 0.5488721804511277
train: step: 2308, loss: 0.6963769793510437, acc: 0.55, recall: 0.55, precision: 0.5666666666666667, f_beta: 0.52
train: step: 2309, loss: 0.6898882985115051, acc: 0.6, recall: 0.6, precision: 0.7777777777777778, f_beta: 0.5238095238095238
train: step: 2310, loss: 0.685771107673645, acc: 0.525, recall: 0.525, precision: 0.5432900432900433, f_beta: 0.46890286512928026
train: step: 2311, loss: 0.6717156171798706, acc: 0.625, recall: 0.625, precision: 0.6792114695340502, f_beta: 0.5943204868154157
train: step: 2312, loss: 0.6949900984764099, acc: 0.5, recall: 0.5, precision: 0.5, f_beta: 0.4884910485933504
train: step: 2313, loss: 0.6932483911514282, acc: 0.425, recall: 0.42500000000000004, precision: 0.42000000000000004, f_beta: 0.4158730158730159
train: step: 2314, loss: 0.6923065781593323, acc: 0.525, recall: 0.525, precision: 0.5266666666666666, f_beta: 0.5174603174603175
train: step: 2315, loss: 0.6937898993492126, acc: 0.5, recall: 0.5, precision: 0.5, f_beta: 0.4987468671679198
train: step: 2316, loss: 0.6882315874099731, acc: 0.6, recall: 0.6, precision: 0.696078431372549, f_beta: 0.5441595441595442
train: step: 2317, loss: 0.6931203603744507, acc: 0.525, recall: 0.525, precision: 0.5571428571428572, f_beta: 0.44727272727272727
train: step: 2318, loss: 0.692906379699707, acc: 0.525, recall: 0.525, precision: 0.5266666666666666, f_beta: 0.5174603174603175
train: step: 2319, loss: 0.6879044771194458, acc: 0.6, recall: 0.6, precision: 0.6098901098901099, f_beta: 0.5907928388746804
train: step: 2320, loss: 0.6920773386955261, acc: 0.6, recall: 0.6000000000000001, precision: 0.601010101010101, f_beta: 0.5989974937343359
train: step: 2321, loss: 0.6942622065544128, acc: 0.5, recall: 0.5, precision: 0.5, f_beta: 0.4884910485933504
train: step: 2322, loss: 0.6957076191902161, acc: 0.475, recall: 0.475, precision: 0.47150997150997154, f_beta: 0.4584139264990329
train: step: 2323, loss: 0.6881431937217712, acc: 0.55, recall: 0.55, precision: 0.554945054945055, f_beta: 0.5396419437340153
train: step: 2324, loss: 0.6966927647590637, acc: 0.425, recall: 0.425, precision: 0.3924731182795699, f_beta: 0.37795807978363755
train: step: 2325, loss: 0.6933068037033081, acc: 0.475, recall: 0.475, precision: 0.474937343358396, f_beta: 0.474671669793621
train: step: 2326, loss: 0.6933245062828064, acc: 0.45, recall: 0.45, precision: 0.4494949494949495, f_beta: 0.4486215538847118
train: step: 2327, loss: 0.6749317646026611, acc: 0.6, recall: 0.6, precision: 0.7777777777777778, f_beta: 0.5238095238095238
train: step: 2328, loss: 0.6984518766403198, acc: 0.5, recall: 0.5, precision: 0.5, f_beta: 0.4884910485933504
train: step: 2329, loss: 0.6916632652282715, acc: 0.55, recall: 0.55, precision: 0.578125, f_beta: 0.5054945054945055
train: step: 2330, loss: 0.6926723122596741, acc: 0.55, recall: 0.55, precision: 0.5505050505050505, f_beta: 0.5488721804511277
train: step: 2331, loss: 0.6926719546318054, acc: 0.525, recall: 0.525, precision: 0.525062656641604, f_beta: 0.5247029393370857
train: step: 2332, loss: 0.6930330395698547, acc: 0.5, recall: 0.5, precision: 0.5, f_beta: 0.5
train: step: 2333, loss: 0.6896569728851318, acc: 0.625, recall: 0.625, precision: 0.6567398119122256, f_beta: 0.6050032916392363
train: step: 2334, loss: 0.6939173936843872, acc: 0.5, recall: 0.5, precision: 0.5, f_beta: 0.4666666666666667
train: step: 2335, loss: 0.688141942024231, acc: 0.55, recall: 0.55, precision: 0.5980392156862745, f_beta: 0.4871794871794872
train: step: 2336, loss: 0.6924150586128235, acc: 0.5, recall: 0.5, precision: 0.5, f_beta: 0.4884910485933504
train: step: 2337, loss: 0.7014241218566895, acc: 0.325, recall: 0.32499999999999996, precision: 0.32456140350877194, f_beta: 0.324577861163227
train: step: 2338, loss: 0.6835260987281799, acc: 0.6, recall: 0.6000000000000001, precision: 0.6190476190476191, f_beta: 0.5833333333333333
train: step: 2339, loss: 0.6914680600166321, acc: 0.525, recall: 0.525, precision: 0.5255754475703325, f_beta: 0.5223130106851037
train: step: 2340, loss: 0.6897162199020386, acc: 0.525, recall: 0.525, precision: 0.5571428571428572, f_beta: 0.44727272727272727
train: step: 2341, loss: 0.6805656552314758, acc: 0.55, recall: 0.55, precision: 0.763157894736842, f_beta: 0.4357366771159875
train: step: 2342, loss: 0.6926827430725098, acc: 0.475, recall: 0.475, precision: 0.474937343358396, f_beta: 0.474671669793621
train: step: 2343, loss: 0.6869331002235413, acc: 0.55, recall: 0.55, precision: 0.5980392156862745, f_beta: 0.4871794871794872
train: step: 2344, loss: 0.694440484046936, acc: 0.425, recall: 0.42500000000000004, precision: 0.424812030075188, f_beta: 0.4246404002501563
train: step: 2345, loss: 0.6938674449920654, acc: 0.5, recall: 0.5, precision: 0.5, f_beta: 0.4666666666666667
train: step: 2346, loss: 0.686905562877655, acc: 0.525, recall: 0.5249999999999999, precision: 0.5284900284900285, f_beta: 0.5099935525467441
train: step: 2347, loss: 0.6940776705741882, acc: 0.45, recall: 0.44999999999999996, precision: 0.43333333333333335, f_beta: 0.41333333333333333
train: step: 2348, loss: 0.6985776424407959, acc: 0.525, recall: 0.525, precision: 0.5571428571428572, f_beta: 0.44727272727272727
train: step: 2349, loss: 0.6915950179100037, acc: 0.575, recall: 0.575, precision: 0.60752688172043, f_beta: 0.5402298850574712
train: step: 2350, loss: 0.6912607550621033, acc: 0.55, recall: 0.55, precision: 0.55, f_beta: 0.55
train: step: 2351, loss: 0.6934831142425537, acc: 0.5, recall: 0.5, precision: 0.5, f_beta: 0.4884910485933504
train: step: 2352, loss: 0.6930803060531616, acc: 0.575, recall: 0.575, precision: 0.575187969924812, f_beta: 0.5747342088805503
train: step: 2353, loss: 0.6871567368507385, acc: 0.6, recall: 0.6, precision: 0.696078431372549, f_beta: 0.5441595441595442
train: step: 2354, loss: 0.6941227316856384, acc: 0.475, recall: 0.47500000000000003, precision: 0.40990990990990994, f_beta: 0.35926773455377575
train: step: 2355, loss: 0.6935475468635559, acc: 0.475, recall: 0.47500000000000003, precision: 0.4744245524296675, f_beta: 0.47203016970458833
train: step: 2356, loss: 0.6970912218093872, acc: 0.5, recall: 0.5, precision: 0.5, f_beta: 0.40476190476190477
train: step: 2357, loss: 0.6922498941421509, acc: 0.4, recall: 0.4, precision: 0.3901098901098901, f_beta: 0.38618925831202044
train: step: 2358, loss: 0.6752491593360901, acc: 0.6, recall: 0.6, precision: 0.6333333333333333, f_beta: 0.5733333333333334
train: step: 2359, loss: 0.6926550269126892, acc: 0.55, recall: 0.5499999999999999, precision: 0.6388888888888888, f_beta: 0.46428571428571425
train: step: 2360, loss: 0.69404137134552, acc: 0.375, recall: 0.375, precision: 0.3666666666666667, f_beta: 0.3650793650793651
train: step: 2361, loss: 0.693458080291748, acc: 0.525, recall: 0.525, precision: 0.5266666666666666, f_beta: 0.5174603174603175
train: step: 2362, loss: 0.6943255066871643, acc: 0.45, recall: 0.45, precision: 0.4494949494949495, f_beta: 0.4486215538847118
train: step: 2363, loss: 0.6840981245040894, acc: 0.65, recall: 0.65, precision: 0.7, f_beta: 0.6266666666666667
train: step: 2364, loss: 0.6941647529602051, acc: 0.425, recall: 0.425, precision: 0.42327365728900257, f_beta: 0.4217473287240729
train: step: 2365, loss: 0.6935028433799744, acc: 0.475, recall: 0.475, precision: 0.44285714285714284, f_beta: 0.38909090909090915
train: step: 2366, loss: 0.6966890096664429, acc: 0.45, recall: 0.45, precision: 0.4019607843137255, f_beta: 0.37321937321937326
train: step: 2367, loss: 0.6988973617553711, acc: 0.45, recall: 0.44999999999999996, precision: 0.43333333333333335, f_beta: 0.41333333333333333
train: step: 2368, loss: 0.6771892309188843, acc: 0.6, recall: 0.6, precision: 0.6098901098901099, f_beta: 0.5907928388746804
train: step: 2369, loss: 0.6920248866081238, acc: 0.6, recall: 0.6000000000000001, precision: 0.601010101010101, f_beta: 0.5989974937343359
train: step: 2370, loss: 0.6958343982696533, acc: 0.45, recall: 0.45, precision: 0.44047619047619047, f_beta: 0.42708333333333337
train: step: 2371, loss: 0.6941112875938416, acc: 0.425, recall: 0.425, precision: 0.42327365728900257, f_beta: 0.4217473287240729
train: step: 2372, loss: 0.6910759806632996, acc: 0.625, recall: 0.625, precision: 0.6424501424501424, f_beta: 0.6131528046421664
train: step: 2373, loss: 0.6932218074798584, acc: 0.575, recall: 0.575, precision: 0.5767263427109974, f_beta: 0.5725958516656191
train: step: 2374, loss: 0.697435736656189, acc: 0.4, recall: 0.4, precision: 0.398989898989899, f_beta: 0.39849624060150374
train: step: 2375, loss: 0.6802862286567688, acc: 0.625, recall: 0.625, precision: 0.6567398119122256, f_beta: 0.6050032916392363
train: step: 2376, loss: 0.677049458026886, acc: 0.625, recall: 0.625, precision: 0.6278772378516624, f_beta: 0.6228786926461345
train: step: 2377, loss: 0.6940702795982361, acc: 0.575, recall: 0.575, precision: 0.575187969924812, f_beta: 0.5747342088805503
train: step: 2378, loss: 0.6928186416625977, acc: 0.475, recall: 0.47500000000000003, precision: 0.4744245524296675, f_beta: 0.47203016970458833
train: step: 2379, loss: 0.6922978162765503, acc: 0.475, recall: 0.475, precision: 0.47333333333333333, f_beta: 0.4666666666666666
train: step: 2380, loss: 0.6947054862976074, acc: 0.375, recall: 0.375, precision: 0.3575498575498576, f_beta: 0.3552546744036106
train: step: 2381, loss: 0.6928618550300598, acc: 0.5, recall: 0.5, precision: 0.5, f_beta: 0.4949494949494949
train: step: 2382, loss: 0.6950021386146545, acc: 0.4, recall: 0.4, precision: 0.39583333333333337, f_beta: 0.3939393939393939
train: step: 2383, loss: 0.6916325688362122, acc: 0.45, recall: 0.44999999999999996, precision: 0.43333333333333335, f_beta: 0.41333333333333333
train: step: 2384, loss: 0.7010067105293274, acc: 0.35, recall: 0.35000000000000003, precision: 0.265625, f_beta: 0.28571428571428575
train: step: 2385, loss: 0.6933092474937439, acc: 0.525, recall: 0.525, precision: 0.5255754475703325, f_beta: 0.5223130106851037
train: step: 2386, loss: 0.7009825706481934, acc: 0.475, recall: 0.475, precision: 0.46415770609318996, f_beta: 0.43204868154158216
train: step: 2387, loss: 0.6924716830253601, acc: 0.6, recall: 0.6, precision: 0.6041666666666667, f_beta: 0.595959595959596
train: step: 2388, loss: 0.675425112247467, acc: 0.65, recall: 0.65, precision: 0.6785714285714286, f_beta: 0.6354166666666667
train: step: 2389, loss: 0.7164595723152161, acc: 0.3, recall: 0.3, precision: 0.2802197802197802, f_beta: 0.28388746803069054
train: step: 2390, loss: 0.662950873374939, acc: 0.65, recall: 0.65, precision: 0.6648351648351649, f_beta: 0.6419437340153452
train: step: 2391, loss: 0.6702862977981567, acc: 0.675, recall: 0.675, precision: 0.7508960573476702, f_beta: 0.6484110885733605
train: step: 2392, loss: 0.6914107203483582, acc: 0.55, recall: 0.55, precision: 0.55, f_beta: 0.55
train: step: 2393, loss: 0.6812079548835754, acc: 0.625, recall: 0.625, precision: 0.6567398119122256, f_beta: 0.6050032916392363
train: step: 2394, loss: 0.6830502152442932, acc: 0.575, recall: 0.575, precision: 0.5854700854700855, f_beta: 0.5615731785944552
train: step: 2395, loss: 0.6777154207229614, acc: 0.65, recall: 0.65, precision: 0.65, f_beta: 0.65
train: step: 2396, loss: 0.6965268850326538, acc: 0.5, recall: 0.5, precision: 0.5, f_beta: 0.4666666666666667
train: step: 2397, loss: 0.7193194627761841, acc: 0.3, recall: 0.3, precision: 0.297979797979798, f_beta: 0.2982456140350877
train: step: 2398, loss: 0.6893928647041321, acc: 0.575, recall: 0.575, precision: 0.5800000000000001, f_beta: 0.5682539682539682
train: step: 2399, loss: 0.6643126606941223, acc: 0.625, recall: 0.625, precision: 0.6424501424501424, f_beta: 0.6131528046421664
train: step: 2400, loss: 0.6886381506919861, acc: 0.625, recall: 0.625, precision: 0.6278772378516624, f_beta: 0.6228786926461345
train: step: 2401, loss: 0.6867682337760925, acc: 0.575, recall: 0.575, precision: 0.5940438871473355, f_beta: 0.5523370638578011
train: step: 2402, loss: 0.6935763955116272, acc: 0.475, recall: 0.475, precision: 0.474937343358396, f_beta: 0.474671669793621
train: step: 2403, loss: 0.7057853937149048, acc: 0.475, recall: 0.475, precision: 0.47150997150997154, f_beta: 0.4584139264990329
train: step: 2404, loss: 0.6929537057876587, acc: 0.5, recall: 0.5, precision: 0.5, f_beta: 0.4666666666666667
train: step: 2405, loss: 0.692310094833374, acc: 0.525, recall: 0.525, precision: 0.5432900432900433, f_beta: 0.46890286512928026
train: step: 2406, loss: 0.6839767694473267, acc: 0.575, recall: 0.575, precision: 0.5940438871473355, f_beta: 0.5523370638578011
train: step: 2407, loss: 0.7207258939743042, acc: 0.4, recall: 0.39999999999999997, precision: 0.34375, f_beta: 0.34065934065934067
train: step: 2408, loss: 0.6754733324050903, acc: 0.575, recall: 0.575, precision: 0.5800000000000001, f_beta: 0.5682539682539682
train: step: 2409, loss: 0.6982617974281311, acc: 0.5, recall: 0.5, precision: 0.5, f_beta: 0.4884910485933504
train: step: 2410, loss: 0.6812185049057007, acc: 0.55, recall: 0.55, precision: 0.55, f_beta: 0.55
train: step: 2411, loss: 0.6942342519760132, acc: 0.475, recall: 0.475, precision: 0.47333333333333333, f_beta: 0.4666666666666666
train: step: 2412, loss: 0.6930520534515381, acc: 0.475, recall: 0.475, precision: 0.474937343358396, f_beta: 0.474671669793621
train: step: 2413, loss: 0.6934075355529785, acc: 0.475, recall: 0.47500000000000003, precision: 0.4744245524296675, f_beta: 0.47203016970458833
train: step: 2414, loss: 0.6917198896408081, acc: 0.55, recall: 0.55, precision: 0.554945054945055, f_beta: 0.5396419437340153
train: step: 2415, loss: 0.6678099036216736, acc: 0.7, recall: 0.7, precision: 0.7197802197802198, f_beta: 0.6930946291560103
train: step: 2416, loss: 0.695207953453064, acc: 0.475, recall: 0.475, precision: 0.474937343358396, f_beta: 0.474671669793621
train: step: 2417, loss: 0.6611124277114868, acc: 0.6, recall: 0.6, precision: 0.6333333333333333, f_beta: 0.5733333333333334
train: step: 2418, loss: 0.6908909678459167, acc: 0.525, recall: 0.525, precision: 0.5313479623824451, f_beta: 0.4996708360763661
train: step: 2419, loss: 0.6863420605659485, acc: 0.55, recall: 0.55, precision: 0.5595238095238095, f_beta: 0.53125
train: step: 2420, loss: 0.6935335397720337, acc: 0.45, recall: 0.44999999999999996, precision: 0.44505494505494503, f_beta: 0.4373401534526854
train: step: 2421, loss: 0.6443396806716919, acc: 0.675, recall: 0.675, precision: 0.6994301994301995, f_beta: 0.6647324306898775
train: step: 2422, loss: 0.6833909749984741, acc: 0.575, recall: 0.575, precision: 0.5940438871473355, f_beta: 0.5523370638578011
train: step: 2423, loss: 0.6922522783279419, acc: 0.525, recall: 0.525, precision: 0.53584229390681, f_beta: 0.4861392832995267
train: step: 2424, loss: 0.704479455947876, acc: 0.425, recall: 0.42500000000000004, precision: 0.4059561128526646, f_beta: 0.39433838051349573
train: step: 2425, loss: 0.6866275072097778, acc: 0.575, recall: 0.575, precision: 0.5940438871473355, f_beta: 0.5523370638578011
train: step: 2426, loss: 0.6810774207115173, acc: 0.55, recall: 0.55, precision: 0.5595238095238095, f_beta: 0.53125
train: step: 2427, loss: 0.6901239156723022, acc: 0.525, recall: 0.525, precision: 0.53584229390681, f_beta: 0.4861392832995267
train: step: 2428, loss: 0.6941030025482178, acc: 0.45, recall: 0.45, precision: 0.4494949494949495, f_beta: 0.4486215538847118
train: step: 2429, loss: 0.6871625781059265, acc: 0.475, recall: 0.475, precision: 0.44285714285714284, f_beta: 0.38909090909090915
train: step: 2430, loss: 0.6990253329277039, acc: 0.5, recall: 0.5, precision: 0.5, f_beta: 0.4949494949494949
train: step: 2431, loss: 0.6935304403305054, acc: 0.525, recall: 0.525, precision: 0.5255754475703325, f_beta: 0.5223130106851037
train: step: 2432, loss: 0.6881479024887085, acc: 0.5, recall: 0.5, precision: 0.5, f_beta: 0.4884910485933504
train: step: 2433, loss: 0.7036970257759094, acc: 0.3, recall: 0.3, precision: 0.2619047619047619, f_beta: 0.2708333333333333
train: step: 2434, loss: 0.6754364967346191, acc: 0.65, recall: 0.65, precision: 0.7, f_beta: 0.6266666666666667
train: step: 2435, loss: 0.7045655250549316, acc: 0.5, recall: 0.5, precision: 0.5, f_beta: 0.40476190476190477
train: step: 2436, loss: 0.6928687691688538, acc: 0.475, recall: 0.475, precision: 0.47150997150997154, f_beta: 0.4584139264990329
train: step: 2437, loss: 0.6559556126594543, acc: 0.675, recall: 0.675, precision: 0.6994301994301995, f_beta: 0.6647324306898775
train: step: 2438, loss: 0.698706865310669, acc: 0.525, recall: 0.525, precision: 0.5313479623824451, f_beta: 0.4996708360763661
train: step: 2439, loss: 0.7199365496635437, acc: 0.425, recall: 0.42500000000000004, precision: 0.424812030075188, f_beta: 0.4246404002501563
train: step: 2440, loss: 0.715990424156189, acc: 0.45, recall: 0.45, precision: 0.4494949494949495, f_beta: 0.4486215538847118
train: step: 2441, loss: 0.6859937906265259, acc: 0.575, recall: 0.575, precision: 0.5940438871473355, f_beta: 0.5523370638578011
train: step: 2442, loss: 0.7037142515182495, acc: 0.525, recall: 0.525, precision: 0.5313479623824451, f_beta: 0.4996708360763661
train: step: 2443, loss: 0.6932893395423889, acc: 0.475, recall: 0.475, precision: 0.47150997150997154, f_beta: 0.4584139264990329
train: step: 2444, loss: 0.6915606260299683, acc: 0.525, recall: 0.525, precision: 0.53584229390681, f_beta: 0.4861392832995267
train: step: 2445, loss: 0.6943939924240112, acc: 0.475, recall: 0.475, precision: 0.46865203761755486, f_beta: 0.4470046082949308
train: step: 2446, loss: 0.6916044354438782, acc: 0.45, recall: 0.45, precision: 0.421875, f_beta: 0.39560439560439553
train: step: 2447, loss: 0.6935884356498718, acc: 0.45, recall: 0.45, precision: 0.44791666666666663, f_beta: 0.4444444444444444
train: step: 2448, loss: 0.6984177827835083, acc: 0.525, recall: 0.525, precision: 0.5313479623824451, f_beta: 0.4996708360763661
train: step: 2449, loss: 0.7125879526138306, acc: 0.45, recall: 0.45, precision: 0.44047619047619047, f_beta: 0.42708333333333337
train: step: 2450, loss: 0.6929011344909668, acc: 0.525, recall: 0.525, precision: 0.525062656641604, f_beta: 0.5247029393370857
train: step: 2451, loss: 0.699928879737854, acc: 0.425, recall: 0.42500000000000004, precision: 0.42000000000000004, f_beta: 0.4158730158730159
train: step: 2452, loss: 0.6874926686286926, acc: 0.55, recall: 0.5499999999999999, precision: 0.6388888888888888, f_beta: 0.46428571428571425
train: step: 2453, loss: 0.7110081911087036, acc: 0.4, recall: 0.39999999999999997, precision: 0.34375, f_beta: 0.34065934065934067
train: step: 2454, loss: 0.6749950647354126, acc: 0.7, recall: 0.7, precision: 0.7666666666666666, f_beta: 0.6799999999999999
train: step: 2455, loss: 0.689391016960144, acc: 0.55, recall: 0.55, precision: 0.5666666666666667, f_beta: 0.52
train: step: 2456, loss: 0.6950987577438354, acc: 0.35, recall: 0.35, precision: 0.34375, f_beta: 0.3434343434343434
train: step: 2457, loss: 0.6856560707092285, acc: 0.6, recall: 0.6, precision: 0.696078431372549, f_beta: 0.5441595441595442
train: step: 2458, loss: 0.6966747045516968, acc: 0.475, recall: 0.47500000000000003, precision: 0.45670995670995673, f_beta: 0.41299790356394134
train: step: 2459, loss: 0.6936454772949219, acc: 0.5, recall: 0.5, precision: 0.5, f_beta: 0.4987468671679198
train: step: 2460, loss: 0.6825283169746399, acc: 0.575, recall: 0.575, precision: 0.5800000000000001, f_beta: 0.5682539682539682
train: step: 2461, loss: 0.6653065085411072, acc: 0.625, recall: 0.625, precision: 0.6567398119122256, f_beta: 0.6050032916392363
train: step: 2462, loss: 0.6943327188491821, acc: 0.65, recall: 0.65, precision: 0.6785714285714286, f_beta: 0.6354166666666667
train: step: 2463, loss: 0.6890724897384644, acc: 0.525, recall: 0.525, precision: 0.5266666666666666, f_beta: 0.5174603174603175
train: step: 2464, loss: 0.7065362930297852, acc: 0.425, recall: 0.425, precision: 0.42327365728900257, f_beta: 0.4217473287240729
train: step: 2465, loss: 0.6639907360076904, acc: 0.65, recall: 0.6499999999999999, precision: 0.6515151515151515, f_beta: 0.6491228070175439
train: step: 2466, loss: 0.6981104612350464, acc: 0.475, recall: 0.475, precision: 0.474937343358396, f_beta: 0.474671669793621
train: step: 2467, loss: 0.6448183655738831, acc: 0.8, recall: 0.8, precision: 0.803030303030303, f_beta: 0.7994987468671679
train: step: 2468, loss: 0.7488180994987488, acc: 0.375, recall: 0.375, precision: 0.3721227621483376, f_beta: 0.3714644877435575
train: step: 2469, loss: 0.6974723935127258, acc: 0.5, recall: 0.5, precision: 0.5, f_beta: 0.4987468671679198
train: step: 2470, loss: 0.7036248445510864, acc: 0.525, recall: 0.525, precision: 0.5266666666666666, f_beta: 0.5174603174603175
train: step: 2471, loss: 0.6169508099555969, acc: 0.75, recall: 0.75, precision: 0.7525252525252526, f_beta: 0.7493734335839599
train: step: 2472, loss: 0.6903700828552246, acc: 0.55, recall: 0.55, precision: 0.5595238095238095, f_beta: 0.53125
train: step: 2473, loss: 0.692949116230011, acc: 0.525, recall: 0.525, precision: 0.525062656641604, f_beta: 0.5247029393370857
train: step: 2474, loss: 0.6892359852790833, acc: 0.55, recall: 0.55, precision: 0.5520833333333333, f_beta: 0.5454545454545454
train: step: 2475, loss: 0.6694610714912415, acc: 0.8, recall: 0.8, precision: 0.8571428571428572, f_beta: 0.7916666666666665
train: step: 2476, loss: 0.6934192180633545, acc: 0.5, recall: 0.5, precision: 0.5, f_beta: 0.4505494505494506
train: step: 2477, loss: 0.6753013730049133, acc: 0.65, recall: 0.6499999999999999, precision: 0.6515151515151515, f_beta: 0.6491228070175439
train: step: 2478, loss: 0.6882222890853882, acc: 0.575, recall: 0.575, precision: 0.575187969924812, f_beta: 0.5747342088805503
train: step: 2479, loss: 0.7022282481193542, acc: 0.45, recall: 0.45, precision: 0.45, f_beta: 0.45
train: step: 2480, loss: 0.6921319961547852, acc: 0.6, recall: 0.6000000000000001, precision: 0.6190476190476191, f_beta: 0.5833333333333333
train: step: 2481, loss: 0.707426905632019, acc: 0.4, recall: 0.4, precision: 0.38095238095238093, f_beta: 0.375
train: step: 2482, loss: 0.6834477186203003, acc: 0.6, recall: 0.6, precision: 0.6, f_beta: 0.6
train: step: 2483, loss: 0.6535808444023132, acc: 0.65, recall: 0.6499999999999999, precision: 0.6515151515151515, f_beta: 0.6491228070175439
train: step: 2484, loss: 0.689601719379425, acc: 0.625, recall: 0.625, precision: 0.6278772378516624, f_beta: 0.6228786926461345
train: step: 2485, loss: 0.691588819026947, acc: 0.65, recall: 0.65, precision: 0.7, f_beta: 0.6266666666666667
train: step: 2486, loss: 0.6910392045974731, acc: 0.675, recall: 0.675, precision: 0.6994301994301995, f_beta: 0.6647324306898775
train: step: 2487, loss: 0.6790370941162109, acc: 0.6, recall: 0.6, precision: 0.6333333333333333, f_beta: 0.5733333333333334
train: step: 2488, loss: 0.6940003633499146, acc: 0.5, recall: 0.5, precision: 0.5, f_beta: 0.4666666666666667
train: step: 2489, loss: 0.6925298571586609, acc: 0.4, recall: 0.4, precision: 0.38095238095238093, f_beta: 0.375
train: step: 2490, loss: 0.6987013816833496, acc: 0.475, recall: 0.475, precision: 0.47150997150997154, f_beta: 0.4584139264990329
train: step: 2491, loss: 0.6998070478439331, acc: 0.375, recall: 0.375, precision: 0.3721227621483376, f_beta: 0.3714644877435575
train: step: 2492, loss: 0.6915630102157593, acc: 0.65, recall: 0.65, precision: 0.6648351648351649, f_beta: 0.6419437340153452
train: step: 2493, loss: 0.6927086114883423, acc: 0.5, recall: 0.5, precision: 0.5, f_beta: 0.4884910485933504
train: step: 2494, loss: 0.6889458298683167, acc: 0.55, recall: 0.55, precision: 0.5505050505050505, f_beta: 0.5488721804511277
train: step: 2495, loss: 0.6914119124412537, acc: 0.5, recall: 0.5, precision: 0.5, f_beta: 0.4949494949494949
train: step: 2496, loss: 0.6839447021484375, acc: 0.625, recall: 0.625, precision: 0.62531328320802, f_beta: 0.6247654784240151
train: step: 2497, loss: 0.6905602216720581, acc: 0.55, recall: 0.55, precision: 0.554945054945055, f_beta: 0.5396419437340153
train: step: 2498, loss: 0.6898884773254395, acc: 0.525, recall: 0.5249999999999999, precision: 0.5284900284900285, f_beta: 0.5099935525467441
train: step: 2499, loss: 0.6910207867622375, acc: 0.55, recall: 0.55, precision: 0.55, f_beta: 0.55
train: step: 2500, loss: 0.6878668069839478, acc: 0.625, recall: 0.625, precision: 0.62531328320802, f_beta: 0.6247654784240151
train: step: 2501, loss: 0.7113674879074097, acc: 0.475, recall: 0.475, precision: 0.474937343358396, f_beta: 0.474671669793621
train: step: 2502, loss: 0.7071622014045715, acc: 0.475, recall: 0.475, precision: 0.47333333333333333, f_beta: 0.4666666666666666
train: step: 2503, loss: 0.6922965049743652, acc: 0.525, recall: 0.5249999999999999, precision: 0.5284900284900285, f_beta: 0.5099935525467441
train: step: 2504, loss: 0.6873747110366821, acc: 0.575, recall: 0.575, precision: 0.5767263427109974, f_beta: 0.5725958516656191
train: step: 2505, loss: 0.7249064445495605, acc: 0.475, recall: 0.475, precision: 0.47333333333333333, f_beta: 0.4666666666666666
train: step: 2506, loss: 0.6832553148269653, acc: 0.6, recall: 0.6, precision: 0.6098901098901099, f_beta: 0.5907928388746804
train: step: 2507, loss: 0.6920952796936035, acc: 0.575, recall: 0.575, precision: 0.5767263427109974, f_beta: 0.5725958516656191
train: step: 2508, loss: 0.673253059387207, acc: 0.625, recall: 0.625, precision: 0.6278772378516624, f_beta: 0.6228786926461345
train: step: 2509, loss: 0.6826866865158081, acc: 0.6, recall: 0.6, precision: 0.6, f_beta: 0.6
train: step: 2510, loss: 0.7001661062240601, acc: 0.375, recall: 0.375, precision: 0.3666666666666667, f_beta: 0.3650793650793651
train: step: 2511, loss: 0.7009990811347961, acc: 0.45, recall: 0.45, precision: 0.44047619047619047, f_beta: 0.42708333333333337
train: step: 2512, loss: 0.6929207444190979, acc: 0.475, recall: 0.475, precision: 0.474937343358396, f_beta: 0.474671669793621
train: step: 2513, loss: 0.6826326251029968, acc: 0.6, recall: 0.6, precision: 0.6041666666666667, f_beta: 0.595959595959596
train: step: 2514, loss: 0.6884272694587708, acc: 0.55, recall: 0.55, precision: 0.5505050505050505, f_beta: 0.5488721804511277
train: step: 2515, loss: 0.7273812294006348, acc: 0.35, recall: 0.35, precision: 0.34375, f_beta: 0.3434343434343434
train: step: 2516, loss: 0.6941918730735779, acc: 0.5, recall: 0.5, precision: 0.5, f_beta: 0.4949494949494949
train: step: 2517, loss: 0.6716116666793823, acc: 0.625, recall: 0.625, precision: 0.6278772378516624, f_beta: 0.6228786926461345
train: step: 2518, loss: 0.6936683058738708, acc: 0.45, recall: 0.44999999999999996, precision: 0.43333333333333335, f_beta: 0.41333333333333333
train: step: 2519, loss: 0.6872244477272034, acc: 0.625, recall: 0.625, precision: 0.6333333333333333, f_beta: 0.6190476190476191
train: step: 2520, loss: 0.6485215425491333, acc: 0.75, recall: 0.75, precision: 0.7525252525252526, f_beta: 0.7493734335839599
train: step: 2521, loss: 0.6505569219589233, acc: 0.65, recall: 0.65, precision: 0.65625, f_beta: 0.6464646464646464
train: step: 2522, loss: 0.7189565896987915, acc: 0.5, recall: 0.5, precision: 0.5, f_beta: 0.47916666666666663
train: step: 2523, loss: 0.6966444849967957, acc: 0.4, recall: 0.4, precision: 0.398989898989899, f_beta: 0.39849624060150374
train: step: 2524, loss: 0.6908727884292603, acc: 0.625, recall: 0.625, precision: 0.62531328320802, f_beta: 0.6247654784240151
train: step: 2525, loss: 0.7040590047836304, acc: 0.5, recall: 0.5, precision: 0.5, f_beta: 0.4884910485933504
train: step: 2526, loss: 0.6721910238265991, acc: 0.6, recall: 0.6, precision: 0.6098901098901099, f_beta: 0.5907928388746804
train: step: 2527, loss: 0.6960947513580322, acc: 0.5, recall: 0.5, precision: 0.5, f_beta: 0.4987468671679198
train: step: 2528, loss: 0.7008166313171387, acc: 0.45, recall: 0.45, precision: 0.44791666666666663, f_beta: 0.4444444444444444
train: step: 2529, loss: 0.6803978681564331, acc: 0.625, recall: 0.625, precision: 0.6424501424501424, f_beta: 0.6131528046421664
train: step: 2530, loss: 0.681378960609436, acc: 0.6, recall: 0.6000000000000001, precision: 0.601010101010101, f_beta: 0.5989974937343359
train: step: 2531, loss: 0.6830703616142273, acc: 0.575, recall: 0.575, precision: 0.60752688172043, f_beta: 0.5402298850574712
train: step: 2532, loss: 0.7019913792610168, acc: 0.475, recall: 0.475, precision: 0.44285714285714284, f_beta: 0.38909090909090915
train: step: 2533, loss: 0.6961193680763245, acc: 0.425, recall: 0.42500000000000004, precision: 0.32857142857142857, f_beta: 0.33090909090909093
train: step: 2534, loss: 0.6908356547355652, acc: 0.525, recall: 0.525, precision: 0.5313479623824451, f_beta: 0.4996708360763661
train: step: 2535, loss: 0.6883825063705444, acc: 0.575, recall: 0.575, precision: 0.5854700854700855, f_beta: 0.5615731785944552
train: step: 2536, loss: 0.6949737668037415, acc: 0.475, recall: 0.475, precision: 0.46865203761755486, f_beta: 0.4470046082949308
train: step: 2537, loss: 0.6929458379745483, acc: 0.55, recall: 0.55, precision: 0.5520833333333333, f_beta: 0.5454545454545454
train: step: 2538, loss: 0.6861542463302612, acc: 0.575, recall: 0.575, precision: 0.60752688172043, f_beta: 0.5402298850574712
train: step: 2539, loss: 0.6848703622817993, acc: 0.625, recall: 0.625, precision: 0.6567398119122256, f_beta: 0.6050032916392363
train: step: 2540, loss: 0.6826921701431274, acc: 0.6, recall: 0.6, precision: 0.6098901098901099, f_beta: 0.5907928388746804
train: step: 2541, loss: 0.679513692855835, acc: 0.625, recall: 0.625, precision: 0.6567398119122256, f_beta: 0.6050032916392363
train: step: 2542, loss: 0.6933590173721313, acc: 0.475, recall: 0.475, precision: 0.47333333333333333, f_beta: 0.4666666666666666
train: step: 2543, loss: 0.6851350665092468, acc: 0.65, recall: 0.6499999999999999, precision: 0.6515151515151515, f_beta: 0.6491228070175439
train: step: 2544, loss: 0.6651296019554138, acc: 0.65, recall: 0.65, precision: 0.7, f_beta: 0.6266666666666667
train: step: 2545, loss: 0.7125144004821777, acc: 0.525, recall: 0.525, precision: 0.5255754475703325, f_beta: 0.5223130106851037
train: step: 2546, loss: 0.6883394122123718, acc: 0.575, recall: 0.575, precision: 0.575187969924812, f_beta: 0.5747342088805503
train: step: 2547, loss: 0.6699347496032715, acc: 0.625, recall: 0.625, precision: 0.62531328320802, f_beta: 0.6247654784240151
train: step: 2548, loss: 0.6864474415779114, acc: 0.55, recall: 0.55, precision: 0.5595238095238095, f_beta: 0.53125
train: step: 2549, loss: 0.6564412713050842, acc: 0.7, recall: 0.7, precision: 0.7197802197802198, f_beta: 0.6930946291560103
train: step: 2550, loss: 0.6960030198097229, acc: 0.475, recall: 0.475, precision: 0.47150997150997154, f_beta: 0.4584139264990329
train: step: 2551, loss: 0.6871944665908813, acc: 0.55, recall: 0.55, precision: 0.578125, f_beta: 0.5054945054945055
train: step: 2552, loss: 0.6921135783195496, acc: 0.55, recall: 0.55, precision: 0.554945054945055, f_beta: 0.5396419437340153
train: step: 2553, loss: 0.6907238960266113, acc: 0.55, recall: 0.55, precision: 0.554945054945055, f_beta: 0.5396419437340153
train: step: 2554, loss: 0.657019317150116, acc: 0.675, recall: 0.675, precision: 0.6790281329923273, f_beta: 0.6731615336266499
train: step: 2555, loss: 0.6903688311576843, acc: 0.575, recall: 0.575, precision: 0.5767263427109974, f_beta: 0.5725958516656191
train: step: 2556, loss: 0.645098090171814, acc: 0.675, recall: 0.675, precision: 0.6790281329923273, f_beta: 0.6731615336266499
train: step: 2557, loss: 0.6938363313674927, acc: 0.5, recall: 0.5, precision: 0.5, f_beta: 0.4949494949494949
train: step: 2558, loss: 0.6856303811073303, acc: 0.625, recall: 0.625, precision: 0.62531328320802, f_beta: 0.6247654784240151
train: step: 2559, loss: 0.6923226118087769, acc: 0.5, recall: 0.5, precision: 0.5, f_beta: 0.4884910485933504
train: step: 2560, loss: 0.699143648147583, acc: 0.475, recall: 0.47500000000000003, precision: 0.4744245524296675, f_beta: 0.47203016970458833
train: step: 2561, loss: 0.7020084857940674, acc: 0.425, recall: 0.42500000000000004, precision: 0.424812030075188, f_beta: 0.4246404002501563
train: step: 2562, loss: 0.6530641913414001, acc: 0.675, recall: 0.675, precision: 0.6866666666666666, f_beta: 0.6698412698412699
train: step: 2563, loss: 0.6936629414558411, acc: 0.5, recall: 0.5, precision: 0.5, f_beta: 0.4884910485933504
train: step: 2564, loss: 0.683894157409668, acc: 0.625, recall: 0.625, precision: 0.6424501424501424, f_beta: 0.6131528046421664
train: step: 2565, loss: 0.6757467985153198, acc: 0.675, recall: 0.675, precision: 0.719435736677116, f_beta: 0.6576695194206715
train: step: 2566, loss: 0.6881193518638611, acc: 0.55, recall: 0.55, precision: 0.554945054945055, f_beta: 0.5396419437340153
train: step: 2567, loss: 0.6987165212631226, acc: 0.475, recall: 0.47500000000000003, precision: 0.4744245524296675, f_beta: 0.47203016970458833
train: step: 2568, loss: 0.6932526230812073, acc: 0.5, recall: 0.5, precision: 0.5, f_beta: 0.4949494949494949
train: step: 2569, loss: 0.683724045753479, acc: 0.575, recall: 0.575, precision: 0.575187969924812, f_beta: 0.5747342088805503
train: step: 2570, loss: 0.7259895205497742, acc: 0.35, recall: 0.35, precision: 0.34375, f_beta: 0.3434343434343434
train: step: 2571, loss: 0.6872153282165527, acc: 0.525, recall: 0.525, precision: 0.5266666666666666, f_beta: 0.5174603174603175
train: step: 2572, loss: 0.7071835398674011, acc: 0.475, recall: 0.475, precision: 0.46865203761755486, f_beta: 0.4470046082949308
train: step: 2573, loss: 0.6393278241157532, acc: 0.725, recall: 0.7250000000000001, precision: 0.7564102564102564, f_beta: 0.7163120567375887
train: step: 2574, loss: 0.6924717426300049, acc: 0.5, recall: 0.5, precision: 0.5, f_beta: 0.4505494505494506
train: step: 2575, loss: 0.7160395383834839, acc: 0.375, recall: 0.375, precision: 0.3721227621483376, f_beta: 0.3714644877435575
train: step: 2576, loss: 0.6841797828674316, acc: 0.575, recall: 0.575, precision: 0.5940438871473355, f_beta: 0.5523370638578011
train: step: 2577, loss: 0.6908797025680542, acc: 0.525, recall: 0.525, precision: 0.5313479623824451, f_beta: 0.4996708360763661
train: step: 2578, loss: 0.7025391459465027, acc: 0.525, recall: 0.525, precision: 0.5432900432900433, f_beta: 0.46890286512928026
train: step: 2579, loss: 0.6794407963752747, acc: 0.675, recall: 0.675, precision: 0.6790281329923273, f_beta: 0.6731615336266499
train: step: 2580, loss: 0.690365195274353, acc: 0.65, recall: 0.65, precision: 0.65625, f_beta: 0.6464646464646464
train: step: 2581, loss: 0.686632513999939, acc: 0.55, recall: 0.55, precision: 0.5520833333333333, f_beta: 0.5454545454545454
train: step: 2582, loss: 0.6883544921875, acc: 0.6, recall: 0.6, precision: 0.6041666666666667, f_beta: 0.595959595959596
train: step: 2583, loss: 0.7114129066467285, acc: 0.325, recall: 0.325, precision: 0.24910394265232974, f_beta: 0.2697768762677485
train: step: 2584, loss: 0.6798325777053833, acc: 0.65, recall: 0.65, precision: 0.6648351648351649, f_beta: 0.6419437340153452
train: step: 2585, loss: 0.6777914762496948, acc: 0.575, recall: 0.575, precision: 0.5854700854700855, f_beta: 0.5615731785944552
train: step: 2586, loss: 0.7341785430908203, acc: 0.45, recall: 0.45, precision: 0.4494949494949495, f_beta: 0.4486215538847118
train: step: 2587, loss: 0.6917138695716858, acc: 0.475, recall: 0.475, precision: 0.474937343358396, f_beta: 0.474671669793621
train: step: 2588, loss: 0.7333025336265564, acc: 0.275, recall: 0.275, precision: 0.26, f_beta: 0.2634920634920635
train: step: 2589, loss: 0.661535918712616, acc: 0.65, recall: 0.65, precision: 0.6785714285714286, f_beta: 0.6354166666666667
train: step: 2590, loss: 0.706261157989502, acc: 0.4, recall: 0.4, precision: 0.39583333333333337, f_beta: 0.3939393939393939
train: step: 2591, loss: 0.6915601491928101, acc: 0.55, recall: 0.55, precision: 0.55, f_beta: 0.55
train: step: 2592, loss: 0.6998929977416992, acc: 0.275, recall: 0.275, precision: 0.2698209718670077, f_beta: 0.2708988057825267
train: step: 2593, loss: 0.6986922025680542, acc: 0.425, recall: 0.425, precision: 0.4145299145299145, f_beta: 0.40683430045132174
train: step: 2594, loss: 0.6671109199523926, acc: 0.65, recall: 0.65, precision: 0.65625, f_beta: 0.6464646464646464
train: step: 2595, loss: 0.657814621925354, acc: 0.725, recall: 0.7250000000000001, precision: 0.7301790281329923, f_beta: 0.7234443746071653
train: step: 2596, loss: 0.6972692012786865, acc: 0.475, recall: 0.47500000000000003, precision: 0.4744245524296675, f_beta: 0.47203016970458833
train: step: 2597, loss: 0.6891366243362427, acc: 0.625, recall: 0.625, precision: 0.6424501424501424, f_beta: 0.6131528046421664
train: step: 2598, loss: 0.6848770976066589, acc: 0.575, recall: 0.575, precision: 0.575187969924812, f_beta: 0.5747342088805503
train: step: 2599, loss: 0.676876425743103, acc: 0.7, recall: 0.7, precision: 0.702020202020202, f_beta: 0.6992481203007519
train: step: 2600, loss: 0.6910125613212585, acc: 0.575, recall: 0.575, precision: 0.60752688172043, f_beta: 0.5402298850574712
train: step: 2601, loss: 0.6701653599739075, acc: 0.625, recall: 0.625, precision: 0.6567398119122256, f_beta: 0.6050032916392363
train: step: 2602, loss: 0.7177156805992126, acc: 0.3, recall: 0.3, precision: 0.2802197802197802, f_beta: 0.28388746803069054
train: step: 2603, loss: 0.6938188672065735, acc: 0.525, recall: 0.525, precision: 0.5255754475703325, f_beta: 0.5223130106851037
train: step: 2604, loss: 0.7440133690834045, acc: 0.3, recall: 0.3, precision: 0.2619047619047619, f_beta: 0.2708333333333333
train: step: 2605, loss: 0.693637490272522, acc: 0.475, recall: 0.475, precision: 0.46415770609318996, f_beta: 0.43204868154158216
train: step: 2606, loss: 0.6952387094497681, acc: 0.425, recall: 0.42500000000000004, precision: 0.42000000000000004, f_beta: 0.4158730158730159
train: step: 2607, loss: 0.698184609413147, acc: 0.5, recall: 0.5, precision: 0.5, f_beta: 0.40476190476190477
train: step: 2608, loss: 0.7029100060462952, acc: 0.5, recall: 0.5, precision: 0.5, f_beta: 0.43019943019943013
train: step: 2609, loss: 0.7546866536140442, acc: 0.275, recall: 0.275, precision: 0.1774193548387097, f_beta: 0.21568627450980393
train: step: 2610, loss: 0.6932119131088257, acc: 0.525, recall: 0.525, precision: 0.5266666666666666, f_beta: 0.5174603174603175
train: step: 2611, loss: 0.6677773594856262, acc: 0.675, recall: 0.675, precision: 0.6994301994301995, f_beta: 0.6647324306898775
train: step: 2612, loss: 0.7001609802246094, acc: 0.45, recall: 0.45, precision: 0.4019607843137255, f_beta: 0.37321937321937326
train: step: 2613, loss: 0.6950693726539612, acc: 0.4, recall: 0.4, precision: 0.38095238095238093, f_beta: 0.375
train: step: 2614, loss: 0.6831404566764832, acc: 0.65, recall: 0.6499999999999999, precision: 0.734375, f_beta: 0.6153846153846154
train: step: 2615, loss: 0.6771327257156372, acc: 0.625, recall: 0.625, precision: 0.6278772378516624, f_beta: 0.6228786926461345
train: step: 2616, loss: 0.6912668943405151, acc: 0.5, recall: 0.5, precision: 0.5, f_beta: 0.4505494505494506
train: step: 2617, loss: 0.6714980006217957, acc: 0.65, recall: 0.65, precision: 0.65625, f_beta: 0.6464646464646464
train: step: 2618, loss: 0.6924470067024231, acc: 0.5, recall: 0.5, precision: 0.5, f_beta: 0.3730407523510972
train: step: 2619, loss: 0.6825833320617676, acc: 0.55, recall: 0.55, precision: 0.578125, f_beta: 0.5054945054945055
train: step: 2620, loss: 0.6873282194137573, acc: 0.575, recall: 0.575, precision: 0.6714285714285715, f_beta: 0.5054545454545454
train: step: 2621, loss: 0.6861968040466309, acc: 0.625, recall: 0.625, precision: 0.6567398119122256, f_beta: 0.6050032916392363
train: step: 2622, loss: 0.6939109563827515, acc: 0.525, recall: 0.525, precision: 0.5255754475703325, f_beta: 0.5223130106851037
train: step: 2623, loss: 0.6504994630813599, acc: 0.675, recall: 0.675, precision: 0.6994301994301995, f_beta: 0.6647324306898775
train: step: 2624, loss: 0.6928180456161499, acc: 0.625, recall: 0.625, precision: 0.6278772378516624, f_beta: 0.6228786926461345
train: step: 2625, loss: 0.7022411227226257, acc: 0.4, recall: 0.4, precision: 0.38095238095238093, f_beta: 0.375
train: step: 2626, loss: 0.6920415759086609, acc: 0.625, recall: 0.625, precision: 0.6424501424501424, f_beta: 0.6131528046421664
train: step: 2627, loss: 0.674149215221405, acc: 0.625, recall: 0.625, precision: 0.6792114695340502, f_beta: 0.5943204868154157
train: step: 2628, loss: 0.6832830905914307, acc: 0.625, recall: 0.625, precision: 0.6567398119122256, f_beta: 0.6050032916392363
train: step: 2629, loss: 0.6788251399993896, acc: 0.675, recall: 0.675, precision: 0.719435736677116, f_beta: 0.6576695194206715
train: step: 2630, loss: 0.6892994046211243, acc: 0.625, recall: 0.625, precision: 0.6792114695340502, f_beta: 0.5943204868154157
train: step: 2631, loss: 0.6964722871780396, acc: 0.5, recall: 0.5, precision: 0.5, f_beta: 0.5
train: step: 2632, loss: 0.6898429989814758, acc: 0.525, recall: 0.525, precision: 0.5432900432900433, f_beta: 0.46890286512928026
train: step: 2633, loss: 0.6952498555183411, acc: 0.525, recall: 0.525, precision: 0.5266666666666666, f_beta: 0.5174603174603175
train: step: 2634, loss: 0.6854705810546875, acc: 0.55, recall: 0.55, precision: 0.5980392156862745, f_beta: 0.4871794871794872
train: step: 2635, loss: 0.6950095891952515, acc: 0.525, recall: 0.525, precision: 0.5266666666666666, f_beta: 0.5174603174603175
train: step: 2636, loss: 0.6710831522941589, acc: 0.6, recall: 0.6000000000000001, precision: 0.601010101010101, f_beta: 0.5989974937343359
train: step: 2637, loss: 0.7021918892860413, acc: 0.375, recall: 0.375, precision: 0.3721227621483376, f_beta: 0.3714644877435575
train: step: 2638, loss: 0.6969768404960632, acc: 0.475, recall: 0.47500000000000003, precision: 0.4744245524296675, f_beta: 0.47203016970458833
train: step: 2639, loss: 0.6794541478157043, acc: 0.55, recall: 0.55, precision: 0.5980392156862745, f_beta: 0.4871794871794872
train: step: 2640, loss: 0.6970654129981995, acc: 0.475, recall: 0.475, precision: 0.46865203761755486, f_beta: 0.4470046082949308
train: step: 2641, loss: 0.6976341009140015, acc: 0.5, recall: 0.5, precision: 0.5, f_beta: 0.4884910485933504
train: step: 2642, loss: 0.683220624923706, acc: 0.6, recall: 0.6, precision: 0.6, f_beta: 0.6
train: step: 2643, loss: 0.6926370859146118, acc: 0.525, recall: 0.525, precision: 0.525062656641604, f_beta: 0.5247029393370857
train: step: 2644, loss: 0.6918829083442688, acc: 0.525, recall: 0.525, precision: 0.525062656641604, f_beta: 0.5247029393370857
train: step: 2645, loss: 0.6944757103919983, acc: 0.45, recall: 0.45, precision: 0.4494949494949495, f_beta: 0.4486215538847118
train: step: 2646, loss: 0.6537849307060242, acc: 0.7, recall: 0.7, precision: 0.702020202020202, f_beta: 0.6992481203007519
train: step: 2647, loss: 0.698723554611206, acc: 0.475, recall: 0.475, precision: 0.47333333333333333, f_beta: 0.4666666666666666
train: step: 2648, loss: 0.6689366698265076, acc: 0.725, recall: 0.725, precision: 0.74, f_beta: 0.7206349206349207
train: step: 2649, loss: 0.7173364758491516, acc: 0.45, recall: 0.45, precision: 0.45, f_beta: 0.45
train: step: 2650, loss: 0.6914763450622559, acc: 0.55, recall: 0.55, precision: 0.5520833333333333, f_beta: 0.5454545454545454
train: step: 2651, loss: 0.6681579351425171, acc: 0.65, recall: 0.6499999999999999, precision: 0.6515151515151515, f_beta: 0.6491228070175439
train: step: 2652, loss: 0.6799276471138, acc: 0.575, recall: 0.575, precision: 0.5940438871473355, f_beta: 0.5523370638578011
train: step: 2653, loss: 0.6825634837150574, acc: 0.625, recall: 0.625, precision: 0.6278772378516624, f_beta: 0.6228786926461345
train: step: 2654, loss: 0.7169511914253235, acc: 0.425, recall: 0.425, precision: 0.4145299145299145, f_beta: 0.40683430045132174
train: step: 2655, loss: 0.6827560663223267, acc: 0.6, recall: 0.6000000000000001, precision: 0.601010101010101, f_beta: 0.5989974937343359
train: step: 2656, loss: 0.7146979570388794, acc: 0.375, recall: 0.375, precision: 0.3666666666666667, f_beta: 0.3650793650793651
train: step: 2657, loss: 0.679442286491394, acc: 0.65, recall: 0.6499999999999999, precision: 0.734375, f_beta: 0.6153846153846154
train: step: 2658, loss: 0.6924819946289062, acc: 0.55, recall: 0.55, precision: 0.5595238095238095, f_beta: 0.53125
train: step: 2659, loss: 0.6914094090461731, acc: 0.625, recall: 0.625, precision: 0.62531328320802, f_beta: 0.6247654784240151
train: step: 2660, loss: 0.685889482498169, acc: 0.575, recall: 0.575, precision: 0.6298701298701299, f_beta: 0.5248078266946192
train: step: 2661, loss: 0.687474250793457, acc: 0.575, recall: 0.575, precision: 0.5767263427109974, f_beta: 0.5725958516656191
train: step: 2662, loss: 0.6702380180358887, acc: 0.65, recall: 0.6499999999999999, precision: 0.6515151515151515, f_beta: 0.6491228070175439
train: step: 2663, loss: 0.6948146224021912, acc: 0.425, recall: 0.42500000000000004, precision: 0.4059561128526646, f_beta: 0.39433838051349573
train: step: 2664, loss: 0.6934338808059692, acc: 0.475, recall: 0.475, precision: 0.46865203761755486, f_beta: 0.4470046082949308
train: step: 2665, loss: 0.6946043968200684, acc: 0.35, recall: 0.35, precision: 0.30000000000000004, f_beta: 0.30666666666666664
train: step: 2666, loss: 0.6937627196311951, acc: 0.5, recall: 0.5, precision: 0.5, f_beta: 0.4884910485933504
train: step: 2667, loss: 0.6928297281265259, acc: 0.6, recall: 0.6000000000000001, precision: 0.601010101010101, f_beta: 0.5989974937343359
train: step: 2668, loss: 0.6771450042724609, acc: 0.6, recall: 0.6, precision: 0.6333333333333333, f_beta: 0.5733333333333334
train: step: 2669, loss: 0.7013900876045227, acc: 0.5, recall: 0.5, precision: 0.5, f_beta: 0.4987468671679198
train: step: 2670, loss: 0.6577739119529724, acc: 0.7, recall: 0.7, precision: 0.7197802197802198, f_beta: 0.6930946291560103
train: step: 2671, loss: 0.6757780909538269, acc: 0.625, recall: 0.625, precision: 0.6278772378516624, f_beta: 0.6228786926461345
train: step: 2672, loss: 0.6872345209121704, acc: 0.475, recall: 0.47500000000000003, precision: 0.40990990990990994, f_beta: 0.35926773455377575
train: step: 2673, loss: 0.6897411346435547, acc: 0.55, recall: 0.55, precision: 0.5505050505050505, f_beta: 0.5488721804511277
train: step: 2674, loss: 0.6844616532325745, acc: 0.55, recall: 0.55, precision: 0.5666666666666667, f_beta: 0.52
train: step: 2675, loss: 0.7074109315872192, acc: 0.375, recall: 0.375, precision: 0.37468671679197996, f_beta: 0.37460913070669166
train: step: 2676, loss: 0.6963945627212524, acc: 0.45, recall: 0.45, precision: 0.44047619047619047, f_beta: 0.42708333333333337
train: step: 2677, loss: 0.6545580625534058, acc: 0.7, recall: 0.7, precision: 0.702020202020202, f_beta: 0.6992481203007519
train: step: 2678, loss: 0.6823145151138306, acc: 0.6, recall: 0.6, precision: 0.6041666666666667, f_beta: 0.595959595959596
train: step: 2679, loss: 0.6934863924980164, acc: 0.575, recall: 0.575, precision: 0.5800000000000001, f_beta: 0.5682539682539682
train: step: 2680, loss: 0.6736377477645874, acc: 0.625, recall: 0.625, precision: 0.6278772378516624, f_beta: 0.6228786926461345
train: step: 2681, loss: 0.7171470522880554, acc: 0.275, recall: 0.275, precision: 0.26, f_beta: 0.2634920634920635
train: step: 2682, loss: 0.695115864276886, acc: 0.375, recall: 0.375, precision: 0.34326018808777425, f_beta: 0.34167215273206053
train: step: 2683, loss: 0.687919557094574, acc: 0.525, recall: 0.525, precision: 0.5571428571428572, f_beta: 0.44727272727272727
train: step: 2684, loss: 0.6784268617630005, acc: 0.575, recall: 0.575, precision: 0.5800000000000001, f_beta: 0.5682539682539682
train: step: 2685, loss: 0.6459909677505493, acc: 0.725, recall: 0.725, precision: 0.7255639097744361, f_beta: 0.7248280175109443
train: step: 2686, loss: 0.6853540539741516, acc: 0.575, recall: 0.575, precision: 0.60752688172043, f_beta: 0.5402298850574712
train: step: 2687, loss: 0.6380005478858948, acc: 0.725, recall: 0.725, precision: 0.7255639097744361, f_beta: 0.7248280175109443
train: step: 2688, loss: 0.6929186582565308, acc: 0.55, recall: 0.55, precision: 0.5520833333333333, f_beta: 0.5454545454545454
train: step: 2689, loss: 0.687613844871521, acc: 0.55, recall: 0.55, precision: 0.5505050505050505, f_beta: 0.5488721804511277
train: step: 2690, loss: 0.6904968023300171, acc: 0.55, recall: 0.55, precision: 0.5520833333333333, f_beta: 0.5454545454545454
train: step: 2691, loss: 0.6947267651557922, acc: 0.525, recall: 0.525, precision: 0.5266666666666666, f_beta: 0.5174603174603175
train: step: 2692, loss: 0.6960074305534363, acc: 0.425, recall: 0.42500000000000004, precision: 0.424812030075188, f_beta: 0.4246404002501563
train: step: 2693, loss: 0.6762121319770813, acc: 0.65, recall: 0.65, precision: 0.6785714285714286, f_beta: 0.6354166666666667
train: step: 2694, loss: 0.6460738778114319, acc: 0.675, recall: 0.675, precision: 0.6866666666666666, f_beta: 0.6698412698412699
train: step: 2695, loss: 0.6926136016845703, acc: 0.6, recall: 0.6, precision: 0.6041666666666667, f_beta: 0.595959595959596
train: step: 2696, loss: 0.6948341131210327, acc: 0.5, recall: 0.5, precision: 0.5, f_beta: 0.47916666666666663
train: step: 2697, loss: 0.7402591705322266, acc: 0.375, recall: 0.375, precision: 0.3721227621483376, f_beta: 0.3714644877435575
train: step: 2698, loss: 0.707731306552887, acc: 0.425, recall: 0.42500000000000004, precision: 0.424812030075188, f_beta: 0.4246404002501563
train: step: 2699, loss: 0.694308876991272, acc: 0.5, recall: 0.5, precision: 0.5, f_beta: 0.4666666666666667
train: step: 2700, loss: 0.7095020413398743, acc: 0.35, recall: 0.35, precision: 0.3484848484848485, f_beta: 0.3483709273182958
train: step: 2701, loss: 0.6937970519065857, acc: 0.5, recall: 0.5, precision: 0.5, f_beta: 0.4949494949494949
train: step: 2702, loss: 0.6578957438468933, acc: 0.675, recall: 0.675, precision: 0.6866666666666666, f_beta: 0.6698412698412699
train: step: 2703, loss: 0.645584762096405, acc: 0.725, recall: 0.725, precision: 0.74, f_beta: 0.7206349206349207
train: step: 2704, loss: 0.6910420656204224, acc: 0.55, recall: 0.55, precision: 0.5520833333333333, f_beta: 0.5454545454545454
train: step: 2705, loss: 0.69120854139328, acc: 0.55, recall: 0.55, precision: 0.5595238095238095, f_beta: 0.53125
train: step: 2706, loss: 0.6839473843574524, acc: 0.625, recall: 0.625, precision: 0.6278772378516624, f_beta: 0.6228786926461345
train: step: 2707, loss: 0.6936216354370117, acc: 0.475, recall: 0.47500000000000003, precision: 0.45670995670995673, f_beta: 0.41299790356394134
train: step: 2708, loss: 0.6713871359825134, acc: 0.625, recall: 0.625, precision: 0.6278772378516624, f_beta: 0.6228786926461345
train: step: 2709, loss: 0.6510955095291138, acc: 0.65, recall: 0.65, precision: 0.65625, f_beta: 0.6464646464646464
train: step: 2710, loss: 0.686233639717102, acc: 0.55, recall: 0.55, precision: 0.554945054945055, f_beta: 0.5396419437340153
train: step: 2711, loss: 0.6914006471633911, acc: 0.6, recall: 0.6000000000000001, precision: 0.601010101010101, f_beta: 0.5989974937343359
train: step: 2712, loss: 0.6933610439300537, acc: 0.5, recall: 0.5, precision: 0.5, f_beta: 0.5
train: step: 2713, loss: 0.6918179392814636, acc: 0.525, recall: 0.525, precision: 0.5266666666666666, f_beta: 0.5174603174603175
train: step: 2714, loss: 0.6396867036819458, acc: 0.7, recall: 0.7, precision: 0.7083333333333333, f_beta: 0.6969696969696968
train: step: 2715, loss: 0.6765415072441101, acc: 0.575, recall: 0.575, precision: 0.5940438871473355, f_beta: 0.5523370638578011
train: step: 2716, loss: 0.6920687556266785, acc: 0.5, recall: 0.5, precision: 0.5, f_beta: 0.4987468671679198
train: step: 2717, loss: 0.6716026067733765, acc: 0.6, recall: 0.6, precision: 0.6098901098901099, f_beta: 0.5907928388746804
train: step: 2718, loss: 0.692673921585083, acc: 0.5, recall: 0.5, precision: 0.5, f_beta: 0.47916666666666663
train: step: 2719, loss: 0.7033891677856445, acc: 0.5, recall: 0.5, precision: 0.5, f_beta: 0.5
train: step: 2720, loss: 0.6948822736740112, acc: 0.45, recall: 0.45, precision: 0.45, f_beta: 0.45
train: step: 2721, loss: 0.7086742520332336, acc: 0.475, recall: 0.47500000000000003, precision: 0.4744245524296675, f_beta: 0.47203016970458833
train: step: 2722, loss: 0.6878617405891418, acc: 0.4, recall: 0.4, precision: 0.39583333333333337, f_beta: 0.3939393939393939
train: step: 2723, loss: 0.6626182794570923, acc: 0.7, recall: 0.7, precision: 0.7083333333333333, f_beta: 0.6969696969696968
train: step: 2724, loss: 0.6925103664398193, acc: 0.525, recall: 0.525, precision: 0.5255754475703325, f_beta: 0.5223130106851037
train: step: 2725, loss: 0.6955810785293579, acc: 0.425, recall: 0.425, precision: 0.3701298701298701, f_beta: 0.35709294199860236
train: step: 2726, loss: 0.6763706803321838, acc: 0.6, recall: 0.6, precision: 0.65625, f_beta: 0.5604395604395604
train: step: 2727, loss: 0.7008563280105591, acc: 0.45, recall: 0.45, precision: 0.4494949494949495, f_beta: 0.4486215538847118
train: step: 2728, loss: 0.6860474348068237, acc: 0.6, recall: 0.6000000000000001, precision: 0.6190476190476191, f_beta: 0.5833333333333333
train: step: 2729, loss: 0.6945347785949707, acc: 0.525, recall: 0.525, precision: 0.5255754475703325, f_beta: 0.5223130106851037
train: step: 2730, loss: 0.7196313738822937, acc: 0.375, recall: 0.375, precision: 0.3721227621483376, f_beta: 0.3714644877435575
train: step: 2731, loss: 0.6870474815368652, acc: 0.55, recall: 0.55, precision: 0.554945054945055, f_beta: 0.5396419437340153
train: step: 2732, loss: 0.6943444013595581, acc: 0.425, recall: 0.425, precision: 0.3701298701298701, f_beta: 0.35709294199860236
train: step: 2733, loss: 0.688060462474823, acc: 0.55, recall: 0.55, precision: 0.5595238095238095, f_beta: 0.53125
train: step: 2734, loss: 0.694119393825531, acc: 0.5, recall: 0.5, precision: 0.5, f_beta: 0.4666666666666667
train: step: 2735, loss: 0.6887832880020142, acc: 0.5, recall: 0.5, precision: 0.5, f_beta: 0.43019943019943013
train: step: 2736, loss: 0.6923171877861023, acc: 0.525, recall: 0.525, precision: 0.5255754475703325, f_beta: 0.5223130106851037
train: step: 2737, loss: 0.6934451460838318, acc: 0.55, recall: 0.55, precision: 0.55, f_beta: 0.55
train: step: 2738, loss: 0.6861361861228943, acc: 0.575, recall: 0.575, precision: 0.5767263427109974, f_beta: 0.5725958516656191
train: step: 2739, loss: 0.6843050122261047, acc: 0.575, recall: 0.575, precision: 0.5940438871473355, f_beta: 0.5523370638578011
train: step: 2740, loss: 0.6930559873580933, acc: 0.525, recall: 0.525, precision: 0.5255754475703325, f_beta: 0.5223130106851037
train: step: 2741, loss: 0.6888606548309326, acc: 0.55, recall: 0.55, precision: 0.5666666666666667, f_beta: 0.52
train: step: 2742, loss: 0.677437424659729, acc: 0.625, recall: 0.625, precision: 0.6333333333333333, f_beta: 0.6190476190476191
train: step: 2743, loss: 0.7020683288574219, acc: 0.475, recall: 0.475, precision: 0.46865203761755486, f_beta: 0.4470046082949308
train: step: 2744, loss: 0.6910516023635864, acc: 0.525, recall: 0.525, precision: 0.5255754475703325, f_beta: 0.5223130106851037
train: step: 2745, loss: 0.6737127304077148, acc: 0.675, recall: 0.675, precision: 0.719435736677116, f_beta: 0.6576695194206715
train: step: 2746, loss: 0.6847478151321411, acc: 0.575, recall: 0.575, precision: 0.575187969924812, f_beta: 0.5747342088805503
train: step: 2747, loss: 0.6966351270675659, acc: 0.425, recall: 0.42500000000000004, precision: 0.4059561128526646, f_beta: 0.39433838051349573
train: step: 2748, loss: 0.7098356485366821, acc: 0.45, recall: 0.45, precision: 0.45, f_beta: 0.45
train: step: 2749, loss: 0.6958533525466919, acc: 0.325, recall: 0.32499999999999996, precision: 0.32456140350877194, f_beta: 0.324577861163227
train: step: 2750, loss: 0.7281621694564819, acc: 0.25, recall: 0.25, precision: 0.24747474747474746, f_beta: 0.24812030075187969
train: step: 2751, loss: 0.6939511895179749, acc: 0.475, recall: 0.475, precision: 0.44285714285714284, f_beta: 0.38909090909090915
train: step: 2752, loss: 0.665926456451416, acc: 0.65, recall: 0.65, precision: 0.65, f_beta: 0.65
train: step: 2753, loss: 0.7122333645820618, acc: 0.375, recall: 0.375, precision: 0.3575498575498576, f_beta: 0.3552546744036106
train: step: 2754, loss: 0.6507288217544556, acc: 0.675, recall: 0.675, precision: 0.6754385964912281, f_beta: 0.6747967479674797
train: step: 2755, loss: 0.7348023653030396, acc: 0.425, recall: 0.425, precision: 0.4145299145299145, f_beta: 0.40683430045132174
train: step: 2756, loss: 0.6913498640060425, acc: 0.475, recall: 0.475, precision: 0.47333333333333333, f_beta: 0.4666666666666666
train: step: 2757, loss: 0.6703653335571289, acc: 0.7, recall: 0.7, precision: 0.7380952380952381, f_beta: 0.6875
train: step: 2758, loss: 0.7016643285751343, acc: 0.475, recall: 0.475, precision: 0.47150997150997154, f_beta: 0.4584139264990329
train: step: 2759, loss: 0.6920968890190125, acc: 0.6, recall: 0.6000000000000001, precision: 0.601010101010101, f_beta: 0.5989974937343359
train: step: 2760, loss: 0.6848673224449158, acc: 0.6, recall: 0.6, precision: 0.6, f_beta: 0.6
train: step: 2761, loss: 0.6932340860366821, acc: 0.6, recall: 0.6000000000000001, precision: 0.6190476190476191, f_beta: 0.5833333333333333
train: step: 2762, loss: 0.6783206462860107, acc: 0.6, recall: 0.6, precision: 0.6098901098901099, f_beta: 0.5907928388746804
train: step: 2763, loss: 0.7113317847251892, acc: 0.425, recall: 0.425, precision: 0.3924731182795699, f_beta: 0.37795807978363755
train: step: 2764, loss: 0.6883508563041687, acc: 0.55, recall: 0.55, precision: 0.5666666666666667, f_beta: 0.52
train: step: 2765, loss: 0.6938210725784302, acc: 0.5, recall: 0.5, precision: 0.5, f_beta: 0.4987468671679198
train: step: 2766, loss: 0.6672183275222778, acc: 0.625, recall: 0.625, precision: 0.6792114695340502, f_beta: 0.5943204868154157
train: step: 2767, loss: 0.6588383913040161, acc: 0.625, recall: 0.625, precision: 0.6278772378516624, f_beta: 0.6228786926461345
train: step: 2768, loss: 0.7297598719596863, acc: 0.375, recall: 0.375, precision: 0.3575498575498576, f_beta: 0.3552546744036106
train: step: 2769, loss: 0.6521698832511902, acc: 0.675, recall: 0.675, precision: 0.6790281329923273, f_beta: 0.6731615336266499
train: step: 2770, loss: 0.7087321877479553, acc: 0.45, recall: 0.45, precision: 0.421875, f_beta: 0.39560439560439553
train: step: 2771, loss: 0.7103634476661682, acc: 0.425, recall: 0.42500000000000004, precision: 0.4059561128526646, f_beta: 0.39433838051349573
train: step: 2772, loss: 0.691653847694397, acc: 0.675, recall: 0.675, precision: 0.6754385964912281, f_beta: 0.6747967479674797
train: step: 2773, loss: 0.719192385673523, acc: 0.425, recall: 0.42500000000000004, precision: 0.32857142857142857, f_beta: 0.33090909090909093
train: step: 2774, loss: 0.7293254733085632, acc: 0.45, recall: 0.44999999999999996, precision: 0.44505494505494503, f_beta: 0.4373401534526854
train: step: 2775, loss: 0.6994950771331787, acc: 0.475, recall: 0.47500000000000003, precision: 0.4744245524296675, f_beta: 0.47203016970458833
train: step: 2776, loss: 0.6956923604011536, acc: 0.475, recall: 0.47500000000000003, precision: 0.4744245524296675, f_beta: 0.47203016970458833
train: step: 2777, loss: 0.6978024244308472, acc: 0.55, recall: 0.55, precision: 0.55, f_beta: 0.55
train: step: 2778, loss: 0.6878750920295715, acc: 0.525, recall: 0.525, precision: 0.525062656641604, f_beta: 0.5247029393370857
train: step: 2779, loss: 0.696111798286438, acc: 0.4, recall: 0.4, precision: 0.398989898989899, f_beta: 0.39849624060150374
train: step: 2780, loss: 0.6724165081977844, acc: 0.65, recall: 0.6499999999999999, precision: 0.6515151515151515, f_beta: 0.6491228070175439
train: step: 2781, loss: 0.6994418501853943, acc: 0.45, recall: 0.45, precision: 0.4494949494949495, f_beta: 0.4486215538847118
train: step: 2782, loss: 0.6650272607803345, acc: 0.65, recall: 0.65, precision: 0.65625, f_beta: 0.6464646464646464
train: step: 2783, loss: 0.6828852891921997, acc: 0.625, recall: 0.625, precision: 0.62531328320802, f_beta: 0.6247654784240151
train: step: 2784, loss: 0.692257285118103, acc: 0.575, recall: 0.575, precision: 0.5854700854700855, f_beta: 0.5615731785944552
train: step: 2785, loss: 0.6941800117492676, acc: 0.475, recall: 0.475, precision: 0.47150997150997154, f_beta: 0.4584139264990329
train: step: 2786, loss: 0.6504071354866028, acc: 0.675, recall: 0.675, precision: 0.6790281329923273, f_beta: 0.6731615336266499
train: step: 2787, loss: 0.69548100233078, acc: 0.475, recall: 0.47500000000000003, precision: 0.4744245524296675, f_beta: 0.47203016970458833
train: step: 2788, loss: 0.666274905204773, acc: 0.6, recall: 0.6, precision: 0.6098901098901099, f_beta: 0.5907928388746804
train: step: 2789, loss: 0.6959436535835266, acc: 0.375, recall: 0.375, precision: 0.3666666666666667, f_beta: 0.3650793650793651
train: step: 2790, loss: 0.6628750562667847, acc: 0.6, recall: 0.6, precision: 0.696078431372549, f_beta: 0.5441595441595442
train: step: 2791, loss: 0.6950411200523376, acc: 0.425, recall: 0.425, precision: 0.42327365728900257, f_beta: 0.4217473287240729
train: step: 2792, loss: 0.6911143064498901, acc: 0.575, recall: 0.575, precision: 0.575187969924812, f_beta: 0.5747342088805503
train: step: 2793, loss: 0.699691891670227, acc: 0.5, recall: 0.5, precision: 0.5, f_beta: 0.4884910485933504
train: step: 2794, loss: 0.6905096769332886, acc: 0.6, recall: 0.6, precision: 0.6098901098901099, f_beta: 0.5907928388746804
train: step: 2795, loss: 0.6647946238517761, acc: 0.65, recall: 0.65, precision: 0.65, f_beta: 0.65
train: step: 2796, loss: 0.6923167109489441, acc: 0.625, recall: 0.625, precision: 0.62531328320802, f_beta: 0.6247654784240151
train: step: 2797, loss: 0.7056344747543335, acc: 0.425, recall: 0.425, precision: 0.42327365728900257, f_beta: 0.4217473287240729
train: step: 2798, loss: 0.7324784994125366, acc: 0.375, recall: 0.375, precision: 0.37468671679197996, f_beta: 0.37460913070669166
train: step: 2799, loss: 0.6805849671363831, acc: 0.65, recall: 0.65, precision: 0.6785714285714286, f_beta: 0.6354166666666667
train: step: 2800, loss: 0.6788397431373596, acc: 0.575, recall: 0.575, precision: 0.5854700854700855, f_beta: 0.5615731785944552
train: step: 2801, loss: 0.6835170388221741, acc: 0.575, recall: 0.575, precision: 0.5767263427109974, f_beta: 0.5725958516656191
train: step: 2802, loss: 0.6479797959327698, acc: 0.7, recall: 0.7, precision: 0.7083333333333333, f_beta: 0.6969696969696968
train: step: 2803, loss: 0.6960590481758118, acc: 0.3, recall: 0.3, precision: 0.297979797979798, f_beta: 0.2982456140350877
train: step: 2804, loss: 0.7008808851242065, acc: 0.4, recall: 0.4, precision: 0.3901098901098901, f_beta: 0.38618925831202044
train: step: 2805, loss: 0.7012958526611328, acc: 0.475, recall: 0.475, precision: 0.47333333333333333, f_beta: 0.4666666666666666
train: step: 2806, loss: 0.701287567615509, acc: 0.525, recall: 0.525, precision: 0.5255754475703325, f_beta: 0.5223130106851037
train: step: 2807, loss: 0.6976640820503235, acc: 0.475, recall: 0.47500000000000003, precision: 0.45670995670995673, f_beta: 0.41299790356394134
train: step: 2808, loss: 0.6921869516372681, acc: 0.5, recall: 0.5, precision: 0.5, f_beta: 0.4505494505494506
train: step: 2809, loss: 0.69298255443573, acc: 0.575, recall: 0.575, precision: 0.5767263427109974, f_beta: 0.5725958516656191
train: step: 2810, loss: 0.6856077909469604, acc: 0.6, recall: 0.6, precision: 0.6041666666666667, f_beta: 0.595959595959596
train: step: 2811, loss: 0.6915698051452637, acc: 0.575, recall: 0.575, precision: 0.5767263427109974, f_beta: 0.5725958516656191
train: step: 2812, loss: 0.675665557384491, acc: 0.65, recall: 0.6499999999999999, precision: 0.6515151515151515, f_beta: 0.6491228070175439
train: step: 2813, loss: 0.7434719204902649, acc: 0.4, recall: 0.4, precision: 0.39583333333333337, f_beta: 0.3939393939393939
train: step: 2814, loss: 0.7127450704574585, acc: 0.35, recall: 0.35, precision: 0.3351648351648352, f_beta: 0.3350383631713555
train: step: 2815, loss: 0.6932377815246582, acc: 0.55, recall: 0.55, precision: 0.5520833333333333, f_beta: 0.5454545454545454
train: step: 2816, loss: 0.669391930103302, acc: 0.725, recall: 0.725, precision: 0.74, f_beta: 0.7206349206349207
train: step: 2817, loss: 0.6741225123405457, acc: 0.625, recall: 0.625, precision: 0.6278772378516624, f_beta: 0.6228786926461345
train: step: 2818, loss: 0.691683292388916, acc: 0.6, recall: 0.6, precision: 0.7777777777777778, f_beta: 0.5238095238095238
train: step: 2819, loss: 0.6829004883766174, acc: 0.625, recall: 0.625, precision: 0.6424501424501424, f_beta: 0.6131528046421664
train: step: 2820, loss: 0.6767774820327759, acc: 0.6, recall: 0.6000000000000001, precision: 0.601010101010101, f_beta: 0.5989974937343359
train: step: 2821, loss: 0.6875923871994019, acc: 0.55, recall: 0.55, precision: 0.5505050505050505, f_beta: 0.5488721804511277
train: step: 2822, loss: 0.6804845929145813, acc: 0.625, recall: 0.625, precision: 0.62531328320802, f_beta: 0.6247654784240151
train: step: 2823, loss: 0.678714394569397, acc: 0.6, recall: 0.6, precision: 0.6041666666666667, f_beta: 0.595959595959596
train: step: 2824, loss: 0.6921887993812561, acc: 0.525, recall: 0.5249999999999999, precision: 0.5284900284900285, f_beta: 0.5099935525467441
train: step: 2825, loss: 0.6607764959335327, acc: 0.7, recall: 0.7, precision: 0.702020202020202, f_beta: 0.6992481203007519
train: step: 2826, loss: 0.7042621970176697, acc: 0.45, recall: 0.44999999999999996, precision: 0.43333333333333335, f_beta: 0.41333333333333333
train: step: 2827, loss: 0.6961042881011963, acc: 0.45, recall: 0.45, precision: 0.45, f_beta: 0.45
train: step: 2828, loss: 0.6885462999343872, acc: 0.55, recall: 0.55, precision: 0.5595238095238095, f_beta: 0.53125
train: step: 2829, loss: 0.6839011311531067, acc: 0.575, recall: 0.575, precision: 0.5800000000000001, f_beta: 0.5682539682539682
train: step: 2830, loss: 0.6858417391777039, acc: 0.575, recall: 0.575, precision: 0.6298701298701299, f_beta: 0.5248078266946192
train: step: 2831, loss: 0.5930892825126648, acc: 0.875, recall: 0.875, precision: 0.8759398496240601, f_beta: 0.8749218261413383
train: step: 2832, loss: 0.6727989912033081, acc: 0.6, recall: 0.6, precision: 0.6041666666666667, f_beta: 0.595959595959596
train: step: 2833, loss: 0.7000013589859009, acc: 0.3, recall: 0.30000000000000004, precision: 0.29166666666666663, f_beta: 0.29292929292929293
train: step: 2834, loss: 0.6753933429718018, acc: 0.65, recall: 0.65, precision: 0.65625, f_beta: 0.6464646464646464
train: step: 2835, loss: 0.7091379761695862, acc: 0.375, recall: 0.375, precision: 0.3666666666666667, f_beta: 0.3650793650793651
train: step: 2836, loss: 0.6741966605186462, acc: 0.65, recall: 0.65, precision: 0.6785714285714286, f_beta: 0.6354166666666667
train: step: 2837, loss: 0.68803471326828, acc: 0.575, recall: 0.575, precision: 0.5940438871473355, f_beta: 0.5523370638578011
train: step: 2838, loss: 0.6375864744186401, acc: 0.7, recall: 0.7, precision: 0.7666666666666666, f_beta: 0.6799999999999999
train: step: 2839, loss: 0.6784870028495789, acc: 0.625, recall: 0.625, precision: 0.6278772378516624, f_beta: 0.6228786926461345
train: step: 2840, loss: 0.6683389544487, acc: 0.675, recall: 0.675, precision: 0.6754385964912281, f_beta: 0.6747967479674797
train: step: 2841, loss: 0.6875864267349243, acc: 0.575, recall: 0.575, precision: 0.575187969924812, f_beta: 0.5747342088805503
train: step: 2842, loss: 0.6585097312927246, acc: 0.675, recall: 0.675, precision: 0.6866666666666666, f_beta: 0.6698412698412699
train: step: 2843, loss: 0.6799759864807129, acc: 0.625, recall: 0.625, precision: 0.6567398119122256, f_beta: 0.6050032916392363
train: step: 2844, loss: 0.6876789927482605, acc: 0.55, recall: 0.55, precision: 0.578125, f_beta: 0.5054945054945055
train: step: 2845, loss: 0.6824115514755249, acc: 0.575, recall: 0.575, precision: 0.6298701298701299, f_beta: 0.5248078266946192
train: step: 2846, loss: 0.7187239527702332, acc: 0.375, recall: 0.375, precision: 0.34326018808777425, f_beta: 0.34167215273206053
train: step: 2847, loss: 0.6909104585647583, acc: 0.55, recall: 0.55, precision: 0.5666666666666667, f_beta: 0.52
train: step: 2848, loss: 0.6898346543312073, acc: 0.55, recall: 0.55, precision: 0.5520833333333333, f_beta: 0.5454545454545454
train: step: 2849, loss: 0.6951870918273926, acc: 0.5, recall: 0.5, precision: 0.5, f_beta: 0.5
train: step: 2850, loss: 0.7177539467811584, acc: 0.275, recall: 0.275, precision: 0.2698209718670077, f_beta: 0.2708988057825267
train: step: 2851, loss: 0.6931582689285278, acc: 0.45, recall: 0.45, precision: 0.4494949494949495, f_beta: 0.4486215538847118
train: step: 2852, loss: 0.6473545432090759, acc: 0.675, recall: 0.675, precision: 0.6790281329923273, f_beta: 0.6731615336266499
train: step: 2853, loss: 0.7489727139472961, acc: 0.3, recall: 0.3, precision: 0.297979797979798, f_beta: 0.2982456140350877
train: step: 2854, loss: 0.6967397928237915, acc: 0.5, recall: 0.5, precision: 0.5, f_beta: 0.4505494505494506
train: step: 2855, loss: 0.6989456415176392, acc: 0.5, recall: 0.5, precision: 0.5, f_beta: 0.4949494949494949
train: step: 2856, loss: 0.7349085807800293, acc: 0.275, recall: 0.275, precision: 0.2744360902255639, f_beta: 0.27454659161976236
train: step: 2857, loss: 0.6931626200675964, acc: 0.525, recall: 0.525, precision: 0.5432900432900433, f_beta: 0.46890286512928026
train: step: 2858, loss: 0.7035796642303467, acc: 0.425, recall: 0.425, precision: 0.3701298701298701, f_beta: 0.35709294199860236
train: step: 2859, loss: 0.6954184770584106, acc: 0.45, recall: 0.45, precision: 0.4494949494949495, f_beta: 0.4486215538847118
train: step: 2860, loss: 0.6798408627510071, acc: 0.575, recall: 0.575, precision: 0.5940438871473355, f_beta: 0.5523370638578011
train: step: 2861, loss: 0.6887142062187195, acc: 0.575, recall: 0.575, precision: 0.5940438871473355, f_beta: 0.5523370638578011
train: step: 2862, loss: 0.6895829439163208, acc: 0.525, recall: 0.525, precision: 0.5900900900900901, f_beta: 0.4202898550724638
train: step: 2863, loss: 0.6936535835266113, acc: 0.5, recall: 0.5, precision: 0.5, f_beta: 0.4884910485933504
train: step: 2864, loss: 0.6916434168815613, acc: 0.55, recall: 0.55, precision: 0.5520833333333333, f_beta: 0.5454545454545454
train: step: 2865, loss: 0.6762955188751221, acc: 0.575, recall: 0.575, precision: 0.6298701298701299, f_beta: 0.5248078266946192
train: step: 2866, loss: 0.658241868019104, acc: 0.65, recall: 0.6499999999999999, precision: 0.6515151515151515, f_beta: 0.6491228070175439
train: step: 2867, loss: 0.6934695243835449, acc: 0.45, recall: 0.44999999999999996, precision: 0.44505494505494503, f_beta: 0.4373401534526854
train: step: 2868, loss: 0.6779576539993286, acc: 0.625, recall: 0.625, precision: 0.6792114695340502, f_beta: 0.5943204868154157
train: step: 2869, loss: 0.6831372976303101, acc: 0.575, recall: 0.575, precision: 0.7702702702702703, f_beta: 0.4813119755911518
train: step: 2870, loss: 0.6525807976722717, acc: 0.7, recall: 0.7, precision: 0.7380952380952381, f_beta: 0.6875
train: step: 2871, loss: 0.691875696182251, acc: 0.6, recall: 0.6, precision: 0.6098901098901099, f_beta: 0.5907928388746804
train: step: 2872, loss: 0.6623350977897644, acc: 0.7, recall: 0.7, precision: 0.7666666666666666, f_beta: 0.6799999999999999
train: step: 2873, loss: 0.6943634748458862, acc: 0.525, recall: 0.525, precision: 0.53584229390681, f_beta: 0.4861392832995267
train: step: 2874, loss: 0.6567934155464172, acc: 0.625, recall: 0.625, precision: 0.6567398119122256, f_beta: 0.6050032916392363
train: step: 2875, loss: 0.693082332611084, acc: 0.45, recall: 0.44999999999999996, precision: 0.43333333333333335, f_beta: 0.41333333333333333
train: step: 2876, loss: 0.6974307298660278, acc: 0.275, recall: 0.275, precision: 0.2698209718670077, f_beta: 0.2708988057825267
train: step: 2877, loss: 0.6911484599113464, acc: 0.55, recall: 0.55, precision: 0.554945054945055, f_beta: 0.5396419437340153
train: step: 2878, loss: 0.69660884141922, acc: 0.375, recall: 0.375, precision: 0.3721227621483376, f_beta: 0.3714644877435575
train: step: 2879, loss: 0.6795393824577332, acc: 0.575, recall: 0.575, precision: 0.5940438871473355, f_beta: 0.5523370638578011
train: step: 2880, loss: 0.6907246708869934, acc: 0.625, recall: 0.625, precision: 0.6333333333333333, f_beta: 0.6190476190476191
train: step: 2881, loss: 0.6725173592567444, acc: 0.725, recall: 0.725, precision: 0.7255639097744361, f_beta: 0.7248280175109443
train: step: 2882, loss: 0.7008545994758606, acc: 0.525, recall: 0.525, precision: 0.53584229390681, f_beta: 0.4861392832995267
train: step: 2883, loss: 0.6847420930862427, acc: 0.575, recall: 0.575, precision: 0.5800000000000001, f_beta: 0.5682539682539682
train: step: 2884, loss: 0.6948663592338562, acc: 0.525, recall: 0.525, precision: 0.53584229390681, f_beta: 0.4861392832995267
train: step: 2885, loss: 0.6935769319534302, acc: 0.45, recall: 0.45, precision: 0.4494949494949495, f_beta: 0.4486215538847118
train: step: 2886, loss: 0.6877167224884033, acc: 0.55, recall: 0.55, precision: 0.5666666666666667, f_beta: 0.52
train: step: 2887, loss: 0.6858450174331665, acc: 0.6, recall: 0.6, precision: 0.65625, f_beta: 0.5604395604395604
train: step: 2888, loss: 0.6927100419998169, acc: 0.55, recall: 0.55, precision: 0.5666666666666667, f_beta: 0.52
train: step: 2889, loss: 0.6786854863166809, acc: 0.6, recall: 0.6, precision: 0.6041666666666667, f_beta: 0.595959595959596
train: step: 2890, loss: 0.6871312856674194, acc: 0.575, recall: 0.575, precision: 0.5767263427109974, f_beta: 0.5725958516656191
train: step: 2891, loss: 0.6993465423583984, acc: 0.425, recall: 0.42500000000000004, precision: 0.42000000000000004, f_beta: 0.4158730158730159
train: step: 2892, loss: 0.6825088262557983, acc: 0.6, recall: 0.6, precision: 0.6098901098901099, f_beta: 0.5907928388746804
train: step: 2893, loss: 0.6509073376655579, acc: 0.65, recall: 0.6499999999999999, precision: 0.734375, f_beta: 0.6153846153846154
train: step: 2894, loss: 0.696671187877655, acc: 0.5, recall: 0.5, precision: 0.5, f_beta: 0.4949494949494949
train: step: 2895, loss: 0.7086092829704285, acc: 0.375, recall: 0.375, precision: 0.34326018808777425, f_beta: 0.34167215273206053
train: step: 2896, loss: 0.6901381611824036, acc: 0.55, recall: 0.55, precision: 0.5505050505050505, f_beta: 0.5488721804511277
train: step: 2897, loss: 0.6847308278083801, acc: 0.55, recall: 0.55, precision: 0.578125, f_beta: 0.5054945054945055
train: step: 2898, loss: 0.6990195512771606, acc: 0.4, recall: 0.4, precision: 0.38095238095238093, f_beta: 0.375
train: step: 2899, loss: 0.6766561269760132, acc: 0.725, recall: 0.7250000000000001, precision: 0.7301790281329923, f_beta: 0.7234443746071653
train: step: 2900, loss: 0.6872034072875977, acc: 0.475, recall: 0.47500000000000003, precision: 0.45670995670995673, f_beta: 0.41299790356394134
train: step: 2901, loss: 0.635334312915802, acc: 0.775, recall: 0.775, precision: 0.7756892230576441, f_beta: 0.774859287054409
train: step: 2902, loss: 0.6850914359092712, acc: 0.6, recall: 0.6, precision: 0.6333333333333333, f_beta: 0.5733333333333334
train: step: 2903, loss: 0.6747537851333618, acc: 0.65, recall: 0.65, precision: 0.65625, f_beta: 0.6464646464646464
train: step: 2904, loss: 0.6447222232818604, acc: 0.75, recall: 0.75, precision: 0.7525252525252526, f_beta: 0.7493734335839599
train: step: 2905, loss: 0.6906351447105408, acc: 0.6, recall: 0.6000000000000001, precision: 0.601010101010101, f_beta: 0.5989974937343359
train: step: 2906, loss: 0.6933503150939941, acc: 0.525, recall: 0.525, precision: 0.5900900900900901, f_beta: 0.4202898550724638
train: step: 2907, loss: 0.6941878795623779, acc: 0.55, recall: 0.55, precision: 0.5520833333333333, f_beta: 0.5454545454545454
train: step: 2908, loss: 0.7104407548904419, acc: 0.4, recall: 0.4, precision: 0.3666666666666667, f_beta: 0.36
train: step: 2909, loss: 0.692516028881073, acc: 0.475, recall: 0.475, precision: 0.47333333333333333, f_beta: 0.4666666666666666
train: step: 2910, loss: 0.7273473143577576, acc: 0.425, recall: 0.42500000000000004, precision: 0.424812030075188, f_beta: 0.4246404002501563
train: step: 2911, loss: 0.5922528505325317, acc: 0.775, recall: 0.775, precision: 0.7756892230576441, f_beta: 0.774859287054409
train: step: 2912, loss: 0.690426766872406, acc: 0.55, recall: 0.55, precision: 0.554945054945055, f_beta: 0.5396419437340153
train: step: 2913, loss: 0.6695801615715027, acc: 0.65, recall: 0.65, precision: 0.65625, f_beta: 0.6464646464646464
train: step: 2914, loss: 0.6700916290283203, acc: 0.6, recall: 0.6, precision: 0.696078431372549, f_beta: 0.5441595441595442
train: step: 2915, loss: 0.692231297492981, acc: 0.575, recall: 0.575, precision: 0.60752688172043, f_beta: 0.5402298850574712
train: step: 2916, loss: 0.6889050006866455, acc: 0.575, recall: 0.575, precision: 0.575187969924812, f_beta: 0.5747342088805503
train: step: 2917, loss: 0.6647430658340454, acc: 0.65, recall: 0.6499999999999999, precision: 0.734375, f_beta: 0.6153846153846154
train: step: 2918, loss: 0.6868652105331421, acc: 0.575, recall: 0.575, precision: 0.5940438871473355, f_beta: 0.5523370638578011
train: step: 2919, loss: 0.6005889773368835, acc: 0.75, recall: 0.75, precision: 0.7604166666666667, f_beta: 0.7474747474747475
train: step: 2920, loss: 0.6832138299942017, acc: 0.55, recall: 0.5499999999999999, precision: 0.6388888888888888, f_beta: 0.46428571428571425
train: step: 2921, loss: 0.6937204599380493, acc: 0.425, recall: 0.425, precision: 0.42327365728900257, f_beta: 0.4217473287240729
train: step: 2922, loss: 0.6987336277961731, acc: 0.525, recall: 0.5249999999999999, precision: 0.5284900284900285, f_beta: 0.5099935525467441
train: step: 2923, loss: 0.6823368072509766, acc: 0.525, recall: 0.525, precision: 0.5432900432900433, f_beta: 0.46890286512928026
train: step: 2924, loss: 0.683356761932373, acc: 0.575, recall: 0.575, precision: 0.5767263427109974, f_beta: 0.5725958516656191
train: step: 2925, loss: 0.6921790838241577, acc: 0.525, recall: 0.525, precision: 0.5571428571428572, f_beta: 0.44727272727272727
train: step: 2926, loss: 0.7187579274177551, acc: 0.375, recall: 0.375, precision: 0.3575498575498576, f_beta: 0.3552546744036106
train: step: 2927, loss: 0.6964874863624573, acc: 0.5, recall: 0.5, precision: 0.5, f_beta: 0.40476190476190477
train: step: 2928, loss: 0.6587449312210083, acc: 0.675, recall: 0.675, precision: 0.6754385964912281, f_beta: 0.6747967479674797
train: step: 2929, loss: 0.692150890827179, acc: 0.475, recall: 0.47500000000000003, precision: 0.45670995670995673, f_beta: 0.41299790356394134
train: step: 2930, loss: 0.6964083909988403, acc: 0.475, recall: 0.475, precision: 0.47333333333333333, f_beta: 0.4666666666666666
train: step: 2931, loss: 0.6933619976043701, acc: 0.475, recall: 0.47500000000000003, precision: 0.40990990990990994, f_beta: 0.35926773455377575
train: step: 2932, loss: 0.689409613609314, acc: 0.65, recall: 0.65, precision: 0.65625, f_beta: 0.6464646464646464
train: step: 2933, loss: 0.6908247470855713, acc: 0.55, recall: 0.55, precision: 0.5595238095238095, f_beta: 0.53125
train: step: 2934, loss: 0.6908904910087585, acc: 0.55, recall: 0.55, precision: 0.5666666666666667, f_beta: 0.52
train: step: 2935, loss: 0.6963419914245605, acc: 0.3, recall: 0.3, precision: 0.3, f_beta: 0.3
train: step: 2936, loss: 0.6851853132247925, acc: 0.575, recall: 0.575, precision: 0.575187969924812, f_beta: 0.5747342088805503
train: step: 2937, loss: 0.6932483315467834, acc: 0.425, recall: 0.425, precision: 0.3924731182795699, f_beta: 0.37795807978363755
train: step: 2938, loss: 0.6914240121841431, acc: 0.525, recall: 0.525, precision: 0.525062656641604, f_beta: 0.5247029393370857
train: step: 2939, loss: 0.6952235698699951, acc: 0.55, recall: 0.55, precision: 0.5980392156862745, f_beta: 0.4871794871794872
train: step: 2940, loss: 0.6930302381515503, acc: 0.475, recall: 0.47500000000000003, precision: 0.45670995670995673, f_beta: 0.41299790356394134
train: step: 2941, loss: 0.6927032470703125, acc: 0.5, recall: 0.5, precision: 0.5, f_beta: 0.47916666666666663
train: step: 2942, loss: 0.6946098804473877, acc: 0.4, recall: 0.4, precision: 0.3901098901098901, f_beta: 0.38618925831202044
train: step: 2943, loss: 0.703339695930481, acc: 0.475, recall: 0.475, precision: 0.46865203761755486, f_beta: 0.4470046082949308
train: step: 2944, loss: 0.6674889326095581, acc: 0.675, recall: 0.675, precision: 0.6994301994301995, f_beta: 0.6647324306898775
train: step: 2945, loss: 0.6934926509857178, acc: 0.45, recall: 0.45, precision: 0.44791666666666663, f_beta: 0.4444444444444444
