{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "data process\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import json\n",
    "import random\n",
    "import copy\n",
    "from collections import Counter\n",
    "from itertools import chain\n",
    "from typing import Dict, Tuple, Optional, List, Union\n",
    "\n",
    "import gensim\n",
    "import numpy as np\n",
    "\n",
    "DEBUG_PRINT = False\n",
    "\n",
    "class PrototypicalData(object):\n",
    "    def __init__(self, output_path: str, sequence_length: int = 100, num_classes: int = 2, num_support: int = 5,\n",
    "                 num_queries: int = 50, num_tasks: int = 1000, num_eval_tasks: int = 100,\n",
    "                 stop_word_path: Optional[str] = None,\n",
    "                 embedding_size: Optional[int] = None, low_freq: int = 5,\n",
    "                 word_vector_path: Optional[str] = None, is_training: bool = True):\n",
    "        \"\"\"\n",
    "        init method\n",
    "        :param output_path: path of train/eval data\n",
    "        :param num_classes: number of support class\n",
    "        :param num_support: number of support sample per class\n",
    "        :param num_queries: number of query sample per class\n",
    "        :param num_tasks: number of pre-sampling tasks, this will speeding up train\n",
    "        :param num_eval_tasks: number of pre-sampling tasks in eval stage\n",
    "        :param stop_word_path: path of stop word file\n",
    "        :param embedding_size: embedding size\n",
    "        :param low_freq: frequency of words\n",
    "        :param word_vector_path: path of word vector file(eg. word2vec, glove)\n",
    "        :param is_training: bool\n",
    "        \"\"\"\n",
    "\n",
    "        self.__output_path = output_path\n",
    "        if not os.path.exists(self.__output_path):\n",
    "            os.makedirs(self.__output_path)\n",
    "\n",
    "        self.__sequence_length = sequence_length\n",
    "        self.__num_classes = num_classes\n",
    "        self.__num_support = num_support\n",
    "        self.__num_queries = num_queries\n",
    "        self.__num_tasks = num_tasks\n",
    "        self.__num_eval_tasks = num_eval_tasks\n",
    "        self.__stop_word_path = stop_word_path\n",
    "        self.__embedding_size = embedding_size\n",
    "        self.__low_freq = low_freq\n",
    "        self.__word_vector_path = word_vector_path\n",
    "        self.__is_training = is_training\n",
    "\n",
    "        self.vocab_size = None\n",
    "        self.word_vectors = None\n",
    "        self.current_category_index = 0  # record current sample category\n",
    "\n",
    "        print(\"stop word path: \", self.__stop_word_path)\n",
    "        print(\"word vector path: \", self.__word_vector_path)\n",
    "\n",
    "    @staticmethod\n",
    "    def load_data(data_path: str) -> Dict[str, Dict[str, List[List[str]]]]:\n",
    "        \"\"\"\n",
    "        read train/eval data\n",
    "        :param data_path:\n",
    "        :return: dict. {class_name: {sentiment: [[]], }, ...}\n",
    "        \"\"\"\n",
    "        category_files = os.listdir(data_path)\n",
    "        categories_data = {}\n",
    "        for category_file in category_files:\n",
    "            file_path = os.path.join(data_path, category_file)\n",
    "            sentiment_data = {}\n",
    "            with open(file_path, \"r\", encoding=\"utf8\") as fr:\n",
    "                for line in fr.readlines():\n",
    "                    content, label = line.strip().split(\"\\t\")\n",
    "                    if sentiment_data.get(label, None):\n",
    "                        sentiment_data[label].append(content.split(\" \"))\n",
    "                    else:\n",
    "                        sentiment_data[label] = [content.split(\" \")]\n",
    "\n",
    "            # print(\"task name: \", category_file)\n",
    "            # print(\"pos samples length: \", len(sentiment_data[\"1\"]))\n",
    "            # print(\"neg samples length: \", len(sentiment_data[\"-1\"]))\n",
    "            categories_data[category_file] = sentiment_data\n",
    "        return categories_data\n",
    "\n",
    "    def remove_stop_word(self, data: Dict[str, Dict[str, List[List[str]]]]) -> List[str]:\n",
    "        \"\"\"\n",
    "        remove low frequency words and stop words, construct vocab\n",
    "        :param data: {class_name: {sentiment: [[]], }, ...}\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        all_words = []\n",
    "        for category, category_data in data.items():\n",
    "            for sentiment, sentiment_data in category_data.items():\n",
    "                all_words.extend(list(chain(*sentiment_data)))\n",
    "        word_count = Counter(all_words)  # statistic the frequency of words\n",
    "        sort_word_count = sorted(word_count.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "        # remove low frequency word\n",
    "        words = [item[0] for item in sort_word_count if item[1] > self.__low_freq]\n",
    "\n",
    "        # if stop word file exists, then remove stop words\n",
    "        if self.__stop_word_path:\n",
    "            with open(self.__stop_word_path, \"r\", encoding=\"utf8\") as fr:\n",
    "                stop_words = [line.strip() for line in fr.readlines()]\n",
    "            words = [word for word in words if word not in stop_words]\n",
    "\n",
    "        return words\n",
    "\n",
    "    def get_word_vectors(self, vocab: List[str]) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        load word vector file,\n",
    "        :param vocab: vocab\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        pad_vector = np.zeros(self.__embedding_size)  # set the \"<pad>\" vector to 0\n",
    "        word_vectors = (1 / np.sqrt(len(vocab) - 1) * (2 * np.random.rand(len(vocab) - 1, self.__embedding_size) - 1))\n",
    "        word_vectors = np.vstack((pad_vector, word_vectors))\n",
    "        if DEBUG_PRINT:\n",
    "            # print(vocab)\n",
    "            print(f\"get_word_vectors word_vectors={word_vectors.shape}\")\n",
    "        \n",
    "        # load glove vectors\n",
    "        # glove_vector = {}\n",
    "        # with open(self.__word_vector_path, \"r\", encoding=\"utf8\") as fr:\n",
    "        #     for line in fr.readlines():\n",
    "        #         line_list = line.strip().split(\" \")\n",
    "        #         glove_vector[line_list[0]] = line_list[1:]\n",
    "\n",
    "        # for i in range(1, len(vocab)):\n",
    "        #     if glove_vector.get(vocab[i], None):\n",
    "        #         word_vectors[i, :] = glove_vector[vocab[i]]\n",
    "        #     else:\n",
    "        #         print(vocab[i] + \"not exist word vector file\")\n",
    "\n",
    "        # # load gensim word2vec vectors\n",
    "        # if os.path.splitext(self.__word_vector_path)[-1] == \".bin\":\n",
    "        #     word_vec = gensim.models.KeyedVectors.load_word2vec_format(self.__word_vector_path, binary=True)\n",
    "        # else:\n",
    "        #     word_vec = gensim.models.KeyedVectors.load_word2vec_format(self.__word_vector_path)\n",
    "        #\n",
    "        # for i in range(1, len(vocab)):\n",
    "        #     try:\n",
    "        #         vector = word_vec.wv[vocab[i]]\n",
    "        #         word_vectors[i, :] = vector\n",
    "        #     except:\n",
    "        #         print(vocab[i] + \"not exist word vector file\")\n",
    "\n",
    "        return word_vectors\n",
    "\n",
    "    def gen_vocab(self, words: List[str]) -> Dict[str, int]:\n",
    "        \"\"\"\n",
    "        generate word_to_index mapping table\n",
    "        :param words:\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        if self.__is_training:\n",
    "            vocab = [\"<pad>\", \"<unk>\"] + words\n",
    "\n",
    "            self.vocab_size = len(vocab)\n",
    "\n",
    "            if self.__word_vector_path:\n",
    "                word_vectors = self.get_word_vectors(vocab)\n",
    "                self.word_vectors = word_vectors\n",
    "                # save word vector to npy file\n",
    "                np.save(os.path.join(self.__output_path, \"word_vectors.npy\"), self.word_vectors)\n",
    "\n",
    "            word_to_index = dict(zip(vocab, list(range(len(vocab)))))\n",
    "\n",
    "            # save word_to_index to json file\n",
    "            with open(os.path.join(self.__output_path, \"word_to_index.json\"), \"w\") as f:\n",
    "                json.dump(word_to_index, f)\n",
    "        else:\n",
    "            with open(os.path.join(self.__output_path, \"word_to_index.json\"), \"r\") as f:\n",
    "                word_to_index = json.load(f)\n",
    "\n",
    "        return word_to_index\n",
    "\n",
    "    @staticmethod\n",
    "    def trans_to_index(data: Dict[str, Dict[str, List[List[str]]]], word_to_index: Dict[str, int]) -> \\\n",
    "            Dict[str, Dict[str, List[List[int]]]]:\n",
    "        \"\"\"\n",
    "        transformer token to id\n",
    "        :param data:\n",
    "        :param word_to_index:\n",
    "        :return: {class_name: [[], [], ], ..}\n",
    "        \"\"\"\n",
    "        data_ids = {category: {sentiment: [[word_to_index.get(token, word_to_index[\"<unk>\"]) for token in line]\n",
    "                                           for line in sentiment_data]\n",
    "                               for sentiment, sentiment_data in category_data.items()}\n",
    "                    for category, category_data in data.items()}\n",
    "        return data_ids\n",
    "\n",
    "    def choice_support_query(self, task_data: Dict[str, List[List[int]]])\\\n",
    "            -> Tuple[List[List[List[int]]], List[List[int]], List[int]]:\n",
    "        \"\"\"\n",
    "        randomly selecting support set, query set form a task.\n",
    "        :param task_data: all data for a task\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        label_to_index = {\"1\": 0, \"-1\": 1}\n",
    "        # if self.__is_training:\n",
    "        #     with open(os.path.join(self.__output_path, \"label_to_index.json\"), \"w\") as f:\n",
    "        #         json.dump(label_to_index, f)\n",
    "\n",
    "        pos_samples = task_data[\"1\"]\n",
    "        neg_samples = task_data[\"-1\"]\n",
    "        pos_support = random.sample(pos_samples, self.__num_support)\n",
    "        neg_support = random.sample(neg_samples, self.__num_support)\n",
    "\n",
    "        pos_others = copy.copy(pos_samples)\n",
    "        [pos_others.remove(data) for data in pos_support]\n",
    "\n",
    "        neg_others = copy.copy(neg_samples)\n",
    "        [neg_others.remove(data) for data in neg_support]\n",
    "\n",
    "        pos_query = random.sample(pos_others, self.__num_queries)\n",
    "        neg_query = random.sample(neg_others, self.__num_queries)\n",
    "\n",
    "        # padding\n",
    "        pos_support = self.padding(pos_support)\n",
    "        neg_support = self.padding(neg_support)\n",
    "        pos_query = self.padding(pos_query)\n",
    "        neg_query = self.padding(neg_query)\n",
    "\n",
    "        support_set = [pos_support, neg_support]  # [num_classes, num_support, sequence_length]\n",
    "        query_set = pos_query + neg_query  # [num_classes * num_queries, sequence_length]\n",
    "        labels = [label_to_index[\"1\"]] * len(pos_query) + [label_to_index[\"-1\"]] * len(neg_query)\n",
    "\n",
    "        return support_set, query_set, labels\n",
    "\n",
    "    def samples(self, data_ids: Dict[str, Dict[str, List[List[int]]]]) \\\n",
    "            -> List[Dict[str, Union[List[List[List[int]]], List[List[int]], List[int]]]]:\n",
    "        \"\"\"\n",
    "        positive and negative sample from raw data\n",
    "        :param data_ids:\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        # product name list\n",
    "        category_list = list(data_ids.keys())\n",
    "\n",
    "        tasks = []\n",
    "        if self.__is_training:\n",
    "            num_tasks = self.__num_tasks\n",
    "        else:\n",
    "            num_tasks = self.__num_eval_tasks\n",
    "        for i in range(num_tasks):\n",
    "            # randomly choice a category to construct train sample\n",
    "            try:\n",
    "                support_category = random.choice(category_list)\n",
    "                support_set, query_set, labels = self.choice_support_query(data_ids[support_category])\n",
    "                tasks.append(dict(support=support_set, queries=query_set, labels=labels))\n",
    "            except:\n",
    "                pass\n",
    "        return tasks\n",
    "\n",
    "    def gen_data(self, file_path: str) -> Dict[str, Dict[str, List[List[int]]]]:\n",
    "        \"\"\"\n",
    "        Generate data that is eventually input to the model\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        # load data\n",
    "        data = self.load_data(file_path)\n",
    "        # remove stop word\n",
    "        words = self.remove_stop_word(data)\n",
    "        word_to_index = self.gen_vocab(words)\n",
    "\n",
    "        data_ids = self.trans_to_index(data, word_to_index)\n",
    "        return data_ids\n",
    "\n",
    "    def padding(self, sentences: List[List[int]]) -> List[List[int]]:\n",
    "        \"\"\"\n",
    "        padding according to the predefined sequence length\n",
    "        :param sentences:\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        sentence_pad = [sentence[:self.__sequence_length] if len(sentence) > self.__sequence_length\n",
    "                        else sentence + [0] * (self.__sequence_length - len(sentence))\n",
    "                        for sentence in sentences]\n",
    "        return sentence_pad\n",
    "\n",
    "    def next_batch(self, data_ids: Dict[str, Dict[str, List[List[int]]]]) \\\n",
    "            -> Dict[str, Union[List[List[List[int]]], List[List[int]], List[int]]]:\n",
    "        \"\"\"\n",
    "        train a task at every turn\n",
    "        :param data_ids:\n",
    "        :return:\n",
    "        \"\"\"\n",
    "\n",
    "        tasks = self.samples(data_ids)\n",
    "\n",
    "        for task in tasks:\n",
    "            yield task\n",
    "\n",
    "config = {\n",
    "  \"model_name\": \"prototypical\",\n",
    "  \"epochs\": 30,\n",
    "  \"checkpoint_every\": 100,\n",
    "  \"eval_every\": 500,\n",
    "  \"learning_rate\": 1e-3,\n",
    "  \"optimization\": \"adam\",\n",
    "  \"embedding_size\": 300,\n",
    "  \"hidden_sizes\": [128],\n",
    "  \"attention_size\": 64,\n",
    "  \"num_support\": 10,\n",
    "  \"num_queries\": 50,\n",
    "  \"num_classes\": 2,\n",
    "  \"num_tasks\": 200,\n",
    "  \"num_eval_tasks\": 100,\n",
    "  \"low_freq\": 3,\n",
    "  \"sequence_length\": 200,\n",
    "  \"keep_prob\": 0.7,\n",
    "  \"l2_reg_lambda\": 0.0,\n",
    "  \"max_grad_norm\": 5.0,\n",
    "  \"train_data\": \"./reviews/newtrain\",\n",
    "  \"eval_data\": \"./reviews/neweval\",\n",
    "  \"stop_word_path\": \"./reviews/english\",\n",
    "  \"output_path\": \"./output/prototypical\",\n",
    "  \"word_vector_path\": \"./word_embedded/new_word2vec_model.txt\",\n",
    "  \"ckpt_model_path\": \"./output/prototypical/ckpt_model\",\n",
    "  \"pb_model_path\": \"./output/prototypical/pb_model\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# data_loader = PrototypicalData(output_path=config[\"output_path\"],\n",
    "#                                     sequence_length=config[\"sequence_length\"],\n",
    "#                                     num_classes=config[\"num_classes\"],\n",
    "#                                     num_support=config[\"num_support\"],\n",
    "#                                     num_queries=config[\"num_queries\"],\n",
    "#                                     num_tasks=config[\"num_tasks\"],\n",
    "#                                     num_eval_tasks=config[\"num_eval_tasks\"],\n",
    "#                                     embedding_size=config[\"embedding_size\"],\n",
    "#                                     stop_word_path=config[\"stop_word_path\"],\n",
    "#                                     word_vector_path=config[\"word_vector_path\"],\n",
    "#                                     is_training=True)\n",
    "# eval_loader = PrototypicalData(output_path=config[\"output_path\"],\n",
    "#                                     sequence_length=config[\"sequence_length\"],\n",
    "#                                     num_classes=config[\"num_classes\"],\n",
    "#                                     num_support=config[\"num_support\"],\n",
    "#                                     num_queries=config[\"num_queries\"],\n",
    "#                                     num_tasks=config[\"num_tasks\"],\n",
    "#                                     num_eval_tasks=config[\"num_eval_tasks\"],\n",
    "#                                     embedding_size=config[\"embedding_size\"],\n",
    "#                                     stop_word_path=config[\"stop_word_path\"],\n",
    "#                                     word_vector_path=config[\"word_vector_path\"],\n",
    "#                                     is_training=False)\n",
    "# train_tasks = data_loader.gen_data(config[\"train_data\"])\n",
    "# eval_tasks = eval_loader.gen_data(config[\"eval_data\"])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "performance metrics function\n",
    "\"\"\"\n",
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "\n",
    "def accuracy(pred_y: torch.Tensor, true_y: torch.Tensor):\n",
    "    \"\"\"\n",
    "    Calculate accuracy\n",
    "    :param pred_y: predict result\n",
    "    :param true_y: true result\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    return np.sum(pred_y == true_y) / len(pred_y)\n",
    "\n",
    "\n",
    "def binary_precision(pred_y: torch.Tensor, true_y: torch.Tensor, positive=1):\n",
    "    \"\"\"\n",
    "    Calculate the precision of binary classification\n",
    "    :param pred_y: predict result\n",
    "    :param true_y: true result\n",
    "    :param positive: index of positive label\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    tp = np.sum((pred_y == positive) & (true_y == positive))\n",
    "    fp = np.sum((pred_y == positive) & (true_y != positive))\n",
    "    # fn = np.sum((pred_y != positive) & (true_y == positive))\n",
    "    # tn = np.sum((pred_y != positive) & (true_y != positive))\n",
    "    if (tp + fp) != 0:\n",
    "        return tp / (tp + fp)\n",
    "    else:\n",
    "        return 0.0\n",
    "\n",
    "\n",
    "def binary_recall(pred_y, true_y, positive=1):\n",
    "    \"\"\"\n",
    "    Calculate the recall of binary classification\n",
    "    :param pred_y: predict result\n",
    "    :param true_y: true result\n",
    "    :param positive: index of positive label\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    tp = np.sum((pred_y == positive) & (true_y == positive))\n",
    "    # fp = np.sum((pred_y == positive) & (true_y != positive))\n",
    "    fn = np.sum((pred_y != positive) & (true_y == positive))\n",
    "    # tn = np.sum((pred_y != positive) & (true_y != positive))\n",
    "    if (tp + fn) != 0:\n",
    "        return tp / (tp + fn)\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "\n",
    "def binary_f_beta(pred_y, true_y, beta=1.0, positive=1):\n",
    "    \"\"\"\n",
    "    Calculate the f beta of binary classification\n",
    "    :param pred_y: predict result\n",
    "    :param beta: beta parameter\n",
    "    :param true_y: true result\n",
    "    :param positive: index of positive label\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    precision = binary_precision(pred_y, true_y, positive)\n",
    "    recall = binary_recall(pred_y, true_y, positive)\n",
    "    if (beta * beta * precision + recall) != 0:\n",
    "        return (1 + beta * beta) * precision * recall / (beta * beta * precision + recall)\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "\n",
    "def get_binary_metrics(pred_y, true_y, f_beta=1.0):\n",
    "    \"\"\"\n",
    "    Calculate various performance metrics of binary classification\n",
    "    :param pred_y: predict result\n",
    "    :param true_y: true result\n",
    "    :param f_beta: beta parameter\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    acc = accuracy(pred_y, true_y)\n",
    "    recall = binary_recall(pred_y, true_y)\n",
    "    precision = binary_precision(pred_y, true_y)\n",
    "    f_beta = binary_f_beta(pred_y, true_y, f_beta)\n",
    "    return acc, recall, precision, f_beta\n",
    "\n",
    "\n",
    "def multi_precision(pred_y, true_y, labels):\n",
    "    \"\"\"\n",
    "    Calculate the precision of multi classification\n",
    "    :param pred_y: predict result\n",
    "    :param true_y: true result\n",
    "    :param labels: label list\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    precisions = [binary_precision(pred_y, true_y, label) for label in labels]\n",
    "    prec = np.mean(precisions)\n",
    "    return prec\n",
    "\n",
    "\n",
    "def multi_recall(pred_y, true_y, labels):\n",
    "    \"\"\"\n",
    "    Calculate the recall of multi classification\n",
    "    :param pred_y: predict result\n",
    "    :param true_y: true result\n",
    "    :param labels: label list\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    recalls = [binary_recall(pred_y, true_y, label) for label in labels]\n",
    "    rec = np.mean(recalls)\n",
    "    return rec\n",
    "\n",
    "\n",
    "def multi_f_beta(pred_y, true_y, labels, beta=1.0):\n",
    "    \"\"\"\n",
    "    Calculate the f value of multi classification\n",
    "    :param pred_y: predict result\n",
    "    :param true_y: true result\n",
    "    :param labels: label list\n",
    "    :param beta: beta parameter\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    f_betas = [binary_f_beta(pred_y, true_y, beta, label) for label in labels]\n",
    "    f_beta = np.mean(f_betas)\n",
    "    return f_beta\n",
    "\n",
    "\n",
    "def get_multi_metrics(pred_y, true_y, labels, f_beta=1.0):\n",
    "    \"\"\"\n",
    "    Calculate various performance metrics of multi classification\n",
    "    :param pred_y: predict result\n",
    "    :param true_y: true result\n",
    "    :param labels: label list\n",
    "    :param beta: beta parameter\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    acc = accuracy(pred_y, true_y)\n",
    "    recall = multi_recall(pred_y, true_y, labels)\n",
    "    precision = multi_precision(pred_y, true_y, labels)\n",
    "    f_beta = multi_f_beta(pred_y, true_y, labels, f_beta)\n",
    "    return acc, recall, precision, f_beta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import os\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "\n",
    "def extract_batch(batch: dict) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor]:\n",
    "    supports = torch.tensor(batch['support'], device=device)\n",
    "    query = torch.tensor(batch['queries'], device=device)\n",
    "    labels = torch.tensor(batch['labels'], device=device)\n",
    "    \n",
    "    # https://github.com/jakesnell/prototypical-networks/blob/master/protonets/models/few_shot.py\n",
    "    # https://github.com/sicara/easy-few-shot-learning/blob/master/notebooks/my_first_few_shot_classifier.ipynb\n",
    "\n",
    "    # supports = [num_classes, batch, length]\n",
    "    pos_support = supports[0, :, :]\n",
    "    pos_support_labels = torch.zeros(pos_support.size(0))\n",
    "    neg_support = supports[1, :, :]\n",
    "    neg_support_labels = torch.ones(neg_support.size(0))\n",
    "    supports = torch.cat([pos_support, neg_support], dim=0).to(device=device)\n",
    "    supports_labels = torch.cat([pos_support_labels, neg_support_labels], dim=0).to(device=device)\n",
    "    \n",
    "    return supports, supports_labels, query, labels\n",
    "\n",
    "class CNN(nn.Module):\n",
    "    \n",
    "    def __init__(self, vocab_size: int, embedding_dim: int, fs: List[int], channels: int, output_dim: int) -> None:\n",
    "        super(CNN, self).__init__()\n",
    "        \n",
    "        self.embed = nn.Embedding(vocab_size, embedding_dim)\n",
    "        \n",
    "        self.convs = nn.ModuleList([\n",
    "            nn.Conv2d(in_channels=1, out_channels=channels, kernel_size=(n, embedding_dim)) for n in fs\n",
    "        ])\n",
    "        \n",
    "        self.fc = nn.Linear(len(fs) * channels, output_dim)\n",
    "        \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        if DEBUG_PRINT:\n",
    "            print(f\"CNN x={x.size()}\")\n",
    "        embedded = self.embed(x)\n",
    "        if DEBUG_PRINT:\n",
    "            print(f\"CNN embeded={embedded.size()}\")\n",
    "        embedded = embedded.unsqueeze(1)\n",
    "        conved = [F.relu(conv(embedded)).squeeze(3) for conv in self.convs]\n",
    "        pooled = [F.max_pool1d(conv, conv.shape[2]).squeeze(2) for conv in conved]\n",
    "        output = torch.cat(pooled, dim=1)\n",
    "        return self.fc(output)\n",
    "    \n",
    "    def predict(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        return self(x).argmax(1)\n",
    "    \n",
    "    def scores(self, batch: dict) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n",
    "        supports, supports_labels, query, query_labels = extract_batch(batch)\n",
    "        supports_out = model(supports)\n",
    "        query_out = model(query)\n",
    "\n",
    "        if DEBUG_PRINT:\n",
    "            print(f\"train support_out={supports_out.size()} query_out={query_out.size()}\")\n",
    "        \n",
    "        if DEBUG_PRINT:\n",
    "            print(f\"train indices={torch.nonzero(supports_labels == 0).size()}\")\n",
    "        proto_pos = supports_out[torch.nonzero(supports_labels == 0)]\n",
    "        proto_neg = supports_out[torch.nonzero(supports_labels == 1)]\n",
    "        \n",
    "        if DEBUG_PRINT:\n",
    "            print(f\"train proto_pos={proto_pos.size()}\")\n",
    "            \n",
    "        proto_pos = proto_pos.mean(0)\n",
    "        proto_neg = proto_neg.mean(0)\n",
    "        \n",
    "        if DEBUG_PRINT:\n",
    "            print(f\"train proto_pos_meaned={proto_pos.size()}\")\n",
    "            \n",
    "        proto = torch.cat([proto_pos, proto_neg], dim=0).to(dtype=torch.float)\n",
    "        if DEBUG_PRINT:\n",
    "            print(f\"train proto={proto.size()}\")\n",
    "            \n",
    "        dists = torch.cdist(query_out, proto)\n",
    "        if DEBUG_PRINT:\n",
    "            print(f\"train dists={dists.size()} labels={query_labels.size()}\")\n",
    "\n",
    "        scores = -dists\n",
    "        return scores, query_out, query_labels\n",
    "    \n",
    "data_loader = PrototypicalData(output_path=config[\"output_path\"],\n",
    "                                    sequence_length=config[\"sequence_length\"],\n",
    "                                    num_classes=config[\"num_classes\"],\n",
    "                                    num_support=config[\"num_support\"],\n",
    "                                    num_queries=config[\"num_queries\"],\n",
    "                                    num_tasks=config[\"num_tasks\"],\n",
    "                                    num_eval_tasks=config[\"num_eval_tasks\"],\n",
    "                                    embedding_size=config[\"embedding_size\"],\n",
    "                                    stop_word_path=config[\"stop_word_path\"],\n",
    "                                    word_vector_path=config[\"word_vector_path\"],\n",
    "                                    is_training=True)\n",
    "eval_loader = PrototypicalData(output_path=config[\"output_path\"],\n",
    "                                    sequence_length=config[\"sequence_length\"],\n",
    "                                    num_classes=config[\"num_classes\"],\n",
    "                                    num_support=config[\"num_support\"],\n",
    "                                    num_queries=config[\"num_queries\"],\n",
    "                                    num_tasks=config[\"num_tasks\"],\n",
    "                                    num_eval_tasks=config[\"num_eval_tasks\"],\n",
    "                                    embedding_size=config[\"embedding_size\"],\n",
    "                                    stop_word_path=config[\"stop_word_path\"],\n",
    "                                    word_vector_path=config[\"word_vector_path\"],\n",
    "                                    is_training=False)\n",
    "train_tasks = data_loader.gen_data(config[\"train_data\"])\n",
    "eval_tasks = eval_loader.gen_data(config[\"eval_data\"])\n",
    "\n",
    "epochs = config[\"epochs\"]\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model = CNN(data_loader.vocab_size, 512, [1,2,4], 128, 2).to(device)\n",
    "lr = 0.01\n",
    "opt = torch.optim.SGD(model.parameters(), lr=lr)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "total_loss = 0\n",
    "interval = config[\"checkpoint_every\"]\n",
    "current_step = 0\n",
    "\n",
    "ckpt_path = Path(config['ckpt_model_path'])\n",
    "\n",
    "\n",
    "for ep in range(1, epochs+1):\n",
    "    for task in data_loader.next_batch(train_tasks):\n",
    "        model.zero_grad()\n",
    "        # https://github.com/jakesnell/prototypical-networks/blob/master/protonets/models/few_shot.py\n",
    "        # https://github.com/sicara/easy-few-shot-learning/blob/master/notebooks/my_first_few_shot_classifier.ipynb\n",
    "    \n",
    "        scores, _, labels = model.scores(task)\n",
    "        \n",
    "        loss = loss_fn(scores, labels)\n",
    "        loss.backward()\n",
    "        \n",
    "        opt.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        \n",
    "        current_step += 1\n",
    "        \n",
    "        if current_step % interval == 0:\n",
    "            with torch.no_grad():\n",
    "                val_losses = []\n",
    "                val_accs = []\n",
    "                val_recalls = []\n",
    "                val_precs = []\n",
    "                val_fbeta = []\n",
    "                for task in eval_loader.next_batch(eval_tasks):\n",
    "                    scores, preds_out, labels = model.scores(task)\n",
    "                    preds = preds_out.argmax(1)\n",
    "                    val_loss = loss_fn(scores, labels)\n",
    "                    val_losses.append(val_loss.item())\n",
    "                    \n",
    "                    # Move to cpu for faster computation\n",
    "                    acc, recall, prec, f_beta = get_multi_metrics(pred_y=preds.cpu().numpy(),\n",
    "                                                                 true_y=labels.cpu().numpy(),\n",
    "                                                                 labels=np.array([0, 1]))\n",
    "                    val_accs.append(acc)\n",
    "                    val_recalls.append(recall)\n",
    "                    val_precs.append(prec)\n",
    "                    val_fbeta.append(f_beta)\n",
    "                    \n",
    "                print(f\"VAL: epoch={ep} loss={np.mean(val_losses):.03f} accuracy={np.mean(val_accs):.03f} recalls={np.mean(val_recalls):.03f} precs={np.mean(val_precs):.03f} fbeta={np.mean(val_fbeta):.03f}\")\n",
    "        \n",
    "        opt.step()\n",
    "    print(f\"TRAIN: epoch={ep} loss={loss} avg_loss={total_loss/current_step}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ce7455",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
